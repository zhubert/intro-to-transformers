# An Introduction to Transformers

**Understanding Modern AI from the Inside Out**

This is an educational resource that teaches how modern AI systems work by building them yourself. No black boxesâ€”just the actual math and code that powers language models and image generators.

ðŸ“– **[Read Online](https://www.zhubert.com/intro-to-transformers/)**

## What's Inside

### Understanding Gradients

Calculate a complete transformer forward and backward pass **by hand**. Using only basic Python (no NumPy, no PyTorch), you compute every matrix multiplication, every activation function, every gradient. By the end, you understand transformers not because someone explained them in abstract terms, but because you calculated every operation yourself.

- Tokenization & Embeddings â€” How text becomes vectors
- QKV Projections â€” What Query, Key, Value actually mean
- Attention â€” The softmax-weighted sum that made transformers possible
- Multi-Head Attention â€” Running parallel attention operations
- Feed-Forward Network â€” The MLP that processes attended information
- Layer Normalization â€” Stabilizing activations for training
- Cross-Entropy Loss â€” Measuring prediction quality
- Loss Gradients & Backpropagation â€” The complete backward pass
- AdamW Optimizer â€” How weights actually get updated

### Building a Transformer

Build a complete GPT-style transformer in PyTorch. This section covers the architecture that powers modern language models, from embeddings to interpretability tools.

- Embeddings & Positions â€” Token embeddings, ALiBi, RoPE
- Scaled Dot-Product Attention â€” Attention with causal masking
- Multi-Head Attention â€” Parallel attention heads
- Feed-Forward Networks â€” Position-wise MLPs with GELU
- Transformer Block â€” Pre-LN, residuals, and all components combined
- Complete Model â€” Stacking blocks with embedding and output layers
- Training at Scale â€” Gradient accumulation, validation splits
- KV-Cache â€” Fast inference through caching
- Interpretability â€” Logit lens, attention analysis, induction heads

### Fine-Tuning a Transformer

Take a pretrained model and adapt it for specific tasks using modern techniques.

- **Supervised Fine-Tuning (SFT)** â€” Instruction formatting, loss masking, LoRA
- **Reward Modeling** â€” Preference data, training reward models, evaluation
- **RLHF** â€” PPO algorithm, KL penalty, training dynamics, reference models
- **DPO** â€” Direct Preference Optimization as an alternative to RLHF
- **Advanced Topics** â€” Memory optimization, hyperparameter tuning, evaluation metrics, common pitfalls

### Reasoning with Transformers

Explore how models can "think" before answering. This section covers techniques from simple prompting to training your own reasoning model.

- Chain-of-Thought â€” Prompting models to show their work
- Self-Consistency â€” Sampling multiple chains, majority voting
- Tree of Thoughts â€” Exploring and pruning reasoning paths
- Process Reward Models â€” Scoring individual reasoning steps
- Best-of-N Sampling â€” Using verification to select best solutions
- Monte Carlo Tree Search â€” Search algorithms for reasoning
- Budget Forcing â€” Controlling reasoning length with "Wait" tokens
- GRPO â€” RL without a critic (DeepSeek's approach)
- Distillation â€” Transferring reasoning to smaller models

### From Noise to Images

Learn how AI generates images from text prompts. This section builds from flow matching fundamentals to a working latent diffusion model.

- Flow Matching â€” Velocity fields, noise-to-data paths
- Diffusion Transformer â€” Patchifying images, attention for generation
- Class Conditioning â€” Classifier-free guidance (CFG)
- Text Conditioning â€” CLIP embeddings, cross-attention
- Latent Diffusion â€” VAE compression, scaling to larger images

## Philosophy

These materials follow a simple principle: **the best way to understand something is to build it yourself**.

- No hand-waving. Every operation is explicit.
- No magic. You see exactly what the computer sees.
- No shortcuts. Understanding comes from doing the work.

The architecture that powers GPT, Claude, Stable Diffusion, and other frontier models isn't beyond comprehension. It's just matrix multiplications, attention mechanisms, and optimizationâ€”repeated at scale.

## Getting Started

### Prerequisites

- Python 3.12
- A GPU is recommended for the PyTorch sections (CUDA, ROCm, or MPS)
- Basic understanding of calculus (chain rule, partial derivatives)
- Familiarity with Python

### Installation

```bash
# Clone the repository
git clone https://github.com/zhubert/intro-to-transformers.git
cd intro-to-transformers

# Install dependencies with uv (recommended)
uv sync

# Or with pip
pip install -e .
```

### Running the Notebooks

```bash
# Activate the virtual environment
source .venv/bin/activate

# Start Jupyter
jupyter notebook
```

### Building the Book

The book is built with [MyST](https://mystmd.org/):

```bash
# Install MyST CLI
npm install

# Start the development server
npx myst start

# Build for production
npx myst build --html
```

## Project Structure

```
intro-to-transformers/
â”œâ”€â”€ understanding-gradients/      # Hand-calculated forward & backward pass
â”œâ”€â”€ building-a-transformer/       # PyTorch implementation from scratch
â”œâ”€â”€ fine-tuning-a-transformer/    # SFT, RLHF, DPO techniques
â”œâ”€â”€ from-noise-to-images/         # Diffusion models and flow matching
â”œâ”€â”€ reasoning-with-transformers/  # CoT, MCTS, GRPO, distillation
â”œâ”€â”€ intro.md                      # Book landing page
â”œâ”€â”€ myst.yml                      # MyST configuration
â””â”€â”€ pyproject.toml                # Python dependencies
```

## Dependencies

Core dependencies:

- `torch` â€” Deep learning framework
- `transformers` â€” Hugging Face transformers library
- `datasets` â€” Hugging Face datasets
- `tiktoken` â€” OpenAI's tokenizer
- `matplotlib` â€” Visualization
- `peft` â€” Parameter-efficient fine-tuning (LoRA)
- `accelerate` â€” Training utilities
- `bitsandbytes` â€” Quantization

## Authors

- [Zack Hubert](https://www.zhubert.com) ([@zhubert](https://github.com/zhubert))
- [Claude Code](https://claude.ai/product/claude-code)

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- The original "Attention Is All You Need" paper (Vaswani et al., 2017)
- Andrej Karpathy's educational videos and nanoGPT
- The PyTorch and Hugging Face teams
- Everyone who has worked to make AI more understandable
