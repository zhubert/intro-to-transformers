{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Token Embeddings & Positional Encoding\n",
    "\n",
    "**Converting text to vectors and adding position information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embeddings\n",
    "\n",
    "**What are tokens?** Before we can process text with a neural network, we need to break it into pieces called tokens. A token might be a word (\"hello\"), a subword (\"ing\"), or even a single character.\n",
    "\n",
    "**Why do we need embeddings?** Computers can't directly understand token IDs—they're just arbitrary numbers. We need to convert them into meaningful representations that capture semantic relationships.\n",
    "\n",
    "**What is an embedding?** An embedding is a learned vector representation for each token. Instead of representing \"cat\" as ID 142, we represent it as a dense vector like [0.2, -0.5, 0.8, ...] with `d_model` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Convert token indices to dense vectors.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len) - token indices\n",
    "        # returns: (batch, seq_len, d_model) - embeddings\n",
    "        return self.embedding(x)\n",
    "\n",
    "# Example\n",
    "vocab_size = 1000\n",
    "d_model = 64\n",
    "embed = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "# Fake tokens\n",
    "tokens = torch.tensor([[5, 142, 89, 256]])  # (batch=1, seq_len=4)\n",
    "embeddings = embed(tokens)\n",
    "\n",
    "print(f\"Input tokens: {tokens.shape}\")\n",
    "print(f\"Output embeddings: {embeddings.shape}\")\n",
    "print(f\"\\nFirst token embedding (first 8 dims):\")\n",
    "print(f\"  {embeddings[0, 0, :8]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why We Need Positional Information\n",
    "\n",
    "Consider: \"The cat ate the mouse\" vs \"The mouse ate the cat\"—same words, completely different meanings! The order matters.\n",
    "\n",
    "Transformers process all tokens simultaneously in parallel, so they have no inherent notion of position. We need to give the model information about where each token appears.\n",
    "\n",
    "There are several modern approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: ALiBi (Attention with Linear Biases)\n",
    "\n",
    "The simplest and very effective! Instead of modifying embeddings, ALiBi adds distance-based penalties directly to attention scores:\n",
    "\n",
    "```\n",
    "attention_score[i,j] = Q·K / √d_k - slope × |i - j|\n",
    "```\n",
    "\n",
    "The further apart two positions are, the more negative the penalty → lower attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALiBiPositionalBias(nn.Module):\n",
    "    \"\"\"ALiBi: Attention with Linear Biases.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        # Geometric sequence of slopes for different heads\n",
    "        # Head 0 = strongest penalty (local), last head = weakest (global)\n",
    "        slopes = torch.tensor([\n",
    "            2 ** (-8 * (i + 1) / num_heads) \n",
    "            for i in range(num_heads)\n",
    "        ])\n",
    "        self.register_buffer('slopes', slopes)\n",
    "    \n",
    "    def forward(self, seq_len):\n",
    "        # Create position indices\n",
    "        positions = torch.arange(seq_len)\n",
    "        \n",
    "        # Compute pairwise distances: |i - j|\n",
    "        distances = torch.abs(positions.unsqueeze(0) - positions.unsqueeze(1))\n",
    "        \n",
    "        # Apply slope to get biases: -slope × distance\n",
    "        # Shape: (num_heads, seq_len, seq_len)\n",
    "        biases = -self.slopes.view(-1, 1, 1) * distances.unsqueeze(0).float()\n",
    "        \n",
    "        return biases\n",
    "\n",
    "# Example\n",
    "alibi = ALiBiPositionalBias(num_heads=4)\n",
    "biases = alibi(seq_len=8)\n",
    "\n",
    "print(f\"ALiBi biases shape: {biases.shape}\")\n",
    "print(f\"\\nHead 0 biases (strong local focus):\")\n",
    "print(biases[0, :4, :4].numpy().round(2))\n",
    "print(f\"\\nHead 3 biases (weak penalty, global focus):\")\n",
    "print(biases[3, :4, :4].numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Learned Positional Embeddings\n",
    "\n",
    "Used in GPT-2 and BERT. Each position gets its own learnable embedding that's added to the token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    \"\"\"Learned positional embeddings (GPT-2 style).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Create position indices: [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        \n",
    "        # Get position embeddings and ADD to input\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        return x + pos_emb\n",
    "\n",
    "# Example\n",
    "pos_enc = LearnedPositionalEncoding(d_model=64)\n",
    "x = torch.randn(2, 10, 64)  # (batch=2, seq_len=10, d_model=64)\n",
    "x_with_pos = pos_enc(x)\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"With position encoding: {x_with_pos.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: RoPE (Rotary Position Embeddings)\n",
    "\n",
    "Used in LLaMA, Mistral. Instead of adding position info, we *rotate* query and key vectors by an angle proportional to their position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"RoPE: Rotary Position Embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000, base=10000):\n",
    "        super().__init__()\n",
    "        # Compute inverse frequencies\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Precompute cos/sin for all positions\n",
    "        positions = torch.arange(max_seq_len)\n",
    "        freqs = torch.outer(positions, inv_freq)\n",
    "        self.register_buffer('cos_cached', freqs.cos())\n",
    "        self.register_buffer('sin_cached', freqs.sin())\n",
    "    \n",
    "    def forward(self, x, seq_len):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        cos = self.cos_cached[:seq_len]\n",
    "        sin = self.sin_cached[:seq_len]\n",
    "        return cos, sin\n",
    "\n",
    "def apply_rotary_emb(x, cos, sin):\n",
    "    \"\"\"Apply rotation to x using precomputed cos/sin.\"\"\"\n",
    "    # Split into pairs and rotate\n",
    "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "    x_rotated = torch.stack([\n",
    "        x1 * cos - x2 * sin,\n",
    "        x1 * sin + x2 * cos\n",
    "    ], dim=-1).flatten(-2)\n",
    "    return x_rotated\n",
    "\n",
    "# Example\n",
    "rope = RotaryPositionalEmbedding(d_model=64)\n",
    "cos, sin = rope(None, seq_len=10)\n",
    "print(f\"Cos cache shape: {cos.shape}\")\n",
    "print(f\"Sin cache shape: {sin.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "| Method | Parameters | Position Type | Extrapolation | Used In |\n",
    "|--------|------------|--------------|---------------|--------|\n",
    "| **ALiBi** | 0 | Relative | Best | BLOOM, MPT |\n",
    "| **RoPE** | 0 | Relative | Excellent | LLaMA, Mistral |\n",
    "| **Learned** | Many | Absolute | Limited | GPT-2, BERT |\n",
    "\n",
    "For modern models, ALiBi and RoPE are preferred because:\n",
    "- Zero parameters to learn\n",
    "- Better extrapolation to longer sequences\n",
    "- Encode relative position (distance matters, not absolute position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Attention\n",
    "\n",
    "Now that we can convert tokens to embeddings and encode position, we're ready to implement the core innovation of transformers: the attention mechanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
