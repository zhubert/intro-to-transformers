{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer from Scratch\n",
    "\n",
    "**An educational journey through the architecture that powers modern AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Transformer?\n",
    "\n",
    "A transformer is a type of neural network architecture introduced in the landmark paper *\"Attention is All You Need\"* (Vaswani et al., 2017). It revolutionized artificial intelligence and is now the foundation of virtually all modern large language models, including GPT, BERT, Claude, and many others.\n",
    "\n",
    "**What makes transformers special?** Previous approaches to language modeling used recurrent neural networks (RNNs), which process text one word at a time in sequence—like reading a sentence from left to right. Transformers instead use a mechanism called **attention** that allows them to process all words simultaneously *while still understanding their relationships*. This parallel processing makes them much faster to train and more effective at capturing long-range dependencies in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Path\n",
    "\n",
    "In this section, we'll build a complete transformer in PyTorch:\n",
    "\n",
    "1. **Token Embeddings** — Convert text to vectors and add position information\n",
    "2. **Attention** — Learn how tokens attend to each other using Query, Key, Value\n",
    "3. **Multi-Head Attention** — Run parallel attention heads to capture different relationships\n",
    "4. **Feed-Forward Networks** — Process attended information through position-wise MLPs\n",
    "5. **Transformer Block** — Combine attention, FFN, layer norm, and residual connections\n",
    "6. **Complete Model** — Stack blocks and add embedding/output layers\n",
    "7. **Training** — Use gradient accumulation and validation for stable training\n",
    "8. **KV-Cache** — Optimize inference speed by caching key-value pairs\n",
    "9. **Interpretability** — Analyze attention patterns and understand what the model learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251124+rocm7.1\n",
      "CUDA available: True\n",
      "CUDA device: Radeon RX 7900 XTX\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch is available\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"MPS (Apple Silicon) available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Overview\n",
    "\n",
    "A decoder-only transformer (like GPT) consists of:\n",
    "\n",
    "```\n",
    "Input tokens\n",
    "    ↓\n",
    "[Token Embedding + Positional Encoding]\n",
    "    ↓\n",
    "┌─────────────────────────────────┐\n",
    "│      Transformer Block × N      │  ← Repeated N times\n",
    "│  ┌───────────────────────────┐  │\n",
    "│  │ Multi-Head Self-Attention │  │\n",
    "│  │ + Residual + LayerNorm    │  │\n",
    "│  └───────────────────────────┘  │\n",
    "│  ┌───────────────────────────┐  │\n",
    "│  │ Feed-Forward Network      │  │\n",
    "│  │ + Residual + LayerNorm    │  │\n",
    "│  └───────────────────────────┘  │\n",
    "└─────────────────────────────────┘\n",
    "    ↓\n",
    "[Linear → Vocabulary]\n",
    "    ↓\n",
    "Output probabilities\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Hyperparameters\n",
    "\n",
    "| Parameter | Typical Value | Description |\n",
    "|-----------|--------------|-------------|\n",
    "| `d_model` | 512-4096 | Embedding dimension |\n",
    "| `n_heads` | 8-32 | Number of attention heads |\n",
    "| `n_layers` | 6-96 | Number of transformer blocks |\n",
    "| `d_ff` | 4 × d_model | Feed-forward hidden dimension |\n",
    "| `vocab_size` | 32K-100K | Size of token vocabulary |\n",
    "| `max_seq_len` | 512-128K | Maximum sequence length |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "  d_model: 256\n",
      "  n_heads: 4\n",
      "  n_layers: 4\n",
      "  d_ff: 1024\n",
      "  vocab_size: 10000\n",
      "  max_seq_len: 512\n",
      "  dropout: 0.1\n",
      "\n",
      "Estimated parameters: 5,705,728 (5.7M)\n"
     ]
    }
   ],
   "source": [
    "# Example configuration for a small educational model\n",
    "config = {\n",
    "    'd_model': 256,      # Embedding dimension\n",
    "    'n_heads': 4,        # Number of attention heads\n",
    "    'n_layers': 4,       # Number of transformer blocks\n",
    "    'd_ff': 1024,        # Feed-forward dimension (4 × d_model)\n",
    "    'vocab_size': 10000, # Vocabulary size\n",
    "    'max_seq_len': 512,  # Maximum sequence length\n",
    "    'dropout': 0.1,      # Dropout rate\n",
    "}\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Estimate parameter count\n",
    "embed_params = config['vocab_size'] * config['d_model']  # Token embeddings\n",
    "attn_params = config['n_layers'] * 4 * config['d_model'] ** 2  # Q, K, V, O projections\n",
    "ffn_params = config['n_layers'] * 2 * config['d_model'] * config['d_ff']  # Up and down projections\n",
    "total_params = embed_params + attn_params + ffn_params\n",
    "\n",
    "print(f\"\\nEstimated parameters: {total_params:,} ({total_params/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Build!\n",
    "\n",
    "In the following notebooks, we'll implement each component step by step, with executable code you can run and modify.\n",
    "\n",
    "Ready? Let's start with embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
