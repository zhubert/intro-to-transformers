{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Building a Transformer from Scratch\n",
    "\n",
    "**An educational journey through the architecture that powers modern AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Architecture That Changed Everything\n",
    "\n",
    "In 2017, a team at Google published a paper with an unusually bold title: *\"Attention Is All You Need.\"* The paper introduced the transformer architecture, and it's no exaggeration to say it changed the trajectory of artificial intelligence.\n",
    "\n",
    "Before transformers, the dominant approach for language tasks was **recurrent neural networks (RNNs)**. RNNs process text sequentially—one word at a time, left to right—maintaining a hidden state that carries information forward. It's intuitive: that's how we read, after all.\n",
    "\n",
    "But sequential processing has a fatal flaw: **it's slow**. You can't start processing word 5 until you've finished word 4. On modern GPUs—which excel at parallel computation—this is a massive waste. Training large RNNs took weeks.\n",
    "\n",
    "The transformer's key insight was to replace recurrence with **attention**: a mechanism that lets every position look at every other position simultaneously. Instead of processing sequentially, transformers process all positions in parallel, using learned attention patterns to capture relationships. A sentence that took an RNN 100 sequential steps can be processed by a transformer in one parallel operation.\n",
    "\n",
    "This parallelism unlocked scale. Suddenly you could train on billions of tokens. GPT-2, GPT-3, GPT-4, Claude, LLaMA, Gemini—all transformers. The architecture proved so effective that it's now used not just for language, but for images, audio, video, and even protein structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## What We're Building\n",
    "\n",
    "In this chapter, we'll build a complete transformer from scratch in PyTorch. Not a toy model that skips the hard parts—the real thing, with every component implemented and explained.\n",
    "\n",
    "We're building a **decoder-only transformer** (the architecture used by GPT, Claude, and LLaMA). \"Decoder-only\" means it generates text autoregressively—predicting one token at a time, each prediction conditioned on all previous tokens. The original transformer paper had both an encoder and decoder (for translation); modern language models found that decoder-only works great and is simpler.\n",
    "\n",
    "Here's the high-level architecture:\n",
    "\n",
    "```\n",
    "Input tokens: [The, cat, sat, on, the]\n",
    "                    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│     Token Embedding + Position      │  Convert tokens to vectors\n",
    "└─────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│       Transformer Block × N         │  The core of the model\n",
    "│  ┌────────────────────────────────┐ │\n",
    "│  │   Multi-Head Self-Attention    │ │  Tokens communicate\n",
    "│  │   + Residual + LayerNorm       │ │\n",
    "│  └────────────────────────────────┘ │\n",
    "│  ┌────────────────────────────────┐ │\n",
    "│  │   Feed-Forward Network         │ │  Tokens compute\n",
    "│  │   + Residual + LayerNorm       │ │\n",
    "│  └────────────────────────────────┘ │\n",
    "└─────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│     Final LayerNorm + Linear        │  Project to vocabulary\n",
    "└─────────────────────────────────────┘\n",
    "                    ↓\n",
    "Output logits: [0.1, 0.3, 8.2, ...]    Scores for each possible next token\n",
    "```\n",
    "\n",
    "Each transformer block contains two sub-layers:\n",
    "1. **Multi-Head Self-Attention**: Where tokens \"talk\" to each other. Each position can attend to all previous positions, gathering relevant context.\n",
    "2. **Feed-Forward Network**: Where tokens \"think\" independently. A two-layer MLP processes each position's representation.\n",
    "\n",
    "Both sub-layers use **residual connections** (adding the input to the output) and **layer normalization** (stabilizing activations). These aren't optional nice-to-haves—they're essential for training deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The Learning Path\n",
    "\n",
    "We'll build each component step by step:\n",
    "\n",
    "| Notebook | Topic | What You'll Learn |\n",
    "|----------|-------|-------------------|\n",
    "| **01** | Token Embeddings | How text becomes vectors; positional encoding approaches (learned, ALiBi, RoPE) |\n",
    "| **02** | Attention | The Q, K, V mechanism; scaled dot-product attention; causal masking |\n",
    "| **03** | Multi-Head Attention | Running parallel attention heads; why multiple heads help |\n",
    "| **04** | Feed-Forward Networks | The \"thinking\" layer; GELU activation; the 4× expansion pattern |\n",
    "| **05** | Transformer Block | Combining attention + FFN with residuals and layer norm |\n",
    "| **06** | Complete Model | Stacking blocks; the full forward pass; parameter counting |\n",
    "| **07** | Training | Gradient accumulation; validation; the training loop |\n",
    "| **08** | KV-Cache | Efficient inference; trading memory for speed |\n",
    "| **09** | Interpretability | Looking inside: attention patterns, logit lens, activation patching |\n",
    "\n",
    "Each notebook builds on the previous ones. By the end, you'll have a working transformer that you could (with enough compute) train on real text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Key Hyperparameters\n",
    "\n",
    "Before diving in, let's establish the key dimensions that define a transformer. Understanding these will help you read the code and scale models up or down.\n",
    "\n",
    "| Parameter | Symbol | Description | Our Model | GPT-2 Small | GPT-3 |\n",
    "|-----------|--------|-------------|-----------|-------------|-------|\n",
    "| **Embedding dimension** | $d_{model}$ | Size of token representations | 256 | 768 | 12,288 |\n",
    "| **Number of heads** | $n_{heads}$ | Parallel attention mechanisms | 4 | 12 | 96 |\n",
    "| **Head dimension** | $d_k = d_{model}/n_{heads}$ | Size per attention head | 64 | 64 | 128 |\n",
    "| **Number of layers** | $n_{layers}$ | Stacked transformer blocks | 4 | 12 | 96 |\n",
    "| **FFN hidden size** | $d_{ff}$ | Feed-forward inner dimension | 1024 | 3072 | 49,152 |\n",
    "| **Vocabulary size** | $|V|$ | Number of unique tokens | 10,000 | 50,257 | 50,257 |\n",
    "| **Context length** | $n_{ctx}$ | Maximum sequence length | 512 | 1,024 | 2,048 |\n",
    "\n",
    "A few patterns to notice:\n",
    "- **$d_{ff} \\approx 4 \\times d_{model}$**: The feed-forward layer expands to 4× the embedding size, then projects back down. This expansion gives the model more capacity to learn complex transformations.\n",
    "- **$d_k = d_{model} / n_{heads}$**: Each attention head operates on a slice of the embedding. More heads means each head is smaller, but you get more parallel \"perspectives.\"\n",
    "- **Depth scales with width**: Larger models use both more layers and larger dimensions. GPT-3's 96 layers would be unstable with GPT-2's smaller dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:23.027073Z",
     "iopub.status.busy": "2025-12-10T21:17:23.026996Z",
     "iopub.status.idle": "2025-12-10T21:17:23.742119Z",
     "shell.execute_reply": "2025-12-10T21:17:23.741740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251124+rocm7.1\n",
      "CUDA available: True\n",
      "CUDA device: Radeon RX 7900 XTX\n"
     ]
    }
   ],
   "source": [
    "# Let's verify PyTorch is set up correctly\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"MPS (Apple Silicon) available\")\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:23.743072Z",
     "iopub.status.busy": "2025-12-10T21:17:23.742968Z",
     "iopub.status.idle": "2025-12-10T21:17:23.744984Z",
     "shell.execute_reply": "2025-12-10T21:17:23.744722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration\n",
      "========================================\n",
      "  d_model         = 256\n",
      "  n_heads         = 4\n",
      "  n_layers        = 4\n",
      "  d_ff            = 1024\n",
      "  vocab_size      = 10000\n",
      "  max_seq_len     = 512\n",
      "  dropout         = 0.1\n"
     ]
    }
   ],
   "source": [
    "# Our model configuration\n",
    "config = {\n",
    "    'd_model': 256,       # Embedding dimension\n",
    "    'n_heads': 4,         # Number of attention heads\n",
    "    'n_layers': 4,        # Number of transformer blocks\n",
    "    'd_ff': 1024,         # Feed-forward hidden dimension (4 × d_model)\n",
    "    'vocab_size': 10000,  # Vocabulary size\n",
    "    'max_seq_len': 512,   # Maximum sequence length\n",
    "    'dropout': 0.1,       # Dropout rate\n",
    "}\n",
    "\n",
    "print(\"Model Configuration\")\n",
    "print(\"=\" * 40)\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k:15} = {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Parameter Count: Where Do All the Numbers Live?\n",
    "\n",
    "Understanding where parameters come from helps you reason about model capacity and memory. Let's count them:\n",
    "\n",
    "**Token Embeddings**: $|V| \\times d_{model}$\n",
    "- One $d_{model}$-dimensional vector per vocabulary token\n",
    "- For us: $10,000 \\times 256 = 2,560,000$ parameters\n",
    "\n",
    "**Position Embeddings** (if learned): $n_{ctx} \\times d_{model}$\n",
    "- One vector per position\n",
    "- For us: $512 \\times 256 = 131,072$ parameters\n",
    "\n",
    "**Per Transformer Block**:\n",
    "- **Attention**: $4 \\times d_{model}^2$ (for Q, K, V, and output projections)\n",
    "- **FFN**: $2 \\times d_{model} \\times d_{ff}$ (up projection and down projection)\n",
    "- **LayerNorm**: $4 \\times d_{model}$ (two layer norms, each with scale and shift)\n",
    "\n",
    "**Output Projection**: $d_{model} \\times |V|$\n",
    "- Projects from embedding space back to vocabulary (often tied with input embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:23.745882Z",
     "iopub.status.busy": "2025-12-10T21:17:23.745795Z",
     "iopub.status.idle": "2025-12-10T21:17:23.749018Z",
     "shell.execute_reply": "2025-12-10T21:17:23.748755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Count Breakdown\n",
      "==================================================\n",
      "Token embeddings:         2,560,000 (2.56M)\n",
      "Position embeddings:        131,072 (0.13M)\n",
      "\n",
      "Per transformer block:\n",
      "  Attention (Q,K,V,O):      262,144\n",
      "  Feed-forward:             524,288\n",
      "  LayerNorm (×2):             1,024\n",
      "  Block total:              787,456\n",
      "\n",
      "All 4 blocks:             3,149,824 (3.15M)\n",
      "Final LayerNorm:                512\n",
      "==================================================\n",
      "Total parameters:         5,841,408 (5.84M)\n"
     ]
    }
   ],
   "source": [
    "# Let's count parameters for our configuration\n",
    "d_model = config['d_model']\n",
    "n_heads = config['n_heads']\n",
    "n_layers = config['n_layers']\n",
    "d_ff = config['d_ff']\n",
    "vocab_size = config['vocab_size']\n",
    "max_seq_len = config['max_seq_len']\n",
    "\n",
    "# Embeddings\n",
    "token_embed_params = vocab_size * d_model\n",
    "pos_embed_params = max_seq_len * d_model\n",
    "\n",
    "# Per block\n",
    "attention_params = 4 * d_model * d_model  # Q, K, V, O projections (ignoring biases for simplicity)\n",
    "ffn_params = 2 * d_model * d_ff           # Up and down projections\n",
    "layernorm_params = 4 * d_model            # 2 layer norms × (scale + shift)\n",
    "block_params = attention_params + ffn_params + layernorm_params\n",
    "\n",
    "# Total\n",
    "total_block_params = n_layers * block_params\n",
    "output_proj_params = d_model * vocab_size  # Often tied with token embeddings\n",
    "final_ln_params = 2 * d_model\n",
    "\n",
    "total_params = token_embed_params + pos_embed_params + total_block_params + final_ln_params\n",
    "# Note: if we tie embeddings, we don't count output_proj_params separately\n",
    "\n",
    "print(\"Parameter Count Breakdown\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Token embeddings:      {token_embed_params:>12,} ({token_embed_params/1e6:.2f}M)\")\n",
    "print(f\"Position embeddings:   {pos_embed_params:>12,} ({pos_embed_params/1e6:.2f}M)\")\n",
    "print()\n",
    "print(f\"Per transformer block:\")\n",
    "print(f\"  Attention (Q,K,V,O): {attention_params:>12,}\")\n",
    "print(f\"  Feed-forward:        {ffn_params:>12,}\")\n",
    "print(f\"  LayerNorm (×2):      {layernorm_params:>12,}\")\n",
    "print(f\"  Block total:         {block_params:>12,}\")\n",
    "print()\n",
    "print(f\"All {n_layers} blocks:          {total_block_params:>12,} ({total_block_params/1e6:.2f}M)\")\n",
    "print(f\"Final LayerNorm:       {final_ln_params:>12,}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total parameters:      {total_params:>12,} ({total_params/1e6:.2f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## A Note on Scale\n",
    "\n",
    "Our model is tiny—about 5-6 million parameters. For context:\n",
    "\n",
    "| Model | Parameters | Relative Size |\n",
    "|-------|------------|---------------|\n",
    "| **Our model** | ~5M | 1× |\n",
    "| GPT-2 Small | 117M | 20× |\n",
    "| GPT-2 Large | 774M | 130× |\n",
    "| GPT-3 | 175B | 30,000× |\n",
    "| GPT-4 | ~1.8T (rumored) | 300,000× |\n",
    "\n",
    "But here's the beautiful thing: **the architecture is identical**. The same attention mechanism, the same feed-forward structure, the same residual connections. GPT-3 is just our model with bigger matrices and more layers.\n",
    "\n",
    "Understanding our 5M parameter model means understanding GPT-3. The math scales; the concepts don't change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## What You'll Need\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python and PyTorch (tensors, modules, autograd)\n",
    "- Linear algebra fundamentals (matrix multiplication, transpose, dot products)\n",
    "- Some calculus (we'll compute gradients, but PyTorch does the heavy lifting)\n",
    "\n",
    "**Mindset:**\n",
    "- Run the code cells—this is meant to be interactive\n",
    "- Modify things and see what breaks\n",
    "- The goal is understanding, not just running\n",
    "\n",
    "Each notebook is self-contained with working code. You can run them in order, or jump to a specific topic if you want to understand one component in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Let's Build\n",
    "\n",
    "We'll start where every language model starts: turning text into numbers.\n",
    "\n",
    "In the next notebook, we'll implement token embeddings and explore three different approaches to positional encoding—learned embeddings (GPT-2 style), ALiBi (BLOOM style), and RoPE (LLaMA style). Each has its trade-offs, and understanding them will give you intuition for how transformers represent sequences.\n",
    "\n",
    "Ready? Let's go."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
