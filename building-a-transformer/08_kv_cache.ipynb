{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# KV-Cache\n\n**Optimizing inference from O(n²) to O(n)**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Slow Autoregressive Generation\n",
    "\n",
    "When generating text, transformers produce one token at a time. After generating each token, we feed the entire sequence back through the model to predict the next token. This means we repeatedly recompute the same values!\n",
    "\n",
    "### Example without cache:\n",
    "\n",
    "```python\n",
    "# Generate \"The cat sat\"\n",
    "\n",
    "# Step 1: Generate token 3\n",
    "Input: [The, cat]\n",
    "Compute: K[The], V[The], K[cat], V[cat]\n",
    "Output: \"sat\" ✓\n",
    "\n",
    "# Step 2: Generate token 4\n",
    "Input: [The, cat, sat]\n",
    "Compute: K[The], V[The], K[cat], V[cat], K[sat], V[sat]  ← Redundant!\n",
    "Output: \"on\"\n",
    "\n",
    "# Step 3: Generate token 5\n",
    "Input: [The, cat, sat, on]\n",
    "Compute: K[The], V[The], K[cat], V[cat], K[sat], V[sat], K[on], V[on]  ← Redundant!\n",
    "Output: \"the\"\n",
    "```\n",
    "\n",
    "For generating n tokens, we process 1 + 2 + 3 + ... + n = **O(n²)** tokens total. Very slow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution: KV-Cache\n",
    "\n",
    "**Key Insight:** In attention, K (Key) and V (Value) for past tokens never change! Only the new token's query matters. We can cache K and V from previous steps and reuse them.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "**Two Modes:**\n",
    "\n",
    "- **PREFILL:** Process initial prompt, compute and cache K, V for all tokens\n",
    "- **DECODE:** For each new token, compute only its K, V, concatenate with cached values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "class AttentionWithCache(nn.Module):\n",
    "    \"\"\"Multi-head attention with KV-cache support.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model) - new tokens to process\n",
    "            kv_cache: tuple of (K_cached, V_cached) or None for first pass\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            new_kv_cache: tuple of updated (K, V) tensors\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K, V for new tokens\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K_new = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V_new = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Concatenate with cache if available\n",
    "        if kv_cache is not None:\n",
    "            K_cached, V_cached = kv_cache\n",
    "            K = torch.cat([K_cached, K_new], dim=2)  # Append new K\n",
    "            V = torch.cat([V_cached, V_new], dim=2)  # Append new V\n",
    "        else:\n",
    "            K = K_new\n",
    "            V = V_new\n",
    "        \n",
    "        # Standard attention (Q attends to ALL K, V including cached)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Reshape and project\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        # Return output AND updated cache\n",
    "        return output, (K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREFILL: Process initial prompt ===\n",
      "Input: torch.Size([1, 5, 64])\n",
      "Output: torch.Size([1, 5, 64])\n",
      "Cache K shape: torch.Size([1, 4, 5, 16])\n",
      "Cache V shape: torch.Size([1, 4, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate KV-cache in action\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "attn = AttentionWithCache(d_model, num_heads)\n",
    "\n",
    "print(\"=== PREFILL: Process initial prompt ===\")\n",
    "prompt = torch.randn(1, 5, d_model)  # 5-token prompt\n",
    "output, kv_cache = attn(prompt, kv_cache=None)\n",
    "print(f\"Input: {prompt.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Cache K shape: {kv_cache[0].shape}\")\n",
    "print(f\"Cache V shape: {kv_cache[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DECODE: Generate tokens one at a time ===\n",
      "\n",
      "Step 1:\n",
      "  New token input: torch.Size([1, 1, 64])\n",
      "  Output: torch.Size([1, 1, 64])\n",
      "  Cache K shape: torch.Size([1, 4, 6, 16])\n",
      "  (Cache grows by 1 position each step)\n",
      "\n",
      "Step 2:\n",
      "  New token input: torch.Size([1, 1, 64])\n",
      "  Output: torch.Size([1, 1, 64])\n",
      "  Cache K shape: torch.Size([1, 4, 7, 16])\n",
      "  (Cache grows by 1 position each step)\n",
      "\n",
      "Step 3:\n",
      "  New token input: torch.Size([1, 1, 64])\n",
      "  Output: torch.Size([1, 1, 64])\n",
      "  Cache K shape: torch.Size([1, 4, 8, 16])\n",
      "  (Cache grows by 1 position each step)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== DECODE: Generate tokens one at a time ===\")\n",
    "for i in range(3):\n",
    "    # Only process the NEW token\n",
    "    new_token = torch.randn(1, 1, d_model)\n",
    "    output, kv_cache = attn(new_token, kv_cache=kv_cache)\n",
    "    \n",
    "    print(f\"\\nStep {i + 1}:\")\n",
    "    print(f\"  New token input: {new_token.shape}\")\n",
    "    print(f\"  Output: {output.shape}\")\n",
    "    print(f\"  Cache K shape: {kv_cache[0].shape}\")\n",
    "    print(f\"  (Cache grows by 1 position each step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory vs Speed Tradeoff\n",
    "\n",
    "**Memory Cost:** For each layer, we cache K and V tensors with shape `(batch, num_heads, seq_len, d_k)`. For a 6-layer model with d_model=256, 4 heads, and 200-token sequence, this is only ~3 MB per example. Very affordable!\n",
    "\n",
    "**Speed Benefit:** Reduces time complexity from O(n²) to O(n) for generating n tokens.\n",
    "\n",
    "Typical speedups:\n",
    "- Short sequences (10-20 tokens): 2-5x faster\n",
    "- Medium sequences (50-100 tokens): 10-20x faster\n",
    "- Long sequences (200+ tokens): 20-50x faster\n",
    "\n",
    "**Why ALL production LLMs use KV-cache:** The memory cost is tiny compared to the model weights, but the speed improvement is massive. Every production system (GPT, Claude, etc.) uses KV-cache for generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: with vs without cache\n",
    "import time\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Attention WITHOUT cache (for comparison).\"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "# Setup\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_tokens_to_generate = 50\n",
    "\n",
    "attn_no_cache = SimpleAttention(d_model, num_heads)\n",
    "attn_with_cache = AttentionWithCache(d_model, num_heads)\n",
    "\n",
    "# Share weights for fair comparison\n",
    "attn_with_cache.W_q = attn_no_cache.W_q\n",
    "attn_with_cache.W_k = attn_no_cache.W_k\n",
    "attn_with_cache.W_v = attn_no_cache.W_v\n",
    "attn_with_cache.W_o = attn_no_cache.W_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT cache:\n",
      "  Generated 50 tokens in 0.010s\n",
      "  Final sequence length: 60\n"
     ]
    }
   ],
   "source": [
    "# Benchmark WITHOUT cache\n",
    "prompt = torch.randn(1, 10, d_model)\n",
    "sequence = prompt.clone()\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(num_tokens_to_generate):\n",
    "    # Recompute attention over ENTIRE sequence each time\n",
    "    output = attn_no_cache(sequence)\n",
    "    new_token = output[:, -1:, :]  # Take last position\n",
    "    sequence = torch.cat([sequence, new_token], dim=1)\n",
    "time_no_cache = time.time() - start\n",
    "\n",
    "print(f\"WITHOUT cache:\")\n",
    "print(f\"  Generated {num_tokens_to_generate} tokens in {time_no_cache:.3f}s\")\n",
    "print(f\"  Final sequence length: {sequence.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH cache:\n",
      "  Generated 50 tokens in 0.007s\n",
      "  Final cache length: 60\n",
      "\n",
      "Speedup: 1.6x faster!\n"
     ]
    }
   ],
   "source": [
    "# Benchmark WITH cache\n",
    "prompt = torch.randn(1, 10, d_model)\n",
    "\n",
    "start = time.time()\n",
    "# Prefill\n",
    "output, kv_cache = attn_with_cache(prompt, kv_cache=None)\n",
    "last_token = output[:, -1:, :]\n",
    "\n",
    "# Decode\n",
    "for _ in range(num_tokens_to_generate):\n",
    "    # Only process the NEW token\n",
    "    output, kv_cache = attn_with_cache(last_token, kv_cache=kv_cache)\n",
    "    last_token = output\n",
    "time_with_cache = time.time() - start\n",
    "\n",
    "print(f\"WITH cache:\")\n",
    "print(f\"  Generated {num_tokens_to_generate} tokens in {time_with_cache:.3f}s\")\n",
    "print(f\"  Final cache length: {kv_cache[0].shape[2]}\")\n",
    "print(f\"\\nSpeedup: {time_no_cache / time_with_cache:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Implementation Detail\n",
    "\n",
    "The cache must correctly handle positional encodings! When processing token at position N, it must receive position embedding for N, not 0. Implementations track the cache length and adjust positions automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Interpretability\n",
    "\n",
    "Now that we can efficiently generate text, let's explore what our model has actually learned using interpretability techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}