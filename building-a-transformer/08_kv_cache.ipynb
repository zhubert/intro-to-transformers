{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KV-Cache\n",
    "\n",
    "**How transformers generate text efficiently**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained a transformer. Now we want to *use* it—to generate text, token by token. But there's a problem: the naive approach is horrifically slow.\n",
    "\n",
    "This notebook explains the problem and its elegant solution: **KV-caching**. This single optimization takes generation from $O(n^2)$ to $O(n)$—making it practical for real-world use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Redundant Computation\n",
    "\n",
    "Transformers generate text **autoregressively**—one token at a time. Each new token depends on all previous tokens:\n",
    "\n",
    "```\n",
    "Step 1: [The] → predict \"cat\"\n",
    "Step 2: [The, cat] → predict \"sat\"\n",
    "Step 3: [The, cat, sat] → predict \"on\"\n",
    "Step 4: [The, cat, sat, on] → predict \"the\"\n",
    "...\n",
    "```\n",
    "\n",
    "### The Wasteful Part\n",
    "\n",
    "At each step, we need the model to attend to all previous tokens. Without optimization, this means:\n",
    "\n",
    "| Step | Tokens Processed | K, V Computed For |\n",
    "|------|-----------------|-------------------|\n",
    "| 1 | \"The\" | \"The\" |\n",
    "| 2 | \"The cat\" | \"The\" (again!), \"cat\" |\n",
    "| 3 | \"The cat sat\" | \"The\" (again!), \"cat\" (again!), \"sat\" |\n",
    "| 4 | \"The cat sat on\" | \"The\" (again!), \"cat\" (again!), \"sat\" (again!), \"on\" |\n",
    "\n",
    "We keep recomputing K and V for tokens we've already processed! To generate $n$ tokens, we process $1 + 2 + 3 + ... + n = \\frac{n(n+1)}{2}$ token computations.\n",
    "\n",
    "**Time complexity:** $O(n^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Insight\n",
    "\n",
    "Look back at the attention formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "For autoregressive generation, **K and V for past tokens never change**. Once we've computed $K_{\\text{\"The\"}}$ and $V_{\\text{\"The\"}}$, those values are fixed forever. Only the new token needs fresh computation.\n",
    "\n",
    "The solution: **cache K and V from previous steps and reuse them**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV-Cache: The Solution\n",
    "\n",
    "The KV-cache stores computed K and V tensors across generation steps:\n",
    "\n",
    "**Phase 1: Prefill** (process the initial prompt)\n",
    "- Input: \"The cat\" (2 tokens)\n",
    "- Compute: $K_0, V_0$ for \"The\"; $K_1, V_1$ for \"cat\"\n",
    "- Cache: $[K_0, K_1]$, $[V_0, V_1]$\n",
    "\n",
    "**Phase 2: Decode** (generate one token at a time)\n",
    "- Input: just the NEW token (\"sat\")\n",
    "- Compute: $K_2, V_2$ for \"sat\" only\n",
    "- Retrieve: $K_{\\text{cached}} = [K_0, K_1]$\n",
    "- Concatenate: $K_{\\text{full}} = [K_0, K_1, K_2]$\n",
    "- Attention: New token's Q attends to all K, V\n",
    "\n",
    "Now we process exactly 1 token per step. **Time complexity:** $O(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:49:02.792845Z",
     "iopub.status.busy": "2025-12-08T22:49:02.792763Z",
     "iopub.status.idle": "2025-12-08T22:49:03.467255Z",
     "shell.execute_reply": "2025-12-08T22:49:03.466888Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's build attention with KV-cache support. The key change: the forward method accepts an optional cache and returns an updated cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:49:03.468461Z",
     "iopub.status.busy": "2025-12-08T22:49:03.468364Z",
     "iopub.status.idle": "2025-12-08T22:49:03.471452Z",
     "shell.execute_reply": "2025-12-08T22:49:03.471172Z"
    }
   },
   "outputs": [],
   "source": [
    "class CachedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention with KV-cache for efficient generation.\n",
    "    \n",
    "    During generation:\n",
    "    - First call (prefill): Process full prompt, initialize cache\n",
    "    - Subsequent calls (decode): Process single token, update cache\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model) - tokens to process\n",
    "               During prefill: full prompt\n",
    "               During decode: single new token (seq_len=1)\n",
    "            kv_cache: tuple of (K_cached, V_cached) or None\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            new_cache: tuple of (K_updated, V_updated)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        # Shape: (batch, seq_len, d_model)\n",
    "        Q = self.W_q(x)\n",
    "        K_new = self.W_k(x)\n",
    "        V_new = self.W_v(x)\n",
    "        \n",
    "        # Reshape for multi-head: (batch, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K_new = K_new.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V_new = V_new.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # === THE CACHING MAGIC ===\n",
    "        if kv_cache is not None:\n",
    "            # Append new K, V to cached values\n",
    "            K_cached, V_cached = kv_cache\n",
    "            K = torch.cat([K_cached, K_new], dim=2)  # Concat along seq dimension\n",
    "            V = torch.cat([V_cached, V_new], dim=2)\n",
    "        else:\n",
    "            # First call: no cache yet\n",
    "            K = K_new\n",
    "            V = V_new\n",
    "        \n",
    "        # Standard scaled dot-product attention\n",
    "        # Q: (batch, heads, seq_q, d_k) - could be 1 during decode\n",
    "        # K: (batch, heads, seq_kv, d_k) - grows each step\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        # Return output AND updated cache\n",
    "        return output, (K, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Critical Lines\n",
    "\n",
    "The entire optimization happens here:\n",
    "\n",
    "```python\n",
    "if kv_cache is not None:\n",
    "    K = torch.cat([K_cached, K_new], dim=2)\n",
    "    V = torch.cat([V_cached, V_new], dim=2)\n",
    "```\n",
    "\n",
    "Instead of recomputing K and V for all previous tokens, we just concatenate the cached values with the new token's K and V. The attention mechanism then works exactly as before—it doesn't know or care that we assembled K and V from cached parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "Let's trace through how the cache grows during generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:49:03.472316Z",
     "iopub.status.busy": "2025-12-08T22:49:03.472245Z",
     "iopub.status.idle": "2025-12-08T22:49:03.476133Z",
     "shell.execute_reply": "2025-12-08T22:49:03.475723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 1: PREFILL (process initial prompt)\n",
      "============================================================\n",
      "Input shape:  torch.Size([1, 5, 64])  (batch=1, seq=5, d_model=64)\n",
      "Output shape: torch.Size([1, 5, 64])\n",
      "\n",
      "Cache initialized:\n",
      "  K cache: torch.Size([1, 4, 5, 16])  (batch=1, heads=4, seq=5, d_k=16)\n",
      "  V cache: torch.Size([1, 4, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "attn = CachedMultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: PREFILL (process initial prompt)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate a 5-token prompt\n",
    "prompt = torch.randn(1, 5, d_model)  # batch=1, seq=5\n",
    "output, kv_cache = attn(prompt, kv_cache=None)\n",
    "\n",
    "print(f\"Input shape:  {prompt.shape}  (batch=1, seq=5, d_model={d_model})\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nCache initialized:\")\n",
    "print(f\"  K cache: {kv_cache[0].shape}  (batch=1, heads={num_heads}, seq=5, d_k={d_model//num_heads})\")\n",
    "print(f\"  V cache: {kv_cache[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:49:03.492137Z",
     "iopub.status.busy": "2025-12-08T22:49:03.492045Z",
     "iopub.status.idle": "2025-12-08T22:49:03.504444Z",
     "shell.execute_reply": "2025-12-08T22:49:03.504137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 2: DECODE (generate tokens one at a time)\n",
      "============================================================\n",
      "\n",
      "Step 1:\n",
      "  Input: torch.Size([1, 1, 64])  (just 1 new token!)\n",
      "  Output: torch.Size([1, 1, 64])\n",
      "  K cache: torch.Size([1, 4, 6, 16])  (seq grew to 6)\n",
      "\n",
      "Step 2:\n",
      "  Input: torch.Size([1, 1, 64])  (just 1 new token!)\n",
      "  Output: torch.Size([1, 1, 64])\n",
      "  K cache: torch.Size([1, 4, 7, 16])  (seq grew to 7)\n",
      "\n",
      "Step 3:\n",
      "  Input: torch.Size([1, 1, 64])  (just 1 new token!)\n",
      "  Output: torch.Size([1, 1, 64])\n",
      "  K cache: torch.Size([1, 4, 8, 16])  (seq grew to 8)\n",
      "\n",
      "============================================================\n",
      "After 3 decode steps: cache has 8 positions\n",
      "Started with 5 (prompt) + generated 3 = 8 total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECODE (generate tokens one at a time)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for step in range(1, 4):\n",
    "    # Simulate generating one new token\n",
    "    # In practice, this would be the embedding of the predicted token\n",
    "    new_token = torch.randn(1, 1, d_model)  # seq_len = 1!\n",
    "    \n",
    "    # Process ONLY the new token, using cached K, V\n",
    "    output, kv_cache = attn(new_token, kv_cache=kv_cache)\n",
    "    \n",
    "    print(f\"\\nStep {step}:\")\n",
    "    print(f\"  Input: {new_token.shape}  (just 1 new token!)\")\n",
    "    print(f\"  Output: {output.shape}\")\n",
    "    print(f\"  K cache: {kv_cache[0].shape}  (seq grew to {kv_cache[0].shape[2]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"After 3 decode steps: cache has {kv_cache[0].shape[2]} positions\")\n",
    "print(f\"Started with 5 (prompt) + generated 3 = 8 total\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity Analysis\n",
    "\n",
    "Let's quantify the improvement. Consider generating $n$ tokens after a prompt of length $p$:\n",
    "\n",
    "### Without KV-Cache\n",
    "\n",
    "| Step | Tokens Processed | K, V Computed |\n",
    "|------|-----------------|---------------|\n",
    "| 1 | p + 1 | p + 1 |\n",
    "| 2 | p + 2 | p + 2 |\n",
    "| ... | ... | ... |\n",
    "| n | p + n | p + n |\n",
    "\n",
    "**Total K, V computations:**\n",
    "$$\\sum_{i=1}^{n} (p + i) = np + \\frac{n(n+1)}{2} = O(n^2 + np)$$\n",
    "\n",
    "### With KV-Cache\n",
    "\n",
    "| Step | Tokens Processed | K, V Computed |\n",
    "|------|-----------------|---------------|\n",
    "| Prefill | p | p |\n",
    "| 1 | 1 | 1 |\n",
    "| 2 | 1 | 1 |\n",
    "| ... | ... | ... |\n",
    "| n | 1 | 1 |\n",
    "\n",
    "**Total K, V computations:**\n",
    "$$p + n = O(n + p)$$\n",
    "\n",
    "**Speedup factor:** $\\frac{O(n^2)}{O(n)} = O(n)$\n",
    "\n",
    "For generating 100 tokens, this is ~50× fewer computations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Cost\n",
    "\n",
    "KV-caching trades memory for speed. For each layer, we store:\n",
    "\n",
    "$$\\text{Cache size} = 2 \\times \\text{batch} \\times \\text{num\\_heads} \\times \\text{seq\\_len} \\times d_k \\times 4 \\text{ bytes}$$\n",
    "\n",
    "For our example model (6 layers, 4 heads, d_model=256, seq_len=512):\n",
    "\n",
    "$$= 6 \\times 2 \\times 1 \\times 4 \\times 512 \\times 64 \\times 4 = 6.3 \\text{ MB}$$\n",
    "\n",
    "This is tiny compared to model weights (~100MB for a small model). The tradeoff is overwhelmingly worth it!\n",
    "\n",
    "| Component | Memory |\n",
    "|-----------|--------|\n",
    "| Model weights | ~100 MB |\n",
    "| KV cache (512 tokens) | ~6 MB |\n",
    "| **Speedup** | **~50×** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "Let's measure the actual speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:49:03.505186Z",
     "iopub.status.busy": "2025-12-08T22:49:03.505111Z",
     "iopub.status.idle": "2025-12-08T22:49:03.507482Z",
     "shell.execute_reply": "2025-12-08T22:49:03.507184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attention WITHOUT caching (for comparison)\n",
    "class NoCacheAttention(nn.Module):\n",
    "    \"\"\"Standard attention that recomputes everything each time.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:49:03.508197Z",
     "iopub.status.busy": "2025-12-08T22:49:03.508124Z",
     "iopub.status.idle": "2025-12-08T22:49:03.512246Z",
     "shell.execute_reply": "2025-12-08T22:49:03.511977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark: Generate 50 tokens after 10-token prompt\n",
      "Model: d_model=256, num_heads=8\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "# Setup benchmark\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "prompt_len = 10\n",
    "generate_tokens = 50\n",
    "\n",
    "attn_no_cache = NoCacheAttention(d_model, num_heads)\n",
    "attn_with_cache = CachedMultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Share weights for fair comparison\n",
    "attn_with_cache.W_q = attn_no_cache.W_q\n",
    "attn_with_cache.W_k = attn_no_cache.W_k\n",
    "attn_with_cache.W_v = attn_no_cache.W_v\n",
    "attn_with_cache.W_o = attn_no_cache.W_o\n",
    "\n",
    "print(f\"Benchmark: Generate {generate_tokens} tokens after {prompt_len}-token prompt\")\n",
    "print(f\"Model: d_model={d_model}, num_heads={num_heads}\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:49:03.512946Z",
     "iopub.status.busy": "2025-12-08T22:49:03.512870Z",
     "iopub.status.idle": "2025-12-08T22:49:03.525076Z",
     "shell.execute_reply": "2025-12-08T22:49:03.524792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT KV-Cache:\n",
      "  Final sequence length: 60\n",
      "  Time: 10.2 ms\n"
     ]
    }
   ],
   "source": [
    "# WITHOUT cache: must reprocess entire sequence each step\n",
    "prompt = torch.randn(1, prompt_len, d_model)\n",
    "sequence = prompt.clone()\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(generate_tokens):\n",
    "    # Process ENTIRE sequence each time\n",
    "    output = attn_no_cache(sequence)\n",
    "    # Take last position's output as \"next token\" (simplified)\n",
    "    new_token = output[:, -1:, :]\n",
    "    sequence = torch.cat([sequence, new_token], dim=1)\n",
    "time_no_cache = time.time() - start\n",
    "\n",
    "print(f\"\\nWITHOUT KV-Cache:\")\n",
    "print(f\"  Final sequence length: {sequence.shape[1]}\")\n",
    "print(f\"  Time: {time_no_cache*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:49:03.525786Z",
     "iopub.status.busy": "2025-12-08T22:49:03.525713Z",
     "iopub.status.idle": "2025-12-08T22:49:03.533848Z",
     "shell.execute_reply": "2025-12-08T22:49:03.533541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH KV-Cache:\n",
      "  Final cache length: 60\n",
      "  Time: 6.1 ms\n",
      "\n",
      "=======================================================\n",
      "Speedup: 1.7x faster!\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "# WITH cache: process only new tokens\n",
    "prompt = torch.randn(1, prompt_len, d_model)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Prefill: process entire prompt\n",
    "output, kv_cache = attn_with_cache(prompt, kv_cache=None)\n",
    "last_output = output[:, -1:, :]\n",
    "\n",
    "# Decode: process one token at a time\n",
    "for _ in range(generate_tokens):\n",
    "    output, kv_cache = attn_with_cache(last_output, kv_cache=kv_cache)\n",
    "    last_output = output  # Already shape (1, 1, d_model)\n",
    "\n",
    "time_with_cache = time.time() - start\n",
    "\n",
    "print(f\"\\nWITH KV-Cache:\")\n",
    "print(f\"  Final cache length: {kv_cache[0].shape[2]}\")\n",
    "print(f\"  Time: {time_with_cache*1000:.1f} ms\")\n",
    "\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"Speedup: {time_no_cache/time_with_cache:.1f}x faster!\")\n",
    "print(f\"{'='*55}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Implementation Details\n",
    "\n",
    "### 1. Position Encodings\n",
    "\n",
    "When using learned or sinusoidal position encodings, the new token must receive the correct position index. If you've generated tokens 0-9 and are now generating token 10, it must get position embedding 10, not 0.\n",
    "\n",
    "```python\n",
    "# Track position during generation\n",
    "current_pos = prompt_length\n",
    "for _ in range(num_tokens):\n",
    "    pos_embed = get_position_embedding(current_pos)\n",
    "    ...\n",
    "    current_pos += 1\n",
    "```\n",
    "\n",
    "### 2. Causal Masking\n",
    "\n",
    "During prefill, you still need causal masking so position $i$ only attends to positions $\\leq i$. During decode, no mask is needed—each new token naturally attends only to past tokens (the cache) and itself.\n",
    "\n",
    "### 3. Multi-Layer Caching\n",
    "\n",
    "A full transformer has multiple layers, each with its own KV cache:\n",
    "\n",
    "```python\n",
    "# Cache is a list of (K, V) tuples, one per layer\n",
    "kv_caches = [None] * num_layers\n",
    "\n",
    "for i, layer in enumerate(layers):\n",
    "    x, kv_caches[i] = layer(x, kv_cache=kv_caches[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Every Production System Uses This\n",
    "\n",
    "KV-caching isn't optional for practical LLM deployment. Consider:\n",
    "\n",
    "| Scenario | Without Cache | With Cache |\n",
    "|----------|--------------|------------|\n",
    "| Generate 100 tokens | 5,050 computations | 100 computations |\n",
    "| Generate 1000 tokens | 500,500 computations | 1,000 computations |\n",
    "| Generate 4096 tokens | 8,390,656 computations | 4,096 computations |\n",
    "\n",
    "At 4096 tokens (common context length), caching provides a **2048× reduction** in K, V computations. The memory cost (~100MB for a large model) is negligible compared to this speedup.\n",
    "\n",
    "GPT, Claude, Gemini—every production system uses KV-caching. It's not an optimization; it's a requirement for usable latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Aspect | Without Cache | With Cache |\n",
    "|--------|--------------|------------|\n",
    "| K, V computation | All tokens every step | 1 token per step |\n",
    "| Time complexity | $O(n^2)$ | $O(n)$ |\n",
    "| Memory | Minimal | Cache grows with context |\n",
    "| Practical speedup | Baseline | 10-1000× faster |\n",
    "\n",
    "**Key insight:** K and V for past tokens are deterministic—compute once, reuse forever.\n",
    "\n",
    "**The tradeoff:** Memory for speed. Almost always worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Interpretability\n",
    "\n",
    "We've built, trained, and optimized a transformer. But what has it actually *learned*? The final notebook explores **mechanistic interpretability**—techniques for peeking inside the black box to understand what patterns and circuits the model has discovered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
