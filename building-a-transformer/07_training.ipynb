{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training at Scale\n",
    "\n",
    "**Gradient accumulation and validation for stable training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the transformer architecture is only half the battle. To train it effectively, we need techniques that make training stable, prevent overfitting, and work within the constraints of hobby-scale hardware.\n",
    "\n",
    "This section covers two critical techniques: **gradient accumulation** and **validation splits**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge: Small Batches, Noisy Training\n",
    "\n",
    "**What is a batch?** During training, we process multiple examples together in a \"batch.\" The model makes predictions for all examples, we compute the average loss, then calculate gradients and update weights. Larger batches give us more stable gradient estimates because we're averaging over more examples.\n",
    "\n",
    "**The problem with small batches:** On hobby hardware (like an M1 Mac or consumer GPU), we're limited to small batches—typically just 8 sequences at a time. Small batches lead to *noisy gradients*: each batch gives a slightly different signal about which direction to update the weights, causing erratic training.\n",
    "\n",
    "**Memory bottleneck:** Why can't we just use bigger batches? Each example requires storing activations in memory for the backward pass. M1 Macs have ~8GB unified memory, and a batch of 8 sequences already uses ~4GB. Doubling to 16 would run out of memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation: Large Batches Without the Memory Cost\n",
    "\n",
    "**The key insight:** We don't need to process all examples simultaneously! Gradient accumulation lets us simulate large batch sizes by accumulating gradients over multiple small batches before updating weights.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Process batch 1:** Forward pass → Loss → Backward pass → Store gradients (don't update yet!)\n",
    "2. **Process batch 2:** Forward pass → Loss → Backward pass → *Add* gradients to stored ones\n",
    "3. **Repeat** for N batches (e.g., 16 times)\n",
    "4. **Update weights:** Use the accumulated (averaged) gradients\n",
    "\n",
    "**Why this works mathematically:** Gradients are linear, so averaging gradients from N separate batches gives the same result as computing the gradient on one large batch:\n",
    "\n",
    "$$\\nabla(L_1 + L_2 + ... + L_n) = \\nabla L_1 + \\nabla L_2 + ... + \\nabla L_n$$\n",
    "\n",
    "By accumulating gradients over 16 batches of 8 sequences each, we get gradients equivalent to a batch of 128 sequences—16× more stable!—while only ever holding 8 sequences in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simple model for demonstration\n",
    "model = nn.Linear(64, 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Simulated data\n",
    "def get_batch():\n",
    "    x = torch.randn(8, 64)  # batch_size=8\n",
    "    y = torch.randint(0, 10, (8,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT accumulation (noisy - updates every batch)\n",
    "print(\"Training WITHOUT gradient accumulation:\")\n",
    "print(\"  Each batch = 8 examples\")\n",
    "print(\"  Updates every batch (noisy gradients)\\n\")\n",
    "\n",
    "for step in range(4):\n",
    "    x, y = get_batch()\n",
    "    loss = F.cross_entropy(model(x), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"  Step {step}: loss = {loss.item():.4f}, updated weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH accumulation (stable - updates every N batches)\n",
    "print(\"Training WITH gradient accumulation (16 steps):\")\n",
    "print(\"  Each batch = 8 examples\")\n",
    "print(\"  Effective batch = 8 × 16 = 128 examples\")\n",
    "print(\"  Updates every 16 batches (stable gradients)\\n\")\n",
    "\n",
    "accumulation_steps = 16\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for step in range(32):\n",
    "    x, y = get_batch()\n",
    "    loss = F.cross_entropy(model(x), y)\n",
    "    loss = loss / accumulation_steps  # Scale for correct averaging\n",
    "    loss.backward()                   # Accumulate gradients\n",
    "    \n",
    "    if (step + 1) % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"  Step {step}: accumulated loss = {loss.item() * accumulation_steps:.4f}, updated weights\")\n",
    "    else:\n",
    "        print(f\"  Step {step}: accumulated loss = {loss.item() * accumulation_steps:.4f}, accumulating...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: Detecting Overfitting\n",
    "\n",
    "### The Problem: Memorization vs. Learning\n",
    "\n",
    "Imagine a student preparing for an exam. They could:\n",
    "\n",
    "- **Memorize answers** to practice problems → Fails on new problems (overfitting)\n",
    "- **Learn concepts** from practice problems → Succeeds on new problems (good generalization)\n",
    "\n",
    "The same happens with neural networks. As training progresses, the model might start memorizing the training data instead of learning general patterns. This is called **overfitting**.\n",
    "\n",
    "### The Solution: Validation Split\n",
    "\n",
    "We set aside 10% of our data that the model *never* sees during training. After each epoch, we evaluate on this \"validation\" data. If the model is truly learning patterns, it should perform well on both training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Interpret the Curves\n",
    "\n",
    "### Good Training\n",
    "```\n",
    "Train: 5.0 → 4.0 → 3.0\n",
    "Val:   5.2 → 4.2 → 3.2\n",
    "```\n",
    "Both losses decreasing together. Model is learning general patterns!\n",
    "\n",
    "### Underfitting\n",
    "```\n",
    "Train: 5.0 → 4.8 → 4.7\n",
    "Val:   5.2 → 5.0 → 4.9\n",
    "```\n",
    "Both losses barely improving. Model is too simple or needs more training.\n",
    "\n",
    "### Overfitting\n",
    "```\n",
    "Train: 5.0 → 3.0 → 1.5\n",
    "Val:   5.2 → 3.5 → 4.0  ← Going up!\n",
    "```\n",
    "Training loss decreasing but validation increasing. Model is memorizing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulate training curves\n",
    "epochs = np.arange(1, 21)\n",
    "\n",
    "# Good training\n",
    "train_good = 5.0 * np.exp(-0.15 * epochs) + 1.5\n",
    "val_good = 5.2 * np.exp(-0.14 * epochs) + 1.7\n",
    "\n",
    "# Overfitting\n",
    "train_overfit = 5.0 * np.exp(-0.2 * epochs) + 0.5\n",
    "val_overfit = np.concatenate([5.2 * np.exp(-0.15 * epochs[:8]) + 2.0, \n",
    "                              np.linspace(2.8, 4.5, 12)])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Good training\n",
    "axes[0].plot(epochs, train_good, 'b-', label='Training', linewidth=2)\n",
    "axes[0].plot(epochs, val_good, 'r--', label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Good Training: Both Decrease Together')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting\n",
    "axes[1].plot(epochs, train_overfit, 'b-', label='Training', linewidth=2)\n",
    "axes[1].plot(epochs, val_overfit, 'r--', label='Validation', linewidth=2)\n",
    "axes[1].axvline(x=8, color='orange', linestyle=':', label='Overfitting starts')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Overfitting: Validation Goes Up!')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with validation\n",
    "def training_loop_with_validation(model, train_data, val_data, epochs):\n",
    "    \"\"\"\n",
    "    Pseudocode for training with validation monitoring.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_data:\n",
    "            loss = compute_loss(model, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase (no gradient updates!)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_data:\n",
    "                loss = compute_loss(model, batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train={train_loss:.2f}, Val={val_loss:.2f}\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        if val_loss > train_loss * 1.3:\n",
    "            print(\"Warning: Possible overfitting!\")\n",
    "\n",
    "print(\"Training loop structure (pseudocode above)\")\n",
    "print(\"\\nKey points:\")\n",
    "print(\"  - model.train() enables dropout, batch norm training mode\")\n",
    "print(\"  - model.eval() disables them for evaluation\")\n",
    "print(\"  - torch.no_grad() prevents gradient computation (faster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Improvements\n",
    "\n",
    "With gradient accumulation and validation:\n",
    "\n",
    "- **20-30% lower final loss** due to stable training\n",
    "- **Smoother training curves** that are easier to debug\n",
    "- **Confidence in generalization** by monitoring validation\n",
    "- **Early stopping** when validation stops improving\n",
    "- **Works on hobby hardware** without expensive GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: KV-Cache\n",
    "\n",
    "Now we'll look at how to speed up text generation by caching key-value pairs during inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
