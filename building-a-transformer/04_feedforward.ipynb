{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Position-Wise Feed-Forward Networks\n",
    "\n",
    "**Processing attended information through MLPs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After attention gathers information from across the sequence, we need to actually *process* that information. The feed-forward network (FFN) is a simple two-layer neural network—also called a Multi-Layer Perceptron (MLP)—that transforms each token's representation independently.\n",
    "\n",
    "Think of attention as \"communication\" between tokens—gathering relevant context. The FFN is the \"computation\" step—processing that gathered information to extract useful features. Without the FFN, the model would only shuffle information around without transforming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture\n",
    "\n",
    "1. **Expand:** Project from `d_model` (e.g., 512) to `d_ff` (typically 4× larger, e.g., 2048). This expansion gives the model more \"capacity\" to learn complex patterns.\n",
    "\n",
    "2. **Activate:** Apply GELU activation—a smooth nonlinear function that allows the model to learn non-linear relationships. Without this nonlinearity, stacking layers would be pointless (multiple linear transformations collapse to one).\n",
    "\n",
    "3. **Project back:** Compress back down from `d_ff` to `d_model` so the output shape matches the input, allowing us to stack more layers.\n",
    "\n",
    "**Position-wise:** The *same* FFN (same weights) is applied to every position independently. This is efficient and helps the model learn general transformations that work regardless of position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Expand dimension\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        # GELU activation (used in GPT-2, GPT-3)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Project back to d_model\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = self.linear1(x)        # → (batch, seq_len, d_ff)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)        # → (batch, seq_len, d_model)\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "d_model = 64\n",
    "d_ff = 256  # 4× expansion\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "\n",
    "ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "# Random input (imagine this came from attention)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nInternal expansion: {d_model} → {d_ff} → {d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division of Labor\n",
    "\n",
    "Attention answers \"What should I pay attention to?\" while the FFN answers \"Now that I have this information, what should I do with it?\"\n",
    "\n",
    "```\n",
    "Token representations\n",
    "        ↓\n",
    "   [Attention]  ← \"Gather relevant context\"\n",
    "        ↓\n",
    "      [FFN]     ← \"Process and transform\"\n",
    "        ↓\n",
    "Richer representations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GELU vs ReLU activation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_vals = np.linspace(-4, 4, 100)\n",
    "\n",
    "# GELU: x * Φ(x) where Φ is standard normal CDF\n",
    "from scipy.stats import norm\n",
    "gelu = x_vals * norm.cdf(x_vals)\n",
    "\n",
    "# ReLU: max(0, x)\n",
    "relu = np.maximum(0, x_vals)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x_vals, gelu, label='GELU (used in GPT)', linewidth=2)\n",
    "plt.plot(x_vals, relu, '--', label='ReLU (older models)', linewidth=2)\n",
    "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('GELU vs ReLU Activation Functions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"GELU is smooth near 0, allowing small negative values through.\")\n",
    "print(\"This helps with gradient flow during training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"Feed-Forward Network Parameters:\")\n",
    "print(f\"  linear1: {d_model} × {d_ff} + {d_ff} (bias) = {d_model * d_ff + d_ff:,}\")\n",
    "print(f\"  linear2: {d_ff} × {d_model} + {d_model} (bias) = {d_ff * d_model + d_model:,}\")\n",
    "print(f\"  Total: {total_params:,} parameters\")\n",
    "print(f\"\\nNote: FFN has ~2× more parameters than multi-head attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Transformer Block\n",
    "\n",
    "Now we'll combine attention and FFN with layer normalization and residual connections to create the complete transformer block."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
