{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transformer Block\n",
    "\n",
    "**Combining all components into one repeatable unit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformer block combines all our components into one repeatable unit. The full transformer model is just many of these blocks stacked on top of each other (GPT-3 has 96 blocks!).\n",
    "\n",
    "## What's in a Block?\n",
    "\n",
    "Each block contains four key components:\n",
    "\n",
    "- **Multi-head attention:** Communication layer—tokens gather information from other tokens\n",
    "- **Feed-forward network:** Computation layer—each token processes its gathered information\n",
    "- **Layer normalization:** Stabilizes training by normalizing activations (prevents them from growing too large or small)\n",
    "- **Residual connections:** \"Skip connections\" that create gradient highways for training deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-LN Architecture\n",
    "\n",
    "We use the Pre-LN (Pre-Layer Normalization) approach used in modern models like GPT-2 and GPT-3. This means we apply layer normalization *before* each sub-layer (attention or FFN) rather than after. This makes training more stable, especially for very deep networks.\n",
    "\n",
    "```\n",
    "Input x\n",
    "   │\n",
    "   ├──────────────────────┐\n",
    "   │                      │ (residual)\n",
    "   ↓                      │\n",
    "[LayerNorm]               │\n",
    "   ↓                      │\n",
    "[Multi-Head Attention]    │\n",
    "   ↓                      │\n",
    "[Dropout]                 │\n",
    "   ↓                      │\n",
    "   + ←────────────────────┘\n",
    "   │\n",
    "   ├──────────────────────┐\n",
    "   │                      │ (residual)\n",
    "   ↓                      │\n",
    "[LayerNorm]               │\n",
    "   ↓                      │\n",
    "[Feed-Forward]            │\n",
    "   ↓                      │\n",
    "[Dropout]                 │\n",
    "   ↓                      │\n",
    "   + ←────────────────────┘\n",
    "   │\n",
    "   ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention (from previous notebook).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 1, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.W_o(context)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network (from previous notebook).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer block with Pre-LN architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer norms (applied BEFORE attention and FFN)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Attention and FFN\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Dropout for residual connections\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # First sub-layer: Multi-head attention with residual\n",
    "        residual = x\n",
    "        x = self.norm1(x)                    # Pre-LN\n",
    "        x = self.attention(x, mask=mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = x + residual                     # Residual connection\n",
    "        \n",
    "        # Second sub-layer: Feed-forward with residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)                    # Pre-LN\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x + residual                     # Residual connection\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "d_ff = 256\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "\n",
    "block = TransformerBlock(d_model, num_heads, d_ff)\n",
    "\n",
    "# Input (imagine this came from embeddings)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "\n",
    "output = block(x, mask=mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nShape is preserved: {x.shape == output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Residual Connections?\n",
    "\n",
    "Residual connections create gradient \"highways\" that allow gradients to flow directly from the output back to early layers. Without them, deep networks struggle to learn.\n",
    "\n",
    "The math is beautiful:\n",
    "\n",
    "$$\\frac{\\partial(x + f(x))}{\\partial x} = 1 + \\frac{\\partial f(x)}{\\partial x}$$\n",
    "\n",
    "The \"1\" ensures gradients *always* flow, even if $\\frac{\\partial f(x)}{\\partial x}$ is tiny!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate residual gradient flow\n",
    "x = torch.randn(1, 4, 64, requires_grad=True)\n",
    "\n",
    "# Without residual: gradients can vanish\n",
    "linear = nn.Linear(64, 64)\n",
    "y_no_res = linear(x)\n",
    "y_no_res.sum().backward()\n",
    "grad_no_res = x.grad.abs().mean().item()\n",
    "\n",
    "# Reset\n",
    "x.grad = None\n",
    "\n",
    "# With residual: gradient is guaranteed to flow\n",
    "y_with_res = x + linear(x)\n",
    "y_with_res.sum().backward()\n",
    "grad_with_res = x.grad.abs().mean().item()\n",
    "\n",
    "print(f\"Average gradient magnitude without residual: {grad_no_res:.4f}\")\n",
    "print(f\"Average gradient magnitude with residual: {grad_with_res:.4f}\")\n",
    "print(f\"\\nResiduals ensure gradients always flow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters in a transformer block\n",
    "total_params = sum(p.numel() for p in block.parameters())\n",
    "\n",
    "print(f\"Transformer Block Parameters:\")\n",
    "print(f\"  Attention (Q, K, V, O): {4 * d_model * d_model + 4 * d_model:,}\")\n",
    "print(f\"  FFN (up, down): {2 * d_model * d_ff + d_ff + d_model:,}\")\n",
    "print(f\"  LayerNorm (2×): {4 * d_model:,}\")\n",
    "print(f\"  Total: {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Blocks\n",
    "\n",
    "The transformer is just N of these blocks stacked together:\n",
    "\n",
    "| Model | Blocks | d_model | Heads |\n",
    "|-------|--------|---------|-------|\n",
    "| GPT-2 Small | 12 | 768 | 12 |\n",
    "| GPT-2 Large | 36 | 1280 | 20 |\n",
    "| GPT-3 | 96 | 12288 | 96 |\n",
    "\n",
    "Each block refines the representations, building increasingly abstract understanding of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack multiple blocks\n",
    "num_layers = 4\n",
    "blocks = nn.ModuleList([\n",
    "    TransformerBlock(d_model, num_heads, d_ff)\n",
    "    for _ in range(num_layers)\n",
    "])\n",
    "\n",
    "# Forward through all blocks\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "for i, block in enumerate(blocks):\n",
    "    x = block(x, mask=mask)\n",
    "    print(f\"After block {i}: shape = {x.shape}, mean = {x.mean():.4f}\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in blocks.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Complete Model\n",
    "\n",
    "Now we'll assemble everything—embeddings, transformer blocks, and output projection—into a complete language model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
