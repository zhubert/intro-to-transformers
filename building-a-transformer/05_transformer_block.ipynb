{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# The Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The Building Block of Transformers\n\nWe now have the two core components:\n- **Multi-Head Attention**: Communication between positions\n- **Feed-Forward Network**: Computation at each position\n\nBut if we just stack these directly, training fails. Deep networks suffer from two problems:\n\n1. **Vanishing/exploding gradients**: As we add layers, gradients get multiplied at each layer. They can shrink to zero or grow to infinity.\n\n2. **Internal covariate shift**: The distribution of layer inputs keeps changing during training, making it hard to learn. When you update weights in layer 1, it changes the distribution of inputs to layer 2. Layer 2 has to constantly adapt to these shifting distributions instead of making stable progress. This is like trying to learn a new skill while the rules keep changing. You waste effort readjusting instead of improving. Layer normalization addresses this by standardizing each layer's inputs to have consistent statistics (mean 0, variance 1), giving each layer a stable foundation to build on.\n\nThe solution: **residual connections** and **layer normalization**. These aren't optional additions. They're essential for training deep transformers."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:37.702832Z",
     "iopub.status.busy": "2025-12-10T21:17:37.702749Z",
     "iopub.status.idle": "2025-12-10T21:17:38.538691Z",
     "shell.execute_reply": "2025-12-10T21:17:38.538313Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## Residual Connections: The Skip Path\n\nA residual connection adds the input directly to the output:\n\n$$\\text{output} = \\text{sublayer}(x) + x$$\n\nInstead of learning the transformation directly, the sublayer learns the **residual**: what to add to the input.\n\n**Why does this help?**\n\nConsider the gradient flow. With a residual:\n$$\\frac{\\partial \\text{output}}{\\partial x} = \\frac{\\partial \\text{sublayer}(x)}{\\partial x} + 1$$\n\nThat \"+1\" is crucial. Even if the sublayer's gradient vanishes, there's still a direct path for gradients to flow backward. The network can always fall back to the identity function.\n\nVisually:\n```\n    x \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                          \u2502 (skip connection)\n    \u2193                          \u2502\n[sublayer]                     \u2502\n    \u2502                          \u2502\n    \u2193                          \u2193\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500[ + ]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 output\n```"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:38.540169Z",
     "iopub.status.busy": "2025-12-10T21:17:38.540060Z",
     "iopub.status.idle": "2025-12-10T21:17:38.571984Z",
     "shell.execute_reply": "2025-12-10T21:17:38.571633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient magnitude comparison:\n",
      "  Without residual: 0.067134\n",
      "  With residual:    8.002588\n",
      "\n",
      "Residual connection gives 119\u00d7 stronger gradient!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating gradient flow with residuals\n",
    "class WithResidual(nn.Module):\n",
    "    def __init__(self, sublayer):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sublayer(x) + x  # The key: add input to output\n",
    "\n",
    "# Create a layer that nearly kills gradients\n",
    "layer = nn.Linear(64, 64)\n",
    "nn.init.normal_(layer.weight, std=0.001)  # Very small weights = near-zero gradients\n",
    "\n",
    "# Without residual\n",
    "x = torch.randn(1, 64, requires_grad=True)\n",
    "out_no_res = layer(x)\n",
    "loss = out_no_res.sum()\n",
    "loss.backward()\n",
    "grad_norm_no_res = x.grad.norm().item()\n",
    "\n",
    "# With residual\n",
    "x2 = torch.randn(1, 64, requires_grad=True)\n",
    "wrapped = WithResidual(layer)\n",
    "out_with_res = wrapped(x2)\n",
    "loss2 = out_with_res.sum()\n",
    "loss2.backward()\n",
    "grad_norm_with_res = x2.grad.norm().item()\n",
    "\n",
    "print(\"Gradient magnitude comparison:\")\n",
    "print(f\"  Without residual: {grad_norm_no_res:.6f}\")\n",
    "print(f\"  With residual:    {grad_norm_with_res:.6f}\")\n",
    "print(f\"\\nResidual connection gives {grad_norm_with_res/grad_norm_no_res:.0f}\u00d7 stronger gradient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Layer Normalization: Stabilizing Activations\n",
    "\n",
    "**Layer normalization** normalizes activations across the feature dimension (not the batch dimension like batch norm).\n",
    "\n",
    "For each position's vector $x \\in \\mathbb{R}^{d_{model}}$:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "where:\n",
    "- $\\mu = \\frac{1}{d} \\sum_i x_i$ (mean across features)\n",
    "- $\\sigma^2 = \\frac{1}{d} \\sum_i (x_i - \\mu)^2$ (variance across features)\n",
    "- $\\gamma, \\beta$ are learned scale and shift parameters\n",
    "- $\\epsilon$ is a small constant for numerical stability\n",
    "\n",
    "**What does it do?**\n",
    "\n",
    "It keeps activations in a consistent range:\n",
    "- Center around 0 (by subtracting mean)\n",
    "- Scale to unit variance (by dividing by std)\n",
    "- Then allow learned rescaling ($\\gamma$, $\\beta$)\n",
    "\n",
    "This prevents activations from drifting to extreme values during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:38.572834Z",
     "iopub.status.busy": "2025-12-10T21:17:38.572738Z",
     "iopub.status.idle": "2025-12-10T21:17:38.575005Z",
     "shell.execute_reply": "2025-12-10T21:17:38.574707Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization.\n",
    "    \n",
    "    Normalizes across the feature dimension (last dimension).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Learned parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))   # Scale\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))   # Shift\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute mean and variance across last dimension\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.gamma * x_norm + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:38.575819Z",
     "iopub.status.busy": "2025-12-10T21:17:38.575737Z",
     "iopub.status.idle": "2025-12-10T21:17:38.578885Z",
     "shell.execute_reply": "2025-12-10T21:17:38.578635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LayerNorm:\n",
      "  Mean: [5.11 5.48 4.57 4.77 4.15]\n",
      "  Std:  [10.75  9.68  9.43 10.18  9.9 ]\n",
      "\n",
      "After LayerNorm:\n",
      "  Mean: [ 0. -0. -0.  0.  0.]\n",
      "  Std:  [1. 1. 1. 1. 1.]\n",
      "\n",
      "LayerNorm centers each position to mean\u22480, std\u22481\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate layer norm\n",
    "d_model = 256\n",
    "ln = LayerNorm(d_model)\n",
    "\n",
    "# Create input with varying scales\n",
    "x = torch.randn(2, 5, d_model) * 10 + 5  # Mean ~5, std ~10\n",
    "\n",
    "x_norm = ln(x)\n",
    "\n",
    "print(\"Before LayerNorm:\")\n",
    "print(f\"  Mean: {x.mean(dim=-1)[0].detach().numpy().round(2)}\")\n",
    "print(f\"  Std:  {x.std(dim=-1)[0].detach().numpy().round(2)}\")\n",
    "print()\n",
    "print(\"After LayerNorm:\")\n",
    "print(f\"  Mean: {x_norm.mean(dim=-1)[0].detach().numpy().round(4)}\")\n",
    "print(f\"  Std:  {x_norm.std(dim=-1)[0].detach().numpy().round(2)}\")\n",
    "print()\n",
    "print(\"LayerNorm centers each position to mean\u22480, std\u22481\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Pre-Norm vs Post-Norm\n\nThere are two ways to combine layer norm with residuals:\n\n**Post-Norm** (original transformer):\n$$x = \\text{LayerNorm}(x + \\text{sublayer}(x))$$\n\n**Pre-Norm** (GPT-2 and later):\n$$x = x + \\text{sublayer}(\\text{LayerNorm}(x))$$\n\n```\nPost-Norm:                    Pre-Norm:\nx \u2500\u2500\u2500\u2500\u2510                       x \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     \u2502                       \u2502                \u2502\n\u2193     \u2502                       \u2193                \u2502\n[sublayer]                [LayerNorm]          \u2502\n\u2502     \u2502                       \u2502                \u2502\n\u2193     \u2193                       \u2193                \u2502\n[ + ]\u2190\u2500                   [sublayer]           \u2502\n\u2502                             \u2502                \u2502\n\u2193                             \u2193                \u2193\n[LayerNorm]                   \u2514\u2500\u2500\u2500\u2500[ + ]\u2190\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502                                  \u2502\n\u2193                                  \u2193\noutput                          output\n```\n\n**Pre-norm is more stable** because the residual path is completely clean. No normalization interfering with gradient flow. This matters especially for very deep models."
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## The Complete Transformer Block\n",
    "\n",
    "Now we can assemble the full block:\n",
    "\n",
    "```\n",
    "Input x\n",
    "    \u2502\n",
    "    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2193                               \u2502 (residual 1)\n",
    "[LayerNorm]                         \u2502\n",
    "    \u2193                               \u2502\n",
    "[Multi-Head Attention]              \u2502\n",
    "    \u2193                               \u2502\n",
    "[Dropout]                           \u2502\n",
    "    \u2193                               \u2193\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500[ + ]\u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                     \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502                               \u2502 (residual 2)\n",
    "    \u2193                               \u2502\n",
    "[LayerNorm]                         \u2502\n",
    "    \u2193                               \u2502\n",
    "[Feed-Forward Network]              \u2502\n",
    "    \u2193                               \u2502\n",
    "[Dropout]                           \u2502\n",
    "    \u2193                               \u2193\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500[ + ]\u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                     \u2502\n",
    "                     \u2193\n",
    "                  Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:38.579648Z",
     "iopub.status.busy": "2025-12-10T21:17:38.579563Z",
     "iopub.status.idle": "2025-12-10T21:17:38.583178Z",
     "shell.execute_reply": "2025-12-10T21:17:38.582852Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention (from previous notebook)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.W_o(context)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network (from previous notebook)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.dropout(self.linear2(F.gelu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:38.583893Z",
     "iopub.status.busy": "2025-12-10T21:17:38.583808Z",
     "iopub.status.idle": "2025-12-10T21:17:38.586212Z",
     "shell.execute_reply": "2025-12-10T21:17:38.585940Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \n",
    "    Architecture (Pre-Norm):\n",
    "        x \u2192 LayerNorm \u2192 Attention \u2192 Dropout \u2192 (+x) \u2192 \n",
    "          \u2192 LayerNorm \u2192 FFN \u2192 Dropout \u2192 (+) \u2192 output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        num_heads: int, \n",
    "        d_ff: int, \n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention sublayer\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # FFN sublayer\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.ffn_dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            mask: causal mask\n",
    "        \n",
    "        Returns:\n",
    "            (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Attention block with residual\n",
    "        attn_out = self.attention(self.attn_norm(x), mask)\n",
    "        x = x + self.attn_dropout(attn_out)\n",
    "        \n",
    "        # FFN block with residual\n",
    "        ffn_out = self.ffn(self.ffn_norm(x))\n",
    "        x = x + self.ffn_dropout(ffn_out)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:38.586874Z",
     "iopub.status.busy": "2025-12-10T21:17:38.586805Z",
     "iopub.status.idle": "2025-12-10T21:17:38.596198Z",
     "shell.execute_reply": "2025-12-10T21:17:38.595899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 8, 256])\n",
      "Output shape: torch.Size([2, 8, 256])\n",
      "\n",
      "Output has same shape as input \u2713\n",
      "This allows us to stack blocks!\n"
     ]
    }
   ],
   "source": [
    "# Test the transformer block\n",
    "d_model = 256\n",
    "num_heads = 4\n",
    "d_ff = 1024\n",
    "\n",
    "block = TransformerBlock(d_model, num_heads, d_ff, dropout=0.0)\n",
    "\n",
    "# Input\n",
    "x = torch.randn(2, 8, d_model)\n",
    "\n",
    "# Causal mask\n",
    "seq_len = 8\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "\n",
    "# Forward\n",
    "output = block(x, mask)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutput has same shape as input \u2713\")\n",
    "print(f\"This allows us to stack blocks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Parameter Count\n",
    "\n",
    "Let's break down the parameters in one block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:38.597120Z",
     "iopub.status.busy": "2025-12-10T21:17:38.597046Z",
     "iopub.status.idle": "2025-12-10T21:17:38.599531Z",
     "shell.execute_reply": "2025-12-10T21:17:38.599268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Block Parameters\n",
      "==================================================\n",
      "Attention sublayer:\n",
      "  LayerNorm:            512\n",
      "  Attention:        262,144\n",
      "\n",
      "FFN sublayer:\n",
      "  LayerNorm:            512\n",
      "  FFN:              525,568\n",
      "==================================================\n",
      "Total:              788,736\n",
      "\n",
      "Proportion:\n",
      "  Attention: 33.2%\n",
      "  FFN:       66.6%\n"
     ]
    }
   ],
   "source": [
    "# Count parameters by component\n",
    "def count_params(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "print(\"Transformer Block Parameters\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Attention sublayer:\")\n",
    "print(f\"  LayerNorm:   {count_params(block.attn_norm):>12,}\")\n",
    "print(f\"  Attention:   {count_params(block.attention):>12,}\")\n",
    "print()\n",
    "print(f\"FFN sublayer:\")\n",
    "print(f\"  LayerNorm:   {count_params(block.ffn_norm):>12,}\")\n",
    "print(f\"  FFN:         {count_params(block.ffn):>12,}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total:         {count_params(block):>12,}\")\n",
    "print()\n",
    "\n",
    "# Show breakdown\n",
    "attn_params = count_params(block.attention)\n",
    "ffn_params = count_params(block.ffn)\n",
    "total_params = count_params(block)\n",
    "\n",
    "print(f\"Proportion:\")\n",
    "print(f\"  Attention: {attn_params/total_params*100:.1f}%\")\n",
    "print(f\"  FFN:       {ffn_params/total_params*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Stacking Blocks\n",
    "\n",
    "The power of transformers comes from stacking multiple blocks. Each block refines the representations:\n",
    "\n",
    "- **Layer 1**: Basic patterns (adjacent words, simple grammar)\n",
    "- **Layer 2-3**: Higher-level patterns (phrases, basic semantics)\n",
    "- **Layer 4+**: Abstract reasoning (long-range dependencies, complex relations)\n",
    "\n",
    "More layers = more refinement = better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:38.600327Z",
     "iopub.status.busy": "2025-12-10T21:17:38.600255Z",
     "iopub.status.idle": "2025-12-10T21:17:38.615968Z",
     "shell.execute_reply": "2025-12-10T21:17:38.615639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 8, 256])\n",
      "Output shape: torch.Size([2, 8, 256])\n",
      "\n",
      "Total parameters in 4-layer stack: 3,154,944\n"
     ]
    }
   ],
   "source": [
    "class TransformerStack(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of transformer blocks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        num_layers: int,\n",
    "        d_model: int, \n",
    "        num_heads: int, \n",
    "        d_ff: int, \n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "# Create a stack\n",
    "stack = TransformerStack(\n",
    "    num_layers=4,\n",
    "    d_model=256,\n",
    "    num_heads=4,\n",
    "    d_ff=1024,\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 8, 256)\n",
    "output = stack(x, mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nTotal parameters in 4-layer stack: {count_params(stack):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Why Residuals and LayerNorm Matter\n",
    "\n",
    "Let's see what happens without them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:38.616781Z",
     "iopub.status.busy": "2025-12-10T21:17:38.616693Z",
     "iopub.status.idle": "2025-12-10T21:17:38.685266Z",
     "shell.execute_reply": "2025-12-10T21:17:38.684893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient magnitude after 8 layers:\n",
      "  Without residuals: 0.000000\n",
      "  With residuals:    73.678993\n",
      "\n",
      "Residuals preserve gradient signal ~11590260933\u00d7 better!\n"
     ]
    }
   ],
   "source": [
    "# Compare gradient flow with and without residuals\n",
    "class BlockWithoutResidual(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.attention(x, mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "# Stack 8 blocks without residuals\n",
    "blocks_no_res = nn.Sequential(*[\n",
    "    BlockWithoutResidual(256, 4, 1024) for _ in range(8)\n",
    "])\n",
    "\n",
    "# Stack 8 blocks with residuals (our proper implementation)\n",
    "blocks_with_res = TransformerStack(8, 256, 4, 1024, dropout=0.0)\n",
    "\n",
    "# Test gradient magnitude through the network\n",
    "x = torch.randn(1, 8, 256, requires_grad=True)\n",
    "\n",
    "# Without residuals\n",
    "out = blocks_no_res(x)\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "grad_no_res = x.grad.norm().item()\n",
    "\n",
    "# With residuals\n",
    "x2 = torch.randn(1, 8, 256, requires_grad=True)\n",
    "out2 = blocks_with_res(x2, None)\n",
    "loss2 = out2.sum()\n",
    "loss2.backward()\n",
    "grad_with_res = x2.grad.norm().item()\n",
    "\n",
    "print(\"Gradient magnitude after 8 layers:\")\n",
    "print(f\"  Without residuals: {grad_no_res:.6f}\")\n",
    "print(f\"  With residuals:    {grad_with_res:.6f}\")\n",
    "print(f\"\\nResiduals preserve gradient signal ~{grad_with_res/grad_no_res:.0f}\u00d7 better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Residual connections** add input directly to output, preserving gradient flow\n",
    "\n",
    "2. **Layer normalization** keeps activations in a stable range\n",
    "\n",
    "3. **Pre-norm** (normalize before sublayer) is more stable than post-norm\n",
    "\n",
    "4. **Each block has the same structure**: Attention + FFN, each with residual and norm\n",
    "\n",
    "5. **Stacking blocks** builds increasingly abstract representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "## Next: Complete Model\n\nWe have the transformer block, the fundamental building unit. Now we need to wrap it with:\n- Input embeddings (token + position)\n- Output projection (to vocabulary logits)\n- Final layer norm\n\nIn the next notebook, we'll assemble the complete model and count every parameter."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Combines multi-head attention and feedforward layers with residual connections and layer normalization."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}