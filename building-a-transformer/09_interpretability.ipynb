{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Model Interpretability\n",
    "\n",
    "**Understanding what transformers learn through mechanistic analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've built and trained a transformer, how do we understand **what it has learned**? Mechanistic interpretability provides tools to peek inside the \"black box\" and discover the circuits and patterns the model uses.\n",
    "\n",
    "This section covers four powerful techniques: Logit Lens, Attention Analysis, Induction Heads, and Activation Patching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Mechanistic Interpretability?\n",
    "\n",
    "Instead of just asking \"does the model work?\", we ask:\n",
    "\n",
    "- **When** does the model \"know\" the answer? (which layer?)\n",
    "- **How** does information flow through the network?\n",
    "- **Which** components are responsible for specific behaviors?\n",
    "- **What** patterns or circuits has the model learned?\n",
    "\n",
    "This connects to cutting-edge research from Anthropic, OpenAI, and academic labs exploring how LLMs actually work under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logit Lens: Seeing Predictions Evolve\n",
    "\n",
    "The **logit lens** technique lets us visualize what the model would predict if we stopped at each layer.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Normally, we only see the final output:\n",
    "\n",
    "```\n",
    "Input → Layer 1 → Layer 2 → ... → Layer N → Unembed → Logits\n",
    "```\n",
    "\n",
    "With logit lens, we apply unembedding *at each layer*:\n",
    "\n",
    "```\n",
    "Input → Layer 1 → [Unembed] → \"What now?\"\n",
    "      → Layer 2 → [Unembed] → \"What now?\"\n",
    "      → Layer 3 → [Unembed] → \"What now?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def logit_lens(model, input_ids):\n",
    "    \"\"\"\n",
    "    Apply unembedding at each layer to see how predictions evolve.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model with .blocks, .token_embedding, .output_proj\n",
    "        input_ids: (batch, seq_len) token indices\n",
    "    \n",
    "    Returns:\n",
    "        layer_predictions: list of (batch, seq_len, vocab_size) logits per layer\n",
    "    \"\"\"\n",
    "    layer_predictions = []\n",
    "    \n",
    "    # Get embeddings\n",
    "    x = model.token_embedding(input_ids)\n",
    "    x = model.pos_encoding(x)\n",
    "    \n",
    "    # After embeddings (layer 0)\n",
    "    layer_predictions.append(model.output_proj(model.ln_f(x)))\n",
    "    \n",
    "    # After each transformer block\n",
    "    for block in model.blocks:\n",
    "        x = block(x)\n",
    "        # Apply final layer norm and project to vocabulary\n",
    "        layer_predictions.append(model.output_proj(model.ln_f(x)))\n",
    "    \n",
    "    return layer_predictions\n",
    "\n",
    "print(\"Logit lens shows predictions evolving through layers.\")\n",
    "print(\"\\nExample insight:\")\n",
    "print('  Input: \"The capital of France is\"')\n",
    "print('  Layer 0: \"the\" (15%), \"a\" (12%) → Generic, common words')\n",
    "print('  Layer 2: \"located\" (18%), \"Paris\" (15%) → Starting to understand')\n",
    "print('  Layer 4: \"Paris\" (65%), \"French\" (10%) → Confident, correct!')\n",
    "print('  Layer 6: \"Paris\" (72%), \"France\" (8%) → Final refinement')\n",
    "print('\\nKey insight: The model \"knows\" Paris by Layer 4!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention Analysis: What Do Heads Focus On?\n",
    "\n",
    "Attention weights show which tokens each position \"attends to\". By analyzing these patterns, we discover specialized behaviors:\n",
    "\n",
    "- **Previous token heads:** Always look at position i-1\n",
    "- **Uniform heads:** Spread attention evenly (averaging information)\n",
    "- **Start token heads:** Focus on the beginning of the sequence\n",
    "- **Sparse heads:** Concentrate on very few key tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_attention_patterns():\n",
    "    \"\"\"Visualize different types of attention patterns.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    seq_len = 6\n",
    "    \n",
    "    # Previous token pattern\n",
    "    prev_token = np.zeros((seq_len, seq_len))\n",
    "    for i in range(1, seq_len):\n",
    "        prev_token[i, i-1] = 1.0\n",
    "    prev_token[0, 0] = 1.0\n",
    "    \n",
    "    # Uniform pattern\n",
    "    uniform = np.ones((seq_len, seq_len)) / seq_len\n",
    "    # Apply causal mask\n",
    "    for i in range(seq_len):\n",
    "        uniform[i, :i+1] = 1.0 / (i + 1)\n",
    "        uniform[i, i+1:] = 0\n",
    "    \n",
    "    # Start token pattern\n",
    "    start_token = np.zeros((seq_len, seq_len))\n",
    "    start_token[:, 0] = 1.0\n",
    "    \n",
    "    # Sparse pattern\n",
    "    sparse = np.zeros((seq_len, seq_len))\n",
    "    np.fill_diagonal(sparse, 0.7)\n",
    "    sparse[:, 0] = 0.3\n",
    "    \n",
    "    patterns = [\n",
    "        (prev_token, 'Previous Token Head'),\n",
    "        (uniform, 'Uniform Head'),\n",
    "        (start_token, 'Start Token Head'),\n",
    "        (sparse, 'Sparse Head')\n",
    "    ]\n",
    "    \n",
    "    for ax, (pattern, title) in zip(axes, patterns):\n",
    "        im = ax.imshow(pattern, cmap='Blues', vmin=0, vmax=1)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Key position')\n",
    "        if ax == axes[0]:\n",
    "            ax.set_ylabel('Query position')\n",
    "        ax.set_xticks(range(seq_len))\n",
    "        ax.set_yticks(range(seq_len))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Induction Heads: Pattern Matching Circuits\n",
    "\n",
    "Induction heads implement **in-context learning**—the ability to copy from earlier patterns.\n",
    "\n",
    "### What Are Induction Heads?\n",
    "\n",
    "Given a repeated pattern like:\n",
    "\n",
    "```\n",
    "Input: \"A B C ... A B [?]\"\n",
    "Prediction: \"C\"\n",
    "```\n",
    "\n",
    "Induction heads learn to predict C by recognizing the repeated \"A B\" pattern and copying what came after the first occurrence.\n",
    "\n",
    "### The Circuit\n",
    "\n",
    "Induction typically involves **two heads working together**:\n",
    "\n",
    "1. **Previous Token Head (Layer L):** At position i, attends to i-1. Creates representation of \"what came before\"\n",
    "\n",
    "2. **Induction Head (Layer L+1):** Queries for matches to previous token. Attends to what came AFTER those matches. Predicts the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_induction_pattern(attention_weights, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Detect if attention weights show induction pattern.\n",
    "    \n",
    "    Induction pattern: when processing position i, attend strongly\n",
    "    to position j where token[j-1] == token[i-1] (previous token matches).\n",
    "    \"\"\"\n",
    "    # This is a simplified detector\n",
    "    # Real implementations check across many sequences\n",
    "    \n",
    "    seq_len = attention_weights.shape[-1]\n",
    "    \n",
    "    # Check if attention is concentrated (not uniform)\n",
    "    entropy = -(attention_weights * torch.log(attention_weights + 1e-10)).sum(dim=-1)\n",
    "    max_entropy = np.log(seq_len)\n",
    "    \n",
    "    is_sparse = entropy.mean() < max_entropy * 0.5\n",
    "    \n",
    "    return is_sparse\n",
    "\n",
    "print(\"Induction heads are crucial for few-shot learning.\")\n",
    "print(\"They emerge suddenly during training ('grokking').\")\n",
    "print(\"Almost all transformer language models develop them!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Patching: Causal Interventions\n",
    "\n",
    "We can *observe* what the model does, but which parts are actually **causing** the behavior?\n",
    "\n",
    "**Activation patching answers this through intervention experiments:**\n",
    "\n",
    "1. Run model on \"clean\" input (correct behavior)\n",
    "2. Run model on \"corrupted\" input (incorrect behavior)\n",
    "3. For each component, swap clean activations into corrupted run\n",
    "4. Measure how much this restores correct behavior\n",
    "\n",
    "High recovery = that component is **causally important**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching_example():\n",
    "    \"\"\"\n",
    "    Conceptual example of activation patching.\n",
    "    \"\"\"\n",
    "    print(\"Activation Patching Example:\")\n",
    "    print(\"=\"*50)\n",
    "    print()\n",
    "    print('Clean input:     \"The Eiffel Tower is in\"')\n",
    "    print('                 → Predicts: \"Paris\" (85%)')\n",
    "    print()\n",
    "    print('Corrupted input: \"The Empire State is in\"')\n",
    "    print('                 → Predicts: \"New York\" (78%)')\n",
    "    print()\n",
    "    print(\"Test: Patch Layer 4 activations from clean → corrupted\")\n",
    "    print('      → Predicts: \"Paris\" (82%)')\n",
    "    print()\n",
    "    print(\"Result: Layer 4 recovery = 90%\")\n",
    "    print(\"        Layer 4 is CRITICAL for this task!\")\n",
    "    print()\n",
    "    print(\"This is CAUSAL evidence, not just correlation.\")\n",
    "    print(\"If patching recovers behavior, that layer is necessary.\")\n",
    "\n",
    "activation_patching_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "layers = ['Embed', 'L1', 'L2', 'L3', 'L4', 'L5', 'L6']\n",
    "# Simulated recovery percentages\n",
    "recovery = [5, 12, 25, 48, 90, 82, 75]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(layers, recovery, color='steelblue')\n",
    "bars[4].set_color('darkgreen')  # Highlight most important layer\n",
    "\n",
    "plt.axhline(y=50, color='red', linestyle='--', label='50% threshold')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Recovery %')\n",
    "plt.title('Activation Patching: Which Layers Are Causally Important?')\n",
    "plt.legend()\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "for i, (layer, rec) in enumerate(zip(layers, recovery)):\n",
    "    plt.text(i, rec + 2, f'{rec}%', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Layer 4 shows highest recovery → most causally important!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research References\n",
    "\n",
    "These techniques come from cutting-edge interpretability research:\n",
    "\n",
    "- **Logit Lens:** [nostalgebraist (LessWrong)](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/)\n",
    "- **Transformer Circuits:** [Anthropic](https://transformer-circuits.pub/2021/framework/)\n",
    "- **Induction Heads:** [Anthropic](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/)\n",
    "- **Activation Patching:** [Neel Nanda & Stefan Heimersheim (LessWrong)](https://www.lesswrong.com/posts/FhryNAFknqKAdDcYy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We've Learned\n",
    "\n",
    "We've built a complete transformer from scratch and explored:\n",
    "\n",
    "1. **Embeddings** — Converting tokens to vectors with position information\n",
    "2. **Attention** — The core mechanism for tokens to communicate\n",
    "3. **Multi-Head Attention** — Parallel attention for different relationships\n",
    "4. **Feed-Forward Networks** — Processing gathered information\n",
    "5. **Transformer Blocks** — Combining everything with residuals\n",
    "6. **Complete Model** — Assembling a GPT-style language model\n",
    "7. **Training** — Gradient accumulation and validation\n",
    "8. **KV-Cache** — Fast generation through caching\n",
    "9. **Interpretability** — Understanding what models learn\n",
    "\n",
    "This is the same architecture that powers GPT, Claude, and other frontier models. The components scale, but the fundamentals remain the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
