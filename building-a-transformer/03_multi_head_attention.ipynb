{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-Head Attention\n",
    "\n",
    "**Running parallel attention heads to capture different relationships**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Multiple Heads?\n",
    "\n",
    "A single attention head can only focus on one type of relationship at a time. **Multi-head attention** runs multiple attention operations in parallel, each learning to focus on different aspects:\n",
    "\n",
    "- One head might focus on **syntactic relationships** (subject-verb agreement)\n",
    "- Another on **semantic relationships** (related concepts)\n",
    "- Another on **positional patterns** (nearby words)\n",
    "\n",
    "It's like having multiple experts examining the same data from different perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V, and output\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            mask: optional causal mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            attention_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Reshape to (batch, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 1, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape back: (batch, seq_len, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Random input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "\n",
    "output, attn_weights = mha(x, mask=mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  (batch, num_heads, seq_len, seq_len)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns for different heads\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    weights = attn_weights[0, head_idx].detach().numpy()\n",
    "    im = ax.imshow(weights, cmap='Blues')\n",
    "    ax.set_title(f'Head {head_idx}')\n",
    "    ax.set_xlabel('Key position')\n",
    "    if head_idx == 0:\n",
    "        ax.set_ylabel('Query position')\n",
    "\n",
    "plt.suptitle('Attention Patterns by Head (with causal mask)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Multi-Head Attention Works\n",
    "\n",
    "```\n",
    "Input x: (batch, seq_len, d_model)\n",
    "    │\n",
    "    ├─→ W_q → Q ─┐\n",
    "    ├─→ W_k → K ─┼─→ Split into num_heads\n",
    "    └─→ W_v → V ─┘       │\n",
    "                         ├─→ Head 0: Attention(Q₀, K₀, V₀)\n",
    "                         ├─→ Head 1: Attention(Q₁, K₁, V₁)\n",
    "                         ├─→ ...\n",
    "                         └─→ Head n: Attention(Qₙ, Kₙ, Vₙ)\n",
    "                                     │\n",
    "                         Concatenate all head outputs\n",
    "                                     │\n",
    "                                 W_o → Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"Multi-Head Attention Parameters:\")\n",
    "print(f\"  W_q: {d_model} × {d_model} = {d_model * d_model}\")\n",
    "print(f\"  W_k: {d_model} × {d_model} = {d_model * d_model}\")\n",
    "print(f\"  W_v: {d_model} × {d_model} = {d_model * d_model}\")\n",
    "print(f\"  W_o: {d_model} × {d_model} = {d_model * d_model}\")\n",
    "print(f\"  + biases\")\n",
    "print(f\"  Total: {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Feed-Forward Networks\n",
    "\n",
    "After attention, each position goes through a position-wise feed-forward network that adds non-linearity and increases the model's capacity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
