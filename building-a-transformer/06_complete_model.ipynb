{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# The Complete Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "We've built every piece. Now let's assemble them into a complete, working language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Data Flow Through the Model\n",
    "\n",
    "```\n",
    "Token IDs: [101, 2054, 2003, 2115, 2171]\n",
    "                    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│       Token Embedding               │  Look up vectors\n",
    "│       + Position Embedding          │  Add position info\n",
    "│       + Dropout                     │\n",
    "└─────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│     Transformer Block × N           │  The core processing\n",
    "│                                     │\n",
    "│  (attention + FFN, with residuals)  │\n",
    "└─────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│       Final LayerNorm               │  Stabilize outputs\n",
    "└─────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│       Output Projection             │  d_model → vocab_size\n",
    "└─────────────────────────────────────┘\n",
    "                    ↓\n",
    "Logits: [0.1, 0.3, 8.2, ...]           Scores for each token\n",
    "```\n",
    "\n",
    "Five stages:\n",
    "1. **Embedding**: Convert token IDs to vectors, add position information\n",
    "2. **Transformer Blocks**: N layers of attention + FFN (we use 4; GPT-3 uses 96)\n",
    "3. **Final LayerNorm**: One last normalization for stability\n",
    "4. **Output Projection**: Map from $d_{model}$ to vocabulary size\n",
    "5. **Output Logits**: Raw scores for each possible next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:29.109342Z",
     "iopub.status.busy": "2026-01-22T01:56:29.109342Z",
     "iopub.status.idle": "2026-01-22T01:56:30.313153Z",
     "shell.execute_reply": "2026-01-22T01:56:30.313153Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:30.314187Z",
     "iopub.status.busy": "2026-01-22T01:56:30.314187Z",
     "iopub.status.idle": "2026-01-22T01:56:30.319502Z",
     "shell.execute_reply": "2026-01-22T01:56:30.319502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Components from previous notebooks\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.dropout(self.linear2(F.gelu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = x + self.dropout(self.attention(self.attn_norm(x), mask))\n",
    "        x = x + self.dropout(self.ffn(self.ffn_norm(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## The Complete GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:30.319502Z",
     "iopub.status.busy": "2026-01-22T01:56:30.319502Z",
     "iopub.status.idle": "2026-01-22T01:56:30.325335Z",
     "shell.execute_reply": "2026-01-22T01:56:30.325335Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A complete decoder-only transformer language model.\n",
    "    \n",
    "    This is the same architecture used by GPT-2, GPT-3, and Claude,\n",
    "    just smaller. The fundamentals are identical.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 256,\n",
    "        num_heads: int = 4,\n",
    "        num_layers: int = 4,\n",
    "        d_ff: int = 1024,\n",
    "        max_seq_len: int = 512,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.embed_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output projection: d_model → vocab_size\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: share weights between embedding and output\n",
    "        # This is a common technique that reduces parameters and improves quality\n",
    "        self.output_proj.weight = self.token_embedding.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with small random values.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        token_ids: torch.Tensor,\n",
    "        mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: (batch, seq_len) - integer token IDs\n",
    "            mask: optional causal mask\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, seq_len, vocab_size) - scores for each token\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        device = token_ids.device\n",
    "        \n",
    "        # Create causal mask if not provided\n",
    "        if mask is None:\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
    "        \n",
    "        # 1. Embeddings\n",
    "        positions = torch.arange(seq_len, device=device)\n",
    "        x = self.token_embedding(token_ids) + self.position_embedding(positions)\n",
    "        x = self.embed_dropout(x)\n",
    "        \n",
    "        # 2. Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # 3. Final layer norm\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # 4. Project to vocabulary\n",
    "        logits = self.output_proj(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:30.325335Z",
     "iopub.status.busy": "2026-01-22T01:56:30.325335Z",
     "iopub.status.idle": "2026-01-22T01:56:30.388325Z",
     "shell.execute_reply": "2026-01-22T01:56:30.388325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Model Configuration\n",
      "========================================\n",
      "  vocab_size:   10,000\n",
      "  d_model:      256\n",
      "  num_heads:    4\n",
      "  num_layers:   4\n",
      "  d_ff:         1024 (4 × d_model)\n",
      "  max_seq_len:  512\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = GPT(\n",
    "    vocab_size=10000,\n",
    "    d_model=256,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    d_ff=1024,\n",
    "    max_seq_len=512,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(\"GPT Model Configuration\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  vocab_size:   {model.vocab_size:,}\")\n",
    "print(f\"  d_model:      {model.d_model}\")\n",
    "print(f\"  num_heads:    4\")\n",
    "print(f\"  num_layers:   4\")\n",
    "print(f\"  d_ff:         1024 (4 × d_model)\")\n",
    "print(f\"  max_seq_len:  512\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Counting Every Parameter\n",
    "\n",
    "Let's count exactly where all the parameters come from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:30.388325Z",
     "iopub.status.busy": "2026-01-22T01:56:30.388325Z",
     "iopub.status.idle": "2026-01-22T01:56:30.393620Z",
     "shell.execute_reply": "2026-01-22T01:56:30.393620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Breakdown\n",
      "============================================================\n",
      "Token embeddings:      10000 × 256 =    2,560,000\n",
      "Position embeddings:   512 × 256 =      131,072\n",
      "\n",
      "Per transformer block:\n",
      "  Attention (Q,K,V,O):  4 × 256² =      262,144\n",
      "  FFN:                       525,568\n",
      "  LayerNorm (×2):              1,024\n",
      "  Block total:               788,736\n",
      "\n",
      "All 4 blocks:             3,154,944\n",
      "Final LayerNorm:                 512\n",
      "Output projection:      (tied with token embeddings)\n",
      "============================================================\n",
      "Total (calculated):        5,846,528\n",
      "Total (actual):            5,846,528\n"
     ]
    }
   ],
   "source": [
    "def count_params(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "# Manual calculation\n",
    "vocab_size = 10000\n",
    "d_model = 256\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "d_ff = 1024\n",
    "max_seq_len = 512\n",
    "\n",
    "print(\"Parameter Breakdown\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Embeddings\n",
    "token_embed = vocab_size * d_model\n",
    "pos_embed = max_seq_len * d_model\n",
    "print(f\"Token embeddings:      {vocab_size} × {d_model} = {token_embed:>12,}\")\n",
    "print(f\"Position embeddings:   {max_seq_len} × {d_model} = {pos_embed:>12,}\")\n",
    "\n",
    "# Per block\n",
    "attn_params = 4 * d_model * d_model  # Q, K, V, O\n",
    "ffn_params = 2 * d_model * d_ff + d_ff + d_model  # up + down + biases\n",
    "ln_params = 4 * d_model  # 2 layer norms × (gamma + beta)\n",
    "block_params = attn_params + ffn_params + ln_params\n",
    "\n",
    "print(f\"\\nPer transformer block:\")\n",
    "print(f\"  Attention (Q,K,V,O):  4 × {d_model}² = {attn_params:>12,}\")\n",
    "print(f\"  FFN:                  {ffn_params:>12,}\")\n",
    "print(f\"  LayerNorm (×2):       {ln_params:>12,}\")\n",
    "print(f\"  Block total:          {block_params:>12,}\")\n",
    "\n",
    "# All blocks\n",
    "all_blocks = num_layers * block_params\n",
    "print(f\"\\nAll {num_layers} blocks:          {all_blocks:>12,}\")\n",
    "\n",
    "# Final LN\n",
    "final_ln = 2 * d_model\n",
    "print(f\"Final LayerNorm:        {final_ln:>12,}\")\n",
    "\n",
    "# Output projection (tied with token embedding - no additional params)\n",
    "print(f\"Output projection:      (tied with token embeddings)\")\n",
    "\n",
    "# Total\n",
    "total_calculated = token_embed + pos_embed + all_blocks + final_ln\n",
    "total_actual = count_params(model)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total (calculated):     {total_calculated:>12,}\")\n",
    "print(f\"Total (actual):         {total_actual:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Testing the Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:30.393620Z",
     "iopub.status.busy": "2026-01-22T01:56:30.393620Z",
     "iopub.status.idle": "2026-01-22T01:56:30.404158Z",
     "shell.execute_reply": "2026-01-22T01:56:30.404158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens shape:  torch.Size([2, 16])\n",
      "Output logits shape: torch.Size([2, 16, 10000])\n",
      "\n",
      "For each position, we get 10000 scores (one per vocabulary token).\n"
     ]
    }
   ],
   "source": [
    "# Generate random token IDs (simulating tokenized text)\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Forward pass\n",
    "logits = model(tokens)\n",
    "\n",
    "print(f\"Input tokens shape:  {tokens.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"\\nFor each position, we get {vocab_size} scores (one per vocabulary token).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## What Are Logits?\n",
    "\n",
    "The model outputs **logits**: raw, unnormalized scores for each token in the vocabulary. Higher scores mean the model thinks that token is more likely to come next.\n",
    "\n",
    "To convert logits to probabilities, apply softmax:\n",
    "\n",
    "$$P(\\text{token}_i) = \\frac{e^{\\text{logit}_i}}{\\sum_j e^{\\text{logit}_j}}$$\n",
    "\n",
    "Then you can either:\n",
    "- **Greedy decoding**: Pick the highest-probability token\n",
    "- **Sampling**: Randomly sample from the distribution (for variety)\n",
    "- **Top-k/Top-p sampling**: Sample from the most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:30.405161Z",
     "iopub.status.busy": "2026-01-22T01:56:30.405161Z",
     "iopub.status.idle": "2026-01-22T01:56:30.408233Z",
     "shell.execute_reply": "2026-01-22T01:56:30.408233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predicted next tokens (before training):\n",
      "  Token   339: 0.0003 probability\n",
      "  Token  7951: 0.0003 probability\n",
      "  Token  8281: 0.0003 probability\n",
      "  Token  2075: 0.0003 probability\n",
      "  Token  1483: 0.0003 probability\n",
      "\n",
      "These are random because the model is untrained!\n",
      "With 10,000 tokens, uniform probability = 0.00010\n"
     ]
    }
   ],
   "source": [
    "# Convert logits to probabilities\n",
    "last_position_logits = logits[0, -1, :]  # Last position of first batch\n",
    "probs = F.softmax(last_position_logits, dim=-1)\n",
    "\n",
    "# Top 5 predictions\n",
    "top_probs, top_indices = torch.topk(probs, 5)\n",
    "\n",
    "print(\"Top 5 predicted next tokens (before training):\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    print(f\"  Token {idx.item():>5}: {prob.item():.4f} probability\")\n",
    "\n",
    "print(\"\\nThese are random because the model is untrained!\")\n",
    "print(f\"With 10,000 tokens, uniform probability = {1/10000:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## A Single Training Step\n",
    "\n",
    "Let's see what one training step looks like. We'll compute the loss and update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:30.409240Z",
     "iopub.status.busy": "2026-01-22T01:56:30.409240Z",
     "iopub.status.idle": "2026-01-22T01:56:31.148774Z",
     "shell.execute_reply": "2026-01-22T01:56:31.148774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([2, 15])\n",
      "Target: torch.Size([2, 15])\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training uses next-token prediction:\n",
    "# Input:  [token0, token1, token2, token3, ...]\n",
    "# Target: [token1, token2, token3, token4, ...]  (shifted by 1)\n",
    "\n",
    "# We predict each token from the preceding tokens\n",
    "input_tokens = tokens[:, :-1]   # All but last\n",
    "target_tokens = tokens[:, 1:]   # All but first\n",
    "\n",
    "print(f\"Input:  {input_tokens.shape}\")\n",
    "print(f\"Target: {target_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:31.150518Z",
     "iopub.status.busy": "2026-01-22T01:56:31.148774Z",
     "iopub.status.idle": "2026-01-22T01:56:31.157626Z",
     "shell.execute_reply": "2026-01-22T01:56:31.157626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.2700\n",
      "\n",
      "Expected loss for random predictions: 9.21\n",
      "(Cross-entropy of uniform distribution over 10000 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "logits = model(input_tokens)\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "# Reshape for cross_entropy: (batch × seq_len, vocab_size) vs (batch × seq_len)\n",
    "loss = F.cross_entropy(\n",
    "    logits.view(-1, vocab_size),\n",
    "    target_tokens.reshape(-1)\n",
    ")\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"\\nExpected loss for random predictions: {math.log(vocab_size):.2f}\")\n",
    "print(f\"(Cross-entropy of uniform distribution over {vocab_size} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:31.158630Z",
     "iopub.status.busy": "2026-01-22T01:56:31.158630Z",
     "iopub.status.idle": "2026-01-22T01:56:31.185907Z",
     "shell.execute_reply": "2026-01-22T01:56:31.185907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.3218\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Check gradient magnitudes\n",
    "total_grad_norm = 0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        total_grad_norm += p.grad.norm().item() ** 2\n",
    "total_grad_norm = total_grad_norm ** 0.5\n",
    "\n",
    "print(f\"Total gradient norm: {total_grad_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:56:31.186918Z",
     "iopub.status.busy": "2026-01-22T01:56:31.186918Z",
     "iopub.status.idle": "2026-01-22T01:56:31.200481Z",
     "shell.execute_reply": "2026-01-22T01:56:31.200481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed one training step!\n",
      "\n",
      "To train a language model:\n",
      "  - Repeat this millions of times\n",
      "  - On real text data\n",
      "  - With techniques like gradient accumulation\n"
     ]
    }
   ],
   "source": [
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Completed one training step!\")\n",
    "print(\"\\nTo train a language model:\")\n",
    "print(\"  - Repeat this millions of times\")\n",
    "print(\"  - On real text data\")\n",
    "print(\"  - With techniques like gradient accumulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Model Scale Comparison\n",
    "\n",
    "Our model is tiny. Here's how it compares:\n",
    "\n",
    "| Model | Layers | $d_{model}$ | Heads | Parameters |\n",
    "|-------|--------|-------------|-------|------------|\n",
    "| **Ours** | 4 | 256 | 4 | ~5M |\n",
    "| GPT-2 Small | 12 | 768 | 12 | 117M |\n",
    "| GPT-2 Large | 36 | 1280 | 20 | 774M |\n",
    "| GPT-3 | 96 | 12288 | 96 | 175B |\n",
    "| Claude/GPT-4 | ~? | ~? | ~? | ~1-2T |\n",
    "\n",
    "But the architecture is **identical**. The same attention, the same FFN, the same residuals. GPT-3 is just our model with bigger matrices and more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Five stages**: Embed → Blocks → LayerNorm → Project → Logits\n",
    "\n",
    "2. **Weight tying** shares parameters between embedding and output projection\n",
    "\n",
    "3. **Logits** are raw scores; apply softmax for probabilities\n",
    "\n",
    "4. **Training** predicts each token from preceding tokens (shifted targets)\n",
    "\n",
    "5. **Architecture scales**: Same fundamentals work from 5M to 1T parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Next: Training at Scale\n",
    "\n",
    "We have a complete model. But training it well requires techniques beyond basic gradient descent:\n",
    "\n",
    "- **Gradient accumulation**: Simulate larger batches without more memory\n",
    "- **Validation**: Detect overfitting before it's too late\n",
    "- **Learning rate schedules**: Warm up, then decay\n",
    "\n",
    "In the next notebook, we'll cover practical training strategies."
   ]
  }
 ],
 "metadata": {
  "description": "Assembles full transformer by stacking blocks, adds final layer norm and output projection to vocabulary.",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
