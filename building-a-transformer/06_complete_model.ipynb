{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The Complete Transformer\n",
    "\n",
    "**Assembling all components into a working language model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now assemble all our components into a working decoder-only transformer (GPT-style). This is a complete language model that can be trained to predict the next word in a sequence.\n",
    "\n",
    "**What is \"decoder-only\"?** The original transformer paper had both an encoder (for reading input) and decoder (for generating output), used for translation. Modern language models like GPT use only the decoder part, which is simpler and works great for text generation. The key difference is that decoder-only models use causal masking—they can only look at previous tokens, not future ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Data Flows Through the Model\n",
    "\n",
    "1. **Token Embedding:** Convert input token IDs (integers) to dense vectors\n",
    "\n",
    "2. **Positional Encoding:** Add position information to tell the model where each token is\n",
    "\n",
    "3. **Transformer Blocks (×N):** Stack multiple identical blocks (we use 6; GPT-3 uses 96). Each block refines the representations through attention and feed-forward processing\n",
    "\n",
    "4. **Final LayerNorm:** One last normalization to stabilize the final outputs\n",
    "\n",
    "5. **Output Projection:** Project from d_model dimensions to vocabulary size, giving us scores (logits) for every possible next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Components from previous notebooks\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Learned positional embeddings.\"\"\"\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        return x + self.pos_embedding(positions)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 1, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.linear2(F.gelu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout1(self.attention(self.norm1(x), mask=mask))\n",
    "        x = x + self.dropout2(self.ffn(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"A complete decoder-only transformer language model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        d_model=512, \n",
    "        num_heads=8,\n",
    "        num_layers=6, \n",
    "        d_ff=2048, \n",
    "        max_seq_len=5000, \n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Store config\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def create_causal_mask(self, seq_len):\n",
    "        \"\"\"Create mask to prevent attending to future positions.\"\"\"\n",
    "        return torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len) - token indices\n",
    "            mask: optional causal mask\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # Create causal mask if not provided\n",
    "        if mask is None:\n",
    "            mask = self.create_causal_mask(x.size(1)).to(x.device)\n",
    "        \n",
    "        # 1. Embed tokens and add positions\n",
    "        x = self.token_embedding(x)      # (batch, seq) → (batch, seq, d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 2. Pass through all transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=mask)\n",
    "        \n",
    "        # 3. Final normalization and projection to vocabulary\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.output_proj(x)     # (batch, seq, d_model) → (batch, seq, vocab_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small model for demonstration\n",
    "model = GPTModel(\n",
    "    vocab_size=10000,\n",
    "    d_model=256,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    d_ff=1024,\n",
    "    max_seq_len=512\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  vocab_size: 10,000\")\n",
    "print(f\"  d_model: 256\")\n",
    "print(f\"  num_heads: 4\")\n",
    "print(f\"  num_layers: 4\")\n",
    "print(f\"  d_ff: 1,024\")\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "\n",
    "# Random token IDs (simulating tokenized text)\n",
    "tokens = torch.randint(0, 10000, (batch_size, seq_len))\n",
    "\n",
    "# Get logits\n",
    "logits = model(tokens)\n",
    "\n",
    "print(f\"Input tokens: {tokens.shape}\")\n",
    "print(f\"Output logits: {logits.shape}\")\n",
    "print(f\"\\nLogits are scores for each token in vocabulary:\")\n",
    "print(f\"  For position 0: {logits[0, 0, :5].detach().numpy().round(2)} ... (10,000 values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Logits?\n",
    "\n",
    "The model outputs \"logits\"—raw, unnormalized scores for each token in the vocabulary. Higher scores mean the model thinks that token is more likely to come next.\n",
    "\n",
    "We can convert these to probabilities using softmax, then either:\n",
    "- Pick the highest (greedy decoding)\n",
    "- Sample from the distribution (for more creative generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = F.softmax(logits[0, -1], dim=-1)  # Last position\n",
    "\n",
    "# Top 5 most likely next tokens\n",
    "top_probs, top_indices = torch.topk(probs, 5)\n",
    "\n",
    "print(\"Top 5 predicted next tokens (before training):\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    print(f\"  Token {idx.item()}: {prob.item():.4f} probability\")\n",
    "\n",
    "print(\"\\n(These are random because the model isn't trained yet!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Overview\n",
    "\n",
    "During training, we feed the model sequences of text and ask it to predict the next token at each position:\n",
    "\n",
    "1. **Forward pass:** Compute logits for all positions\n",
    "2. **Loss:** Compare predictions against actual next tokens using cross-entropy loss\n",
    "3. **Backward pass:** Use backpropagation to compute gradients\n",
    "4. **Update:** Adjust all weights using an optimizer (like AdamW)\n",
    "\n",
    "After training on billions of tokens, the model learns to predict plausible next words based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training step example\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Input: [token0, token1, token2, ...]\n",
    "# Target: [token1, token2, token3, ...]  (shifted by 1)\n",
    "input_tokens = tokens[:, :-1]\n",
    "target_tokens = tokens[:, 1:]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_tokens)\n",
    "\n",
    "# Compute loss (cross-entropy)\n",
    "loss = F.cross_entropy(\n",
    "    logits.view(-1, model.vocab_size),\n",
    "    target_tokens.view(-1)\n",
    ")\n",
    "\n",
    "print(f\"Input shape: {input_tokens.shape}\")\n",
    "print(f\"Target shape: {target_tokens.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"\\nExpected loss for random predictions: {math.log(10000):.2f}\")\n",
    "print(\"(Cross-entropy of uniform distribution over 10,000 tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass and update\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Completed one training step!\")\n",
    "print(\"Repeat millions of times on real data to train a language model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Scale\n",
    "\n",
    "Our implementation uses 4 layers with d_model=256, suitable for learning. For comparison:\n",
    "\n",
    "| Model | Layers | d_model | Parameters |\n",
    "|-------|--------|---------|------------|\n",
    "| This notebook | 4 | 256 | ~5M |\n",
    "| GPT-2 Small | 12 | 768 | 117M |\n",
    "| GPT-2 Large | 36 | 1280 | 774M |\n",
    "| GPT-3 | 96 | 12288 | 175B |\n",
    "\n",
    "The architecture scales beautifully—the same fundamental components work at wildly different scales!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Training at Scale\n",
    "\n",
    "We'll look at gradient accumulation and validation strategies for stable training on hobby hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
