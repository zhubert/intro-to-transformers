version: 1
project:
  title: "An Introduction to Transformers"
  authors:
    - name: "Zack Hubert"
      url: "https://www.zhubert.com"
      github: "zhubert"
    - name: "Claude Code"
      url: "https://www.claude.com/product/claude-code"
  exclude:
    - .venv
    - _build
    - "**.ipynb_checkpoints"
    - .git
    - README.md
    - STYLEGUIDE.md
  github: zhubert/intro-to-transformers
  toc:
    - file: intro.md
    - title: "Understanding Gradients"
      children:
        - file: understanding-gradients/00_introduction.ipynb
          title: "Introduction"
        - file: understanding-gradients/01_tokenization_embeddings.ipynb
          title: "Tokenization & Embeddings"
        - file: understanding-gradients/02_qkv_projections.ipynb
          title: "QKV Projections"
        - file: understanding-gradients/03_attention.ipynb
          title: "Attention"
        - file: understanding-gradients/04_multi_head.ipynb
          title: "Multi-Head Attention"
        - file: understanding-gradients/05_feedforward.ipynb
          title: "Feed-Forward Network"
        - file: understanding-gradients/06_layer_norm.ipynb
          title: "Layer Normalization"
        - file: understanding-gradients/07_loss.ipynb
          title: "Cross-Entropy Loss"
        - file: understanding-gradients/08_grad_loss.ipynb
          title: "Loss Gradients"
        - file: understanding-gradients/09_grad_backprop.ipynb
          title: "Backpropagation"
        - file: understanding-gradients/10_optimizer.ipynb
          title: "AdamW Optimizer"
    - title: "Building a Transformer"
      children:
        - file: building-a-transformer/00_introduction.ipynb
          title: "Introduction"
        - file: building-a-transformer/01_embeddings.ipynb
          title: "Embeddings & Positions"
        - file: building-a-transformer/02_attention.ipynb
          title: "Scaled Dot-Product Attention"
        - file: building-a-transformer/03_multi_head_attention.ipynb
          title: "Multi-Head Attention"
        - file: building-a-transformer/04_feedforward.ipynb
          title: "Feed-Forward Networks"
        - file: building-a-transformer/05_transformer_block.ipynb
          title: "Transformer Block"
        - file: building-a-transformer/06_complete_model.ipynb
          title: "Complete Model"
        - file: building-a-transformer/07_training.ipynb
          title: "Training at Scale"
        - file: building-a-transformer/08_kv_cache.ipynb
          title: "KV-Cache"
        - file: building-a-transformer/09_interpretability.ipynb
          title: "Interpretability"
    - title: "Fine-Tuning a Transformer"
      children:
        - file: fine-tuning-a-transformer/00_introduction.ipynb
          title: "Introduction"
        - file: fine-tuning-a-transformer/01_why_post_training.ipynb
          title: "Why Post-Training"
        - file: fine-tuning-a-transformer/02_project_overview.ipynb
          title: "Project Overview"
        - file: fine-tuning-a-transformer/03_sft_introduction.ipynb
          title: "SFT Introduction"
        - file: fine-tuning-a-transformer/04_sft_formatting.ipynb
          title: "Instruction Formatting"
        - file: fine-tuning-a-transformer/05_sft_loss_masking.ipynb
          title: "Loss Masking"
        - file: fine-tuning-a-transformer/06_sft_training.ipynb
          title: "SFT Training"
        - file: fine-tuning-a-transformer/07_sft_lora.ipynb
          title: "LoRA"
        - file: fine-tuning-a-transformer/08_reward_introduction.ipynb
          title: "Reward Modeling"
        - file: fine-tuning-a-transformer/09_reward_preference_data.ipynb
          title: "Preference Data"
        - file: fine-tuning-a-transformer/10_reward_training.ipynb
          title: "Reward Training"
        - file: fine-tuning-a-transformer/11_reward_evaluation.ipynb
          title: "Reward Evaluation"
        - file: fine-tuning-a-transformer/12_rlhf_introduction.ipynb
          title: "RLHF Introduction"
        - file: fine-tuning-a-transformer/13_rlhf_ppo.ipynb
          title: "PPO Algorithm"
        - file: fine-tuning-a-transformer/14_rlhf_kl_penalty.ipynb
          title: "KL Penalty"
        - file: fine-tuning-a-transformer/15_rlhf_dynamics.ipynb
          title: "Training Dynamics"
        - file: fine-tuning-a-transformer/16_rlhf_reference.ipynb
          title: "Reference Models"
        - file: fine-tuning-a-transformer/17_dpo_introduction.ipynb
          title: "DPO Introduction"
        - file: fine-tuning-a-transformer/18_dpo_vs_rlhf.ipynb
          title: "DPO vs RLHF"
        - file: fine-tuning-a-transformer/19_dpo_loss.ipynb
          title: "DPO Loss"
        - file: fine-tuning-a-transformer/20_dpo_training.ipynb
          title: "DPO Training"
        - file: fine-tuning-a-transformer/21_advanced_memory.ipynb
          title: "Memory Optimization"
        - file: fine-tuning-a-transformer/22_advanced_hyperparams.ipynb
          title: "Hyperparameter Tuning"
        - file: fine-tuning-a-transformer/23_advanced_evaluation.ipynb
          title: "Evaluation Metrics"
        - file: fine-tuning-a-transformer/24_advanced_pitfalls.ipynb
          title: "Common Pitfalls"
        - file: fine-tuning-a-transformer/25_try_it.ipynb
          title: "Try It Yourself"
    - title: "From Noise to Images"
      children:
        - file: from-noise-to-images/01_flow_matching_basics.ipynb
          title: "Flow Matching"
        - file: from-noise-to-images/02_diffusion_transformer.ipynb
          title: "Diffusion Transformer"
        - file: from-noise-to-images/03_class_conditioning.ipynb
          title: "Class Conditioning"
        - file: from-noise-to-images/04_text_conditioning.ipynb
          title: "Text Conditioning"
        - file: from-noise-to-images/05_latent_diffusion.ipynb
          title: "Latent Diffusion"
    - title: "Reasoning with Transformers"
      children:
        - file: reasoning-with-transformers/00_introduction.ipynb
          title: "Introduction"
        - file: reasoning-with-transformers/01_chain_of_thought.ipynb
          title: "Chain-of-Thought"
        - file: reasoning-with-transformers/02_self_consistency.ipynb
          title: "Self-Consistency"
        - file: reasoning-with-transformers/03_tree_of_thoughts.ipynb
          title: "Tree of Thoughts"
        - file: reasoning-with-transformers/04_process_reward_model.ipynb
          title: "Process Reward Models"
        - file: reasoning-with-transformers/05_best_of_n.ipynb
          title: "Best-of-N Verification"
        - file: reasoning-with-transformers/06_mcts.ipynb
          title: "Monte Carlo Tree Search"
        - file: reasoning-with-transformers/07_budget_forcing.ipynb
          title: "Budget Forcing"
        - file: reasoning-with-transformers/08_grpo.ipynb
          title: "GRPO Training"
        - file: reasoning-with-transformers/09_distillation.ipynb
          title: "Reasoning Distillation"
site:
  template: book-theme
  parts:
    primary_sidebar_footer: ./empty.md
  options:
    base_url: /intro-to-transformers
    logo: intro.png
    logo_dark: intro.png
    favicon: favicon.ico
    style: ./custom.css
