{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RLHF Training Dynamics\n",
    "\n",
    "**Rollouts, GAE, and the complete training loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Two-Phase Training Loop\n",
    "\n",
    "Each RLHF iteration consists of:\n",
    "\n",
    "1. **Rollout Phase** — Generate data with current policy\n",
    "2. **Update Phase** — Improve policy using PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Phase 1: Rollout Generation\n",
    "\n",
    "```python\n",
    "# For each batch of prompts:\n",
    "1. Generate responses using policy model\n",
    "2. Compute rewards using reward model\n",
    "3. Compute value estimates using value network\n",
    "4. Compute reference log probabilities\n",
    "5. Store in rollout buffer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:00.022864Z",
     "iopub.status.busy": "2025-12-06T23:30:00.022763Z",
     "iopub.status.idle": "2025-12-06T23:30:00.696909Z",
     "shell.execute_reply": "2025-12-06T23:30:00.696494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RolloutBatch stores all data needed for PPO updates:\n",
      "  - Queries and responses\n",
      "  - Log probabilities (policy and reference)\n",
      "  - Values, rewards, advantages, returns\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class RolloutBatch:\n",
    "    \"\"\"A batch of rollout data for PPO training.\"\"\"\n",
    "    \n",
    "    # Input data\n",
    "    query_tensors: torch.Tensor       # Prompts\n",
    "    response_tensors: torch.Tensor    # Generated responses\n",
    "    \n",
    "    # Model outputs during generation\n",
    "    logprobs: torch.Tensor            # Log probs of generated tokens\n",
    "    ref_logprobs: torch.Tensor        # Reference model log probs\n",
    "    values: torch.Tensor              # Value estimates\n",
    "    \n",
    "    # Rewards and advantages\n",
    "    rewards: torch.Tensor             # Reward model scores\n",
    "    advantages: torch.Tensor          # GAE advantages\n",
    "    returns: torch.Tensor             # Discounted returns\n",
    "\n",
    "print(\"RolloutBatch stores all data needed for PPO updates:\")\n",
    "print(\"  - Queries and responses\")\n",
    "print(\"  - Log probabilities (policy and reference)\")\n",
    "print(\"  - Values, rewards, advantages, returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Generalized Advantage Estimation (GAE)\n",
    "\n",
    "GAE efficiently estimates advantages:\n",
    "\n",
    "$$\\hat{A}_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "where $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:00.697887Z",
     "iopub.status.busy": "2025-12-06T23:30:00.697772Z",
     "iopub.status.idle": "2025-12-06T23:30:00.701455Z",
     "shell.execute_reply": "2025-12-06T23:30:00.701199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: torch.Size([2, 10])\n",
      "Advantages shape: torch.Size([2, 10])\n",
      "Returns shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "def compute_gae(\n",
    "    rewards: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    gamma: float = 0.99,\n",
    "    lam: float = 0.95\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation.\n",
    "    \n",
    "    Args:\n",
    "        rewards: Rewards at each timestep, shape (batch, seq_len)\n",
    "        values: Value estimates, shape (batch, seq_len)\n",
    "        gamma: Discount factor\n",
    "        lam: GAE lambda (bias-variance tradeoff)\n",
    "    \n",
    "    Returns:\n",
    "        advantages: GAE advantages\n",
    "        returns: Discounted returns (for value function training)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = rewards.shape\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    last_gae = 0\n",
    "    \n",
    "    # Process backwards through time\n",
    "    for t in reversed(range(seq_len)):\n",
    "        if t == seq_len - 1:\n",
    "            next_value = 0  # Terminal state\n",
    "        else:\n",
    "            next_value = values[:, t + 1]\n",
    "        \n",
    "        # TD error: r + γV(s') - V(s)\n",
    "        delta = rewards[:, t] + gamma * next_value - values[:, t]\n",
    "        \n",
    "        # GAE: accumulate discounted TD errors\n",
    "        last_gae = delta + gamma * lam * last_gae\n",
    "        advantages[:, t] = last_gae\n",
    "    \n",
    "    # Returns = advantages + values\n",
    "    returns = advantages + values\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "# Example\n",
    "batch_size, seq_len = 2, 10\n",
    "rewards = torch.randn(batch_size, seq_len) * 0.5\n",
    "values = torch.randn(batch_size, seq_len)\n",
    "\n",
    "advantages, returns = compute_gae(rewards, values)\n",
    "\n",
    "print(f\"Rewards shape: {rewards.shape}\")\n",
    "print(f\"Advantages shape: {advantages.shape}\")\n",
    "print(f\"Returns shape: {returns.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Advantage Normalization\n",
    "\n",
    "Normalize advantages for stable training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:00.702200Z",
     "iopub.status.busy": "2025-12-06T23:30:00.702127Z",
     "iopub.status.idle": "2025-12-06T23:30:00.704236Z",
     "shell.execute_reply": "2025-12-06T23:30:00.703976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before whitening:\n",
      "  Mean: 1.5281, Std: 0.9532\n",
      "After whitening:\n",
      "  Mean: 0.0000, Std: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def whiten_advantages(advantages: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize advantages to have zero mean and unit variance.\n",
    "    \n",
    "    This stabilizes training by ensuring consistent gradient scales.\n",
    "    \"\"\"\n",
    "    mean = advantages.mean()\n",
    "    std = advantages.std() + 1e-8\n",
    "    return (advantages - mean) / std\n",
    "\n",
    "# Example\n",
    "print(f\"Before whitening:\")\n",
    "print(f\"  Mean: {advantages.mean():.4f}, Std: {advantages.std():.4f}\")\n",
    "\n",
    "whitened = whiten_advantages(advantages)\n",
    "print(f\"After whitening:\")\n",
    "print(f\"  Mean: {whitened.mean():.4f}, Std: {whitened.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Phase 2: PPO Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:00.704898Z",
     "iopub.status.busy": "2025-12-06T23:30:00.704823Z",
     "iopub.status.idle": "2025-12-06T23:30:00.707497Z",
     "shell.execute_reply": "2025-12-06T23:30:00.707248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Update performs:\n",
      "  1. Re-compute log probs with current policy\n",
      "  2. Compute clipped policy loss\n",
      "  3. Compute value loss\n",
      "  4. Add KL penalty\n",
      "  5. Backprop and update\n"
     ]
    }
   ],
   "source": [
    "def ppo_update_step(\n",
    "    policy_model,\n",
    "    value_network,\n",
    "    rollout: RolloutBatch,\n",
    "    optimizer,\n",
    "    config\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform one PPO update step.\n",
    "    \n",
    "    This is called multiple times (ppo_epochs) per rollout.\n",
    "    \"\"\"\n",
    "    # Get current policy outputs\n",
    "    # (Re-compute because policy has changed since rollout)\n",
    "    current_logprobs = policy_model.get_logprobs(\n",
    "        rollout.query_tensors,\n",
    "        rollout.response_tensors\n",
    "    )\n",
    "    \n",
    "    current_values = value_network(\n",
    "        rollout.query_tensors,\n",
    "        rollout.response_tensors\n",
    "    )\n",
    "    \n",
    "    # Compute PPO loss\n",
    "    ratio = torch.exp(current_logprobs - rollout.logprobs)\n",
    "    \n",
    "    # Clipped objective\n",
    "    unclipped = ratio * rollout.advantages\n",
    "    clipped = torch.clamp(ratio, 1 - config['clip_ratio'], 1 + config['clip_ratio'])\n",
    "    clipped = clipped * rollout.advantages\n",
    "    \n",
    "    policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "    \n",
    "    # Value loss\n",
    "    value_loss = ((current_values - rollout.returns) ** 2).mean()\n",
    "    \n",
    "    # KL penalty\n",
    "    kl_penalty = (current_logprobs - rollout.ref_logprobs).mean()\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = (\n",
    "        policy_loss +\n",
    "        config['vf_coef'] * value_loss +\n",
    "        config['kl_coef'] * kl_penalty\n",
    "    )\n",
    "    \n",
    "    # Update\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_model.parameters(), config['max_grad_norm'])\n",
    "    optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        'policy_loss': policy_loss.item(),\n",
    "        'value_loss': value_loss.item(),\n",
    "        'kl': kl_penalty.item()\n",
    "    }\n",
    "\n",
    "print(\"PPO Update performs:\")\n",
    "print(\"  1. Re-compute log probs with current policy\")\n",
    "print(\"  2. Compute clipped policy loss\")\n",
    "print(\"  3. Compute value loss\")\n",
    "print(\"  4. Add KL penalty\")\n",
    "print(\"  5. Backprop and update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:00.708249Z",
     "iopub.status.busy": "2025-12-06T23:30:00.708175Z",
     "iopub.status.idle": "2025-12-06T23:30:00.710141Z",
     "shell.execute_reply": "2025-12-06T23:30:00.709896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF Training Loop Structure:\n",
      "  For each iteration:\n",
      "    1. Generate rollouts (responses + rewards)\n",
      "    2. Compute advantages with GAE\n",
      "    3. Run PPO updates (multiple epochs)\n",
      "    4. Log metrics and checkpoint\n"
     ]
    }
   ],
   "source": [
    "def train_rlhf_loop(config):\n",
    "    \"\"\"\n",
    "    Simplified RLHF training loop structure.\n",
    "    \"\"\"\n",
    "    for iteration in range(config['num_iterations']):\n",
    "        # ===== PHASE 1: ROLLOUT =====\n",
    "        # Sample prompts\n",
    "        # prompts = sample_prompts(config['batch_size'])\n",
    "        \n",
    "        # Generate responses\n",
    "        # responses = policy_model.generate(prompts)\n",
    "        \n",
    "        # Score with reward model\n",
    "        # rewards = reward_model(prompts, responses)\n",
    "        \n",
    "        # Get values and log probs\n",
    "        # values = value_network(prompts, responses)\n",
    "        # policy_logprobs = policy_model.get_logprobs(prompts, responses)\n",
    "        # ref_logprobs = reference_model.get_logprobs(prompts, responses)\n",
    "        \n",
    "        # Compute GAE\n",
    "        # advantages, returns = compute_gae(rewards, values)\n",
    "        # advantages = whiten_advantages(advantages)\n",
    "        \n",
    "        # Store in rollout buffer\n",
    "        # rollout = RolloutBatch(...)\n",
    "        \n",
    "        # ===== PHASE 2: PPO UPDATE =====\n",
    "        # for epoch in range(config['ppo_epochs']):\n",
    "        #     metrics = ppo_update_step(policy_model, value_network, rollout, optimizer, config)\n",
    "        \n",
    "        # ===== LOGGING =====\n",
    "        # log_metrics(metrics)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    print(\"RLHF Training Loop Structure:\")\n",
    "    print(\"  For each iteration:\")\n",
    "    print(\"    1. Generate rollouts (responses + rewards)\")\n",
    "    print(\"    2. Compute advantages with GAE\")\n",
    "    print(\"    3. Run PPO updates (multiple epochs)\")\n",
    "    print(\"    4. Log metrics and checkpoint\")\n",
    "\n",
    "train_rlhf_loop({'num_iterations': 1000, 'ppo_epochs': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Key Hyperparameters\n",
    "\n",
    "| Parameter | Typical Value | Notes |\n",
    "|-----------|---------------|-------|\n",
    "| `gamma` | 0.99 | Discount factor |\n",
    "| `gae_lambda` | 0.95 | GAE bias-variance tradeoff |\n",
    "| `ppo_epochs` | 4 | Updates per rollout |\n",
    "| `batch_size` | 4-16 | Prompts per iteration |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Let's learn about reference models — how to create and manage them for stable RLHF training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
