{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RLHF Training Dynamics: Where the Magic Happens\n",
    "\n",
    "**Understanding rollouts, advantages, and how RLHF actually trains**\n",
    "\n",
    "You've seen the pieces — policy models, reward models, value networks, PPO. Now let's watch them dance together.\n",
    "\n",
    "This is where RLHF gets *real*. We're going to break down exactly what happens during training, step by step, no hand-waving. By the end, you'll understand the heartbeat of RLHF: generate, evaluate, improve, repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Two-Phase Dance\n",
    "\n",
    "RLHF training has a rhythm. Each iteration does two things:\n",
    "\n",
    "**Phase 1: Rollout** — Your policy generates responses. You score them. You remember what happened.\n",
    "\n",
    "**Phase 2: Update** — You use that data to make your policy better. Then you do it again.\n",
    "\n",
    "Think of it like learning to cook. First you make a dish (rollout). Then you taste it, figure out what worked and what didn't, and adjust your technique (update). Rinse and repeat until you're a chef.\n",
    "\n",
    "The key insight? You don't learn *while* you're generating. You generate first, *then* learn from what you generated. This separation is what makes PPO stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## What's a Rollout, Anyway?\n",
    "\n",
    "Here's a word you'll hear constantly in RL: **rollout**.\n",
    "\n",
    "A rollout is just... running your policy and recording what happens. That's it.\n",
    "\n",
    "In RLHF terms:\n",
    "- You give your model some prompts\n",
    "- It generates responses (one token at a time)\n",
    "- You score those responses with your reward model\n",
    "- You ask your value network what it thinks\n",
    "- You save everything to memory\n",
    "\n",
    "Think of it like recording a practice session. You're not evaluating yourself mid-sentence — you finish your full response, *then* you look at what you did. The recording (the rollout) lets you study your performance afterward.\n",
    "\n",
    "Here's what we store in each rollout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:11:28.738195Z",
     "iopub.status.busy": "2025-12-07T20:11:28.738125Z",
     "iopub.status.idle": "2025-12-07T20:11:29.428378Z",
     "shell.execute_reply": "2025-12-07T20:11:29.428090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A RolloutBatch is like a student's exam:\n",
      "\n",
      "  query_tensors     →  The questions\n",
      "  response_tensors  →  The student's answers\n",
      "  logprobs          →  Student's confidence in each answer\n",
      "  ref_logprobs      →  What a baseline student would've done\n",
      "  values            →  Student's prediction of their grade\n",
      "  rewards           →  The actual grade from the teacher\n",
      "  advantages        →  Did they do better or worse than expected?\n",
      "  returns           →  What the grade *should've* been\n",
      "\n",
      "We store all of this so we can learn from it later.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class RolloutBatch:\n",
    "    \"\"\"Everything we need to remember from generating responses.\"\"\"\n",
    "    \n",
    "    # What we started with\n",
    "    query_tensors: torch.Tensor       # The prompts we gave the model\n",
    "    response_tensors: torch.Tensor    # What the model generated\n",
    "    \n",
    "    # What the models thought at generation time\n",
    "    logprobs: torch.Tensor            # How confident was our policy?\n",
    "    ref_logprobs: torch.Tensor        # How confident was the reference model?\n",
    "    values: torch.Tensor              # What did the value network predict?\n",
    "    \n",
    "    # What we computed afterward\n",
    "    rewards: torch.Tensor             # Reward model scores (the \"grades\")\n",
    "    advantages: torch.Tensor          # \"How much better than expected?\" (we'll explain this!)\n",
    "    returns: torch.Tensor             # Target values for training the value network\n",
    "\n",
    "# Let's visualize what a rollout looks like\n",
    "print(\"A RolloutBatch is like a student's exam:\")\n",
    "print()\n",
    "print(\"  query_tensors     →  The questions\")\n",
    "print(\"  response_tensors  →  The student's answers\")\n",
    "print(\"  logprobs          →  Student's confidence in each answer\")\n",
    "print(\"  ref_logprobs      →  What a baseline student would've done\")\n",
    "print(\"  values            →  Student's prediction of their grade\")\n",
    "print(\"  rewards           →  The actual grade from the teacher\")\n",
    "print(\"  advantages        →  Did they do better or worse than expected?\")\n",
    "print(\"  returns           →  What the grade *should've* been\")\n",
    "print()\n",
    "print(\"We store all of this so we can learn from it later.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Advantages: The Secret Sauce\n",
    "\n",
    "Now we get to the really clever bit. Let's talk about **advantages**.\n",
    "\n",
    "Here's the core question in reinforcement learning: *which actions were actually good?*\n",
    "\n",
    "You might think \"just use the rewards!\" But that's too naive. Imagine you're playing basketball, and your team wins 100-98. Every action you took gets associated with \"winning\" — but some of your shots were terrible and you just got lucky.\n",
    "\n",
    "What you really want to know is: **which actions were better than expected?**\n",
    "\n",
    "That's an advantage. It's not the absolute reward. It's the *surprise*.\n",
    "\n",
    "If your value network predicted you'd get a reward of 5, and you got 7, your advantage is +2. You did better than expected! Reinforce that behavior.\n",
    "\n",
    "If it predicted 8 and you got 7, your advantage is -1. You did worse than expected. Maybe don't do that again.\n",
    "\n",
    "This is where **GAE** comes in — **Generalized Advantage Estimation**. (Fancy name, but the intuition is simple: it's a smart way to compute these \"better than expected\" signals.)\n",
    "\n",
    "The formula looks scary, but let's break it down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:11:29.429498Z",
     "iopub.status.busy": "2025-12-07T20:11:29.429398Z",
     "iopub.status.idle": "2025-12-07T20:11:29.434161Z",
     "shell.execute_reply": "2025-12-07T20:11:29.433878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example rollout:\n",
      "  Rewards shape: torch.Size([2, 10]) — what actually happened\n",
      "  Values shape:  torch.Size([2, 10])  — what we predicted\n",
      "\n",
      "  Advantages shape: torch.Size([2, 10]) — surprise signal (positive = better than expected)\n",
      "  Returns shape:    torch.Size([2, 10])    — target for value network training\n",
      "\n",
      "Sample advantages (first sequence, first 5 timesteps):\n",
      "tensor([-0.1139, -0.1449, -1.1203, -0.0052, -0.4782])\n",
      "\n",
      "Positive = 'do more of this', negative = 'do less of this'\n"
     ]
    }
   ],
   "source": [
    "def compute_gae(\n",
    "    rewards: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    gamma: float = 0.99,\n",
    "    lam: float = 0.95\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE).\n",
    "    \n",
    "    This answers: \"How much better/worse was each action compared to what we expected?\"\n",
    "    \n",
    "    The math (don't panic):\n",
    "        A_t = Σ (γλ)^l * δ_{t+l}\n",
    "    \n",
    "    Where δ_t is the \"TD error\" (Temporal Difference error):\n",
    "        δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "    \n",
    "    In English:\n",
    "        δ_t = \"actual reward\" + \"discounted future value\" - \"predicted value\"\n",
    "        δ_t = \"what happened\" - \"what we expected\"\n",
    "    \n",
    "    Then we sum up these errors over time, with exponential decay (γλ).\n",
    "    \n",
    "    Args:\n",
    "        rewards: What actually happened, shape (batch, seq_len)\n",
    "        values: What we predicted would happen, shape (batch, seq_len)\n",
    "        gamma: Discount factor (how much we care about future rewards)\n",
    "                Higher = \"future matters\", lower = \"only now matters\"\n",
    "        lam: GAE lambda (bias-variance knob, 0.95 is standard)\n",
    "                Higher = trust multi-step returns, lower = trust single-step\n",
    "    \n",
    "    Returns:\n",
    "        advantages: \"Better or worse than expected?\" for each timestep\n",
    "        returns: What the value *should've* been (for training value network)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = rewards.shape\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    last_gae = 0\n",
    "    \n",
    "    # We go backwards through time (from end to beginning)\n",
    "    # Why? Because we need to know the future to compute \"expected future value\"\n",
    "    for t in reversed(range(seq_len)):\n",
    "        if t == seq_len - 1:\n",
    "            next_value = 0  # End of sequence, no future\n",
    "        else:\n",
    "            next_value = values[:, t + 1]\n",
    "        \n",
    "        # TD error: \"what we got\" - \"what we expected\"\n",
    "        # r_t = immediate reward\n",
    "        # γ * V(s_{t+1}) = discounted predicted future\n",
    "        # V(s_t) = what we thought we'd get total\n",
    "        delta = rewards[:, t] + gamma * next_value - values[:, t]\n",
    "        \n",
    "        # GAE accumulation: sum up errors with exponential decay\n",
    "        # This smooths out noise and balances bias vs variance\n",
    "        last_gae = delta + gamma * lam * last_gae\n",
    "        advantages[:, t] = last_gae\n",
    "    \n",
    "    # Returns are what the value network should've predicted\n",
    "    # They're the \"ground truth\" for training it\n",
    "    returns = advantages + values\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "# Let's see it in action\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "# Imagine we got these rewards (somewhat random, centered near 0)\n",
    "rewards = torch.randn(batch_size, seq_len) * 0.5\n",
    "\n",
    "# And our value network predicted these values\n",
    "values = torch.randn(batch_size, seq_len)\n",
    "\n",
    "advantages, returns = compute_gae(rewards, values)\n",
    "\n",
    "print(\"Example rollout:\")\n",
    "print(f\"  Rewards shape: {rewards.shape} — what actually happened\")\n",
    "print(f\"  Values shape:  {values.shape}  — what we predicted\")\n",
    "print()\n",
    "print(f\"  Advantages shape: {advantages.shape} — surprise signal (positive = better than expected)\")\n",
    "print(f\"  Returns shape:    {returns.shape}    — target for value network training\")\n",
    "print()\n",
    "print(\"Sample advantages (first sequence, first 5 timesteps):\")\n",
    "print(advantages[0, :5])\n",
    "print()\n",
    "print(\"Positive = 'do more of this', negative = 'do less of this'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Whitening: Keeping Things Stable\n",
    "\n",
    "One more trick before we update the policy: **advantage normalization** (also called \"whitening\").\n",
    "\n",
    "Raw advantages can have wildly different scales. One batch might have advantages from -2 to +2. Another might be -100 to +100. This makes training unstable — your gradients are all over the place.\n",
    "\n",
    "Solution? Normalize them. Make them have mean 0 and standard deviation 1.\n",
    "\n",
    "This is like grading on a curve. It doesn't matter if the test was easy or hard — you're comparing students to *each other*, not to an absolute scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:11:29.434888Z",
     "iopub.status.busy": "2025-12-07T20:11:29.434816Z",
     "iopub.status.idle": "2025-12-07T20:11:29.437247Z",
     "shell.execute_reply": "2025-12-07T20:11:29.437010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before whitening:\n",
      "  Mean: -0.6189\n",
      "  Std:  0.4675\n",
      "  Min:  -1.4832\n",
      "  Max:  0.0186\n",
      "\n",
      "After whitening:\n",
      "  Mean: 0.0000  ← always ~0\n",
      "  Std:  1.0000   ← always ~1\n",
      "  Min:  -1.8487\n",
      "  Max:  1.3637\n",
      "\n",
      "Now the scale is consistent across all batches!\n"
     ]
    }
   ],
   "source": [
    "def whiten_advantages(advantages: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize advantages to mean=0, std=1.\n",
    "    \n",
    "    Why? Training stability. We want consistent gradient scales across batches.\n",
    "    \n",
    "    This is \"grading on a curve\" — comparing actions to each other,\n",
    "    not to an absolute scale.\n",
    "    \"\"\"\n",
    "    mean = advantages.mean()\n",
    "    std = advantages.std() + 1e-8  # Small epsilon prevents division by zero\n",
    "    return (advantages - mean) / std\n",
    "\n",
    "# Let's see the transformation\n",
    "print(\"Before whitening:\")\n",
    "print(f\"  Mean: {advantages.mean():.4f}\")\n",
    "print(f\"  Std:  {advantages.std():.4f}\")\n",
    "print(f\"  Min:  {advantages.min():.4f}\")\n",
    "print(f\"  Max:  {advantages.max():.4f}\")\n",
    "\n",
    "whitened = whiten_advantages(advantages)\n",
    "\n",
    "print()\n",
    "print(\"After whitening:\")\n",
    "print(f\"  Mean: {whitened.mean():.4f}  ← always ~0\")\n",
    "print(f\"  Std:  {whitened.std():.4f}   ← always ~1\")\n",
    "print(f\"  Min:  {whitened.min():.4f}\")\n",
    "print(f\"  Max:  {whitened.max():.4f}\")\n",
    "print()\n",
    "print(\"Now the scale is consistent across all batches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Phase 2: The PPO Update\n",
    "\n",
    "Okay! We've generated rollouts. We've computed advantages. We've whitened them.\n",
    "\n",
    "Now comes the actual learning: **the PPO update**.\n",
    "\n",
    "Remember, PPO stands for Proximal Policy Optimization. The key word is *proximal* — we want to update the policy, but not too much. Stay close to the old policy. (That's what the \"clipping\" does, which we'll see in the code.)\n",
    "\n",
    "Here's what happens in each update step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:11:29.438136Z",
     "iopub.status.busy": "2025-12-07T20:11:29.438065Z",
     "iopub.status.idle": "2025-12-07T20:11:29.441178Z",
     "shell.execute_reply": "2025-12-07T20:11:29.440935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Update in 7 steps:\n",
      "\n",
      "  1. Re-compute log probs & values (policy changed since rollout)\n",
      "  2. Compute ratio of new/old policy probabilities\n",
      "  3. Compute clipped objective (the PPO magic)\n",
      "  4. Compute value loss (train value network)\n",
      "  5. Compute KL penalty (don't forget original LM)\n",
      "  6. Combine losses and backprop\n",
      "  7. Clip gradients and step optimizer\n",
      "\n",
      "The clipping is what makes PPO 'proximal' — we update carefully.\n"
     ]
    }
   ],
   "source": [
    "def ppo_update_step(\n",
    "    policy_model,\n",
    "    value_network,\n",
    "    rollout: RolloutBatch,\n",
    "    optimizer,\n",
    "    config\n",
    "):\n",
    "    \"\"\"\n",
    "    One PPO update step. This is the heart of RLHF training.\n",
    "    \n",
    "    We call this multiple times (ppo_epochs) for each rollout.\n",
    "    Why? Because we have limited data, so we squeeze as much learning\n",
    "    from it as we can. (But not too much — that's where clipping helps.)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Re-compute outputs with CURRENT policy\n",
    "    # (The policy has changed since we did the rollout, so we need fresh numbers)\n",
    "    current_logprobs = policy_model.get_logprobs(\n",
    "        rollout.query_tensors,\n",
    "        rollout.response_tensors\n",
    "    )\n",
    "    \n",
    "    current_values = value_network(\n",
    "        rollout.query_tensors,\n",
    "        rollout.response_tensors\n",
    "    )\n",
    "    \n",
    "    # Step 2: Compute the \"ratio\" \n",
    "    # This tells us: how much more likely is the current policy to take\n",
    "    # the same action compared to the old policy?\n",
    "    #\n",
    "    # ratio = P_new(action) / P_old(action)\n",
    "    # = exp(log P_new - log P_old)\n",
    "    ratio = torch.exp(current_logprobs - rollout.logprobs)\n",
    "    \n",
    "    # Step 3: PPO clipped objective (this is THE magic)\n",
    "    # We compute two versions and take the minimum\n",
    "    \n",
    "    # Unclipped: standard policy gradient\n",
    "    # \"How good was this action (advantage) * how much more likely we are to take it (ratio)\"\n",
    "    unclipped = ratio * rollout.advantages\n",
    "    \n",
    "    # Clipped: same thing, but we clip the ratio\n",
    "    # Don't let it get too far from 1.0 (which means \"same probability as old policy\")\n",
    "    # clip_ratio is typically 0.2, so we allow 0.8 to 1.2\n",
    "    clipped = torch.clamp(\n",
    "        ratio, \n",
    "        1 - config['clip_ratio'],  # Lower bound: 0.8\n",
    "        1 + config['clip_ratio']   # Upper bound: 1.2\n",
    "    ) * rollout.advantages\n",
    "    \n",
    "    # Take the minimum — this is conservative, prevents big updates\n",
    "    # We want to improve, but carefully\n",
    "    policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "    # (Negative because we want to maximize, but optimizers minimize)\n",
    "    \n",
    "    # Step 4: Value loss\n",
    "    # Train the value network to predict returns better\n",
    "    # Standard MSE loss\n",
    "    value_loss = ((current_values - rollout.returns) ** 2).mean()\n",
    "    \n",
    "    # Step 5: KL penalty\n",
    "    # Extra regularization: penalize drifting too far from reference model\n",
    "    # This keeps us from forgetting the original language model's knowledge\n",
    "    kl_penalty = (current_logprobs - rollout.ref_logprobs).mean()\n",
    "    \n",
    "    # Step 6: Combine everything\n",
    "    total_loss = (\n",
    "        policy_loss +                        # Main objective: improve policy\n",
    "        config['vf_coef'] * value_loss +     # Train value network (vf_coef ≈ 0.5)\n",
    "        config['kl_coef'] * kl_penalty       # Don't drift too far (kl_coef ≈ 0.1)\n",
    "    )\n",
    "    \n",
    "    # Step 7: Standard PyTorch training step\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Clip gradients to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(\n",
    "        policy_model.parameters(), \n",
    "        config['max_grad_norm']\n",
    "    )\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        'policy_loss': policy_loss.item(),\n",
    "        'value_loss': value_loss.item(),\n",
    "        'kl': kl_penalty.item()\n",
    "    }\n",
    "\n",
    "# Show the structure\n",
    "print(\"PPO Update in 7 steps:\")\n",
    "print()\n",
    "print(\"  1. Re-compute log probs & values (policy changed since rollout)\")\n",
    "print(\"  2. Compute ratio of new/old policy probabilities\")\n",
    "print(\"  3. Compute clipped objective (the PPO magic)\")\n",
    "print(\"  4. Compute value loss (train value network)\")\n",
    "print(\"  5. Compute KL penalty (don't forget original LM)\")\n",
    "print(\"  6. Combine losses and backprop\")\n",
    "print(\"  7. Clip gradients and step optimizer\")\n",
    "print()\n",
    "print(\"The clipping is what makes PPO 'proximal' — we update carefully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## The Complete Training Loop\n",
    "\n",
    "Alright, let's zoom out and see the full picture.\n",
    "\n",
    "Here's the rhythm of RLHF training, iteration by iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:11:29.441875Z",
     "iopub.status.busy": "2025-12-07T20:11:29.441804Z",
     "iopub.status.idle": "2025-12-07T20:11:29.444739Z",
     "shell.execute_reply": "2025-12-07T20:11:29.444487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════\n",
      "RLHF Training Loop — The Big Picture\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "For each iteration:\n",
      "\n",
      "  PHASE 1: ROLLOUT (Generate & Score)\n",
      "    ├─ Sample prompts from dataset\n",
      "    ├─ Generate responses with current policy\n",
      "    ├─ Score with reward model\n",
      "    ├─ Get value predictions\n",
      "    ├─ Compute advantages (GAE)\n",
      "    └─ Package into RolloutBatch\n",
      "\n",
      "  PHASE 2: UPDATE (Learn)\n",
      "    └─ Run PPO updates (4 epochs typically)\n",
      "        ├─ Re-compute log probs with current policy\n",
      "        ├─ Compute clipped policy loss\n",
      "        ├─ Compute value loss\n",
      "        ├─ Add KL penalty\n",
      "        └─ Backprop and step\n",
      "\n",
      "  Repeat until convergence (or you run out of patience/budget)\n",
      "\n",
      "════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "source": [
    "def train_rlhf_loop(config):\n",
    "    \"\"\"\n",
    "    The complete RLHF training loop.\n",
    "    \n",
    "    This is pseudocode showing the structure. In practice, you'd fill in\n",
    "    the actual model calls.\n",
    "    \"\"\"\n",
    "    \n",
    "    for iteration in range(config['num_iterations']):\n",
    "        \n",
    "        # ===== PHASE 1: ROLLOUT =====\n",
    "        # Generate a batch of responses and collect all the data we need\n",
    "        \n",
    "        # Sample some prompts from your dataset\n",
    "        # prompts = sample_prompts(config['batch_size'])\n",
    "        \n",
    "        # Generate responses with the current policy\n",
    "        # responses = policy_model.generate(prompts)\n",
    "        \n",
    "        # Score them with the reward model\n",
    "        # rewards = reward_model(prompts, responses)\n",
    "        \n",
    "        # Get value predictions\n",
    "        # values = value_network(prompts, responses)\n",
    "        \n",
    "        # Get log probabilities from policy and reference model\n",
    "        # policy_logprobs = policy_model.get_logprobs(prompts, responses)\n",
    "        # ref_logprobs = reference_model.get_logprobs(prompts, responses)\n",
    "        \n",
    "        # Compute advantages using GAE\n",
    "        # advantages, returns = compute_gae(rewards, values, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        # advantages = whiten_advantages(advantages)\n",
    "        \n",
    "        # Package everything into a rollout batch\n",
    "        # rollout = RolloutBatch(\n",
    "        #     query_tensors=prompts,\n",
    "        #     response_tensors=responses,\n",
    "        #     logprobs=policy_logprobs,\n",
    "        #     ref_logprobs=ref_logprobs,\n",
    "        #     values=values,\n",
    "        #     rewards=rewards,\n",
    "        #     advantages=advantages,\n",
    "        #     returns=returns\n",
    "        # )\n",
    "        \n",
    "        # ===== PHASE 2: PPO UPDATE =====\n",
    "        # Learn from the rollout, multiple passes over the same data\n",
    "        \n",
    "        # for epoch in range(config['ppo_epochs']):\n",
    "        #     metrics = ppo_update_step(\n",
    "        #         policy_model,\n",
    "        #         value_network,\n",
    "        #         rollout,\n",
    "        #         optimizer,\n",
    "        #         config\n",
    "        #     )\n",
    "        \n",
    "        # ===== LOGGING & CHECKPOINTING =====\n",
    "        # Track progress, save models periodically\n",
    "        \n",
    "        # if iteration % 10 == 0:\n",
    "        #     log_metrics(metrics)\n",
    "        #     print(f\"Iteration {iteration}: KL={metrics['kl']:.4f}, \"\n",
    "        #           f\"Policy Loss={metrics['policy_loss']:.4f}\")\n",
    "        \n",
    "        # if iteration % 100 == 0:\n",
    "        #     save_checkpoint(policy_model, value_network, iteration)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # return policy_model\n",
    "\n",
    "# Show the structure\n",
    "print(\"═\" * 60)\n",
    "print(\"RLHF Training Loop — The Big Picture\")\n",
    "print(\"═\" * 60)\n",
    "print()\n",
    "print(\"For each iteration:\")\n",
    "print()\n",
    "print(\"  PHASE 1: ROLLOUT (Generate & Score)\")\n",
    "print(\"    ├─ Sample prompts from dataset\")\n",
    "print(\"    ├─ Generate responses with current policy\")\n",
    "print(\"    ├─ Score with reward model\")\n",
    "print(\"    ├─ Get value predictions\")\n",
    "print(\"    ├─ Compute advantages (GAE)\")\n",
    "print(\"    └─ Package into RolloutBatch\")\n",
    "print()\n",
    "print(\"  PHASE 2: UPDATE (Learn)\")\n",
    "print(\"    └─ Run PPO updates (4 epochs typically)\")\n",
    "print(\"        ├─ Re-compute log probs with current policy\")\n",
    "print(\"        ├─ Compute clipped policy loss\")\n",
    "print(\"        ├─ Compute value loss\")\n",
    "print(\"        ├─ Add KL penalty\")\n",
    "print(\"        └─ Backprop and step\")\n",
    "print()\n",
    "print(\"  Repeat until convergence (or you run out of patience/budget)\")\n",
    "print()\n",
    "print(\"═\" * 60)\n",
    "\n",
    "# Run with example config\n",
    "config = {\n",
    "    'num_iterations': 1000,\n",
    "    'ppo_epochs': 4,\n",
    "    'batch_size': 8,\n",
    "    'clip_ratio': 0.2,\n",
    "    'vf_coef': 0.5,\n",
    "    'kl_coef': 0.1,\n",
    "    'max_grad_norm': 1.0\n",
    "}\n",
    "\n",
    "train_rlhf_loop(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Key Hyperparameters (And What They Actually Do)\n",
    "\n",
    "Here are the knobs you can turn, and what happens when you turn them:\n",
    "\n",
    "| Parameter | Typical Value | What It Does | When To Change |\n",
    "|-----------|---------------|--------------|----------------|\n",
    "| `gamma` | 0.99 | **Discount factor** — How much you care about future rewards vs immediate ones. Higher = \"patient\", lower = \"myopic\". | Keep at 0.99 for language. You care about the whole response, not just the next token. |\n",
    "| `gae_lambda` | 0.95 | **GAE smoothing** — Bias-variance tradeoff. Higher = trust multi-step returns (less bias, more variance). Lower = trust single-step (more bias, less variance). | 0.95 is the sweet spot. Don't mess with it unless you're doing research. |\n",
    "| `ppo_epochs` | 4 | **How many times to learn from each rollout**. Higher = squeeze more from limited data, but risk overfitting. | 4 is standard. Increase if data is expensive (it is). Decrease if you see KL divergence exploding. |\n",
    "| `clip_ratio` | 0.2 | **How far the policy can change in one update**. The \"proximal\" in PPO. Higher = bigger steps (faster, riskier). Lower = smaller steps (slower, safer). | 0.2 is battle-tested. Don't change unless you know what you're doing. |\n",
    "| `batch_size` | 4-16 | **Prompts per iteration**. Higher = more stable, slower. Lower = faster, noisier. | Limited by GPU memory. RLHF is memory-hungry (you're running 4 models at once). |\n",
    "| `vf_coef` | 0.5 | **Value loss weight**. How much you care about training the value network vs the policy. | 0.5 means \"value loss matters half as much as policy loss.\" Usually fine. |\n",
    "| `kl_coef` | 0.01-0.1 | **KL penalty weight**. How much you penalize drifting from the reference model. Higher = stay closer to original LM (safer but less freedom). | Start low (0.01), increase if you see mode collapse or gibberish. |\n",
    "| `max_grad_norm` | 1.0 | **Gradient clipping threshold**. Prevents exploding gradients. | 1.0 works. Lower if training is unstable. |\n",
    "\n",
    "The defaults are pretty good. RLHF has been tuned by a lot of very smart people spending a lot of GPU-hours. Trust the defaults until you have a specific reason not to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "You now understand the heartbeat of RLHF:\n",
    "\n",
    "1. **Generate** responses (rollout)\n",
    "2. **Evaluate** them (rewards)\n",
    "3. **Compute** advantages (GAE) — \"better or worse than expected?\"\n",
    "4. **Update** the policy (PPO) — carefully, with clipping\n",
    "\n",
    "That's the loop. Do it a few thousand times, and you've got yourself a fine-tuned language model that optimizes for whatever your reward model says is good.\n",
    "\n",
    "Pretty wild, right? You're literally teaching the model through trial and error, just like how you'd train a dog. Except the dog is a billion-parameter transformer and the treats are scalar reward values.\n",
    "\n",
    "Next up: we'll talk about **reference models** — how to keep one frozen copy of your policy to prevent the training from going off the rails. (Spoiler: it's simpler than you think.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
