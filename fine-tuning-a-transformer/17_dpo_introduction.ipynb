{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Introduction to Direct Preference Optimization (DPO)\n",
    "\n",
    "**A simpler alternative to RLHF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is DPO?\n",
    "\n",
    "**Direct Preference Optimization (DPO)** is a simpler approach to aligning language models with human preferences. Unlike RLHF, DPO:\n",
    "\n",
    "- **Skips the reward model** — Directly optimizes on preferences\n",
    "- **No RL** — Uses a supervised classification loss\n",
    "- **Fewer models** — Only policy and reference needed\n",
    "- **More stable** — No reward hacking or training instability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## DPO vs RLHF\n",
    "\n",
    "| Aspect | RLHF | DPO |\n",
    "|--------|------|-----|\n",
    "| **Pipeline** | SFT → RM → PPO | SFT → DPO |\n",
    "| **Models** | 4 (policy, value, reward, ref) | 2 (policy, ref) |\n",
    "| **Complexity** | High | Low |\n",
    "| **Training** | Reinforcement learning | Supervised learning |\n",
    "| **Stability** | Moderate | High |\n",
    "| **Memory** | 4x model size | 2x model size |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The Key Insight\n",
    "\n",
    "DPO derives from the observation that the optimal policy under the RLHF objective has a closed-form solution:\n",
    "\n",
    "$$\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{r(x,y)}{\\beta}\\right)$$\n",
    "\n",
    "Rearranging, we can express the reward in terms of the policy:\n",
    "\n",
    "$$r(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)$$\n",
    "\n",
    "This means we can **directly optimize the policy** without explicitly learning a reward model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## The DPO Loss\n",
    "\n",
    "The DPO objective is:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l)} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right]$$\n",
    "\n",
    "where:\n",
    "- $y_w$ = chosen (winning) response\n",
    "- $y_l$ = rejected (losing) response\n",
    "- $\\beta$ = temperature parameter\n",
    "- $\\pi_{\\text{ref}}$ = frozen reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:07.230078Z",
     "iopub.status.busy": "2025-12-06T23:30:07.229986Z",
     "iopub.status.idle": "2025-12-06T23:30:07.905557Z",
     "shell.execute_reply": "2025-12-06T23:30:07.905149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO Loss: 0.6262\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_dpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    reference_chosen_logps: torch.Tensor,\n",
    "    reference_rejected_logps: torch.Tensor,\n",
    "    beta: float = 0.1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute DPO loss.\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: Log probs of chosen under policy\n",
    "        policy_rejected_logps: Log probs of rejected under policy\n",
    "        reference_chosen_logps: Log probs of chosen under reference\n",
    "        reference_rejected_logps: Log probs of rejected under reference\n",
    "        beta: Temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "        DPO loss\n",
    "    \"\"\"\n",
    "    # Compute log ratios\n",
    "    chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "    rejected_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "    \n",
    "    # DPO loss: -log sigmoid(beta * (chosen_logratio - rejected_logratio))\n",
    "    logits = beta * (chosen_logratios - rejected_logratios)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Example\n",
    "batch_size = 4\n",
    "policy_chosen = torch.tensor([-50.0, -45.0, -55.0, -48.0])\n",
    "policy_rejected = torch.tensor([-52.0, -48.0, -58.0, -46.0])\n",
    "ref_chosen = torch.tensor([-51.0, -46.0, -56.0, -49.0])\n",
    "ref_rejected = torch.tensor([-51.0, -46.0, -56.0, -49.0])\n",
    "\n",
    "loss = compute_dpo_loss(policy_chosen, policy_rejected, ref_chosen, ref_rejected, beta=0.1)\n",
    "print(f\"DPO Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## How DPO Works\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                         DPO Training                          │\n",
    "├──────────────────────────────────────────────────────────────┤\n",
    "│                                                              │\n",
    "│  Input: (prompt, chosen, rejected) preference pairs          │\n",
    "│                                                              │\n",
    "│  ┌─────────────┐    ┌─────────────┐                         │\n",
    "│  │   Policy    │    │  Reference  │                         │\n",
    "│  │  (trainable)│    │   (frozen)  │                         │\n",
    "│  └──────┬──────┘    └──────┬──────┘                         │\n",
    "│         │                  │                                 │\n",
    "│    Log probs           Log probs                            │\n",
    "│         │                  │                                 │\n",
    "│         └────────┬─────────┘                                 │\n",
    "│                  │                                           │\n",
    "│           Compute log ratios                                 │\n",
    "│                  │                                           │\n",
    "│           DPO Loss                                           │\n",
    "│                  │                                           │\n",
    "│           Update Policy                                      │\n",
    "│                                                              │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Implicit Reward Model\n",
    "\n",
    "DPO implicitly defines a reward model:\n",
    "\n",
    "$$r(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$$\n",
    "\n",
    "The reward is the log-ratio between policy and reference, scaled by β. No separate reward model needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:07.906603Z",
     "iopub.status.busy": "2025-12-06T23:30:07.906483Z",
     "iopub.status.idle": "2025-12-06T23:30:07.908472Z",
     "shell.execute_reply": "2025-12-06T23:30:07.908199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit rewards for chosen responses: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "def compute_implicit_reward(\n",
    "    policy_logps: torch.Tensor,\n",
    "    reference_logps: torch.Tensor,\n",
    "    beta: float = 0.1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the implicit reward under DPO.\n",
    "    \"\"\"\n",
    "    return beta * (policy_logps - reference_logps)\n",
    "\n",
    "# Example\n",
    "implicit_reward = compute_implicit_reward(policy_chosen, ref_chosen, beta=0.1)\n",
    "print(f\"Implicit rewards for chosen responses: {implicit_reward.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## When to Use DPO vs RLHF\n",
    "\n",
    "**Use DPO when:**\n",
    "- You want simpler training\n",
    "- Memory is constrained\n",
    "- Stability is a priority\n",
    "- You have good preference data\n",
    "\n",
    "**Use RLHF when:**\n",
    "- You need to iterate on the reward model\n",
    "- You have lots of prompts (for rollouts)\n",
    "- Maximum flexibility is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the following notebooks, we'll cover:\n",
    "\n",
    "1. **DPO vs RLHF** — Detailed comparison\n",
    "2. **DPO Loss** — Deep dive into the math\n",
    "3. **DPO Training** — Complete implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
