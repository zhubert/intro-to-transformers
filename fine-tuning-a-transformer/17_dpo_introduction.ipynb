{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Introduction to DPO (Direct Preference Optimization)\n",
    "\n",
    "Remember RLHF? That whole pipeline with supervised fine-tuning, then training a reward model, then doing reinforcement learning with PPO?\n",
    "\n",
    "Yeah. What if I told you there's a simpler way?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is DPO?\n",
    "\n",
    "DPO stands for **Direct Preference Optimization**. Let's break down what each word actually means:\n",
    "\n",
    "- **Direct** — We skip the middleman (the reward model) and optimize directly on preferences\n",
    "- **Preference** — We're still using the same kind of data: \"this response is better than that one\"\n",
    "- **Optimization** — We're training a model to get better at something (generating preferred responses)\n",
    "\n",
    "DPO accomplishes the same goal as RLHF (aligning models with human preferences), but it does it with a completely different approach.\n",
    "\n",
    "Instead of:\n",
    "1. Train a reward model to predict which responses are good\n",
    "2. Use reinforcement learning to make the language model chase high rewards\n",
    "\n",
    "We just:\n",
    "1. Train the language model directly on preference pairs\n",
    "\n",
    "That's it. One step instead of two.\n",
    "\n",
    "(You might be thinking: \"Wait, if it's that much simpler, why did we do RLHF first?\" Great question! RLHF came first historically, and DPO is a more recent mathematical insight that shows we can skip some steps.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## DPO vs RLHF: The Practical Differences\n\nLet's make this concrete. Here's what you need for each approach:\n\n| Aspect | RLHF | DPO |\n|--------|------|-----|\n| **Pipeline** | SFT → Reward Model → PPO | SFT → DPO |\n| **Models in memory** | 4 copies (policy, value, reward, reference) | 2 copies (policy, reference) |\n| **Training complexity** | High (RL is tricky) | Low (supervised learning) |\n| **Training type** | Reinforcement learning | Classification-style loss |\n| **Stability** | Can be unstable (reward hacking!) | Generally stable |\n| **Memory needed** | ~4x your base model size | ~2x your base model size |\n\nThe memory difference is *huge* if you're training large models. With RLHF, training a 7B model means you need roughly 28GB of model weights in VRAM (4 models × 7B parameters). With DPO, that drops to 14GB (2 models × 7B).\n\nAlso, no reinforcement learning means no worrying about whether your RL algorithm is converging properly, whether the reward model is being \"hacked\" by the policy, or any of the other headaches that come with RL.\n\nDPO is just... simpler. And simpler is often better."
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## The Key Insight (Or: The Math That Makes It All Work)\n\nOkay, here comes the clever bit. The DPO paper showed something beautiful: the optimal policy under the RLHF objective has a closed-form solution.\n\nIn math terms:\n\n$$\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{r(x,y)}{\\beta}\\right)$$\n\nLet me translate that into English:\n\n- $\\pi^*(y|x)$ — The optimal policy: the probability our *best possible model* assigns to response $y$ given prompt $x$\n- $\\pi_{\\text{ref}}(y|x)$ — The reference policy: probability our *starting model* assigns to the same response\n- $r(x,y)$ — The reward: how good is response $y$ for prompt $x$?\n- $\\beta$ — Temperature parameter: controls how much we trust the reward vs staying close to the reference\n- $Z(x)$ — Normalization constant: makes probabilities sum to 1 (we can basically ignore this)\n\nThe equation says: the optimal policy is the reference policy, adjusted by exponentiating the reward.\n\nWe can rearrange this equation to solve for the reward:\n\n$$r(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)$$\n\nWait. Stop. Look at that.\n\nThe reward is just the log-ratio between the optimal policy and the reference policy (plus a constant we can ignore).\n\nThis means: **if we train a policy to match preferences, we're implicitly defining a reward model**. We don't need to train a separate reward model at all!\n\nWe can directly optimize the policy to prefer better responses over worse ones."
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## The DPO Loss Function\n",
    "\n",
    "Alright, so how do we actually train with DPO? Here's the loss function:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l)} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right]$$\n",
    "\n",
    "That looks intimidating, but let's break it down piece by piece:\n",
    "\n",
    "- $\\mathcal{L}_{\\text{DPO}}$ — The DPO loss we're trying to minimize\n",
    "- $\\pi_\\theta$ — Our policy (the model we're training), with parameters $\\theta$\n",
    "- $\\pi_{\\text{ref}}$ — The reference model (frozen, not updated)\n",
    "- $x$ — The prompt (input to the model)\n",
    "- $y_w$ — The \"winning\" response (the one humans preferred)\n",
    "- $y_l$ — The \"losing\" response (the one humans rejected)\n",
    "- $\\beta$ — Temperature parameter (same as before)\n",
    "- $\\sigma$ — The sigmoid function: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- $\\mathbb{E}$ — Expected value (average over all our training examples)\n",
    "\n",
    "The core idea:\n",
    "1. Compute how much more our policy likes the winning response vs the reference model: $\\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}$\n",
    "2. Compute how much more our policy likes the losing response vs the reference model: $\\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}$\n",
    "3. Take the difference (we want the first to be bigger than the second)\n",
    "4. Pass through sigmoid to get a probability\n",
    "5. Take the log and negate it (standard cross-entropy loss trick)\n",
    "\n",
    "In plain English: we're training the model so that the log-ratio for the winning response is larger than the log-ratio for the losing response.\n",
    "\n",
    "The model learns to increase the probability of good responses (relative to the reference) and decrease the probability of bad responses (relative to the reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:45.431520Z",
     "iopub.status.busy": "2025-12-10T21:19:45.431447Z",
     "iopub.status.idle": "2025-12-10T21:19:46.149319Z",
     "shell.execute_reply": "2025-12-10T21:19:46.149003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO Loss: 0.6262\n",
      "\n",
      "Breaking down the loss computation:\n",
      "============================================================\n",
      "Example 1:\n",
      "  Chosen log-ratio:     1.00 (policy vs ref for good response)\n",
      "  Rejected log-ratio:  -1.00 (policy vs ref for bad response)\n",
      "  Difference:           2.00 (✓ good)\n",
      "\n",
      "Example 2:\n",
      "  Chosen log-ratio:     1.00 (policy vs ref for good response)\n",
      "  Rejected log-ratio:  -2.00 (policy vs ref for bad response)\n",
      "  Difference:           3.00 (✓ good)\n",
      "\n",
      "Example 3:\n",
      "  Chosen log-ratio:     1.00 (policy vs ref for good response)\n",
      "  Rejected log-ratio:  -2.00 (policy vs ref for bad response)\n",
      "  Difference:           3.00 (✓ good)\n",
      "\n",
      "Example 4:\n",
      "  Chosen log-ratio:     1.00 (policy vs ref for good response)\n",
      "  Rejected log-ratio:   3.00 (policy vs ref for bad response)\n",
      "  Difference:          -2.00 (✗ bad - policy prefers rejected!)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_dpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    reference_chosen_logps: torch.Tensor,\n",
    "    reference_rejected_logps: torch.Tensor,\n",
    "    beta: float = 0.1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute DPO loss.\n",
    "    \n",
    "    This is the core of DPO training. We take log probabilities from both\n",
    "    the policy (trainable) and reference (frozen) models, then compute\n",
    "    a loss that encourages the policy to prefer chosen over rejected responses.\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: Log probs of chosen responses under policy\n",
    "        policy_rejected_logps: Log probs of rejected responses under policy\n",
    "        reference_chosen_logps: Log probs of chosen responses under reference\n",
    "        reference_rejected_logps: Log probs of rejected responses under reference\n",
    "        beta: Temperature parameter (controls strength of KL penalty)\n",
    "    \n",
    "    Returns:\n",
    "        DPO loss (scalar)\n",
    "    \"\"\"\n",
    "    # Compute log ratios for chosen and rejected responses\n",
    "    # These tell us: how much more does the policy like this response vs the reference?\n",
    "    chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "    rejected_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "    \n",
    "    # The logits for our binary classification:\n",
    "    # We want chosen_logratios > rejected_logratios\n",
    "    logits = beta * (chosen_logratios - rejected_logratios)\n",
    "    \n",
    "    # Standard binary cross-entropy via log-sigmoid\n",
    "    # logsigmoid(x) = log(1 / (1 + exp(-x))) = -log(1 + exp(-x))\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Example: let's create some fake log probabilities\n",
    "# (In reality these come from actually running the model, but we'll simulate them)\n",
    "batch_size = 4\n",
    "\n",
    "# Policy log probs (our trainable model)\n",
    "# More negative = lower probability (log of a small number)\n",
    "policy_chosen = torch.tensor([-50.0, -45.0, -55.0, -48.0])\n",
    "policy_rejected = torch.tensor([-52.0, -48.0, -58.0, -46.0])  # Note: sometimes policy is confused!\n",
    "\n",
    "# Reference log probs (frozen initial model)\n",
    "ref_chosen = torch.tensor([-51.0, -46.0, -56.0, -49.0])\n",
    "ref_rejected = torch.tensor([-51.0, -46.0, -56.0, -49.0])  # Reference assigns similar probs\n",
    "\n",
    "loss = compute_dpo_loss(policy_chosen, policy_rejected, ref_chosen, ref_rejected, beta=0.1)\n",
    "print(f\"DPO Loss: {loss.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Let's understand what's happening:\n",
    "print(\"Breaking down the loss computation:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(batch_size):\n",
    "    chosen_ratio = policy_chosen[i] - ref_chosen[i]\n",
    "    rejected_ratio = policy_rejected[i] - ref_rejected[i]\n",
    "    diff = chosen_ratio - rejected_ratio\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Chosen log-ratio:   {chosen_ratio.item():6.2f} (policy vs ref for good response)\")\n",
    "    print(f\"  Rejected log-ratio: {rejected_ratio.item():6.2f} (policy vs ref for bad response)\")\n",
    "    print(f\"  Difference:         {diff.item():6.2f} ({'✓ good' if diff > 0 else '✗ bad - policy prefers rejected!'})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## How DPO Works: The Big Picture\n",
    "\n",
    "Let me walk you through the training process:\n",
    "\n",
    "**Input**: Preference pairs in the format (prompt, chosen_response, rejected_response)\n",
    "\n",
    "**Setup**:\n",
    "- Start with your policy model (this is what we'll train)\n",
    "- Make a frozen copy to use as the reference model (this stays fixed)\n",
    "\n",
    "**Training loop**:\n",
    "1. For each batch of preference pairs:\n",
    "   - Run the policy model on both chosen and rejected responses → get log probabilities\n",
    "   - Run the reference model on both chosen and rejected responses → get log probabilities\n",
    "   - Compute the DPO loss (encourages policy to prefer chosen over rejected, relative to reference)\n",
    "   - Backpropagate and update the policy model\n",
    "2. Repeat until the model learns to prefer better responses\n",
    "\n",
    "**Key insight**: The reference model provides an anchor. Without it, the model could just assign probability 1.0 to chosen responses and 0.0 to rejected ones. The reference keeps the policy from drifting too far from the original behavior (this is implicitly a KL divergence penalty).\n",
    "\n",
    "Here's a simple diagram:\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────┐\n",
    "│                      DPO Training                           │\n",
    "├────────────────────────────────────────────────────────────┤\n",
    "│                                                            │\n",
    "│  Input: (prompt, chosen, rejected)                         │\n",
    "│                                                            │\n",
    "│  ┌─────────────┐              ┌─────────────┐             │\n",
    "│  │   Policy    │              │  Reference  │             │\n",
    "│  │  πθ (train) │              │  πref (frozen)            │\n",
    "│  └──────┬──────┘              └──────┬──────┘             │\n",
    "│         │                            │                     │\n",
    "│    Run on both                  Run on both               │\n",
    "│  chosen & rejected              chosen & rejected          │\n",
    "│         │                            │                     │\n",
    "│    log P(chosen)                log P(chosen)              │\n",
    "│    log P(rejected)              log P(rejected)            │\n",
    "│         │                            │                     │\n",
    "│         └──────────┬─────────────────┘                     │\n",
    "│                    │                                       │\n",
    "│         Compute log-ratios                                 │\n",
    "│    (policy vs reference)                                   │\n",
    "│                    │                                       │\n",
    "│         Compare chosen vs rejected                         │\n",
    "│                    │                                       │\n",
    "│            DPO Loss                                        │\n",
    "│         (want chosen > rejected)                           │\n",
    "│                    │                                       │\n",
    "│           Backprop & update                                │\n",
    "│            policy only                                     │\n",
    "│                                                            │\n",
    "└────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## The Implicit Reward Model\n",
    "\n",
    "Here's something cool: even though we don't train a separate reward model, DPO implicitly defines one.\n",
    "\n",
    "Remember that mathematical rearrangement from earlier? We can extract an implicit reward at any time:\n",
    "\n",
    "$$r(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$$\n",
    "\n",
    "In English: the reward for a response is just the log-ratio between how much the policy likes it vs how much the reference likes it, scaled by $\\beta$.\n",
    "\n",
    "This means:\n",
    "- If the policy assigns higher probability than the reference → positive reward\n",
    "- If the policy assigns lower probability than the reference → negative reward  \n",
    "- If they assign the same probability → zero reward\n",
    "\n",
    "The reward model is baked into the policy itself. No need for a separate model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:46.150332Z",
     "iopub.status.busy": "2025-12-10T21:19:46.150233Z",
     "iopub.status.idle": "2025-12-10T21:19:46.153559Z",
     "shell.execute_reply": "2025-12-10T21:19:46.153301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit rewards for chosen responses:\n",
      "============================================================\n",
      "Example 1: +0.100\n",
      "  → Policy likes this MORE than reference (good!)\n",
      "Example 2: +0.100\n",
      "  → Policy likes this MORE than reference (good!)\n",
      "Example 3: +0.100\n",
      "  → Policy likes this MORE than reference (good!)\n",
      "Example 4: +0.100\n",
      "  → Policy likes this MORE than reference (good!)\n",
      "\n",
      "Implicit rewards for rejected responses:\n",
      "============================================================\n",
      "Example 1: -0.100\n",
      "  → Policy likes this LESS than reference (good!)\n",
      "Example 2: -0.200\n",
      "  → Policy likes this LESS than reference (good!)\n",
      "Example 3: -0.200\n",
      "  → Policy likes this LESS than reference (good!)\n",
      "Example 4: +0.300\n",
      "  → Policy likes this MORE than reference (bad - still learning)\n",
      "\n",
      "Key insight: We want chosen rewards > rejected rewards!\n",
      "Average chosen reward:   +0.100\n",
      "Average rejected reward: -0.050\n"
     ]
    }
   ],
   "source": [
    "def compute_implicit_reward(\n",
    "    policy_logps: torch.Tensor,\n",
    "    reference_logps: torch.Tensor,\n",
    "    beta: float = 0.1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the implicit reward under DPO.\n",
    "    \n",
    "    This extracts the \"reward\" that DPO is implicitly optimizing.\n",
    "    Even though we never train a separate reward model, we can\n",
    "    compute what the reward would be for any response.\n",
    "    \n",
    "    Args:\n",
    "        policy_logps: Log probabilities under the policy\n",
    "        reference_logps: Log probabilities under the reference\n",
    "        beta: Temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "        Implicit rewards\n",
    "    \"\"\"\n",
    "    return beta * (policy_logps - reference_logps)\n",
    "\n",
    "# Let's compute the implicit rewards for our earlier examples\n",
    "print(\"Implicit rewards for chosen responses:\")\n",
    "print(\"=\" * 60)\n",
    "implicit_reward_chosen = compute_implicit_reward(policy_chosen, ref_chosen, beta=0.1)\n",
    "for i in range(batch_size):\n",
    "    print(f\"Example {i+1}: {implicit_reward_chosen[i].item():+.3f}\")\n",
    "    if implicit_reward_chosen[i] > 0:\n",
    "        print(f\"  → Policy likes this MORE than reference (good!)\")\n",
    "    elif implicit_reward_chosen[i] < 0:\n",
    "        print(f\"  → Policy likes this LESS than reference (needs more training)\")\n",
    "    else:\n",
    "        print(f\"  → Policy and reference agree\")\n",
    "\n",
    "print()\n",
    "print(\"Implicit rewards for rejected responses:\")\n",
    "print(\"=\" * 60)\n",
    "implicit_reward_rejected = compute_implicit_reward(policy_rejected, ref_rejected, beta=0.1)\n",
    "for i in range(batch_size):\n",
    "    print(f\"Example {i+1}: {implicit_reward_rejected[i].item():+.3f}\")\n",
    "    if implicit_reward_rejected[i] < 0:\n",
    "        print(f\"  → Policy likes this LESS than reference (good!)\")\n",
    "    elif implicit_reward_rejected[i] > 0:\n",
    "        print(f\"  → Policy likes this MORE than reference (bad - still learning)\")\n",
    "    else:\n",
    "        print(f\"  → Policy and reference agree\")\n",
    "\n",
    "print()\n",
    "print(\"Key insight: We want chosen rewards > rejected rewards!\")\n",
    "print(f\"Average chosen reward:   {implicit_reward_chosen.mean().item():+.3f}\")\n",
    "print(f\"Average rejected reward: {implicit_reward_rejected.mean().item():+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## When Should You Use DPO vs RLHF?\n",
    "\n",
    "This is the practical question, right? Here's my take:\n",
    "\n",
    "**Use DPO when:**\n",
    "- **You want simplicity** — Fewer moving parts, easier to debug, less can go wrong\n",
    "- **Memory is tight** — You're training a large model and can't afford 4 copies in VRAM\n",
    "- **You value stability** — You don't want to deal with RL training dynamics\n",
    "- **You have good preference data** — DPO is only as good as your (prompt, chosen, rejected) pairs\n",
    "- **You're just getting started** — DPO is easier to understand and implement\n",
    "\n",
    "**Use RLHF when:**\n",
    "- **You need to iterate on rewards** — Sometimes you want to tweak the reward model without retraining everything\n",
    "- **You have a lot of unlabeled prompts** — RLHF can generate responses and learn from them (online RL)\n",
    "- **Maximum control** — You want fine-grained control over the reward function\n",
    "- **You're already using it** — If RLHF is working for you, no need to switch!\n",
    "\n",
    "Honestly? For most people, most of the time, DPO is the better choice. It's simpler, it's more stable, and it gets you 90% of the way there with 50% of the complexity.\n",
    "\n",
    "(That said, the big AI labs still use RLHF variants for their flagship models. They have the resources to handle the complexity and want maximum control. You probably don't need that.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "We've covered the high-level ideas behind DPO. In the following notebooks, we'll dive deeper:\n",
    "\n",
    "1. **DPO vs RLHF** — A detailed comparison of both approaches, mathematically and practically\n",
    "2. **DPO Loss** — Deep dive into the loss function, its gradients, and what it's actually optimizing\n",
    "3. **DPO Training** — Complete implementation: loading models, preparing data, training loop, evaluation\n",
    "\n",
    "By the end, you'll understand not just *what* DPO is, but *why* it works and *how* to use it.\n",
    "\n",
    "Let's go!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}