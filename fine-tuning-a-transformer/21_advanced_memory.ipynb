{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# The Memory Game\n\n**Or: How to fit a 7 billion parameter model on your gaming PC**\n\nLook, here's the thing about training large language models: they're memory hogs. \n\nA 7B parameter model in full precision? That's 28 GB just for the weights. Add in the optimizer state (another 56 GB!), gradients (28 GB), and activations (20+ GB), and you're looking at 130+ GB of memory.\n\nYour RTX 4090 has 24 GB.\n\nSee the problem?\n\nBut here's the good news: with the right tricks, you can train that 7B model on consumer hardware. This notebook is your guide to the memory optimization techniques that make it possible."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Understanding the Memory Problem\n\nFirst, let's break down where all that memory goes during training.\n\nThink of GPU memory like a packed suitcase. You've got limited space, and you need to fit everything in. Here's what's taking up room:\n\n### The Memory Breakdown\n\n```\nTotal GPU Memory During Training:\n├── Model Weights        (~25-30%)  ← The model parameters themselves\n├── Optimizer State      (~50-60%)  ← Momentum & variance (biggest offender!)\n├── Gradients            (~25-30%)  ← One gradient per parameter\n├── Activations          (~10-20%)  ← Saved outputs for backprop\n└── Framework Overhead   (~5%)      ← PyTorch bookkeeping\n```\n\nNotice something? The optimizer state is typically **the largest consumer of memory**. Not the model itself!\n\nWhy? Because modern optimizers like AdamW keep track of two extra values per parameter: a momentum term and a variance term. That's 2x the model size right there.\n\nThis is why a 7B parameter model needs way more than 28 GB of memory for training. It's not just storing the weights—it's storing all the machinery needed to update those weights effectively."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Let's Do the Math\n\nNumbers make this concrete. Let's calculate the exact memory requirements for two models.\n\n### GPT-2 (124M parameters) - Full Fine-Tuning\n\n```\nModel weights (fp32):     124M params × 4 bytes = 496 MB\nOptimizer (AdamW):        124M params × 8 bytes = 992 MB  ← 2x model size!\nGradients (fp32):         124M params × 4 bytes = 496 MB\nActivations (batch=8):                            ~500 MB\nFramework overhead:                               ~100 MB\n─────────────────────────────────────────────────────────\nTotal:                                           ~2.6 GB\n```\n\nNot too bad! This fits comfortably on most GPUs.\n\n### Llama 7B - Full Fine-Tuning\n\n```\nModel weights (fp32):     7B params × 4 bytes = 28 GB\nOptimizer (AdamW):        7B params × 8 bytes = 56 GB  ← Oof.\nGradients (fp32):         7B params × 4 bytes = 28 GB\nActivations (batch=8):                          ~20 GB\nFramework overhead:                             ~2 GB\n─────────────────────────────────────────────────────\nTotal:                                         ~134 GB\n```\n\nWhoops! That's not fitting on consumer hardware.\n\n### Breaking Down the Calculations\n\n**Why 4 bytes for fp32?** Each 32-bit floating point number takes 32 bits = 4 bytes of memory.\n\n**Why 8 bytes for AdamW?** AdamW stores two additional values per parameter (first moment and second moment), each in fp32. So that's 4 bytes + 4 bytes = 8 bytes per parameter just for optimizer state.\n\n**What are activations?** During the forward pass, we save the output of each layer. We need these saved values during backpropagation to compute gradients. More layers, longer sequences, and bigger batch sizes = more activations to store.\n\nThe good news? We can dramatically reduce these numbers with the right techniques."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Technique 1: Mixed Precision Training\n\nThis is the **most impactful technique** you can apply. It cuts memory usage roughly in half with just a few lines of code.\n\n### What is Mixed Precision?\n\nInstead of using 32-bit floating point numbers (fp32) for everything, we use 16-bit numbers (fp16 or bf16) for most operations.\n\nThink of it like this: you're doing carpentry. Sometimes you need a micrometer for precise measurements. But most of the time? A ruler is fine. Mixed precision training uses the \"ruler\" (16-bit) for most work and pulls out the \"micrometer\" (32-bit) only when needed.\n\n### The Three Formats\n\n| Format | Bits | Range | Precision | Memory per Value |\n|--------|------|-------|-----------|------------------|\n| **fp32** | 32 | ±3.4 × 10³⁸ | ~7 decimal digits | 4 bytes |\n| **fp16** | 16 | ±65,504 | ~3 decimal digits | 2 bytes |\n| **bf16** | 16 | ±3.4 × 10³⁸ | ~2 decimal digits | 2 bytes |\n\n**fp16** (Float16): Traditional half precision. Small range, can overflow/underflow easily.\n\n**bf16** (BrainFloat16): Google's format. Same range as fp32 but less precision. This is the sweet spot for modern GPUs (Ampere, Ada, Hopper). No overflow issues, no loss scaling needed.\n\n**Which should you use?** If your GPU supports it (RTX 30-series or newer, A100, H100), use **bf16**. It's simpler and more robust. Otherwise, fp16 works but requires loss scaling to prevent underflow.\n\n### Memory Savings\n\n```\n7B model in fp32:     7B × 4 bytes = 28 GB\n7B model in bf16:     7B × 2 bytes = 14 GB\n                                     ↓\n                           50% reduction!\n```\n\nAnd here's the kicker: training quality is essentially identical. You're getting half the memory usage for free."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom torch.cuda.amp import autocast, GradScaler\n\n# This is what mixed precision training looks like in code.\n# It's surprisingly simple!\n\n# For fp16, we need a GradScaler to prevent underflow\nscaler = GradScaler()  # (not needed for bf16)\n\ndef train_step_mixed_precision(model, batch, optimizer):\n    \"\"\"A training step using mixed precision.\n    \n    The key is the `autocast` context manager - it automatically\n    casts operations to the specified dtype when beneficial.\n    \"\"\"\n    optimizer.zero_grad()\n    \n    # Forward pass in lower precision (bf16 or fp16)\n    # PyTorch automatically figures out which ops should be bf16\n    # and which should stay fp32 (like loss calculation)\n    with autocast(dtype=torch.bfloat16):  # or torch.float16\n        outputs = model(batch[\"input_ids\"])\n        loss = outputs.loss\n    \n    # Backward pass\n    # For fp16: scale the loss to prevent gradient underflow\n    # For bf16: this is unnecessary but doesn't hurt\n    scaler.scale(loss).backward()\n    \n    # Optimizer step with unscaling\n    scaler.step(optimizer)\n    scaler.update()\n    \n    return loss.item()\n\nprint(\"Mixed Precision Memory Savings:\")\nprint(\"  FP32 → BF16: ~50% reduction in memory\")\nprint(\"  FP32 → FP16: ~50% reduction in memory\")\nprint()\nprint(\"Speed bonus: Training is often 20-30% faster too!\")\nprint(\"(Modern GPUs have dedicated hardware for fp16/bf16 operations)\")\nprint()\nprint(\"Quality impact: Negligible for most tasks\")\nprint(\"(We've trained hundreds of models this way - it just works)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Technique 2: LoRA (Low-Rank Adaptation)\n\nIf mixed precision is a memory reducer, LoRA is a memory **destroyer**. In the best way.\n\nRemember how I said the optimizer state is the biggest memory hog? LoRA solves this by **freezing the base model** and training only tiny adapter layers.\n\n### The Core Idea\n\nInstead of updating all 7 billion parameters, we freeze them and add small \"adapter\" matrices that we train instead.\n\nThink of it like this: you've got a massive reference book (the base model). Instead of rewriting the whole book, you add sticky notes (LoRA adapters) with corrections and additions. The book stays the same; only the notes change.\n\n### The Math\n\nFor each weight matrix W, LoRA adds two small matrices A and B:\n\n```\nOriginal:    W (full rank, millions of parameters)\nLoRA adds:   W + (A × B)\n             ↑    ↑   ↑\n         frozen  rank r matrices\n                 (tiny!)\n```\n\nIf W is 4096×4096, that's 16.7M parameters.\nBut A and B with rank r=16? That's only 4096×16 + 16×4096 = 131K parameters!\n\nThat's **128x fewer parameters** to train.\n\n### Memory Impact\n\n```\nFull Fine-Tuning (Llama 7B):\n  Trainable params:   7,000,000,000\n  Optimizer state:    56 GB  (8 bytes per param)\n  Gradients:          28 GB  (4 bytes per param)\n\nLoRA with r=16 (Llama 7B):\n  Trainable params:   16,777,216  (0.24% of model!)\n  Optimizer state:    134 MB      (418x reduction!)\n  Gradients:          67 MB       (418x reduction!)\n```\n\nThe base model stays frozen, so we only need optimizer state and gradients for those tiny adapter matrices.\n\nIt's kind of absurd how well this works."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's see the actual numbers for LoRA memory savings.\n# This is why LoRA has become the standard approach for fine-tuning large models.\n\nprint(\"LoRA Memory Comparison (Llama 7B in BF16)\")\nprint(\"=\" * 60)\nprint()\n\nprint(\"Full Fine-Tuning:\")\nprint(\"  Base model:         14 GB (all parameters trainable)\")\nprint(\"  Optimizer state:    56 GB (momentum + variance for all params)\")\nprint(\"  Gradients:          14 GB (one gradient per parameter)\")\nprint(\"  ─────────────────────────\")\nprint(\"  Total:              84 GB (before activations!)\")\nprint()\n\nprint(\"LoRA (r=16):\")\nprint(\"  Base model:         14 GB (frozen - can even be quantized!)\")\nprint(\"  LoRA adapters:      67 MB (only these are trainable)\")\nprint(\"  Optimizer state:    268 MB (only for LoRA adapters)\")\nprint(\"  Gradients:          67 MB (only for LoRA adapters)\")\nprint(\"  ─────────────────────────\")\nprint(\"  Total:              14.4 GB (before activations)\")\nprint()\nprint(\"  Memory reduction:   5.8x smaller!\")\nprint(\"  And we can combine this with quantization...\")\nprint()\n\nprint(\"Choosing the Rank (r)\")\nprint(\"=\" * 60)\nprint()\nprint(\"The rank controls capacity vs. memory trade-off:\")\nprint()\nprint(\"  r=4:    ~33 MB trainable\")\nprint(\"          Minimal memory, but may underfit complex tasks\")\nprint()\nprint(\"  r=8:    ~67 MB trainable\")\nprint(\"          Good for simple fine-tuning tasks\")\nprint()\nprint(\"  r=16:   ~134 MB trainable\")\nprint(\"          The sweet spot - recommended default\")\nprint()\nprint(\"  r=32:   ~268 MB trainable\")\nprint(\"          High capacity for complex tasks\")\nprint()\nprint(\"  r=64:   ~536 MB trainable\")\nprint(\"          When you need more expressiveness\")\nprint()\nprint(\"Rule of thumb: Start with r=16. Increase if underfitting.\")\nprint(\"(Most tasks work great with r=16, honestly)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Technique 3: Gradient Accumulation\n\nOkay, this one's clever.\n\nYou know how larger batch sizes generally lead to more stable training? But bigger batches need more memory (you're processing more samples at once).\n\nGradient accumulation lets you have your cake and eat it too.\n\n### The Trick\n\nInstead of computing gradients and updating weights after every batch, we accumulate gradients across multiple small batches, then update once.\n\n```\nEffective batch size = per_device_batch_size × gradient_accumulation_steps\n\nMemory usage = per_device_batch_size (not effective batch size!)\n```\n\nSo if you can only fit batch_size=2 in memory, but you want the training stability of batch_size=32, you can do:\n\n```python\nper_device_batch_size = 2\ngradient_accumulation_steps = 16\n# Effective batch size = 2 × 16 = 32\n```\n\nYou get the benefits of a large batch size with the memory footprint of a small one.\n\n### The Trade-off\n\n**Pro:** Train with larger effective batch sizes without OOM errors.\n\n**Con:** Slightly slower (you're doing more forward passes before each update).\n\nBut slower beats \"doesn't fit in memory\" every time."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Here's what gradient accumulation looks like in practice.\n# The key insight: gradients ADD together, so we can accumulate them\n# over multiple batches before applying an update.\n\ndef train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):\n    \"\"\"Train with gradient accumulation.\n    \n    We process `accumulation_steps` batches, accumulating gradients,\n    then update the model once.\n    \"\"\"\n    optimizer.zero_grad()\n    \n    for i, batch in enumerate(dataloader):\n        # Forward pass\n        outputs = model(batch[\"input_ids\"])\n        loss = outputs.loss\n        \n        # Important: Scale the loss by accumulation steps\n        # (so the effective learning rate stays consistent)\n        loss = loss / accumulation_steps\n        \n        # Backward pass - this ADDS to existing gradients\n        loss.backward()\n        \n        # Only update weights every `accumulation_steps` batches\n        if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()  # Clear for next accumulation\n\n# Let's see the impact\nprint(\"Gradient Accumulation Example:\")\nprint(\"=\" * 60)\nprint()\nprint(\"Scenario: You can fit batch_size=4 in memory\")\nprint(\"          But you want effective batch_size=32\")\nprint()\nprint(\"Solution:\")\nprint(\"  per_device_batch_size = 4\")\nprint(\"  gradient_accumulation_steps = 8\")\nprint()\nprint(\"Result:\")\nprint(\"  Effective batch size: 4 × 8 = 32 ✓\")\nprint(\"  Memory usage: Only 4 samples at a time ✓\")\nprint(\"  Training stability: Same as batch_size=32 ✓\")\nprint()\nprint(\"This is why you'll see accumulation_steps in almost every\")\nprint(\"fine-tuning config - it's free effective batch size!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Technique 4: Gradient Checkpointing\n\nThis technique trades computation for memory. It's brilliant in a slightly painful way.\n\n### The Problem\n\nDuring the forward pass, we need to save the output of every layer. Why? Because during backpropagation, we need those saved values to compute gradients.\n\nFor a 32-layer transformer processing a batch of 8 sequences, that's a LOT of saved activations. They can easily use 20+ GB of memory.\n\n### The Solution\n\nWhat if we... didn't save them?\n\nGradient checkpointing only saves activations at certain \"checkpoint\" layers (say, every 4th layer). During backpropagation, when we need an activation we didn't save, we recompute it on the fly.\n\nThink of it like taking notes during a lecture. You could transcribe everything (high memory), or you could write down key points and fill in the details later (low memory, more work).\n\n```\nWithout Gradient Checkpointing:\n  Forward pass:  Compute activations → Save all of them\n  Backward pass: Use saved activations → Fast\n  Memory: HIGH\n\nWith Gradient Checkpointing:\n  Forward pass:  Compute activations → Save only checkpoints\n  Backward pass: Recompute missing activations → Slower\n  Memory: LOW\n```\n\n### The Trade-off\n\nYou're recomputing activations during backprop, so training is slower (typically 20-30% slower).\n\nBut the memory savings are huge: 50-80% reduction in activation memory.\n\nWhen you're memory-constrained, this is a lifesaver."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM\n\n# Gradient checkpointing is ridiculously easy to enable.\n# One method call. That's it.\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nmodel.gradient_checkpointing_enable()\n\nprint(\"Gradient Checkpointing Enabled!\")\nprint(\"=\" * 60)\nprint()\nprint(\"Memory Impact (Llama 7B, batch=8, seq_len=2048):\")\nprint()\nprint(\"  Without checkpointing:\")\nprint(\"    Activations: ~20 GB\")\nprint(\"    (Every layer output saved)\")\nprint()\nprint(\"  With checkpointing:\")\nprint(\"    Activations: ~5 GB\")\nprint(\"    (Only checkpoint layers saved, rest recomputed)\")\nprint()\nprint(\"  Memory reduction: 75%!\")\nprint()\nprint(\"Speed Trade-off:\")\nprint(\"  Training is ~20-30% slower\")\nprint(\"  (We're recomputing activations during backward pass)\")\nprint()\nprint(\"When to use it:\")\nprint(\"  - You're hitting OOM errors\")\nprint(\"  - You're training with long sequences\")\nprint(\"  - You want to increase batch size\")\nprint()\nprint(\"When to skip it:\")\nprint(\"  - You have plenty of memory\")\nprint(\"  - Speed is critical\")\nprint(\"  - You're using very short sequences\")\nprint()\nprint(\"Real talk: If you're fine-tuning on consumer hardware,\")\nprint(\"you're probably using this. The memory savings are just\")\nprint(\"too good to pass up.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Technique 5: Model Quantization\n\nNow we're getting into the heavy artillery.\n\nQuantization is like... aggressive compression for neural networks. Instead of storing weights in 16-bit or 32-bit precision, we use 8-bit or even 4-bit integers.\n\nSounds crazy, right? How can you possibly represent a neural network weight that might be 0.0123456789 using just 4 bits (16 possible values)?\n\n### The Magic\n\nModern quantization techniques (like NormalFloat4, or \"nf4\") are **calibrated** to the distribution of neural network weights. Most weights cluster around zero, with a long tail. NF4 packs more precision where it matters and less where it doesn't.\n\nIt's like how JPEG compression works: throw away information humans won't notice. Here, we throw away precision the model doesn't need.\n\n### The Numbers\n\n| Precision | Memory (7B model) | Quality | Notes |\n|-----------|-------------------|---------|-------|\n| **FP32** | 28 GB | 100% | Original precision |\n| **BF16** | 14 GB | ~99.9% | Standard for training |\n| **INT8** | 7 GB | ~99% | Good for inference |\n| **NF4** | 3.5 GB | 95-98% | The QLoRA breakthrough |\n\nYes, you read that right. A 7B model in 4-bit takes **3.5 GB**. That fits on a laptop GPU.\n\n### The Catch\n\nQuantized models can only be used for **inference** or as frozen base models for fine-tuning (like with LoRA).\n\nYou can't train a quantized model directly. The low precision causes training to diverge.\n\nBut combined with LoRA (which adds trainable adapters on top), you get QLoRA: 4-bit base model + 16-bit LoRA adapters = magic."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's load a quantized model.\n# This is the secret sauce that makes fine-tuning large models\n# accessible to everyone.\n\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\n# Configure 4-bit quantization (the QLoRA approach)\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Use 4-bit quantization\n    \n    # Use NormalFloat4 (nf4) - specifically designed for neural network weights\n    # This gives better quality than standard 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",\n    \n    # Double quantization: quantize the quantization constants too\n    # (yes, really - saves even more memory with minimal quality loss)\n    bnb_4bit_use_double_quant=True,\n    \n    # When we actually compute with these weights (forward pass),\n    # convert them to bf16 for the calculation\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# Load the model in 4-bit\n# Note: This requires the `bitsandbytes` library\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2.5-3B\",\n    quantization_config=quantization_config,\n    device_map=\"auto\",  # Automatically distribute across available devices\n)\n\nprint(\"QLoRA Setup: 4-bit Model + LoRA Adapters\")\nprint(\"=\" * 60)\nprint()\nprint(\"This is the state-of-the-art approach for fine-tuning\")\nprint(\"large models on consumer hardware.\")\nprint()\nprint(\"Memory breakdown (7B model):\")\nprint(\"  Base model (4-bit):     3.5 GB ← Quantized!\")\nprint(\"  LoRA adapters (bf16):   67 MB  ← Trainable\")\nprint(\"  Optimizer state:        268 MB ← Only for LoRA\")\nprint(\"  Gradients:              67 MB  ← Only for LoRA\")\nprint(\"  Activations (bs=8):     ~5 GB  ← With gradient checkpointing\")\nprint(\"  ───────────────────────────────\")\nprint(\"  Total:                  ~9 GB\")\nprint()\nprint(\"That fits on an RTX 3080 (10 GB)!\")\nprint(\"Or even an RTX 3060 (12 GB) with room to spare.\")\nprint()\nprint(\"The quality hit? Surprisingly small. QLoRA models often\")\nprint(\"perform within 1-2% of full precision fine-tuning.\")\nprint()\nprint(\"This technique democratized LLM fine-tuning. Before QLoRA,\")\nprint(\"you needed expensive multi-GPU setups. Now? Your gaming PC\")\nprint(\"can fine-tune a 7B model. Pretty wild.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Profiling Memory Usage\n\nBefore you can optimize memory, you need to measure it.\n\nHere's a simple profiling function that shows exactly where your memory is going. This is invaluable when debugging OOM errors or trying to squeeze more performance out of your GPU."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport gc\n\ndef profile_memory(fn, label=\"Operation\"):\n    \"\"\"Profile GPU memory usage of a function.\n    \n    This shows you:\n    - Starting memory (what was already allocated)\n    - Ending memory (after the operation)\n    - Delta (how much the operation added)\n    - Peak (the highest memory point during execution)\n    \n    The delta vs peak difference tells you about temporary allocations.\n    \"\"\"\n    if not torch.cuda.is_available():\n        print(\"No CUDA available - can't profile GPU memory\")\n        print(\"(This is fine if you're running on CPU)\")\n        return\n    \n    # Clean up before measuring\n    torch.cuda.reset_peak_memory_stats()\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    start_mem = torch.cuda.memory_allocated()\n    \n    # Run the function\n    result = fn()\n    \n    end_mem = torch.cuda.memory_allocated()\n    peak_mem = torch.cuda.max_memory_allocated()\n    \n    print(f\"\\n{label}\")\n    print(f\"  Start:  {start_mem / 1e9:.2f} GB\")\n    print(f\"  End:    {end_mem / 1e9:.2f} GB\")\n    print(f\"  Delta:  {(end_mem - start_mem) / 1e9:.2f} GB\")\n    print(f\"  Peak:   {peak_mem / 1e9:.2f} GB\")\n    \n    if peak_mem > end_mem:\n        print(f\"  (Peak-End: {(peak_mem - end_mem) / 1e9:.2f} GB temporary allocation)\")\n    \n    return result\n\n# Check current memory usage\nprint(\"Current Memory Status:\")\nprint(\"=\" * 60)\n\nif torch.cuda.is_available():\n    current = torch.cuda.memory_allocated()\n    peak = torch.cuda.max_memory_allocated()\n    total = torch.cuda.get_device_properties(0).total_memory\n    \n    print(f\"Current allocation: {current / 1e9:.2f} GB\")\n    print(f\"Peak allocation:    {peak / 1e9:.2f} GB\")\n    print(f\"Total GPU memory:   {total / 1e9:.2f} GB\")\n    print(f\"Available:          {(total - current) / 1e9:.2f} GB\")\n    print()\n    print(\"Use this profiler to understand where your memory goes!\")\n    print()\n    print(\"Example:\")\n    print(\"  profile_memory(\")\n    print(\"      lambda: model.forward(batch),\")\n    print(\"      label='Forward pass'\")\n    print(\"  )\")\nelse:\n    print(\"No CUDA available\")\n    print(\"(Running on CPU - memory profiling won't work)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## When Things Go Wrong: Debugging OOM Errors\n\nThe dreaded \"CUDA out of memory\" error. We've all been there.\n\nHere's your systematic debugging checklist.\n\n### The Quick Fixes (Try These First)\n\n**1. Cut your batch size in half**\n   - If batch_size=8 OOMs, try batch_size=4\n   - Use gradient accumulation to maintain effective batch size\n   - This fixes 80% of OOM issues\n\n**2. Enable gradient checkpointing**\n   - One line: `model.gradient_checkpointing_enable()`\n   - Saves 50-80% on activation memory\n   - Especially important for long sequences\n\n**3. Use gradient accumulation**\n   - Smaller batches, same training dynamics\n   - Costs nothing but a bit of training time\n\n**4. Clear the CUDA cache**\n   - `torch.cuda.empty_cache()`\n   - Defragments memory, sometimes helps\n   - Won't free memory that's actually in use\n\n### Common OOM Causes\n\n| Problem | Symptom | Solution |\n|---------|---------|----------|\n| Batch size too large | OOM during forward pass | Reduce batch_size by 50% |\n| Sequences too long | OOM with long inputs | Truncate to 512 or 1024 tokens |\n| Memory leak | OOM after many steps | Use `.item()` or `.detach()` on tensors |\n| Fragmented memory | OOM despite \"available\" memory | `torch.cuda.empty_cache()` |\n| Multiple models | OOM at model load | Delete old models: `del model; gc.collect()` |\n| Full precision | Just generally tight | Switch to BF16/FP16 |\n\n### The Nuclear Option\n\nIf nothing else works:\n\n```python\n# QLoRA: 4-bit model + LoRA\nquantization_config = BitsAndBytesConfig(load_in_4bit=True, ...)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=quantization_config\n)\nmodel.gradient_checkpointing_enable()\n\n# LoRA config\nlora_config = LoraConfig(r=8, ...)  # Smaller rank if needed\nmodel = get_peft_model(model, lora_config)\n\n# Training config\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1,  # Minimum\n    gradient_accumulation_steps=32,  # Large accumulation\n    ...\n)\n```\n\nThis'll fit almost anything, anywhere. It's slow, but it works."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Putting It All Together: A Memory Optimization Strategy\n\nOkay, we've covered a lot of techniques. How do you actually use them?\n\nHere's my recommended approach, in order. Think of it as a ladder—start at step 1, and only climb higher if you need to.\n\n### Step 1: The Essentials (Always Do This)\n\nThese are free wins. Apply them to every training run.\n\n**1. Mixed precision (BF16 or FP16)**\n   - 50% memory reduction\n   - Often faster training\n   - Literally no downside on modern GPUs\n\n**2. LoRA (if training large models >1B params)**\n   - 80-95% reduction in optimizer memory\n   - No speed penalty\n   - Quality is excellent for most tasks\n\n**3. Find your maximum batch size**\n   - Start with batch_size=8\n   - If OOM, halve it\n   - If comfortable, double it\n   - Use gradient accumulation to hit your target effective batch size\n\n### Step 2: If You're Still Running Out of Memory\n\n**4. Gradient checkpointing**\n   - 50-80% reduction in activation memory\n   - 20-30% slower training\n   - Essential for long sequences\n\n**5. Gradient accumulation**\n   - Lets you use smaller batches\n   - Maintains training stability\n   - Slight speed cost\n\n### Step 3: The Extreme Measures\n\n**6. 4-bit quantization (QLoRA)**\n   - 75% reduction in model memory\n   - Small quality hit (1-2%)\n   - Can only be used with LoRA\n\n**7. CPU offloading (DeepSpeed ZeRO-3)**\n   - Offloads optimizer state to CPU RAM\n   - 50-70% GPU memory reduction\n   - 60-80% slower training\n   - Use as last resort\n\n### The Rule\n\nStart simple. Add complexity only when you hit limits.\n\nDon't start with QLoRA + checkpointing + accumulation + offloading just because you can. Start with BF16 + LoRA, see how far that gets you, and add techniques as needed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Here's what a fully optimized training setup looks like.\n# This is the configuration that lets you train 7B models on consumer GPUs.\n\nprint(\"Complete Memory Optimization Example\")\nprint(\"=\" * 60)\nprint()\nprint(\"Goal: Fine-tune Llama 7B on a 12 GB GPU (RTX 3060/3080)\")\nprint()\nprint(\"The recipe:\")\nprint()\n\nprint(\"\"\"\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nimport torch\n\n# Step 1: Load model in 4-bit (QLoRA)\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",              # NormalFloat4\n    bnb_4bit_use_double_quant=True,         # Double quantization\n    bnb_4bit_compute_dtype=torch.bfloat16   # Compute in bf16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\n# Step 2: Add LoRA adapters\nlora_config = LoraConfig(\n    r=16,              # Rank (balance capacity vs memory)\n    lora_alpha=32,     # Scaling factor\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Which layers to adapt\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)\n\n# Step 3: Enable gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Step 4: Configure training with small batches + accumulation\ntraining_args = TrainingArguments(\n    output_dir=\"./llama-7b-finetuned\",\n    \n    # Memory settings\n    per_device_train_batch_size=4,      # Small batch per GPU\n    gradient_accumulation_steps=8,       # Effective batch = 4 × 8 = 32\n    \n    # Precision\n    bf16=True,                           # Use BF16 (if supported)\n    \n    # Training\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    \n    # Checkpointing\n    save_strategy=\"steps\",\n    save_steps=100,\n)\n\n# Step 5: Train!\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    max_seq_length=512,  # Reasonable sequence length\n)\n\ntrainer.train()\n\"\"\")\n\nprint()\nprint(\"Memory Breakdown:\")\nprint(\"  Base model (4-bit):     ~3.5 GB\")\nprint(\"  LoRA params (bf16):     ~67 MB\")\nprint(\"  Optimizer state:        ~268 MB\")\nprint(\"  Gradients:              ~67 MB\")\nprint(\"  Activations (bs=4):     ~5 GB (with checkpointing)\")\nprint(\"  Framework overhead:     ~500 MB\")\nprint(\"  ──────────────────────────────\")\nprint(\"  Total:                  ~9.4 GB\")\nprint()\nprint(\"Fits comfortably on a 12 GB GPU with room to spare!\")\nprint()\nprint(\"This same approach scales:\")\nprint(\"  - 13B model on 24 GB GPU (RTX 4090)\")\nprint(\"  - 70B model on 48 GB GPU (A6000)\")\nprint()\nprint(\"The QLoRA revolution made this all possible.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Quick Reference: Memory Optimization Techniques\n\nHere's your cheat sheet. Bookmark this cell.\n\n| Technique | Memory Savings | Speed Impact | Quality Impact | When to Use |\n|-----------|----------------|--------------|----------------|-------------|\n| **Mixed Precision (BF16)** | 50% | +20% faster | Negligible | Always |\n| **LoRA** | 80-95% optimizer | None | Excellent | Large models (>1B) |\n| **Gradient Accumulation** | Enables larger effective batch | -10-20% | None | Memory-limited |\n| **Gradient Checkpointing** | 50-80% activations | -20-30% | None | Long sequences or tight memory |\n| **Quantization (4-bit)** | 75% model | -10-20% | Small (~1-2%) | Extreme constraints |\n| **CPU Offloading** | 50-70% optimizer | -60-80% | None | Last resort |\n\n### The Impact Hierarchy\n\n**Most impact for least effort:**\n1. Mixed precision (BF16)\n2. LoRA\n3. Gradient checkpointing\n\n**When you're desperate:**\n4. Quantization (4-bit)\n5. CPU offloading\n\n### Model Size → GPU Requirements\n\nWith all optimizations (QLoRA + gradient checkpointing + BF16):\n\n| Model Size | Min GPU Memory | Example Cards |\n|------------|----------------|---------------|\n| 3B | 6 GB | RTX 3060, RTX 2060 |\n| 7B | 10 GB | RTX 3080, RTX 2080 Ti |\n| 13B | 16 GB | RTX 4080, RTX 3090 |\n| 30B | 24 GB | RTX 4090, A5000 |\n| 70B | 48 GB | A6000, 2×RTX 4090 |\n\nThese are realistic fine-tuning numbers, not theoretical minimums.\n\n### Remember\n\nThe goal isn't to use every technique. The goal is to use the **minimum** number of techniques needed to fit your model in memory while maintaining reasonable training speed.\n\nStart simple. Add complexity only when needed."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What's Next?\n\nYou now have the tools to fit large models on limited hardware. But memory is just one piece of the puzzle.\n\nNext up: **hyperparameter tuning**. Learning rate schedules, warmup steps, weight decay—all the knobs you can turn to make your model actually learn well (not just fit in memory).\n\nBecause a model that fits in memory but doesn't train well is... not very useful.\n\n(But hey, at least it runs!)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}