{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Optimization\n",
    "\n",
    "**Advanced techniques for training large models on limited hardware**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Memory Challenge\n",
    "\n",
    "Training large language models requires substantial GPU memory. Understanding and optimizing memory usage is crucial for:\n",
    "\n",
    "- **Fitting larger models** on your hardware\n",
    "- **Using larger batch sizes** for more stable training\n",
    "- **Faster training** through better GPU utilization\n",
    "- **Cost reduction** by using smaller/cheaper GPUs\n",
    "\n",
    "### Memory Breakdown\n",
    "\n",
    "```\n",
    "Total GPU Memory Usage:\n",
    "+-- Model Weights        (~25-30%)\n",
    "+-- Optimizer State      (~50-60%)  <-- Largest component!\n",
    "+-- Gradients            (~25-30%)\n",
    "+-- Activations          (~10-20%)  <-- Depends on batch size\n",
    "+-- Framework Overhead   (~5%)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Examples\n",
    "\n",
    "**GPT-2 (124M parameters) full fine-tuning:**\n",
    "\n",
    "```\n",
    "Model weights (fp32):     124M x 4 bytes = 496 MB\n",
    "Optimizer (AdamW):        124M x 8 bytes = 992 MB  (momentum + variance)\n",
    "Gradients (fp32):         124M x 4 bytes = 496 MB\n",
    "Activations (batch=8):                    ~500 MB\n",
    "Framework overhead:                       ~100 MB\n",
    "----------------------------------------------------\n",
    "Total:                                    ~2.6 GB\n",
    "```\n",
    "\n",
    "**Llama 7B full fine-tuning:**\n",
    "\n",
    "```\n",
    "Model weights (fp32):     7B x 4 bytes = 28 GB\n",
    "Optimizer (AdamW):        7B x 8 bytes = 56 GB\n",
    "Gradients (fp32):         7B x 4 bytes = 28 GB\n",
    "Activations (batch=8):                   ~20 GB\n",
    "Framework overhead:                      ~2 GB\n",
    "----------------------------------------------------\n",
    "Total:                                   ~134 GB  <-- Won't fit on consumer GPUs!\n",
    "```\n",
    "\n",
    "The optimizer state is typically the largest memory consumer, often requiring 2x the model size for AdamW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 1: Mixed Precision Training\n",
    "\n",
    "**Most impactful technique** - Reduces memory by 50% with minimal code changes.\n",
    "\n",
    "### FP16 vs BF16 vs FP32\n",
    "\n",
    "| Format | Bits | Range | Precision | Memory |\n",
    "|--------|------|-------|-----------|--------|\n",
    "| FP32 | 32 | +/-3.4e38 | ~7 decimal digits | 4 bytes |\n",
    "| FP16 | 16 | +/-65,504 | ~3 decimal digits | 2 bytes |\n",
    "| BF16 | 16 | +/-3.4e38 | ~2 decimal digits | 2 bytes |\n",
    "\n",
    "**BF16** is preferred for modern GPUs (Ampere/Ada) - same range as FP32, no loss scaling needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Mixed precision training example\n",
    "scaler = GradScaler()  # For FP16 only, not needed for BF16\n",
    "\n",
    "def train_step_mixed_precision(model, batch, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass in mixed precision\n",
    "    with autocast(dtype=torch.bfloat16):  # or torch.float16\n",
    "        outputs = model(batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    scaler.scale(loss).backward()  # Scale loss to prevent underflow\n",
    "    \n",
    "    # Optimizer step with unscaling\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\"Memory Savings with Mixed Precision:\")\n",
    "print(\"  FP32 -> BF16: ~50% reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 2: LoRA (Low-Rank Adaptation)\n",
    "\n",
    "**Dramatic memory reduction** by training only a tiny fraction of parameters.\n",
    "\n",
    "```\n",
    "Full Fine-Tuning:\n",
    "  Trainable params: 7,000,000,000\n",
    "  Optimizer state:  56 GB\n",
    "\n",
    "LoRA (r=16):\n",
    "  Trainable params: 16,777,216  (0.24% of model!)\n",
    "  Optimizer state:  134 MB      (418x reduction!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA memory savings\n",
    "print(\"LoRA Memory Savings (Llama 7B):\")\n",
    "print()\n",
    "print(\"Full Fine-Tuning (BF16):\")\n",
    "print(\"  Model:      14 GB (trainable)\")\n",
    "print(\"  Optimizer:  56 GB\")\n",
    "print(\"  Gradients:  14 GB\")\n",
    "print(\"  Total:      84 GB + activations\")\n",
    "print()\n",
    "print(\"LoRA (BF16, r=16):\")\n",
    "print(\"  Model:      14 GB (frozen, can be quantized)\")\n",
    "print(\"  LoRA:       67 MB (trainable)\")\n",
    "print(\"  Optimizer:  268 MB (only for LoRA)\")\n",
    "print(\"  Gradients:  67 MB (only for LoRA)\")\n",
    "print(\"  Total:      14.4 GB + activations (5.8x reduction!)\")\n",
    "print()\n",
    "print(\"Rank selection for memory:\")\n",
    "print(\"  r=4:   ~33 MB (minimum, may underfit)\")\n",
    "print(\"  r=8:   ~67 MB (good for simple tasks)\")\n",
    "print(\"  r=16:  ~134 MB (default, recommended)\")\n",
    "print(\"  r=32:  ~268 MB (high capacity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 3: Gradient Accumulation\n",
    "\n",
    "**Simulate larger batch sizes** without additional memory.\n",
    "\n",
    "```\n",
    "Effective batch size = batch_size x gradient_accumulation_steps\n",
    "Memory usage = batch_size_per_step (not effective_batch_size!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient accumulation implementation\n",
    "def train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Scale loss by accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # Backward pass (accumulates gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights every accumulation_steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "print(\"Gradient Accumulation:\")\n",
    "print(\"  batch_size=4, accumulation_steps=8\")\n",
    "print(\"  Effective batch size: 32\")\n",
    "print(\"  Memory: Only 4 samples at a time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 4: Gradient Checkpointing\n",
    "\n",
    "**Trade computation for memory** by recomputing activations during backward pass.\n",
    "\n",
    "```\n",
    "Without Gradient Checkpointing:\n",
    "  Forward:  Save all activations -> High memory\n",
    "  Backward: Use saved activations -> Fast\n",
    "\n",
    "With Gradient Checkpointing:\n",
    "  Forward:  Save only checkpoint activations -> Low memory\n",
    "  Backward: Recompute from checkpoints -> Slower, low memory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"Gradient Checkpointing Memory Savings:\")\n",
    "print()\n",
    "print(\"Llama 7B training (batch_size=8, seq_length=2048):\")\n",
    "print(\"  Without checkpointing: ~20 GB activations\")\n",
    "print(\"  With checkpointing:    ~5 GB activations\")\n",
    "print(\"  Savings: 75% reduction\")\n",
    "print()\n",
    "print(\"Trade-off: 20-30% slower training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 5: Model Quantization\n",
    "\n",
    "**Load models in reduced precision** (4-bit or 8-bit) to dramatically reduce memory.\n",
    "\n",
    "| Precision | Memory | Quality |\n",
    "|-----------|--------|--------|\n",
    "| FP32 | 28 GB (7B) | 100% |\n",
    "| BF16 | 14 GB | 99.9% |\n",
    "| 8-bit | 7 GB | ~99% |\n",
    "| 4-bit | 3.5 GB | 95-98% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization with bitsandbytes\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 4-bit quantization (QLoRA)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",        # NormalFloat4 (better than standard)\n",
    "    bnb_4bit_use_double_quant=True,   # Double quantization for more savings\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Computation dtype\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"meta-llama/Llama-3.2-7B\",\n",
    "#     quantization_config=quantization_config,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "print(\"QLoRA (4-bit + LoRA) Setup:\")\n",
    "print(\"  Model (4-bit):          3.5 GB\")\n",
    "print(\"  LoRA adapters (BF16):   67 MB\")\n",
    "print(\"  Optimizer state:        268 MB\")\n",
    "print(\"  Gradients:              67 MB\")\n",
    "print(\"  Activations (bs=8):     5 GB (with checkpointing)\")\n",
    "print(\"  ------------------------------------\")\n",
    "print(\"  Total:                  ~9 GB (fits on RTX 3080!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def profile_memory(fn, label=\"\"):\n",
    "    \"\"\"Profile memory usage of a function.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available for profiling\")\n",
    "        return\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    result = fn()\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    \n",
    "    print(f\"\\n{label}\")\n",
    "    print(f\"  Start: {start_mem / 1e9:.2f} GB\")\n",
    "    print(f\"  End:   {end_mem / 1e9:.2f} GB\")\n",
    "    print(f\"  Delta: {(end_mem - start_mem) / 1e9:.2f} GB\")\n",
    "    print(f\"  Peak:  {peak_mem / 1e9:.2f} GB\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Memory monitoring\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"Peak memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging OOM Errors\n",
    "\n",
    "**OOM Debugging Checklist:**\n",
    "\n",
    "1. **Reduce batch size by 50%**\n",
    "2. **Enable gradient checkpointing**\n",
    "3. **Use gradient accumulation**\n",
    "4. **Check for memory leaks** (storing tensors accidentally)\n",
    "5. **Clear cache** with `torch.cuda.empty_cache()`\n",
    "\n",
    "**Common OOM Causes:**\n",
    "\n",
    "| Cause | Solution |\n",
    "|-------|----------|\n",
    "| Batch size too large | Reduce by 50%, use gradient accumulation |\n",
    "| Sequence length too long | Truncate to 512 or 1024 tokens |\n",
    "| Accumulating tensors | Use `.item()` or `.detach()` |\n",
    "| Fragmented memory | `torch.cuda.empty_cache()` |\n",
    "| Multiple models | Delete unused models |\n",
    "| Full precision | Use BF16/FP16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Optimization Strategy\n",
    "\n",
    "**Recommended approach:**\n",
    "\n",
    "### Step 1: Essential Optimizations (Always Apply)\n",
    "1. Mixed precision (BF16/FP16)\n",
    "2. LoRA (if training large models)\n",
    "3. Find max batch size\n",
    "\n",
    "### Step 2: Add If Still OOM\n",
    "4. Gradient checkpointing\n",
    "5. Gradient accumulation\n",
    "\n",
    "### Step 3: Extreme Constraints\n",
    "6. 4-bit quantization (QLoRA)\n",
    "7. CPU offloading (DeepSpeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete optimization example\n",
    "print(\"\"\"Full Optimization Example:\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 4-bit quantization + LoRA\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-7B\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# LoRA\n",
    "lora_config = LoraConfig(r=16, lora_alpha=32, ...)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Training config\n",
    "config = SFTConfig(\n",
    "    batch_size=4,                    # Small batch\n",
    "    gradient_accumulation_steps=8,   # Effective batch = 32\n",
    "    learning_rate=3e-4,\n",
    ")\n",
    "\n",
    "# Result: 7B model on 12 GB GPU!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Memory Optimization Techniques Ranked:**\n",
    "\n",
    "| Technique | Memory Savings | Speed Impact | When to Use |\n",
    "|-----------|----------------|--------------|-------------|\n",
    "| Mixed Precision | 50% | +20% faster | Always |\n",
    "| LoRA | 80-95% optimizer | None | Large models |\n",
    "| Gradient Accumulation | 0% (enables larger batch) | -20-30% | Memory-limited |\n",
    "| Gradient Checkpointing | 50-80% activations | -20-30% | Long sequences |\n",
    "| Quantization (4-bit) | 75% model | -10-20% | Extreme constraints |\n",
    "| CPU Offloading | 50-70% optimizer | -60-80% | Last resort |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now let's explore hyperparameter tuning for optimal training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
