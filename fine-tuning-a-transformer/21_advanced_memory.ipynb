{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Optimization\n",
    "\n",
    "**Advanced techniques for training large models on limited hardware**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Memory Challenge\n",
    "\n",
    "Training large language models requires substantial GPU memory. Understanding and optimizing memory usage is crucial for:\n",
    "\n",
    "- **Fitting larger models** on your hardware\n",
    "- **Using larger batch sizes** for more stable training\n",
    "- **Faster training** through better GPU utilization\n",
    "- **Cost reduction** by using smaller/cheaper GPUs\n",
    "\n",
    "### Memory Breakdown\n",
    "\n",
    "```\n",
    "Total GPU Memory Usage:\n",
    "+-- Model Weights        (~25-30%)\n",
    "+-- Optimizer State      (~50-60%)  <-- Largest component!\n",
    "+-- Gradients            (~25-30%)\n",
    "+-- Activations          (~10-20%)  <-- Depends on batch size\n",
    "+-- Framework Overhead   (~5%)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Examples\n",
    "\n",
    "**GPT-2 (124M parameters) full fine-tuning:**\n",
    "\n",
    "```\n",
    "Model weights (fp32):     124M x 4 bytes = 496 MB\n",
    "Optimizer (AdamW):        124M x 8 bytes = 992 MB  (momentum + variance)\n",
    "Gradients (fp32):         124M x 4 bytes = 496 MB\n",
    "Activations (batch=8):                    ~500 MB\n",
    "Framework overhead:                       ~100 MB\n",
    "----------------------------------------------------\n",
    "Total:                                    ~2.6 GB\n",
    "```\n",
    "\n",
    "**Llama 7B full fine-tuning:**\n",
    "\n",
    "```\n",
    "Model weights (fp32):     7B x 4 bytes = 28 GB\n",
    "Optimizer (AdamW):        7B x 8 bytes = 56 GB\n",
    "Gradients (fp32):         7B x 4 bytes = 28 GB\n",
    "Activations (batch=8):                   ~20 GB\n",
    "Framework overhead:                      ~2 GB\n",
    "----------------------------------------------------\n",
    "Total:                                   ~134 GB  <-- Won't fit on consumer GPUs!\n",
    "```\n",
    "\n",
    "The optimizer state is typically the largest memory consumer, often requiring 2x the model size for AdamW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 1: Mixed Precision Training\n",
    "\n",
    "**Most impactful technique** - Reduces memory by 50% with minimal code changes.\n",
    "\n",
    "### FP16 vs BF16 vs FP32\n",
    "\n",
    "| Format | Bits | Range | Precision | Memory |\n",
    "|--------|------|-------|-----------|--------|\n",
    "| FP32 | 32 | +/-3.4e38 | ~7 decimal digits | 4 bytes |\n",
    "| FP16 | 16 | +/-65,504 | ~3 decimal digits | 2 bytes |\n",
    "| BF16 | 16 | +/-3.4e38 | ~2 decimal digits | 2 bytes |\n",
    "\n",
    "**BF16** is preferred for modern GPUs (Ampere/Ada) - same range as FP32, no loss scaling needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Savings with Mixed Precision:\n",
      "  FP32 -> BF16: ~50% reduction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26738/986624292.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # For FP16 only, not needed for BF16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Mixed precision training example\n",
    "scaler = GradScaler()  # For FP16 only, not needed for BF16\n",
    "\n",
    "def train_step_mixed_precision(model, batch, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass in mixed precision\n",
    "    with autocast(dtype=torch.bfloat16):  # or torch.float16\n",
    "        outputs = model(batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    scaler.scale(loss).backward()  # Scale loss to prevent underflow\n",
    "    \n",
    "    # Optimizer step with unscaling\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\"Memory Savings with Mixed Precision:\")\n",
    "print(\"  FP32 -> BF16: ~50% reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 2: LoRA (Low-Rank Adaptation)\n",
    "\n",
    "**Dramatic memory reduction** by training only a tiny fraction of parameters.\n",
    "\n",
    "```\n",
    "Full Fine-Tuning:\n",
    "  Trainable params: 7,000,000,000\n",
    "  Optimizer state:  56 GB\n",
    "\n",
    "LoRA (r=16):\n",
    "  Trainable params: 16,777,216  (0.24% of model!)\n",
    "  Optimizer state:  134 MB      (418x reduction!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Memory Savings (Llama 7B):\n",
      "\n",
      "Full Fine-Tuning (BF16):\n",
      "  Model:      14 GB (trainable)\n",
      "  Optimizer:  56 GB\n",
      "  Gradients:  14 GB\n",
      "  Total:      84 GB + activations\n",
      "\n",
      "LoRA (BF16, r=16):\n",
      "  Model:      14 GB (frozen, can be quantized)\n",
      "  LoRA:       67 MB (trainable)\n",
      "  Optimizer:  268 MB (only for LoRA)\n",
      "  Gradients:  67 MB (only for LoRA)\n",
      "  Total:      14.4 GB + activations (5.8x reduction!)\n",
      "\n",
      "Rank selection for memory:\n",
      "  r=4:   ~33 MB (minimum, may underfit)\n",
      "  r=8:   ~67 MB (good for simple tasks)\n",
      "  r=16:  ~134 MB (default, recommended)\n",
      "  r=32:  ~268 MB (high capacity)\n"
     ]
    }
   ],
   "source": [
    "# LoRA memory savings\n",
    "print(\"LoRA Memory Savings (Llama 7B):\")\n",
    "print()\n",
    "print(\"Full Fine-Tuning (BF16):\")\n",
    "print(\"  Model:      14 GB (trainable)\")\n",
    "print(\"  Optimizer:  56 GB\")\n",
    "print(\"  Gradients:  14 GB\")\n",
    "print(\"  Total:      84 GB + activations\")\n",
    "print()\n",
    "print(\"LoRA (BF16, r=16):\")\n",
    "print(\"  Model:      14 GB (frozen, can be quantized)\")\n",
    "print(\"  LoRA:       67 MB (trainable)\")\n",
    "print(\"  Optimizer:  268 MB (only for LoRA)\")\n",
    "print(\"  Gradients:  67 MB (only for LoRA)\")\n",
    "print(\"  Total:      14.4 GB + activations (5.8x reduction!)\")\n",
    "print()\n",
    "print(\"Rank selection for memory:\")\n",
    "print(\"  r=4:   ~33 MB (minimum, may underfit)\")\n",
    "print(\"  r=8:   ~67 MB (good for simple tasks)\")\n",
    "print(\"  r=16:  ~134 MB (default, recommended)\")\n",
    "print(\"  r=32:  ~268 MB (high capacity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 3: Gradient Accumulation\n",
    "\n",
    "**Simulate larger batch sizes** without additional memory.\n",
    "\n",
    "```\n",
    "Effective batch size = batch_size x gradient_accumulation_steps\n",
    "Memory usage = batch_size_per_step (not effective_batch_size!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Accumulation:\n",
      "  batch_size=4, accumulation_steps=8\n",
      "  Effective batch size: 32\n",
      "  Memory: Only 4 samples at a time\n"
     ]
    }
   ],
   "source": [
    "# Gradient accumulation implementation\n",
    "def train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Scale loss by accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # Backward pass (accumulates gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights every accumulation_steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "print(\"Gradient Accumulation:\")\n",
    "print(\"  batch_size=4, accumulation_steps=8\")\n",
    "print(\"  Effective batch size: 32\")\n",
    "print(\"  Memory: Only 4 samples at a time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 4: Gradient Checkpointing\n",
    "\n",
    "**Trade computation for memory** by recomputing activations during backward pass.\n",
    "\n",
    "```\n",
    "Without Gradient Checkpointing:\n",
    "  Forward:  Save all activations -> High memory\n",
    "  Backward: Use saved activations -> Fast\n",
    "\n",
    "With Gradient Checkpointing:\n",
    "  Forward:  Save only checkpoint activations -> Low memory\n",
    "  Backward: Recompute from checkpoints -> Slower, low memory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Checkpointing Memory Savings:\n",
      "\n",
      "Llama 7B training (batch_size=8, seq_length=2048):\n",
      "  Without checkpointing: ~20 GB activations\n",
      "  With checkpointing:    ~5 GB activations\n",
      "  Savings: 75% reduction\n",
      "\n",
      "Trade-off: 20-30% slower training\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"Gradient Checkpointing Memory Savings:\")\n",
    "print()\n",
    "print(\"Llama 7B training (batch_size=8, seq_length=2048):\")\n",
    "print(\"  Without checkpointing: ~20 GB activations\")\n",
    "print(\"  With checkpointing:    ~5 GB activations\")\n",
    "print(\"  Savings: 75% reduction\")\n",
    "print()\n",
    "print(\"Trade-off: 20-30% slower training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 5: Model Quantization\n",
    "\n",
    "**Load models in reduced precision** (4-bit or 8-bit) to dramatically reduce memory.\n",
    "\n",
    "| Precision | Memory | Quality |\n",
    "|-----------|--------|--------|\n",
    "| FP32 | 28 GB (7B) | 100% |\n",
    "| BF16 | 14 GB | 99.9% |\n",
    "| 8-bit | 7 GB | ~99% |\n",
    "| 4-bit | 3.5 GB | 95-98% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for bitsandbytes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/metadata/__init__.py:397\u001b[39m, in \u001b[36mDistribution.from_name\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[31mStopIteration\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPackageNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 4-bit quantization (QLoRA)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m quantization_config = \u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnf4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# NormalFloat4 (better than standard)\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Double quantization for more savings\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Computation dtype\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#     \"meta-llama/Llama-3.2-7B\",\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#     quantization_config=quantization_config,\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#     device_map=\"auto\",\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQLoRA (4-bit + LoRA) Setup:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro-to-transformers/.venv/lib/python3.12/site-packages/transformers/utils/quantization_config.py:510\u001b[39m, in \u001b[36mBitsAndBytesConfig.__init__\u001b[39m\u001b[34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    508\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnused kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. These kwargs are not used in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro-to-transformers/.venv/lib/python3.12/site-packages/transformers/utils/quantization_config.py:568\u001b[39m, in \u001b[36mBitsAndBytesConfig.post_init\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.bnb_4bit_use_double_quant, \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version.parse(\u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbitsandbytes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) >= version.parse(\n\u001b[32m    569\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m0.39.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    570\u001b[39m ):\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    572\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    573\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/metadata/__init__.py:889\u001b[39m, in \u001b[36mversion\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mversion\u001b[39m(distribution_name):\n\u001b[32m    883\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[32m    884\u001b[39m \n\u001b[32m    885\u001b[39m \u001b[33;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[32m    886\u001b[39m \u001b[33;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[32m    887\u001b[39m \u001b[33;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[32m    888\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m.version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/metadata/__init__.py:862\u001b[39m, in \u001b[36mdistribution\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdistribution\u001b[39m(distribution_name):\n\u001b[32m    857\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[32m    858\u001b[39m \n\u001b[32m    859\u001b[39m \u001b[33;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[32m    860\u001b[39m \u001b[33;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/metadata/__init__.py:399\u001b[39m, in \u001b[36mDistribution.from_name\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m.discover(name=name))\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[31mPackageNotFoundError\u001b[39m: No package metadata was found for bitsandbytes"
     ]
    }
   ],
   "source": [
    "# Quantization with bitsandbytes\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 4-bit quantization (QLoRA)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",        # NormalFloat4 (better than standard)\n",
    "    bnb_4bit_use_double_quant=True,   # Double quantization for more savings\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Computation dtype\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"meta-llama/Llama-3.2-7B\",\n",
    "#     quantization_config=quantization_config,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "print(\"QLoRA (4-bit + LoRA) Setup:\")\n",
    "print(\"  Model (4-bit):          3.5 GB\")\n",
    "print(\"  LoRA adapters (BF16):   67 MB\")\n",
    "print(\"  Optimizer state:        268 MB\")\n",
    "print(\"  Gradients:              67 MB\")\n",
    "print(\"  Activations (bs=8):     5 GB (with checkpointing)\")\n",
    "print(\"  ------------------------------------\")\n",
    "print(\"  Total:                  ~9 GB (fits on RTX 3080!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def profile_memory(fn, label=\"\"):\n",
    "    \"\"\"Profile memory usage of a function.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available for profiling\")\n",
    "        return\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    result = fn()\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    \n",
    "    print(f\"\\n{label}\")\n",
    "    print(f\"  Start: {start_mem / 1e9:.2f} GB\")\n",
    "    print(f\"  End:   {end_mem / 1e9:.2f} GB\")\n",
    "    print(f\"  Delta: {(end_mem - start_mem) / 1e9:.2f} GB\")\n",
    "    print(f\"  Peak:  {peak_mem / 1e9:.2f} GB\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Memory monitoring\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"Peak memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging OOM Errors\n",
    "\n",
    "**OOM Debugging Checklist:**\n",
    "\n",
    "1. **Reduce batch size by 50%**\n",
    "2. **Enable gradient checkpointing**\n",
    "3. **Use gradient accumulation**\n",
    "4. **Check for memory leaks** (storing tensors accidentally)\n",
    "5. **Clear cache** with `torch.cuda.empty_cache()`\n",
    "\n",
    "**Common OOM Causes:**\n",
    "\n",
    "| Cause | Solution |\n",
    "|-------|----------|\n",
    "| Batch size too large | Reduce by 50%, use gradient accumulation |\n",
    "| Sequence length too long | Truncate to 512 or 1024 tokens |\n",
    "| Accumulating tensors | Use `.item()` or `.detach()` |\n",
    "| Fragmented memory | `torch.cuda.empty_cache()` |\n",
    "| Multiple models | Delete unused models |\n",
    "| Full precision | Use BF16/FP16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Optimization Strategy\n",
    "\n",
    "**Recommended approach:**\n",
    "\n",
    "### Step 1: Essential Optimizations (Always Apply)\n",
    "1. Mixed precision (BF16/FP16)\n",
    "2. LoRA (if training large models)\n",
    "3. Find max batch size\n",
    "\n",
    "### Step 2: Add If Still OOM\n",
    "4. Gradient checkpointing\n",
    "5. Gradient accumulation\n",
    "\n",
    "### Step 3: Extreme Constraints\n",
    "6. 4-bit quantization (QLoRA)\n",
    "7. CPU offloading (DeepSpeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete optimization example\n",
    "print(\"\"\"Full Optimization Example:\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 4-bit quantization + LoRA\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-7B\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# LoRA\n",
    "lora_config = LoraConfig(r=16, lora_alpha=32, ...)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Training config\n",
    "config = SFTConfig(\n",
    "    batch_size=4,                    # Small batch\n",
    "    gradient_accumulation_steps=8,   # Effective batch = 32\n",
    "    learning_rate=3e-4,\n",
    ")\n",
    "\n",
    "# Result: 7B model on 12 GB GPU!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Memory Optimization Techniques Ranked:**\n",
    "\n",
    "| Technique | Memory Savings | Speed Impact | When to Use |\n",
    "|-----------|----------------|--------------|-------------|\n",
    "| Mixed Precision | 50% | +20% faster | Always |\n",
    "| LoRA | 80-95% optimizer | None | Large models |\n",
    "| Gradient Accumulation | 0% (enables larger batch) | -20-30% | Memory-limited |\n",
    "| Gradient Checkpointing | 50-80% activations | -20-30% | Long sequences |\n",
    "| Quantization (4-bit) | 75% model | -10-20% | Extreme constraints |\n",
    "| CPU Offloading | 50-70% optimizer | -60-80% | Last resort |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now let's explore hyperparameter tuning for optimal training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
