{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Memory Game\n",
    "\n",
    "When training large language models, they're memory hogs. \n",
    "\n",
    "A 7B parameter model in full precision? That's 28 GB just for the weights. Add in the optimizer state (another 56 GB!), gradients (28 GB), and activations (20+ GB), and you're looking at 130+ GB of memory.\n",
    "\n",
    "Your RTX 4090 has 24 GB.\n",
    "\n",
    "See the problem?\n",
    "\n",
    "The good news: with the right tricks, you can train that 7B model on consumer hardware. This notebook is your guide to the memory optimization techniques that make it possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Memory Problem\n",
    "\n",
    "First, let's break down where all that memory goes during training.\n",
    "\n",
    "Think of GPU memory like a packed suitcase. You've got limited space, and you need to fit everything in. Here's what's taking up room:\n",
    "\n",
    "### The Memory Breakdown\n",
    "\n",
    "```\n",
    "Total GPU Memory During Training:\n",
    "├── Model Weights        (~25-30%)  ← The model parameters themselves\n",
    "├── Optimizer State      (~50-60%)  ← Momentum & variance (biggest offender!)\n",
    "├── Gradients            (~25-30%)  ← One gradient per parameter\n",
    "├── Activations          (~10-20%)  ← Saved outputs for backprop\n",
    "└── Framework Overhead   (~5%)      ← PyTorch bookkeeping\n",
    "```\n",
    "\n",
    "Notice something? The optimizer state is typically **the largest consumer of memory**. Not the model itself!\n",
    "\n",
    "Why? Because modern optimizers like AdamW keep track of two extra values per parameter: a momentum term and a variance term. That's 2x the model size right there.\n",
    "\n",
    "This is why a 7B parameter model needs way more than 28 GB of memory for training. It's not just storing the weights—it's storing all the machinery needed to update those weights effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Do the Math\n",
    "\n",
    "Numbers make this concrete. Let's calculate the exact memory requirements for two models.\n",
    "\n",
    "### GPT-2 (124M parameters) - Full Fine-Tuning\n",
    "\n",
    "```\n",
    "Model weights (fp32):     124M params × 4 bytes = 496 MB\n",
    "Optimizer (AdamW):        124M params × 8 bytes = 992 MB  ← 2x model size!\n",
    "Gradients (fp32):         124M params × 4 bytes = 496 MB\n",
    "Activations (batch=8):                            ~500 MB\n",
    "Framework overhead:                               ~100 MB\n",
    "─────────────────────────────────────────────────────────\n",
    "Total:                                           ~2.6 GB\n",
    "```\n",
    "\n",
    "Not too bad! This fits comfortably on most GPUs.\n",
    "\n",
    "### Llama 7B - Full Fine-Tuning\n",
    "\n",
    "```\n",
    "Model weights (fp32):     7B params × 4 bytes = 28 GB\n",
    "Optimizer (AdamW):        7B params × 8 bytes = 56 GB  ← Oof.\n",
    "Gradients (fp32):         7B params × 4 bytes = 28 GB\n",
    "Activations (batch=8):                          ~20 GB\n",
    "Framework overhead:                             ~2 GB\n",
    "─────────────────────────────────────────────────────\n",
    "Total:                                         ~134 GB\n",
    "```\n",
    "\n",
    "Whoops! That's not fitting on consumer hardware.\n",
    "\n",
    "### Breaking Down the Calculations\n",
    "\n",
    "**Why 4 bytes for fp32?** Each 32-bit floating point number takes 32 bits = 4 bytes of memory.\n",
    "\n",
    "**Why 8 bytes for AdamW?** AdamW stores two additional values per parameter (first moment and second moment), each in fp32. So that's 4 bytes + 4 bytes = 8 bytes per parameter just for optimizer state.\n",
    "\n",
    "**What are activations?** During the forward pass, we save the output of each layer. We need these saved values during backpropagation to compute gradients. More layers, longer sequences, and bigger batch sizes = more activations to store.\n",
    "\n",
    "The good news? We can dramatically reduce these numbers with the right techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 1: Mixed Precision Training\n",
    "\n",
    "This is the **most impactful technique** you can apply. It cuts memory usage roughly in half with just a few lines of code.\n",
    "\n",
    "### What is Mixed Precision?\n",
    "\n",
    "Instead of using 32-bit floating point numbers (fp32) for everything, we use 16-bit numbers (fp16 or bf16) for most operations.\n",
    "\n",
    "Think of it like this: you're doing carpentry. Sometimes you need a micrometer for precise measurements. But most of the time? A ruler is fine. Mixed precision training uses the \"ruler\" (16-bit) for most work and pulls out the \"micrometer\" (32-bit) only when needed.\n",
    "\n",
    "### The Three Formats\n",
    "\n",
    "| Format | Bits | Range | Precision | Memory per Value |\n",
    "|--------|------|-------|-----------|------------------|\n",
    "| **fp32** | 32 | ±3.4 × 10³⁸ | ~7 decimal digits | 4 bytes |\n",
    "| **fp16** | 16 | ±65,504 | ~3 decimal digits | 2 bytes |\n",
    "| **bf16** | 16 | ±3.4 × 10³⁸ | ~2 decimal digits | 2 bytes |\n",
    "\n",
    "**fp16** (Float16): Traditional half precision. Small range, can overflow/underflow easily.\n",
    "\n",
    "**bf16** (BrainFloat16): Google's format. Same range as fp32 but less precision. This is the sweet spot for modern GPUs (Ampere, Ada, Hopper). No overflow issues, no loss scaling needed.\n",
    "\n",
    "**Which should you use?** If your GPU supports it (RTX 30-series or newer, A100, H100), use **bf16**. It's simpler and more robust. Otherwise, fp16 works but requires loss scaling to prevent underflow.\n",
    "\n",
    "### Memory Savings\n",
    "\n",
    "```\n",
    "7B model in fp32:     7B × 4 bytes = 28 GB\n",
    "7B model in bf16:     7B × 2 bytes = 14 GB\n",
    "                                     ↓\n",
    "                           50% reduction!\n",
    "```\n",
    "\n",
    "And training quality is essentially identical. You're getting half the memory usage for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed Precision Memory Savings:\n",
      "  FP32 → BF16: ~50% reduction in memory\n",
      "  FP32 → FP16: ~50% reduction in memory\n",
      "\n",
      "Speed bonus: Training is often 20-30% faster too!\n",
      "(Modern GPUs have dedicated hardware for fp16/bf16 operations)\n",
      "\n",
      "Quality impact: Negligible for most tasks\n",
      "(We've trained hundreds of models this way - it just works)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# This is what mixed precision training looks like in code.\n",
    "# It's surprisingly simple!\n",
    "\n",
    "# For fp16, we need a GradScaler to prevent underflow\n",
    "scaler = GradScaler('cuda')  # (not needed for bf16)\n",
    "\n",
    "def train_step_mixed_precision(model, batch, optimizer):\n",
    "    \"\"\"A training step using mixed precision.\n",
    "    \n",
    "    The key is the `autocast` context manager - it automatically\n",
    "    casts operations to the specified dtype when beneficial.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass in lower precision (bf16 or fp16)\n",
    "    # PyTorch automatically figures out which ops should be bf16\n",
    "    # and which should stay fp32 (like loss calculation)\n",
    "    with autocast('cuda', dtype=torch.bfloat16):  # or torch.float16\n",
    "        outputs = model(batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    # For fp16: scale the loss to prevent gradient underflow\n",
    "    # For bf16: this is unnecessary but doesn't hurt\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Optimizer step with unscaling\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\"Mixed Precision Memory Savings:\")\n",
    "print(\"  FP32 → BF16: ~50% reduction in memory\")\n",
    "print(\"  FP32 → FP16: ~50% reduction in memory\")\n",
    "print()\n",
    "print(\"Speed bonus: Training is often 20-30% faster too!\")\n",
    "print(\"(Modern GPUs have dedicated hardware for fp16/bf16 operations)\")\n",
    "print()\n",
    "print(\"Quality impact: Negligible for most tasks\")\n",
    "print(\"(We've trained hundreds of models this way - it just works)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 2: LoRA (Low-Rank Adaptation)\n",
    "\n",
    "If mixed precision is a memory reducer, LoRA is a memory **destroyer**. In the best way.\n",
    "\n",
    "Remember how I said the optimizer state is the biggest memory hog? LoRA solves this by **freezing the base model** and training only tiny adapter layers.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Instead of updating all 7 billion parameters, we freeze them and add small \"adapter\" matrices that we train instead.\n",
    "\n",
    "Think of it like this: you've got a massive reference book (the base model). Instead of rewriting the whole book, you add sticky notes (LoRA adapters) with corrections and additions. The book stays the same; only the notes change.\n",
    "\n",
    "### The Math\n",
    "\n",
    "For each weight matrix W, LoRA adds two small matrices A and B:\n",
    "\n",
    "```\n",
    "Original:    W (full rank, millions of parameters)\n",
    "LoRA adds:   W + (A × B)\n",
    "             ↑    ↑   ↑\n",
    "         frozen  rank r matrices\n",
    "                 (tiny!)\n",
    "```\n",
    "\n",
    "If W is 4096×4096, that's 16.7M parameters.\n",
    "But A and B with rank r=16? That's only 4096×16 + 16×4096 = 131K parameters!\n",
    "\n",
    "That's **128x fewer parameters** to train.\n",
    "\n",
    "### Memory Impact\n",
    "\n",
    "```\n",
    "Full Fine-Tuning (Llama 7B):\n",
    "  Trainable params:   7,000,000,000\n",
    "  Optimizer state:    56 GB  (8 bytes per param)\n",
    "  Gradients:          28 GB  (4 bytes per param)\n",
    "\n",
    "LoRA with r=16 (Llama 7B):\n",
    "  Trainable params:   16,777,216  (0.24% of model!)\n",
    "  Optimizer state:    134 MB      (418x reduction!)\n",
    "  Gradients:          67 MB       (418x reduction!)\n",
    "```\n",
    "\n",
    "The base model stays frozen, so we only need optimizer state and gradients for those tiny adapter matrices.\n",
    "\n",
    "It's kind of absurd how well this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Memory Comparison (Llama 7B in BF16)\n",
      "============================================================\n",
      "\n",
      "Full Fine-Tuning:\n",
      "  Base model:         14 GB (all parameters trainable)\n",
      "  Optimizer state:    56 GB (momentum + variance for all params)\n",
      "  Gradients:          14 GB (one gradient per parameter)\n",
      "  ─────────────────────────\n",
      "  Total:              84 GB (before activations!)\n",
      "\n",
      "LoRA (r=16):\n",
      "  Base model:         14 GB (frozen - can even be quantized!)\n",
      "  LoRA adapters:      67 MB (only these are trainable)\n",
      "  Optimizer state:    268 MB (only for LoRA adapters)\n",
      "  Gradients:          67 MB (only for LoRA adapters)\n",
      "  ─────────────────────────\n",
      "  Total:              14.4 GB (before activations)\n",
      "\n",
      "  Memory reduction:   5.8x smaller!\n",
      "  And we can combine this with quantization...\n",
      "\n",
      "Choosing the Rank (r)\n",
      "============================================================\n",
      "\n",
      "The rank controls capacity vs. memory trade-off:\n",
      "\n",
      "  r=4:    ~33 MB trainable\n",
      "          Minimal memory, but may underfit complex tasks\n",
      "\n",
      "  r=8:    ~67 MB trainable\n",
      "          Good for simple fine-tuning tasks\n",
      "\n",
      "  r=16:   ~134 MB trainable\n",
      "          The sweet spot - recommended default\n",
      "\n",
      "  r=32:   ~268 MB trainable\n",
      "          High capacity for complex tasks\n",
      "\n",
      "  r=64:   ~536 MB trainable\n",
      "          When you need more expressiveness\n",
      "\n",
      "Rule of thumb: Start with r=16. Increase if underfitting.\n",
      "(Most tasks work great with r=16, honestly)\n"
     ]
    }
   ],
   "source": [
    "# Let's see the actual numbers for LoRA memory savings.\n",
    "# This is why LoRA has become the standard approach for fine-tuning large models.\n",
    "\n",
    "print(\"LoRA Memory Comparison (Llama 7B in BF16)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "print(\"Full Fine-Tuning:\")\n",
    "print(\"  Base model:         14 GB (all parameters trainable)\")\n",
    "print(\"  Optimizer state:    56 GB (momentum + variance for all params)\")\n",
    "print(\"  Gradients:          14 GB (one gradient per parameter)\")\n",
    "print(\"  ─────────────────────────\")\n",
    "print(\"  Total:              84 GB (before activations!)\")\n",
    "print()\n",
    "\n",
    "print(\"LoRA (r=16):\")\n",
    "print(\"  Base model:         14 GB (frozen - can even be quantized!)\")\n",
    "print(\"  LoRA adapters:      67 MB (only these are trainable)\")\n",
    "print(\"  Optimizer state:    268 MB (only for LoRA adapters)\")\n",
    "print(\"  Gradients:          67 MB (only for LoRA adapters)\")\n",
    "print(\"  ─────────────────────────\")\n",
    "print(\"  Total:              14.4 GB (before activations)\")\n",
    "print()\n",
    "print(\"  Memory reduction:   5.8x smaller!\")\n",
    "print(\"  And we can combine this with quantization...\")\n",
    "print()\n",
    "\n",
    "print(\"Choosing the Rank (r)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"The rank controls capacity vs. memory trade-off:\")\n",
    "print()\n",
    "print(\"  r=4:    ~33 MB trainable\")\n",
    "print(\"          Minimal memory, but may underfit complex tasks\")\n",
    "print()\n",
    "print(\"  r=8:    ~67 MB trainable\")\n",
    "print(\"          Good for simple fine-tuning tasks\")\n",
    "print()\n",
    "print(\"  r=16:   ~134 MB trainable\")\n",
    "print(\"          The sweet spot - recommended default\")\n",
    "print()\n",
    "print(\"  r=32:   ~268 MB trainable\")\n",
    "print(\"          High capacity for complex tasks\")\n",
    "print()\n",
    "print(\"  r=64:   ~536 MB trainable\")\n",
    "print(\"          When you need more expressiveness\")\n",
    "print()\n",
    "print(\"Rule of thumb: Start with r=16. Increase if underfitting.\")\n",
    "print(\"(Most tasks work great with r=16, honestly)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 3: Gradient Accumulation\n",
    "\n",
    "Okay, this one's clever.\n",
    "\n",
    "You know how larger batch sizes generally lead to more stable training? But bigger batches need more memory (you're processing more samples at once).\n",
    "\n",
    "Gradient accumulation lets you have your cake and eat it too.\n",
    "\n",
    "### The Trick\n",
    "\n",
    "Instead of computing gradients and updating weights after every batch, we accumulate gradients across multiple small batches, then update once.\n",
    "\n",
    "```\n",
    "Effective batch size = per_device_batch_size × gradient_accumulation_steps\n",
    "\n",
    "Memory usage = per_device_batch_size (not effective batch size!)\n",
    "```\n",
    "\n",
    "So if you can only fit batch_size=2 in memory, but you want the training stability of batch_size=32, you can do:\n",
    "\n",
    "```python\n",
    "per_device_batch_size = 2\n",
    "gradient_accumulation_steps = 16\n",
    "# Effective batch size = 2 × 16 = 32\n",
    "```\n",
    "\n",
    "You get the benefits of a large batch size with the memory footprint of a small one.\n",
    "\n",
    "### The Trade-off\n",
    "\n",
    "**Pro:** Train with larger effective batch sizes without OOM errors.\n",
    "\n",
    "**Con:** Slightly slower (you're doing more forward passes before each update).\n",
    "\n",
    "But slower beats \"doesn't fit in memory\" every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Accumulation Example:\n",
      "============================================================\n",
      "\n",
      "Scenario: You can fit batch_size=4 in memory\n",
      "          But you want effective batch_size=32\n",
      "\n",
      "Solution:\n",
      "  per_device_batch_size = 4\n",
      "  gradient_accumulation_steps = 8\n",
      "\n",
      "Result:\n",
      "  Effective batch size: 4 × 8 = 32 ✓\n",
      "  Memory usage: Only 4 samples at a time ✓\n",
      "  Training stability: Same as batch_size=32 ✓\n",
      "\n",
      "This is why you'll see accumulation_steps in almost every\n",
      "fine-tuning config - it's free effective batch size!\n"
     ]
    }
   ],
   "source": [
    "# Here's what gradient accumulation looks like in practice.\n",
    "# The key insight: gradients ADD together, so we can accumulate them\n",
    "# over multiple batches before applying an update.\n",
    "\n",
    "def train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):\n",
    "    \"\"\"Train with gradient accumulation.\n",
    "    \n",
    "    We process `accumulation_steps` batches, accumulating gradients,\n",
    "    then update the model once.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Important: Scale the loss by accumulation steps\n",
    "        # (so the effective learning rate stays consistent)\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # Backward pass - this ADDS to existing gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Only update weights every `accumulation_steps` batches\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Clear for next accumulation\n",
    "\n",
    "# Let's see the impact\n",
    "print(\"Gradient Accumulation Example:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Scenario: You can fit batch_size=4 in memory\")\n",
    "print(\"          But you want effective batch_size=32\")\n",
    "print()\n",
    "print(\"Solution:\")\n",
    "print(\"  per_device_batch_size = 4\")\n",
    "print(\"  gradient_accumulation_steps = 8\")\n",
    "print()\n",
    "print(\"Result:\")\n",
    "print(\"  Effective batch size: 4 × 8 = 32 ✓\")\n",
    "print(\"  Memory usage: Only 4 samples at a time ✓\")\n",
    "print(\"  Training stability: Same as batch_size=32 ✓\")\n",
    "print()\n",
    "print(\"This is why you'll see accumulation_steps in almost every\")\n",
    "print(\"fine-tuning config - it's free effective batch size!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 4: Gradient Checkpointing\n",
    "\n",
    "This technique trades computation for memory. It's brilliant in a slightly painful way.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "During the forward pass, we need to save the output of every layer. Why? Because during backpropagation, we need those saved values to compute gradients.\n",
    "\n",
    "For a 32-layer transformer processing a batch of 8 sequences, that's a LOT of saved activations. They can easily use 20+ GB of memory.\n",
    "\n",
    "### The Solution\n",
    "\n",
    "What if we... didn't save them?\n",
    "\n",
    "Gradient checkpointing only saves activations at certain \"checkpoint\" layers (say, every 4th layer). During backpropagation, when we need an activation we didn't save, we recompute it on the fly.\n",
    "\n",
    "Think of it like taking notes during a lecture. You could transcribe everything (high memory), or you could write down key points and fill in the details later (low memory, more work).\n",
    "\n",
    "```\n",
    "Without Gradient Checkpointing:\n",
    "  Forward pass:  Compute activations → Save all of them\n",
    "  Backward pass: Use saved activations → Fast\n",
    "  Memory: HIGH\n",
    "\n",
    "With Gradient Checkpointing:\n",
    "  Forward pass:  Compute activations → Save only checkpoints\n",
    "  Backward pass: Recompute missing activations → Slower\n",
    "  Memory: LOW\n",
    "```\n",
    "\n",
    "### The Trade-off\n",
    "\n",
    "You're recomputing activations during backprop, so training is slower (typically 20-30% slower).\n",
    "\n",
    "But the memory savings are huge: 50-80% reduction in activation memory.\n",
    "\n",
    "When you're memory-constrained, this is a lifesaver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Checkpointing Enabled!\n",
      "============================================================\n",
      "\n",
      "Memory Impact (Llama 7B, batch=8, seq_len=2048):\n",
      "\n",
      "  Without checkpointing:\n",
      "    Activations: ~20 GB\n",
      "    (Every layer output saved)\n",
      "\n",
      "  With checkpointing:\n",
      "    Activations: ~5 GB\n",
      "    (Only checkpoint layers saved, rest recomputed)\n",
      "\n",
      "  Memory reduction: 75%!\n",
      "\n",
      "Speed Trade-off:\n",
      "  Training is ~20-30% slower\n",
      "  (We're recomputing activations during backward pass)\n",
      "\n",
      "When to use it:\n",
      "  - You're hitting OOM errors\n",
      "  - You're training with long sequences\n",
      "  - You want to increase batch size\n",
      "\n",
      "When to skip it:\n",
      "  - You have plenty of memory\n",
      "  - Speed is critical\n",
      "  - You're using very short sequences\n",
      "\n",
      "If you're fine-tuning on consumer hardware, you're probably\n",
      "using this. The memory savings are just too good to pass up.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Gradient checkpointing is ridiculously easy to enable.\n",
    "# One method call. That's it.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"Gradient Checkpointing Enabled!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Memory Impact (Llama 7B, batch=8, seq_len=2048):\")\n",
    "print()\n",
    "print(\"  Without checkpointing:\")\n",
    "print(\"    Activations: ~20 GB\")\n",
    "print(\"    (Every layer output saved)\")\n",
    "print()\n",
    "print(\"  With checkpointing:\")\n",
    "print(\"    Activations: ~5 GB\")\n",
    "print(\"    (Only checkpoint layers saved, rest recomputed)\")\n",
    "print()\n",
    "print(\"  Memory reduction: 75%!\")\n",
    "print()\n",
    "print(\"Speed Trade-off:\")\n",
    "print(\"  Training is ~20-30% slower\")\n",
    "print(\"  (We're recomputing activations during backward pass)\")\n",
    "print()\n",
    "print(\"When to use it:\")\n",
    "print(\"  - You're hitting OOM errors\")\n",
    "print(\"  - You're training with long sequences\")\n",
    "print(\"  - You want to increase batch size\")\n",
    "print()\n",
    "print(\"When to skip it:\")\n",
    "print(\"  - You have plenty of memory\")\n",
    "print(\"  - Speed is critical\")\n",
    "print(\"  - You're using very short sequences\")\n",
    "print()\n",
    "print(\"If you're fine-tuning on consumer hardware, you're probably\")\n",
    "print(\"using this. The memory savings are just too good to pass up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 5: Model Quantization\n",
    "\n",
    "Now we're getting into the heavy artillery.\n",
    "\n",
    "Quantization is like... aggressive compression for neural networks. Instead of storing weights in 16-bit or 32-bit precision, we use 8-bit or even 4-bit integers.\n",
    "\n",
    "Sounds crazy, right? How can you possibly represent a neural network weight that might be 0.0123456789 using just 4 bits (16 possible values)?\n",
    "\n",
    "### The Magic\n",
    "\n",
    "Modern quantization techniques (like NormalFloat4, or \"nf4\") are **calibrated** to the distribution of neural network weights. Most weights cluster around zero, with a long tail. NF4 packs more precision where it matters and less where it doesn't.\n",
    "\n",
    "It's like how JPEG compression works: throw away information humans won't notice. Here, we throw away precision the model doesn't need.\n",
    "\n",
    "### The Numbers\n",
    "\n",
    "| Precision | Memory (7B model) | Quality | Notes |\n",
    "|-----------|-------------------|---------|-------|\n",
    "| **FP32** | 28 GB | 100% | Original precision |\n",
    "| **BF16** | 14 GB | ~99.9% | Standard for training |\n",
    "| **INT8** | 7 GB | ~99% | Good for inference |\n",
    "| **NF4** | 3.5 GB | 95-98% | The QLoRA breakthrough |\n",
    "\n",
    "Yes, you read that right. A 7B model in 4-bit takes **3.5 GB**. That fits on a laptop GPU.\n",
    "\n",
    "### The Catch\n",
    "\n",
    "Quantized models can only be used for **inference** or as frozen base models for fine-tuning (like with LoRA).\n",
    "\n",
    "You can't train a quantized model directly. The low precision causes training to diverge.\n",
    "\n",
    "But combined with LoRA (which adds trainable adapters on top), you get QLoRA: 4-bit base model + 16-bit LoRA adapters = magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbf407b602c494d8b3a4054c1b1d0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLoRA Setup: 4-bit Model + LoRA Adapters\n",
      "============================================================\n",
      "\n",
      "This is the state-of-the-art approach for fine-tuning\n",
      "large models on consumer hardware.\n",
      "\n",
      "Memory breakdown (7B model):\n",
      "  Base model (4-bit):     3.5 GB ← Quantized!\n",
      "  LoRA adapters (bf16):   67 MB  ← Trainable\n",
      "  Optimizer state:        268 MB ← Only for LoRA\n",
      "  Gradients:              67 MB  ← Only for LoRA\n",
      "  Activations (bs=8):     ~5 GB  ← With gradient checkpointing\n",
      "  ───────────────────────────────\n",
      "  Total:                  ~9 GB\n",
      "\n",
      "That fits on an RTX 3080 (10 GB)!\n",
      "Or even an RTX 3060 (12 GB) with room to spare.\n",
      "\n",
      "The quality hit? Surprisingly small. QLoRA models often\n",
      "perform within 1-2% of full precision fine-tuning.\n",
      "\n",
      "This technique democratized LLM fine-tuning. Before QLoRA,\n",
      "you needed expensive multi-GPU setups. Now? Your gaming PC\n",
      "can fine-tune a 7B model. Pretty wild.\n"
     ]
    }
   ],
   "source": [
    "# Let's load a quantized model.\n",
    "# This is what makes fine-tuning large models accessible to everyone.\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization (the QLoRA approach)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    "    \n",
    "    # Use NormalFloat4 (nf4) - specifically designed for neural network weights\n",
    "    # This gives better quality than standard 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    \n",
    "    # Double quantization: quantize the quantization constants too\n",
    "    # (yes, really - saves even more memory with minimal quality loss)\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    \n",
    "    # When we actually compute with these weights (forward pass),\n",
    "    # convert them to bf16 for the calculation\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the model in 4-bit\n",
    "# Note: This requires the `bitsandbytes` library\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",  # Automatically distribute across available devices\n",
    ")\n",
    "\n",
    "print(\"QLoRA Setup: 4-bit Model + LoRA Adapters\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"This is the state-of-the-art approach for fine-tuning\")\n",
    "print(\"large models on consumer hardware.\")\n",
    "print()\n",
    "print(\"Memory breakdown (7B model):\")\n",
    "print(\"  Base model (4-bit):     3.5 GB ← Quantized!\")\n",
    "print(\"  LoRA adapters (bf16):   67 MB  ← Trainable\")\n",
    "print(\"  Optimizer state:        268 MB ← Only for LoRA\")\n",
    "print(\"  Gradients:              67 MB  ← Only for LoRA\")\n",
    "print(\"  Activations (bs=8):     ~5 GB  ← With gradient checkpointing\")\n",
    "print(\"  ───────────────────────────────\")\n",
    "print(\"  Total:                  ~9 GB\")\n",
    "print()\n",
    "print(\"That fits on an RTX 3080 (10 GB)!\")\n",
    "print(\"Or even an RTX 3060 (12 GB) with room to spare.\")\n",
    "print()\n",
    "print(\"The quality hit? Surprisingly small. QLoRA models often\")\n",
    "print(\"perform within 1-2% of full precision fine-tuning.\")\n",
    "print()\n",
    "print(\"This technique democratized LLM fine-tuning. Before QLoRA,\")\n",
    "print(\"you needed expensive multi-GPU setups. Now? Your gaming PC\")\n",
    "print(\"can fine-tune a 7B model. Pretty wild.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Memory Usage\n",
    "\n",
    "Before you can optimize memory, you need to measure it.\n",
    "\n",
    "Here's a simple profiling function that shows exactly where your memory is going. This is invaluable when debugging OOM errors or trying to squeeze more performance out of your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Memory Status:\n",
      "============================================================\n",
      "Current allocation: 2.06 GB\n",
      "Peak allocation:    3.08 GB\n",
      "Total GPU memory:   25.75 GB\n",
      "Available:          23.70 GB\n",
      "\n",
      "Use this profiler to understand where your memory goes!\n",
      "\n",
      "Example:\n",
      "  profile_memory(\n",
      "      lambda: model.forward(batch),\n",
      "      label='Forward pass'\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def profile_memory(fn, label=\"Operation\"):\n",
    "    \"\"\"Profile GPU memory usage of a function.\n",
    "    \n",
    "    This shows you:\n",
    "    - Starting memory (what was already allocated)\n",
    "    - Ending memory (after the operation)\n",
    "    - Delta (how much the operation added)\n",
    "    - Peak (the highest memory point during execution)\n",
    "    \n",
    "    The delta vs peak difference tells you about temporary allocations.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No CUDA available - can't profile GPU memory\")\n",
    "        print(\"(This is fine if you're running on CPU)\")\n",
    "        return\n",
    "    \n",
    "    # Clean up before measuring\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the function\n",
    "    result = fn()\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    \n",
    "    print(f\"\\n{label}\")\n",
    "    print(f\"  Start:  {start_mem / 1e9:.2f} GB\")\n",
    "    print(f\"  End:    {end_mem / 1e9:.2f} GB\")\n",
    "    print(f\"  Delta:  {(end_mem - start_mem) / 1e9:.2f} GB\")\n",
    "    print(f\"  Peak:   {peak_mem / 1e9:.2f} GB\")\n",
    "    \n",
    "    if peak_mem > end_mem:\n",
    "        print(f\"  (Peak-End: {(peak_mem - end_mem) / 1e9:.2f} GB temporary allocation)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Check current memory usage\n",
    "print(\"Current Memory Status:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    current = torch.cuda.memory_allocated()\n",
    "    peak = torch.cuda.max_memory_allocated()\n",
    "    total = torch.cuda.get_device_properties(0).total_memory\n",
    "    \n",
    "    print(f\"Current allocation: {current / 1e9:.2f} GB\")\n",
    "    print(f\"Peak allocation:    {peak / 1e9:.2f} GB\")\n",
    "    print(f\"Total GPU memory:   {total / 1e9:.2f} GB\")\n",
    "    print(f\"Available:          {(total - current) / 1e9:.2f} GB\")\n",
    "    print()\n",
    "    print(\"Use this profiler to understand where your memory goes!\")\n",
    "    print()\n",
    "    print(\"Example:\")\n",
    "    print(\"  profile_memory(\")\n",
    "    print(\"      lambda: model.forward(batch),\")\n",
    "    print(\"      label='Forward pass'\")\n",
    "    print(\"  )\")\n",
    "else:\n",
    "    print(\"No CUDA available\")\n",
    "    print(\"(Running on CPU - memory profiling won't work)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Things Go Wrong: Debugging OOM Errors\n",
    "\n",
    "The dreaded \"CUDA out of memory\" error. We've all been there.\n",
    "\n",
    "Here's your systematic debugging checklist.\n",
    "\n",
    "### The Quick Fixes (Try These First)\n",
    "\n",
    "**1. Cut your batch size in half**\n",
    "   - If batch_size=8 OOMs, try batch_size=4\n",
    "   - Use gradient accumulation to maintain effective batch size\n",
    "   - This fixes 80% of OOM issues\n",
    "\n",
    "**2. Enable gradient checkpointing**\n",
    "   - One line: `model.gradient_checkpointing_enable()`\n",
    "   - Saves 50-80% on activation memory\n",
    "   - Especially important for long sequences\n",
    "\n",
    "**3. Use gradient accumulation**\n",
    "   - Smaller batches, same training dynamics\n",
    "   - Costs nothing but a bit of training time\n",
    "\n",
    "**4. Clear the CUDA cache**\n",
    "   - `torch.cuda.empty_cache()`\n",
    "   - Defragments memory, sometimes helps\n",
    "   - Won't free memory that's actually in use\n",
    "\n",
    "### Common OOM Causes\n",
    "\n",
    "| Problem | Symptom | Solution |\n",
    "|---------|---------|----------|\n",
    "| Batch size too large | OOM during forward pass | Reduce batch_size by 50% |\n",
    "| Sequences too long | OOM with long inputs | Truncate to 512 or 1024 tokens |\n",
    "| Memory leak | OOM after many steps | Use `.item()` or `.detach()` on tensors |\n",
    "| Fragmented memory | OOM despite \"available\" memory | `torch.cuda.empty_cache()` |\n",
    "| Multiple models | OOM at model load | Delete old models: `del model; gc.collect()` |\n",
    "| Full precision | Just generally tight | Switch to BF16/FP16 |\n",
    "\n",
    "### The Nuclear Option\n",
    "\n",
    "If nothing else works:\n",
    "\n",
    "```python\n",
    "# QLoRA: 4-bit model + LoRA\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, ...)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(r=8, ...)  # Smaller rank if needed\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,  # Minimum\n",
    "    gradient_accumulation_steps=32,  # Large accumulation\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "This'll fit almost anything, anywhere. It's slow, but it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together: A Memory Optimization Strategy\n",
    "\n",
    "Okay, we've covered a lot of techniques. How do you actually use them?\n",
    "\n",
    "Here's my recommended approach, in order. Think of it as a ladder—start at step 1, and only climb higher if you need to.\n",
    "\n",
    "### Step 1: The Essentials (Always Do This)\n",
    "\n",
    "These are free wins. Apply them to every training run.\n",
    "\n",
    "**1. Mixed precision (BF16 or FP16)**\n",
    "   - 50% memory reduction\n",
    "   - Often faster training\n",
    "   - Literally no downside on modern GPUs\n",
    "\n",
    "**2. LoRA (if training large models >1B params)**\n",
    "   - 80-95% reduction in optimizer memory\n",
    "   - No speed penalty\n",
    "   - Quality is excellent for most tasks\n",
    "\n",
    "**3. Find your maximum batch size**\n",
    "   - Start with batch_size=8\n",
    "   - If OOM, halve it\n",
    "   - If comfortable, double it\n",
    "   - Use gradient accumulation to hit your target effective batch size\n",
    "\n",
    "### Step 2: If You're Still Running Out of Memory\n",
    "\n",
    "**4. Gradient checkpointing**\n",
    "   - 50-80% reduction in activation memory\n",
    "   - 20-30% slower training\n",
    "   - Essential for long sequences\n",
    "\n",
    "**5. Gradient accumulation**\n",
    "   - Lets you use smaller batches\n",
    "   - Maintains training stability\n",
    "   - Slight speed cost\n",
    "\n",
    "### Step 3: The Extreme Measures\n",
    "\n",
    "**6. 4-bit quantization (QLoRA)**\n",
    "   - 75% reduction in model memory\n",
    "   - Small quality hit (1-2%)\n",
    "   - Can only be used with LoRA\n",
    "\n",
    "**7. CPU offloading (DeepSpeed ZeRO-3)**\n",
    "   - Offloads optimizer state to CPU RAM\n",
    "   - 50-70% GPU memory reduction\n",
    "   - 60-80% slower training\n",
    "   - Use as last resort\n",
    "\n",
    "### The Rule\n",
    "\n",
    "Start simple. Add complexity only when you hit limits.\n",
    "\n",
    "Don't start with QLoRA + checkpointing + accumulation + offloading just because you can. Start with BF16 + LoRA, see how far that gets you, and add techniques as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:20:57.564708Z",
     "iopub.status.busy": "2025-12-10T21:20:57.564632Z",
     "iopub.status.idle": "2025-12-10T21:20:57.567168Z",
     "shell.execute_reply": "2025-12-10T21:20:57.566888Z"
    }
   },
   "source": [
    "## Complete Memory Optimization Example\n",
    "\n",
    "Here's what a fully optimized training setup looks like. This is the configuration that lets you train 7B models on consumer GPUs.\n",
    "\n",
    "**Goal:** Fine-tune Llama 7B on a 12 GB GPU (RTX 3060/3080)\n",
    "\n",
    "**The recipe:**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "# Step 1: Load model in 4-bit (QLoRA)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",              # NormalFloat4\n",
    "    bnb_4bit_use_double_quant=True,         # Double quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16   # Compute in bf16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Step 2: Add LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,              # Rank (balance capacity vs memory)\n",
    "    lora_alpha=32,     # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Which layers to adapt\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Step 3: Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Step 4: Configure training with small batches + accumulation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-7b-finetuned\",\n",
    "    \n",
    "    # Memory settings\n",
    "    per_device_train_batch_size=4,      # Small batch per GPU\n",
    "    gradient_accumulation_steps=8,       # Effective batch = 4 × 8 = 32\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,                           # Use BF16 (if supported)\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    ")\n",
    "\n",
    "# Step 5: Train!\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,  # Reasonable sequence length\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "**Memory Breakdown:**\n",
    "\n",
    "```\n",
    "Base model (4-bit):     ~3.5 GB\n",
    "LoRA params (bf16):     ~67 MB\n",
    "Optimizer state:        ~268 MB\n",
    "Gradients:              ~67 MB\n",
    "Activations (bs=4):     ~5 GB (with checkpointing)\n",
    "Framework overhead:     ~500 MB\n",
    "──────────────────────────────\n",
    "Total:                  ~9.4 GB\n",
    "```\n",
    "\n",
    "Fits comfortably on a 12 GB GPU with room to spare!\n",
    "\n",
    "This same approach scales:\n",
    "- 13B model on 24 GB GPU (RTX 4090)\n",
    "- 70B model on 48 GB GPU (A6000)\n",
    "\n",
    "The QLoRA revolution made this all possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference: Memory Optimization Techniques\n",
    "\n",
    "Here's your cheat sheet. Bookmark this cell.\n",
    "\n",
    "| Technique | Memory Savings | Speed Impact | Quality Impact | When to Use |\n",
    "|-----------|----------------|--------------|----------------|-------------|\n",
    "| **Mixed Precision (BF16)** | 50% | +20% faster | Negligible | Always |\n",
    "| **LoRA** | 80-95% optimizer | None | Excellent | Large models (>1B) |\n",
    "| **Gradient Accumulation** | Enables larger effective batch | -10-20% | None | Memory-limited |\n",
    "| **Gradient Checkpointing** | 50-80% activations | -20-30% | None | Long sequences or tight memory |\n",
    "| **Quantization (4-bit)** | 75% model | -10-20% | Small (~1-2%) | Extreme constraints |\n",
    "| **CPU Offloading** | 50-70% optimizer | -60-80% | None | Last resort |\n",
    "\n",
    "### The Impact Hierarchy\n",
    "\n",
    "**Most impact for least effort:**\n",
    "1. Mixed precision (BF16)\n",
    "2. LoRA\n",
    "3. Gradient checkpointing\n",
    "\n",
    "**When you're desperate:**\n",
    "4. Quantization (4-bit)\n",
    "5. CPU offloading\n",
    "\n",
    "### Model Size → GPU Requirements\n",
    "\n",
    "With all optimizations (QLoRA + gradient checkpointing + BF16):\n",
    "\n",
    "| Model Size | Min GPU Memory | Example Cards |\n",
    "|------------|----------------|---------------|\n",
    "| 3B | 6 GB | RTX 3060, RTX 2060 |\n",
    "| 7B | 10 GB | RTX 3080, RTX 2080 Ti |\n",
    "| 13B | 16 GB | RTX 4080, RTX 3090 |\n",
    "| 30B | 24 GB | RTX 4090, A5000 |\n",
    "| 70B | 48 GB | A6000, 2×RTX 4090 |\n",
    "\n",
    "These are realistic fine-tuning numbers, not theoretical minimums.\n",
    "\n",
    "### Remember\n",
    "\n",
    "The goal isn't to use every technique. The goal is to use the **minimum** number of techniques needed to fit your model in memory while maintaining reasonable training speed.\n",
    "\n",
    "Start simple. Add complexity only when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You now have the tools to fit large models on limited hardware. But memory is just one piece of the puzzle.\n",
    "\n",
    "Next up: **hyperparameter tuning**. Learning rate schedules, warmup steps, weight decay—all the knobs you can turn to make your model actually learn well (not just fit in memory).\n",
    "\n",
    "Because a model that fits in memory but doesn't train well is... not very useful.\n",
    "\n",
    "(But hey, at least it runs!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "306736f6e901439a81b25d911b69c924": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "315a618055614f4b84f9654a73df10de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "38a542970a964ed6b87b85adeebe3e1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4e245ef6d5fc444aa2596d06dfefe2e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_38a542970a964ed6b87b85adeebe3e1b",
       "placeholder": "​",
       "style": "IPY_MODEL_306736f6e901439a81b25d911b69c924",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "7440d3f9eec44502aa1f64cdf4c00ddb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "85c1614e3888467ca16fbb130aa46eb4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ad26197d7604a979f30ab732313240f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d1ec181931cf45feba92949cf8cc2bcd",
       "placeholder": "​",
       "style": "IPY_MODEL_315a618055614f4b84f9654a73df10de",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:04&lt;00:00,  2.07s/it]"
      }
     },
     "add6ee33b2994a739407fff3f03b7657": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d1ec181931cf45feba92949cf8cc2bcd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8d153cca2ac468ba655a09d56a047b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_85c1614e3888467ca16fbb130aa46eb4",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_add6ee33b2994a739407fff3f03b7657",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "ee46c39d45e84eeb9e56c847142afbf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4e245ef6d5fc444aa2596d06dfefe2e6",
        "IPY_MODEL_d8d153cca2ac468ba655a09d56a047b8",
        "IPY_MODEL_9ad26197d7604a979f30ab732313240f"
       ],
       "layout": "IPY_MODEL_7440d3f9eec44502aa1f64cdf4c00ddb",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
