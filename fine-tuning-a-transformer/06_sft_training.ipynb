{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Training a Transformer: The Main Event\n\nAlright. Deep breath.\n\nWe've learned about tokenization, formatting, and all the prep work. Now it's time to actually *train* the thing.\n\nThis is where the magic happens (though honestly, it's more like very careful calculus than magic). We're going to take a pre-trained model and teach it new tricks through supervised fine-tuning."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The Training Journey (A Roadmap)\n\nThink of training as a road trip. You need:\n\n1. **A vehicle** (the pre-trained model and tokenizer)\n2. **Fuel** (the training data, properly formatted)\n3. **A route** (the data loader that feeds examples in batches)\n4. **Navigation** (the optimizer and learning rate scheduler)\n5. **The actual driving** (the training loop where learning happens)\n6. **Rest stops** (evaluation checkpoints)\n7. **Your destination** (the fine-tuned model, saved and ready to use)\n\nEach step matters. Skip one and you're stranded on the side of the road with a confused transformer.\n\nLet's build this piece by piece."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport numpy as np\n\n# Setup device - GPU if available, CPU otherwise\n# (Training on CPU is like walking to San Francisco. Technically possible, but...)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nif device.type == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## The Dataset Class: Preparing Training Examples\n\nRemember our Alpaca template from before? Now we need to wrap it in a PyTorch Dataset class.\n\nWhy? Because the training loop doesn't want to deal with raw data. It wants a nice, standardized interface where it can say \"give me example #42\" and get back properly formatted tensors.\n\nThink of this as a restaurant kitchen. The chef (training loop) doesn't want to deal with whole chickens and raw vegetables. They want prepped ingredients, measured and ready to cook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "ALPACA_TEMPLATE = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n{response}\"\"\"\n\nclass SFTDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\n    \n    This class does three critical things:\n    1. Formats examples with the Alpaca template\n    2. Tokenizes text into numbers the model understands\n    3. Creates labels that mask the prompt (we only train on responses!)\n    \"\"\"\n    \n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        \"\"\"How many examples do we have?\"\"\"\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \"\"\"Get one training example, fully prepped and ready.\"\"\"\n        item = self.data[idx]\n        \n        # Step 1: Format with our template\n        formatted = ALPACA_TEMPLATE.format(\n            instruction=item['instruction'],\n            response=item['output']\n        )\n        \n        # Step 2: Figure out where the response starts\n        # We need this because we DON'T want to train on the instruction part\n        # Only the response should contribute to the loss\n        prompt = ALPACA_TEMPLATE.format(\n            instruction=item['instruction'],\n            response=''  # Empty response to find where it would start\n        )\n        \n        # Step 3: Tokenize the full text\n        full_tokens = self.tokenizer(\n            formatted,\n            max_length=self.max_length,\n            truncation=True,  # Cut off if too long\n            padding='max_length',  # Pad to consistent length\n            return_tensors='pt'\n        )\n        \n        # Step 4: Tokenize just the prompt to find response boundary\n        prompt_tokens = self.tokenizer(\n            prompt,\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        response_start = prompt_tokens['input_ids'].shape[1]\n        \n        # Step 5: Create labels with masking\n        # Here's the key insight: we copy the input_ids but mask the prompt\n        # The -100 value tells PyTorch \"don't calculate loss for these tokens\"\n        labels = full_tokens['input_ids'].clone().squeeze(0)\n        labels[:response_start] = -100  # Mask the instruction\n        \n        # Also mask padding tokens (we don't want to train on padding!)\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        \n        # Return everything the training loop needs\n        return {\n            'input_ids': full_tokens['input_ids'].squeeze(0),\n            'attention_mask': full_tokens['attention_mask'].squeeze(0),\n            'labels': labels\n        }\n\n# Let's test it with a sample\nprint(\"Dataset class defined successfully!\")\nprint(\"\\nKey insight: We mask the instruction with -100 so the model only learns\")\nprint(\"to predict the response. This is supervised fine-tuning in action.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## Training Configuration: All the Knobs and Dials\n\nBefore we train, we need to make some decisions. How fast should we learn? How many examples at once? How many times through the data?\n\nThese are called hyperparameters (fancy name for \"settings we tune by hand\"). They can make or break your training run.\n\nToo fast? The model goes haywire.\nToo slow? You're waiting until the heat death of the universe.\nToo much data at once? Out of memory.\nToo little? Noisy, unstable training.\n\nThe settings below are reasonable defaults for our small demo. In the real world, you'd spend days (weeks?) tuning these."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "from dataclasses import dataclass\n\n@dataclass\nclass SFTConfig:\n    \"\"\"Configuration for SFT training.\n    \n    Let's break down what each setting does:\n    \"\"\"\n    \n    # Which model to start from\n    model_name: str = \"gpt2\"\n    \n    # Maximum sequence length (longer = more memory)\n    max_length: int = 512\n    \n    # Batch size: how many examples to process at once\n    # Bigger = faster but needs more memory\n    batch_size: int = 4\n    \n    # Learning rate: how big of steps to take when updating weights\n    # This is the single most important hyperparameter\n    learning_rate: float = 2e-4  # 0.0002 - small steps, safe and steady\n    \n    # Epochs: how many times to loop through the entire dataset\n    # One epoch = seeing every training example once\n    num_epochs: int = 3\n    \n    # Warmup steps: gradually increase learning rate at the start\n    # Prevents the model from making crazy updates early on\n    warmup_steps: int = 100\n    \n    # Gradient accumulation: simulate bigger batches without more memory\n    # Process this many batches before updating weights\n    # Effective batch size = batch_size * gradient_accumulation_steps\n    gradient_accumulation_steps: int = 4\n    \n    # Gradient clipping: prevent exploding gradients\n    # If gradients get bigger than this, scale them down\n    max_grad_norm: float = 1.0\n    \n    # How often to log progress\n    logging_steps: int = 10\n    \n    # How often to check validation loss\n    eval_steps: int = 100\n    \n    # How often to save checkpoints\n    save_steps: int = 500\n    \n    # Where to save the model\n    output_dir: str = \"./sft_output\"\n\nconfig = SFTConfig()\n\nprint(\"Training configuration:\")\nprint(\"=\" * 60)\nfor k, v in vars(config).items():\n    print(f\"  {k:.<35} {v}\")\nprint(\"=\" * 60)\nprint(f\"\\nEffective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\nprint(\"(That's how many examples we process before each weight update)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## The Training Loop: Where the Magic Happens\n\nOkay, here's the heart of it all. The training loop.\n\nThis is where we actually teach the model. It's a dance with three steps, repeated thousands of times:\n\n1. **Forward pass**: Show the model an example, see what it predicts\n2. **Backward pass**: Calculate how wrong it was, compute gradients\n3. **Update**: Adjust the weights to be a little less wrong next time\n\nRinse and repeat until the model gets good (or you run out of patience/GPU credits).\n\nThe code below looks long, but it's just those three steps wrapped in careful bookkeeping. Let me walk you through it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "def train_sft(model, tokenizer, train_dataset, eval_dataset, config):\n    \"\"\"Complete SFT training loop.\n    \n    This function orchestrates the entire training process.\n    It's like conducting an orchestra - lots of moving parts, all needing\n    to work together in harmony.\n    \"\"\"\n    \n    # Create data loaders\n    # These batch up our data and shuffle it each epoch\n    # Think of this as the assembly line that feeds examples to the model\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,  # Randomize order each epoch - helps training\n        num_workers=0  # Parallel data loading (0 = use main process)\n    )\n    \n    eval_loader = DataLoader(\n        eval_dataset,\n        batch_size=config.batch_size,\n        shuffle=False  # No need to shuffle validation data\n    )\n    \n    # Setup optimizer\n    # The optimizer is what actually updates the model weights\n    # AdamW is the gold standard for transformer training\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config.learning_rate,\n        weight_decay=0.01  # Regularization to prevent overfitting\n    )\n    \n    # Learning rate scheduler\n    # We don't use a fixed learning rate - we adjust it over time\n    # Start with warmup (gradual increase), then linear decay\n    total_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation_steps\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=config.warmup_steps,\n        num_training_steps=total_steps\n    )\n    \n    # Training state\n    model.train()  # Put model in training mode (enables dropout, etc.)\n    global_step = 0  # Total number of weight updates\n    best_eval_loss = float('inf')  # Track best model\n    \n    # The main training loop - iterate over epochs\n    # An epoch = one complete pass through the training data\n    for epoch in range(config.num_epochs):\n        epoch_loss = 0\n        \n        # Progress bar for visual feedback (because watching loss decrease is satisfying)\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n        \n        # Iterate over batches within this epoch\n        for step, batch in enumerate(progress_bar):\n            # Move batch to GPU (if available)\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # ===== FORWARD PASS =====\n            # Feed the input through the model, get predictions and loss\n            outputs = model(\n                input_ids=batch['input_ids'],\n                attention_mask=batch['attention_mask'],\n                labels=batch['labels']  # The model computes loss for us!\n            )\n            \n            # Scale loss by accumulation steps\n            # Why? Because we're going to accumulate gradients across multiple batches\n            loss = outputs.loss / config.gradient_accumulation_steps\n            \n            # ===== BACKWARD PASS =====\n            # Compute gradients - this is where the learning happens\n            # PyTorch automagically calculates how to adjust each weight\n            loss.backward()\n            \n            epoch_loss += loss.item() * config.gradient_accumulation_steps\n            \n            # ===== UPDATE WEIGHTS =====\n            # Only update every gradient_accumulation_steps\n            # This simulates a larger batch size without using more memory\n            if (step + 1) % config.gradient_accumulation_steps == 0:\n                # Clip gradients to prevent explosions\n                # Sometimes gradients get REALLY big - this caps them\n                torch.nn.utils.clip_grad_norm_(\n                    model.parameters(),\n                    config.max_grad_norm\n                )\n                \n                # Take the optimization step\n                optimizer.step()\n                scheduler.step()  # Update learning rate\n                optimizer.zero_grad()  # Clear gradients for next batch\n                \n                global_step += 1\n                \n                # Log progress\n                if global_step % config.logging_steps == 0:\n                    avg_loss = epoch_loss / (step + 1)\n                    progress_bar.set_postfix({\n                        'loss': f'{avg_loss:.4f}',\n                        'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n                    })\n        \n        # End of epoch - check how we're doing on validation data\n        eval_loss = evaluate(model, eval_loader, device)\n        print(f\"\\nEpoch {epoch+1} - Train Loss: {epoch_loss/len(train_loader):.4f}, Eval Loss: {eval_loss:.4f}\")\n        \n        # Save best model\n        # We keep the model with the lowest validation loss\n        if eval_loss < best_eval_loss:\n            best_eval_loss = eval_loss\n            model.save_pretrained(f\"{config.output_dir}/best\")\n            tokenizer.save_pretrained(f\"{config.output_dir}/best\")\n            print(f\"Saved best model with eval loss: {eval_loss:.4f}\")\n    \n    return model\n\n\ndef evaluate(model, eval_loader, device):\n    \"\"\"Evaluate model on validation set.\n    \n    This tells us how well the model generalizes to new data.\n    If training loss goes down but eval loss goes up... that's overfitting.\n    Bad news bears.\n    \"\"\"\n    model.eval()  # Put model in evaluation mode (disables dropout, etc.)\n    total_loss = 0\n    \n    # No gradients needed for evaluation - saves memory and time\n    with torch.no_grad():\n        for batch in eval_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(\n                input_ids=batch['input_ids'],\n                attention_mask=batch['attention_mask'],\n                labels=batch['labels']\n            )\n            total_loss += outputs.loss.item()\n    \n    model.train()  # Put model back in training mode\n    return total_loss / len(eval_loader)\n\nprint(\"Training loop defined!\")\nprint(\"\\nKey concepts:\")\nprint(\"  • Forward pass: Model makes predictions\")\nprint(\"  • Backward pass: Calculate gradients (how to improve)\")\nprint(\"  • Optimizer step: Actually update the weights\")\nprint(\"  • Gradient accumulation: Simulate bigger batches\")\nprint(\"  • Validation: Check if we're learning or just memorizing\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## Setting Up for Training\n\nAlright, let's load our model and data. This is the pre-flight checklist before takeoff.\n\nWe'll use GPT-2 as our base model (it's small enough to train quickly but still powerful), and the Alpaca dataset for training examples.\n\nFor this demo, we're using a tiny subset of the data. In the real world, you'd use the full dataset and let it run for hours (days?). But we're trying to learn here, not max out your electricity bill."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Load model and tokenizer\nmodel_name = \"gpt2\"\nprint(f\"Loading {model_name}...\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# GPT-2 doesn't have a pad token by default, so we'll use the EOS token\n# (This is a common trick - pad tokens are just for batching anyway)\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# Move model to GPU\nmodel.to(device)\nprint(f\"✓ Model loaded and moved to {device}\")\n\n# Load dataset\nprint(\"\\nLoading Alpaca dataset...\")\nraw_data = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n\n# Use a small subset for this demo\n# (Training on 1000 examples instead of 52,000 - your GPU will thank me)\nraw_data = raw_data.select(range(1000))\nprint(f\"✓ Loaded {len(raw_data)} examples\")\n\n# Split into train/eval (90/10 split)\ntrain_size = int(0.9 * len(raw_data))\ntrain_data = raw_data.select(range(train_size))\neval_data = raw_data.select(range(train_size, len(raw_data)))\n\n# Create datasets using our SFTDataset class\n# We use shorter sequences (256) to save memory\ntrain_dataset = SFTDataset(train_data, tokenizer, max_length=256)\neval_dataset = SFTDataset(eval_data, tokenizer, max_length=256)\n\nprint(f\"✓ Created datasets:\")\nprint(f\"  - Training: {len(train_dataset)} examples\")\nprint(f\"  - Validation: {len(eval_dataset)} examples\")\n\n# Let's peek at one example to make sure everything looks right\nsample = train_dataset[0]\nprint(f\"\\nSample training example:\")\nprint(f\"  - Input IDs shape: {sample['input_ids'].shape}\")\nprint(f\"  - Labels shape: {sample['labels'].shape}\")\nprint(f\"  - Number of masked tokens: {(sample['labels'] == -100).sum().item()}\")\nprint(f\"  - Number of trainable tokens: {(sample['labels'] != -100).sum().item()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Adjust config for a quick demo run\nconfig.num_epochs = 1\nconfig.max_length = 256  # Shorter sequences = faster training\n\nprint(\"Starting training...\")\nprint(\"=\" * 60)\n\n# This will take a few minutes\n# Watch the loss decrease - that's learning in action!\nmodel = train_sft(model, tokenizer, train_dataset, eval_dataset, config)\n\nprint(\"=\" * 60)\nprint(\"✓ Training complete!\")\nprint(\"\\nWhat just happened?\")\nprint(\"  • The model saw 900 training examples\")\nprint(\"  • It adjusted its weights to minimize prediction errors\")\nprint(\"  • We validated on 100 held-out examples to check generalization\")\nprint(\"  • The best model was saved to disk\")\nprint(\"\\nNext up: Let's see if it actually learned anything!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "## Testing the Fine-Tuned Model\n\nThe proof is in the pudding. Let's see if our model can actually follow instructions now.\n\nWe'll give it a prompt and watch it generate a response. If training worked, it should follow the Alpaca format and give helpful answers.\n\nIf it starts rambling about random nonsense... well, that's why we have validation loss. (And why we save the best checkpoint, not the final one.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "def generate_response(model, tokenizer, instruction, max_new_tokens=100):\n    \"\"\"Generate a response for an instruction.\n    \n    This is the moment of truth - does the model actually follow instructions?\n    \"\"\"\n    # Format the prompt using our Alpaca template\n    prompt = ALPACA_TEMPLATE.format(instruction=instruction, response='')\n    \n    # Tokenize and move to device\n    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n    \n    # Generate!\n    with torch.no_grad():  # No gradients needed for inference\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,  # Sampling for more natural text\n            temperature=0.7,  # Higher = more random, lower = more conservative\n            top_p=0.9,  # Nucleus sampling - keep top 90% probability mass\n            pad_token_id=tokenizer.pad_token_id\n        )\n    \n    # Decode the generated tokens back to text\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract just the response part (after \"### Response:\")\n    response = response.split(\"### Response:\")[-1].strip()\n    \n    return response\n\n# Test with a few different instructions\ntest_instructions = [\n    \"Explain what machine learning is in simple terms.\",\n    \"Write a haiku about programming.\",\n    \"What are the three branches of the US government?\"\n]\n\nprint(\"Testing the fine-tuned model...\")\nprint(\"=\" * 60)\n\nfor instruction in test_instructions:\n    print(f\"\\nInstruction: {instruction}\")\n    print(\"-\" * 60)\n    response = generate_response(model, tokenizer, instruction)\n    print(f\"Response: {response}\")\n    print(\"=\" * 60)\n\nprint(\"\\nHow'd we do? The model should:\")\nprint(\"  ✓ Follow the instruction format\")\nprint(\"  ✓ Stay on topic\")\nprint(\"  ✓ Generate coherent (if not always perfect) responses\")\nprint(\"\\nRemember: We only trained for one epoch on 900 examples.\")\nprint(\"With more data and training time, it would get much better!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "## What We Learned\n\nCongratulations! You just trained a transformer from scratch (well, fine-tuned one, but that still counts).\n\nLet's recap the key concepts:\n\n**The Training Loop** is three steps repeated thousands of times:\n- Forward pass: Show the model data, get predictions\n- Backward pass: Calculate gradients (how wrong were we?)\n- Optimizer step: Update weights to be less wrong\n\n**Epochs** are complete passes through the training data. More epochs = more learning, but also more risk of overfitting.\n\n**Gradient Accumulation** lets us simulate bigger batch sizes without running out of memory. We accumulate gradients over multiple small batches, then update.\n\n**Learning Rate Scheduling** adjusts how aggressively we update weights. Start with warmup (ease into it), then gradually decrease (make smaller adjustments as we get closer to optimal).\n\n**Validation Loss** tells us if we're actually learning or just memorizing. If it starts going up while training loss goes down, that's overfitting.\n\n**Label Masking** (the -100 trick) means we only train on the parts we care about (the responses), not the prompts.\n\n---\n\nNext up: **LoRA** - a clever way to fine-tune with way fewer parameters. Because not everyone has a server farm in their basement."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}