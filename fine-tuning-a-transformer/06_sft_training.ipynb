{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SFT Training Loop\n",
    "\n",
    "**Complete implementation with best practices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Training Pipeline Overview\n",
    "\n",
    "```\n",
    "1. Load pre-trained model and tokenizer\n",
    "2. Prepare dataset with instruction formatting\n",
    "3. Create data loader with proper collation\n",
    "4. Setup optimizer and learning rate scheduler\n",
    "5. Training loop with loss masking\n",
    "6. Evaluation and checkpointing\n",
    "7. Save fine-tuned model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:14.483200Z",
     "iopub.status.busy": "2025-12-06T23:29:14.483113Z",
     "iopub.status.idle": "2025-12-06T23:29:16.298492Z",
     "shell.execute_reply": "2025-12-06T23:29:16.298148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:16.299532Z",
     "iopub.status.busy": "2025-12-06T23:29:16.299393Z",
     "iopub.status.idle": "2025-12-06T23:29:16.302043Z",
     "shell.execute_reply": "2025-12-06T23:29:16.301802Z"
    }
   },
   "outputs": [],
   "source": [
    "ALPACA_TEMPLATE = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\"\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Format with template\n",
    "        formatted = ALPACA_TEMPLATE.format(\n",
    "            instruction=item['instruction'],\n",
    "            response=item['output']\n",
    "        )\n",
    "        \n",
    "        # Find response start position (before tokenization)\n",
    "        prompt = ALPACA_TEMPLATE.format(\n",
    "            instruction=item['instruction'],\n",
    "            response=''\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        full_tokens = self.tokenizer(\n",
    "            formatted,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        prompt_tokens = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        response_start = prompt_tokens['input_ids'].shape[1]\n",
    "        \n",
    "        # Create labels with masking\n",
    "        labels = full_tokens['input_ids'].clone().squeeze(0)\n",
    "        labels[:response_start] = -100  # Mask prompt tokens\n",
    "        \n",
    "        # Also mask padding\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': full_tokens['input_ids'].squeeze(0),\n",
    "            'attention_mask': full_tokens['attention_mask'].squeeze(0),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:16.302740Z",
     "iopub.status.busy": "2025-12-06T23:29:16.302668Z",
     "iopub.status.idle": "2025-12-06T23:29:16.304932Z",
     "shell.execute_reply": "2025-12-06T23:29:16.304686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  model_name: gpt2\n",
      "  max_length: 512\n",
      "  batch_size: 4\n",
      "  learning_rate: 0.0002\n",
      "  num_epochs: 3\n",
      "  warmup_steps: 100\n",
      "  gradient_accumulation_steps: 4\n",
      "  max_grad_norm: 1.0\n",
      "  logging_steps: 10\n",
      "  eval_steps: 100\n",
      "  save_steps: 500\n",
      "  output_dir: ./sft_output\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig:\n",
    "    \"\"\"Configuration for SFT training.\"\"\"\n",
    "    model_name: str = \"gpt2\"\n",
    "    max_length: int = 512\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 2e-4\n",
    "    num_epochs: int = 3\n",
    "    warmup_steps: int = 100\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    max_grad_norm: float = 1.0\n",
    "    logging_steps: int = 10\n",
    "    eval_steps: int = 100\n",
    "    save_steps: int = 500\n",
    "    output_dir: str = \"./sft_output\"\n",
    "\n",
    "config = SFTConfig()\n",
    "print(\"Training configuration:\")\n",
    "for k, v in vars(config).items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:16.305693Z",
     "iopub.status.busy": "2025-12-06T23:29:16.305624Z",
     "iopub.status.idle": "2025-12-06T23:29:16.309461Z",
     "shell.execute_reply": "2025-12-06T23:29:16.309174Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_sft(model, tokenizer, train_dataset, eval_dataset, config):\n",
    "    \"\"\"Complete SFT training loop.\"\"\"\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    total_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config.warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    best_eval_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss / config.gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_loss += loss.item() * config.gradient_accumulation_steps\n",
    "            \n",
    "            # Update weights every gradient_accumulation_steps\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(),\n",
    "                    config.max_grad_norm\n",
    "                )\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # Logging\n",
    "                if global_step % config.logging_steps == 0:\n",
    "                    avg_loss = epoch_loss / (step + 1)\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': f'{avg_loss:.4f}',\n",
    "                        'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "                    })\n",
    "        \n",
    "        # End of epoch evaluation\n",
    "        eval_loss = evaluate(model, eval_loader, device)\n",
    "        print(f\"\\nEpoch {epoch+1} - Train Loss: {epoch_loss/len(train_loader):.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            model.save_pretrained(f\"{config.output_dir}/best\")\n",
    "            tokenizer.save_pretrained(f\"{config.output_dir}/best\")\n",
    "            print(f\"Saved best model with eval loss: {eval_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    \"\"\"Evaluate model on validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            total_loss += outputs.loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / len(eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Running Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:16.310141Z",
     "iopub.status.busy": "2025-12-06T23:29:16.310072Z",
     "iopub.status.idle": "2025-12-06T23:29:18.603427Z",
     "shell.execute_reply": "2025-12-06T23:29:18.602987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 900\n",
      "Eval samples: 100\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Load dataset (using a small subset for demonstration)\n",
    "raw_data = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "raw_data = raw_data.select(range(1000))  # Small subset for demo\n",
    "\n",
    "# Split into train/eval\n",
    "train_size = int(0.9 * len(raw_data))\n",
    "train_data = raw_data.select(range(train_size))\n",
    "eval_data = raw_data.select(range(train_size, len(raw_data)))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SFTDataset(train_data, tokenizer, max_length=256)\n",
    "eval_dataset = SFTDataset(eval_data, tokenizer, max_length=256)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Eval samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:18.604376Z",
     "iopub.status.busy": "2025-12-06T23:29:18.604282Z",
     "iopub.status.idle": "2025-12-06T23:29:18.605744Z",
     "shell.execute_reply": "2025-12-06T23:29:18.605473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train! (uncomment to run)\n",
    "# config.num_epochs = 1  # Quick test\n",
    "# model = train_sft(model, tokenizer, train_dataset, eval_dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Generating with the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:18.606511Z",
     "iopub.status.busy": "2025-12-06T23:29:18.606428Z",
     "iopub.status.idle": "2025-12-06T23:29:21.024801Z",
     "shell.execute_reply": "2025-12-06T23:29:21.024466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Explain what machine learning is in simple terms.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhubert/intro-to-transformers/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:83: UserWarning: Flash Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:316.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "/home/zhubert/intro-to-transformers/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:83: UserWarning: Mem Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:373.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: How to answer the question.\n",
      "\n",
      "### Method:\n",
      "\n",
      "How to answer the question.\n",
      "\n",
      "### Message:\n",
      "\n",
      "A list of commands.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Explain what machine learning is\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, instruction, max_new_tokens=100):\n",
    "    \"\"\"Generate a response for an instruction.\"\"\"\n",
    "    prompt = ALPACA_TEMPLATE.format(instruction=instruction, response='')\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the response part\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test generation\n",
    "test_instruction = \"Explain what machine learning is in simple terms.\"\n",
    "print(f\"Instruction: {test_instruction}\")\n",
    "print(f\"Response: {generate_response(model, tokenizer, test_instruction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have a complete SFT training loop, let's learn about LoRA for efficient fine-tuning with fewer parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
