{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Training a Transformer\n",
    "\n",
    "We've learned about tokenization, formatting, and all the prep work. Now it's time to actually *train* the thing.\n",
    "\n",
    "We're going to take a pre-trained model and teach it new tricks through supervised fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Training Journey (A Roadmap)\n",
    "\n",
    "Think of training as a road trip. You need:\n",
    "\n",
    "1. **A vehicle** (the pre-trained model and tokenizer)\n",
    "2. **Fuel** (the training data, properly formatted)\n",
    "3. **A route** (the data loader that feeds examples in batches)\n",
    "4. **Navigation** (the optimizer and learning rate scheduler)\n",
    "5. **The actual driving** (the training loop where learning happens)\n",
    "6. **Rest stops** (evaluation checkpoints)\n",
    "7. **Your destination** (the fine-tuned model, saved and ready to use)\n",
    "\n",
    "Each step matters. Skip one and you're stranded on the side of the road with a confused transformer.\n",
    "\n",
    "Let's build this piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:57:57.009671Z",
     "iopub.status.busy": "2026-01-22T01:57:57.009671Z",
     "iopub.status.idle": "2026-01-22T01:58:00.658157Z",
     "shell.execute_reply": "2026-01-22T01:58:00.658157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "Memory: 34.19 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Setup device - GPU if available, CPU otherwise\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The Dataset Class: Preparing Training Examples\n",
    "\n",
    "Remember our Alpaca template from before? Now we need to wrap it in a PyTorch Dataset class.\n",
    "\n",
    "Why? Because the training loop doesn't want to deal with raw data. It wants a nice, standardized interface where it can say \"give me example #42\" and get back properly formatted tensors.\n",
    "\n",
    "Think of this as a restaurant kitchen. The chef (training loop) doesn't want to deal with whole chickens and raw vegetables. They want prepped ingredients, measured and ready to cook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:58:00.658157Z",
     "iopub.status.busy": "2026-01-22T01:58:00.658157Z",
     "iopub.status.idle": "2026-01-22T01:58:00.664342Z",
     "shell.execute_reply": "2026-01-22T01:58:00.664342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined successfully!\n",
      "\n",
      "Key insight: We mask the instruction with -100 so the model only learns\n",
      "to predict the response. This is supervised fine-tuning in action.\n"
     ]
    }
   ],
   "source": [
    "ALPACA_TEMPLATE = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\"\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\n",
    "    \n",
    "    This class does three critical things:\n",
    "    1. Formats examples with the Alpaca template\n",
    "    2. Tokenizes text into numbers the model understands\n",
    "    3. Creates labels that mask the prompt (we only train on responses!)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"How many examples do we have?\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get one training example, fully prepped and ready.\"\"\"\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Step 1: Format with our template\n",
    "        formatted = ALPACA_TEMPLATE.format(\n",
    "            instruction=item['instruction'],\n",
    "            response=item['output']\n",
    "        )\n",
    "        \n",
    "        # Step 2: Figure out where the response starts\n",
    "        # We need this because we DON'T want to train on the instruction part\n",
    "        # Only the response should contribute to the loss\n",
    "        prompt = ALPACA_TEMPLATE.format(\n",
    "            instruction=item['instruction'],\n",
    "            response=''  # Empty response to find where it would start\n",
    "        )\n",
    "        \n",
    "        # Step 3: Tokenize the full text\n",
    "        full_tokens = self.tokenizer(\n",
    "            formatted,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,  # Cut off if too long\n",
    "            padding='max_length',  # Pad to consistent length\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Step 4: Tokenize just the prompt to find response boundary\n",
    "        prompt_tokens = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        response_start = prompt_tokens['input_ids'].shape[1]\n",
    "        \n",
    "        # Step 5: Create labels with masking\n",
    "        # Here's the key insight: we copy the input_ids but mask the prompt\n",
    "        # The -100 value tells PyTorch \"don't calculate loss for these tokens\"\n",
    "        labels = full_tokens['input_ids'].clone().squeeze(0)\n",
    "        labels[:response_start] = -100  # Mask the instruction\n",
    "        \n",
    "        # Also mask padding tokens (we don't want to train on padding!)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Return everything the training loop needs\n",
    "        return {\n",
    "            'input_ids': full_tokens['input_ids'].squeeze(0),\n",
    "            'attention_mask': full_tokens['attention_mask'].squeeze(0),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Let's test it with a sample\n",
    "print(\"Dataset class defined successfully!\")\n",
    "print(\"\\nKey insight: We mask the instruction with -100 so the model only learns\")\n",
    "print(\"to predict the response. This is supervised fine-tuning in action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Training Configuration: All the Knobs and Dials\n",
    "\n",
    "Before we train, we need to make some decisions. How fast should we learn? How many examples at once? How many times through the data?\n",
    "\n",
    "These are called hyperparameters (fancy name for \"settings we tune by hand\"). They can make or break your training run.\n",
    "\n",
    "Too fast? The model goes haywire.\n",
    "Too slow? You're waiting until the heat death of the universe.\n",
    "Too much data at once? Out of memory.\n",
    "Too little? Noisy, unstable training.\n",
    "\n",
    "The settings below are reasonable defaults for our small demo. In the real world, you'd spend days (weeks?) tuning these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:58:00.666085Z",
     "iopub.status.busy": "2026-01-22T01:58:00.664342Z",
     "iopub.status.idle": "2026-01-22T01:58:00.669418Z",
     "shell.execute_reply": "2026-01-22T01:58:00.669418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "============================================================\n",
      "  model_name......................... gpt2\n",
      "  max_length......................... 512\n",
      "  batch_size......................... 4\n",
      "  learning_rate...................... 0.0002\n",
      "  num_epochs......................... 3\n",
      "  warmup_steps....................... 100\n",
      "  gradient_accumulation_steps........ 4\n",
      "  max_grad_norm...................... 1.0\n",
      "  logging_steps...................... 10\n",
      "  eval_steps......................... 100\n",
      "  save_steps......................... 500\n",
      "  output_dir......................... ./sft_output\n",
      "============================================================\n",
      "\n",
      "Effective batch size: 16\n",
      "(That's how many examples we process before each weight update)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig:\n",
    "    \"\"\"Configuration for SFT training.\n",
    "    \n",
    "    Let's break down what each setting does:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Which model to start from\n",
    "    model_name: str = \"gpt2\"\n",
    "    \n",
    "    # Maximum sequence length (longer = more memory)\n",
    "    max_length: int = 512\n",
    "    \n",
    "    # Batch size: how many examples to process at once\n",
    "    # Bigger = faster but needs more memory\n",
    "    batch_size: int = 4\n",
    "    \n",
    "    # Learning rate: how big of steps to take when updating weights\n",
    "    # This is the single most important hyperparameter\n",
    "    learning_rate: float = 2e-4  # 0.0002 - small steps, safe and steady\n",
    "    \n",
    "    # Epochs: how many times to loop through the entire dataset\n",
    "    # One epoch = seeing every training example once\n",
    "    num_epochs: int = 3\n",
    "    \n",
    "    # Warmup steps: gradually increase learning rate at the start\n",
    "    # Prevents the model from making crazy updates early on\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "    # Gradient accumulation: simulate bigger batches without more memory\n",
    "    # Process this many batches before updating weights\n",
    "    # Effective batch size = batch_size * gradient_accumulation_steps\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # Gradient clipping: prevent exploding gradients\n",
    "    # If gradients get bigger than this, scale them down\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # How often to log progress\n",
    "    logging_steps: int = 10\n",
    "    \n",
    "    # How often to check validation loss\n",
    "    eval_steps: int = 100\n",
    "    \n",
    "    # How often to save checkpoints\n",
    "    save_steps: int = 500\n",
    "    \n",
    "    # Where to save the model\n",
    "    output_dir: str = \"./sft_output\"\n",
    "\n",
    "config = SFTConfig()\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(\"=\" * 60)\n",
    "for k, v in vars(config).items():\n",
    "    print(f\"  {k:.<35} {v}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nEffective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(\"(That's how many examples we process before each weight update)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "Okay, here's the heart of it all. The training loop.\n",
    "\n",
    "This is where we actually teach the model. It's a dance with three steps, repeated thousands of times:\n",
    "\n",
    "1. **Forward pass**: Show the model an example, see what it predicts\n",
    "2. **Backward pass**: Calculate how wrong it was, compute gradients\n",
    "3. **Update**: Adjust the weights to be a little less wrong next time\n",
    "\n",
    "Rinse and repeat until the model gets good (or you run out of patience/GPU credits).\n",
    "\n",
    "The code below looks long, but it's just those three steps wrapped in careful bookkeeping. Here's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:58:00.670421Z",
     "iopub.status.busy": "2026-01-22T01:58:00.670421Z",
     "iopub.status.idle": "2026-01-22T01:58:00.676506Z",
     "shell.execute_reply": "2026-01-22T01:58:00.676506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop defined.\n",
      "\n",
      "Key concepts:\n",
      "  • Forward pass: Model makes predictions\n",
      "  • Backward pass: Calculate gradients\n",
      "  • Optimizer step: Update the weights\n",
      "  • Gradient accumulation: Simulate bigger batches\n",
      "  • Validation: Check if we're learning or just memorizing\n"
     ]
    }
   ],
   "source": [
    "def train_sft(model, tokenizer, train_dataset, eval_dataset, config):\n",
    "    \"\"\"Complete SFT training loop.\n",
    "    \n",
    "    This function orchestrates the entire training process.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    total_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config.warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training state\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    best_eval_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss / config.gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_loss += loss.item() * config.gradient_accumulation_steps\n",
    "            \n",
    "            # Update weights\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(),\n",
    "                    config.max_grad_norm\n",
    "                )\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                if global_step % config.logging_steps == 0:\n",
    "                    avg_loss = epoch_loss / (step + 1)\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': f'{avg_loss:.4f}',\n",
    "                        'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "                    })\n",
    "        \n",
    "        # End of epoch - evaluate\n",
    "        eval_loss = evaluate(model, eval_loader, device)\n",
    "        print(f\"\\nEpoch {epoch+1} - Train Loss: {epoch_loss/len(train_loader):.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            model.save_pretrained(f\"{config.output_dir}/best\")\n",
    "            tokenizer.save_pretrained(f\"{config.output_dir}/best\")\n",
    "            print(f\"Saved best model with eval loss: {eval_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    \"\"\"Evaluate model on validation set.\n",
    "    \n",
    "    This tells us how well the model generalizes to new data.\n",
    "    If training loss goes down but eval loss goes up, that's overfitting.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            total_loss += outputs.loss.item()\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / len(eval_loader)\n",
    "\n",
    "print(\"Training loop defined.\")\n",
    "print(\"\\nKey concepts:\")\n",
    "print(\"  • Forward pass: Model makes predictions\")\n",
    "print(\"  • Backward pass: Calculate gradients\")\n",
    "print(\"  • Optimizer step: Update the weights\")\n",
    "print(\"  • Gradient accumulation: Simulate bigger batches\")\n",
    "print(\"  • Validation: Check if we're learning or just memorizing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Setting Up for Training\n",
    "\n",
    "Alright, let's load our model and data. This is the pre-flight checklist before takeoff.\n",
    "\n",
    "We'll use GPT-2 as our base model (it's small enough to train quickly but still powerful), and the Alpaca dataset for training examples.\n",
    "\n",
    "For this demo, we're using a tiny subset of the data. In the real world, you'd use the full dataset and let it run for hours (days?). But we're trying to learn here, not max out your electricity bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:58:00.676506Z",
     "iopub.status.busy": "2026-01-22T01:58:00.676506Z",
     "iopub.status.idle": "2026-01-22T01:58:03.911702Z",
     "shell.execute_reply": "2026-01-22T01:58:03.911702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded and moved to cuda\n",
      "\n",
      "Loading Alpaca dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 1000 examples\n",
      "✓ Created datasets:\n",
      "  - Training: 900 examples\n",
      "  - Validation: 100 examples\n",
      "\n",
      "Sample training example:\n",
      "  - Input IDs shape: torch.Size([256])\n",
      "  - Labels shape: torch.Size([256])\n",
      "  - Number of masked tokens: 106\n",
      "  - Number of trainable tokens: 150\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a pad token by default, so we'll use the EOS token\n",
    "# (This is a common trick - pad tokens are just for batching anyway)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "print(f\"✓ Model loaded and moved to {device}\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"\\nLoading Alpaca dataset...\")\n",
    "raw_data = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "# Use a small subset for this demo\n",
    "# (Training on 1000 examples instead of 52,000 - your GPU will thank me)\n",
    "raw_data = raw_data.select(range(1000))\n",
    "print(f\"✓ Loaded {len(raw_data)} examples\")\n",
    "\n",
    "# Split into train/eval (90/10 split)\n",
    "train_size = int(0.9 * len(raw_data))\n",
    "train_data = raw_data.select(range(train_size))\n",
    "eval_data = raw_data.select(range(train_size, len(raw_data)))\n",
    "\n",
    "# Create datasets using our SFTDataset class\n",
    "# We use shorter sequences (256) to save memory\n",
    "train_dataset = SFTDataset(train_data, tokenizer, max_length=256)\n",
    "eval_dataset = SFTDataset(eval_data, tokenizer, max_length=256)\n",
    "\n",
    "print(f\"✓ Created datasets:\")\n",
    "print(f\"  - Training: {len(train_dataset)} examples\")\n",
    "print(f\"  - Validation: {len(eval_dataset)} examples\")\n",
    "\n",
    "# Let's peek at one example to make sure everything looks right\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample training example:\")\n",
    "print(f\"  - Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  - Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"  - Number of masked tokens: {(sample['labels'] == -100).sum().item()}\")\n",
    "print(f\"  - Number of trainable tokens: {(sample['labels'] != -100).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:58:03.911702Z",
     "iopub.status.busy": "2026-01-22T01:58:03.911702Z",
     "iopub.status.idle": "2026-01-22T01:58:11.934511Z",
     "shell.execute_reply": "2026-01-22T01:58:11.934511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33e9a5513364227bc7c20a610f3d337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - Train Loss: 2.5128, Eval Loss: 2.4273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model with eval loss: 2.4273\n",
      "============================================================\n",
      "Training complete.\n",
      "\n",
      "What happened:\n",
      "  • The model saw 900 training examples\n",
      "  • It adjusted its weights to minimize prediction errors\n",
      "  • We validated on 100 held-out examples to check generalization\n",
      "  • The best model was saved to disk\n"
     ]
    }
   ],
   "source": [
    "# Adjust config for a quick demo run\n",
    "config.num_epochs = 1\n",
    "config.max_length = 256\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = train_sft(model, tokenizer, train_dataset, eval_dataset, config)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training complete.\")\n",
    "print(\"\\nWhat happened:\")\n",
    "print(\"  • The model saw 900 training examples\")\n",
    "print(\"  • It adjusted its weights to minimize prediction errors\")\n",
    "print(\"  • We validated on 100 held-out examples to check generalization\")\n",
    "print(\"  • The best model was saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Testing the Fine-Tuned Model\n",
    "\n",
    "Let's see if our model can actually follow instructions now.\n",
    "\n",
    "We'll give it a prompt and watch it generate a response. If training worked, it should follow the Alpaca format and give helpful answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:58:11.935488Z",
     "iopub.status.busy": "2026-01-22T01:58:11.935488Z",
     "iopub.status.idle": "2026-01-22T01:58:14.498516Z",
     "shell.execute_reply": "2026-01-22T01:58:14.498516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the fine-tuned model...\n",
      "============================================================\n",
      "\n",
      "Instruction: Explain what machine learning is in simple terms.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Machine learning is the study of the algorithms that make up a computer's programming language. The language that the computer uses to solve a problem is called a machine learning model, or ML. ML is a classification system, where a group of neural networks that have the same computational power as the computer can be trained to perform the task. The machine learning model is then used to learn the new algorithms, and it can be used to generate new models.\n",
      "\n",
      "Machine learning is a form of machine learning,\n",
      "============================================================\n",
      "\n",
      "Instruction: Write a haiku about programming.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: In the hiku, a hiku is a collection of letters that have a specific meaning. It is a way to express one's feelings, ideas, and experiences.\n",
      "\n",
      "\n",
      "In the hiku, the letter is a type of letter that is used to convey a message, or to convey a feeling. It can be a simple letter, or a letter that is often used to convey a specific emotion, or a feeling.\n",
      "\n",
      "The hiku is a form of poetry, which is\n",
      "============================================================\n",
      "\n",
      "Instruction: What are the three branches of the US government?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The three branches of the US government are:\n",
      "\n",
      "\n",
      "1. The Federal Reserve\n",
      "\n",
      "2. the Federal Reserve\n",
      "3. the Treasury\n",
      "\n",
      "4. the Federal Reserve Act\n",
      "\n",
      "5. the Federal Reserve Act\n",
      "\n",
      "6. the Federal Reserve Act\n",
      "\n",
      "7. the Federal Reserve Act of 1913\n",
      "\n",
      "8. the Federal Reserve Act of 1913\n",
      "\n",
      "9. the Federal Reserve Act of 1913\n",
      "\n",
      "10. the Federal Reserve Act of 1913\n",
      "\n",
      "11. the Federal Reserve\n",
      "============================================================\n",
      "\n",
      "The model should:\n",
      "  ✓ Follow the instruction format\n",
      "  ✓ Stay on topic\n",
      "  ✓ Generate coherent responses\n",
      "\n",
      "Remember: We only trained for one epoch on 900 examples.\n",
      "With more data and training time, it would get much better.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, instruction, max_new_tokens=100):\n",
    "    \"\"\"Generate a response for an instruction.\"\"\"\n",
    "    prompt = ALPACA_TEMPLATE.format(instruction=instruction, response='')\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test with a few different instructions\n",
    "test_instructions = [\n",
    "    \"Explain what machine learning is in simple terms.\",\n",
    "    \"Write a haiku about programming.\",\n",
    "    \"What are the three branches of the US government?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    print(f\"\\nInstruction: {instruction}\")\n",
    "    print(\"-\" * 60)\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nThe model should:\")\n",
    "print(\"  ✓ Follow the instruction format\")\n",
    "print(\"  ✓ Stay on topic\")\n",
    "print(\"  ✓ Generate coherent responses\")\n",
    "print(\"\\nRemember: We only trained for one epoch on 900 examples.\")\n",
    "print(\"With more data and training time, it would get much better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## What We Learned\n",
    "\n",
    "You just trained a transformer.\n",
    "\n",
    "Key concepts:\n",
    "\n",
    "**The Training Loop** is three steps repeated thousands of times:\n",
    "- Forward pass: Show the model data, get predictions\n",
    "- Backward pass: Calculate gradients (how wrong were we?)\n",
    "- Optimizer step: Update weights to be less wrong\n",
    "\n",
    "**Epochs** are complete passes through the training data. More epochs = more learning, but also more risk of overfitting.\n",
    "\n",
    "**Gradient Accumulation** lets us simulate bigger batch sizes without running out of memory. We accumulate gradients over multiple small batches, then update.\n",
    "\n",
    "**Learning Rate Scheduling** adjusts how aggressively we update weights. Start with warmup (ease into it), then gradually decrease (make smaller adjustments as we get closer to optimal).\n",
    "\n",
    "**Validation Loss** tells us if we're actually learning or just memorizing. If it starts going up while training loss goes down, that's overfitting.\n",
    "\n",
    "**Label Masking** (the -100 trick) means we only train on the parts we care about (the responses), not the prompts.\n",
    "\n",
    "---\n",
    "\n",
    "Next up: **LoRA** - a clever way to fine-tune with way fewer parameters."
   ]
  }
 ],
 "metadata": {
  "description": "Implements full supervised fine-tuning loop with proper data formatting, loss masking, and evaluation.",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "151b9ac219704f0d9827ae790a218342": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b5fb3c0f82c14ee3972e0545a504c529",
       "placeholder": "​",
       "style": "IPY_MODEL_4b88d84f2808440bb3164cd52810b2f8",
       "tabbable": null,
       "tooltip": null,
       "value": " 225/225 [00:07&lt;00:00, 31.49it/s, loss=2.5424, lr=1.00e-04]"
      }
     },
     "20d4e7da177148fdb55a73c126388cf7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4b88d84f2808440bb3164cd52810b2f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "620202e6ff2d49d29547eefa0f5b5112": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "994945116b8f47feb20ac019a486a6b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a4d7a60f432c4357b7b59f03fa5a1929": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a60f374feea34ca888ba036e366a4583": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a4d7a60f432c4357b7b59f03fa5a1929",
       "placeholder": "​",
       "style": "IPY_MODEL_20d4e7da177148fdb55a73c126388cf7",
       "tabbable": null,
       "tooltip": null,
       "value": "Epoch 1/1: 100%"
      }
     },
     "b5fb3c0f82c14ee3972e0545a504c529": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf76874e70484ea8acd655d7961c7b71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_620202e6ff2d49d29547eefa0f5b5112",
       "max": 225.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_994945116b8f47feb20ac019a486a6b1",
       "tabbable": null,
       "tooltip": null,
       "value": 225.0
      }
     },
     "d33e9a5513364227bc7c20a610f3d337": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a60f374feea34ca888ba036e366a4583",
        "IPY_MODEL_cf76874e70484ea8acd655d7961c7b71",
        "IPY_MODEL_151b9ac219704f0d9827ae790a218342"
       ],
       "layout": "IPY_MODEL_e87b794096314cfd81aa68770b0e07e7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e87b794096314cfd81aa68770b0e07e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
