{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# DPO vs RLHF: A Tale of Two Training Methods\n\nSo you've seen both DPO (Direct Preference Optimization) and RLHF (Reinforcement Learning from Human Feedback) in action. They both accomplish the same goal ‚Äî teaching a language model to follow human preferences ‚Äî but they take very different paths to get there.\n\nThink of it like this: RLHF is the scenic route with multiple stops, while DPO is the highway straight to your destination.\n\nLet's break down what makes them different, when you'd choose one over the other, and why DPO has become the darling of the open-source world. (Spoiler: it's simpler and cheaper.)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The Training Pipeline: Where the Paths Diverge\n\nHere's where things get interesting.\n\n### RLHF: The Three-Act Structure\n\nRLHF follows a three-stage process:\n\n```\n1. Supervised Fine-Tuning (SFT) ‚Äî teach the model to respond well\n2. Train a Reward Model ‚Äî teach a critic to score responses\n3. PPO Training ‚Äî let the model learn from the critic's feedback\n```\n\nIt's like training a chef: first you show them how to cook (SFT), then you train a food critic to score dishes (reward model), then the chef practices making dishes while the critic provides feedback (PPO).\n\n### DPO: Cutting Out the Middleman\n\nDPO simplifies this to just two stages:\n\n```\n1. Supervised Fine-Tuning (SFT) ‚Äî teach the model to respond well\n2. DPO on preference pairs ‚Äî directly optimize preferences\n```\n\nSame chef analogy, but skip the critic entirely. Instead, you just show the chef pairs of dishes and tell them \"this one is better than that one\" repeatedly until they get it.\n\nThe reward model stage? Gone. The complex reinforcement learning? Gone. Just you, your model, and preference data.\n\nThis isn't just conceptually simpler ‚Äî it has massive practical implications."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Memory: The Real Budget Killer\n\nLet's talk GPU memory. Because if you've ever tried to fine-tune a large model, you know this is where the rubber meets the road.\n\n### RLHF's Ensemble Cast\n\nDuring PPO training, RLHF needs to keep **four different models** in memory:\n\n1. **Policy Model** (trainable) ‚Äî the model we're actually training\n2. **Value Model** (trainable) ‚Äî estimates future rewards\n3. **Reward Model** (frozen) ‚Äî scores the quality of responses\n4. **Reference Model** (frozen) ‚Äî prevents the policy from drifting too far\n\nFour models. At the same time. In GPU memory.\n\nIf you're training a 7B parameter model, you're effectively using memory for ~28B parameters worth of models. That's... a lot.\n\n### DPO's Minimalist Approach\n\nDPO only needs **two models**:\n\n1. **Policy Model** (trainable) ‚Äî the model we're training\n2. **Reference Model** (frozen) ‚Äî prevents drift from the starting point\n\nThat's it. No reward model, no value model. Just the essentials.\n\n**The math is simple: DPO uses roughly half the memory of RLHF.**\n\nThis isn't just theoretical ‚Äî it means the difference between needing 4x A100s vs 2x A100s. Between \"we can do this\" and \"we need more budget.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:09.754557Z",
     "iopub.status.busy": "2025-12-06T23:30:09.754462Z",
     "iopub.status.idle": "2025-12-06T23:30:10.012468Z",
     "shell.execute_reply": "2025-12-06T23:30:10.012114Z"
    }
   },
   "outputs": [],
   "source": "# Let's visualize that memory difference\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# RLHF visualization\nmodels_rlhf = ['Policy\\n(trainable)', 'Value\\n(trainable)', 'Reward\\n(frozen)', 'Reference\\n(frozen)']\ncolors_rlhf = ['#3498db', '#2ecc71', '#e74c3c', '#95a5a6']\naxes[0].bar(range(4), [1, 1, 1, 1], color=colors_rlhf, edgecolor='black', linewidth=1.5)\naxes[0].set_xticks(range(4))\naxes[0].set_xticklabels(models_rlhf, fontsize=10)\naxes[0].set_ylabel('Relative Memory Usage', fontsize=12, fontweight='bold')\naxes[0].set_title('RLHF: 4 Models in Memory', fontsize=14, fontweight='bold')\naxes[0].set_ylim(0, 1.3)\naxes[0].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\naxes[0].text(1.5, 1.15, 'Total: ~4x base model size', ha='center', fontsize=11, \n             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n# DPO visualization\nmodels_dpo = ['Policy\\n(trainable)', 'Reference\\n(frozen)']\ncolors_dpo = ['#3498db', '#95a5a6']\naxes[1].bar(range(2), [1, 1], color=colors_dpo, edgecolor='black', linewidth=1.5)\naxes[1].set_xticks(range(2))\naxes[1].set_xticklabels(models_dpo, fontsize=10)\naxes[1].set_ylabel('Relative Memory Usage', fontsize=12, fontweight='bold')\naxes[1].set_title('DPO: 2 Models in Memory', fontsize=14, fontweight='bold')\naxes[1].set_ylim(0, 1.3)\naxes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\naxes[1].text(0.5, 1.15, 'Total: ~2x base model size', ha='center', fontsize=11,\n             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMemory Comparison Summary:\")\nprint(\"=\" * 50)\nprint(f\"RLHF requires: 4 models simultaneously\")\nprint(f\"DPO requires:  2 models simultaneously\")\nprint(f\"\\nMemory savings: ~50% with DPO\")\nprint(f\"\\nFor a 7B parameter model:\")\nprint(f\"  RLHF: ~28B parameters worth of memory\")\nprint(f\"  DPO:  ~14B parameters worth of memory\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## Training Stability: The Headache Factor\n\nHere's where DPO really shines. Let me explain through the lens of what can go wrong.\n\n### RLHF's Stability Issues\n\nReinforcement learning is... temperamental. Here are the classic problems:\n\n**Reward Hacking**: Remember how we said the reward model scores responses? Well, the policy model can learn to exploit quirks in the reward model instead of actually getting better. It's like a student who figures out their teacher always gives high marks for using big words, so they start throwing in \"cromulent\" and \"embiggen\" everywhere. Technically high-scoring, actually useless.\n\n**Mode Collapse**: The model might discover one type of response that scores well and just... keep generating that. Over and over. Diversity? Gone.\n\n**Training Instability**: PPO (Proximal Policy Optimization) is complex. It involves multiple training objectives, clipping, advantages, value functions... lots of moving parts. Lots of things that can break. You'll be staring at loss curves wondering why everything suddenly exploded at step 2,437.\n\n**Hyperparameter Sensitivity**: PPO has a *lot* of hyperparameters. Learning rate, KL penalty coefficient, clip range, value function coefficient, entropy bonus... getting them right is part science, part art, part voodoo.\n\n### DPO's Calm Waters\n\nDPO, by contrast, is just supervised learning in fancy clothes.\n\nYou're not doing reinforcement learning. You're not training a reward model. You're just showing the model preference pairs and using a clever loss function to push it toward preferred responses.\n\n**Reward hacking?** Can't happen ‚Äî there's no separate reward model to hack.\n\n**Mode collapse?** Much rarer ‚Äî the direct optimization on preference pairs keeps things grounded.\n\n**Training instability?** Nope ‚Äî it's supervised learning. Your loss goes down. Your model gets better. No surprises.\n\n**Hyperparameter sensitivity?** You've basically got learning rate and the beta parameter (which controls how much to stay close to the reference model). That's it.\n\nThe difference is like comparing flying a commercial airliner (RLHF) versus riding a bicycle (DPO). Sure, the airliner can carry more and go farther, but the bicycle just... works. No pre-flight checklist required."
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## Data Requirements: What You Need to Get Started\n\nThis is a more nuanced tradeoff. Let me break it down.\n\n### RLHF's Data Strategy\n\nRLHF operates in two phases with different data needs:\n\n**Phase 1 (Reward Model Training):** You need preference pairs. Lots of them. Typically thousands to tens of thousands of examples where humans said \"response A is better than response B.\"\n\n**Phase 2 (PPO Training):** You just need prompts. The model generates responses, the reward model scores them, and PPO optimizes based on those scores. You're generating new data on the fly.\n\nThis means once you've trained your reward model, you can throw tons of unlabeled prompts at it. Reddit threads, customer questions, whatever. The reward model will score the generated responses and provide learning signal.\n\n### DPO's Data Strategy\n\nDPO needs preference pairs. Full stop.\n\nEvery training example is a prompt with a chosen response and a rejected response. You can't just throw unlabeled prompts at it ‚Äî you need the human judgments baked in.\n\nThis is both a strength and a limitation:\n- **Strength**: The model learns directly from human preferences, no intermediary\n- **Limitation**: You can't easily scale up with unlabeled data\n\n### The Sample Efficiency Question\n\nHere's where it gets interesting. DPO tends to be more *sample efficient* ‚Äî it learns more from each preference pair because it's optimizing directly on what you care about.\n\nRLHF might need more preference pairs to train the reward model well, but then it can generate millions of synthetic examples during PPO training.\n\nThink of it this way:\n- **DPO**: High-quality personal training. Every data point counts.\n- **RLHF**: Learn the rules once, then practice solo for hours.\n\nWhich is better? Depends on what data you have access to."
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## Flexibility: When You Need to Iterate\n\nThis is where RLHF gets its revenge. Let me explain why.\n\n### Iterating on the Reward Function\n\nImagine you've trained your model, deployed it, and discovered a problem. Maybe it's being too verbose, or not verbose enough. Maybe it's great at coding questions but terrible at creative writing.\n\n**With RLHF**, you can:\n1. Collect new preference data for the problem area\n2. Retrain just the reward model (relatively cheap)\n3. Run PPO again with the updated reward model\n4. Keep the process running, continuously refining\n\nThe reward model is a separate component. You can swap it out, fine-tune it, even run multiple reward models for different objectives (helpfulness, harmlessness, humor, whatever).\n\n**With DPO**, you need to:\n1. Collect new preference data\n2. Combine it with your original data (or not?)\n3. Retrain the entire policy from scratch\n4. Hope it works\n\nThere's no intermediate reward model to iterate on. The preferences are baked directly into the policy training.\n\n### Multi-Objective Optimization\n\nRLHF makes it easy to balance multiple objectives. Want your model to be helpful AND harmless AND honest? Train three reward models and combine their scores. You can weight them, adjust them, experiment.\n\nDPO can do multi-objective optimization, but it's more complex. You're modifying the loss function directly, which means more math and less modularity.\n\n### Online Learning\n\nRLHF naturally supports online learning. Deploy your model, collect new prompts from real users, generate responses, score them with the reward model, continue training. It's a feedback loop.\n\nDPO needs labeled preference pairs, which means you need humans in the loop constantly. Harder to set up a continuous learning pipeline.\n\n### The Flexibility Tax\n\nHere's the thing: all this flexibility comes at a cost. Complexity. Memory. Training time. Potential instability.\n\nRLHF gives you knobs to turn and levers to pull. But you have to actually know which knobs to turn and when. DPO gives you fewer options, but the options you have just... work."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:10.013367Z",
     "iopub.status.busy": "2025-12-06T23:30:10.013275Z",
     "iopub.status.idle": "2025-12-06T23:30:10.015490Z",
     "shell.execute_reply": "2025-12-06T23:30:10.015200Z"
    }
   },
   "outputs": [],
   "source": "# Let's create a comprehensive comparison table\nimport pandas as pd\n\n# Create comparison data\ncomparison_data = {\n    'Criterion': [\n        'Simplicity',\n        'Memory Usage',\n        'Training Stability',\n        'Sample Efficiency',\n        'Flexibility',\n        'Training Time',\n        'Hyperparameters',\n        'Risk of Reward Hacking'\n    ],\n    'RLHF': [\n        'Complex (3 stages)',\n        '~4x base model',\n        'Can be unstable',\n        'Lower per pair',\n        'High',\n        'Longer',\n        'Many (~10+)',\n        'Common'\n    ],\n    'DPO': [\n        'Simple (2 stages)',\n        '~2x base model',\n        'Very stable',\n        'Higher per pair',\n        'Moderate',\n        'Shorter',\n        'Few (~2-3)',\n        'Rare'\n    ],\n    'Winner': [\n        'DPO',\n        'DPO',\n        'DPO',\n        'DPO',\n        'RLHF',\n        'DPO',\n        'DPO',\n        'DPO'\n    ]\n}\n\ndf = pd.DataFrame(comparison_data)\n\n# Display the table\nprint(\"=\" * 80)\nprint(\"COMPREHENSIVE COMPARISON: DPO vs RLHF\")\nprint(\"=\" * 80)\nprint()\nprint(df.to_string(index=False))\nprint()\nprint(\"=\" * 80)\nprint(f\"Score: DPO wins {(df['Winner'] == 'DPO').sum()}/8 categories\")\nprint(\"=\" * 80)\n\n# Visual summary\nprint(\"\\n\" + \"üèÜ \" + \"=\"*76)\nprint(\"KEY TAKEAWAY:\")\nprint(\"=\"*78)\nprint(\"\"\"\nDPO wins on: simplicity, memory, stability, sample efficiency, training time,\n             hyperparameters, and resistance to reward hacking.\n\nRLHF wins on: flexibility and iterative refinement.\n\nFor most practitioners, especially those new to alignment or with limited \ncompute, DPO is the clear choice. Use RLHF when you need maximum control\nand have the infrastructure to support it.\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Decision Time: Which Should You Choose?\n\nLet's make this practical. Here's my opinionated guide on when to use each method.\n\n### Choose DPO if:\n\n**You're new to preference learning**  \nSeriously. Start simple. DPO will teach you the core concepts without the complexity overhead. You can always graduate to RLHF later.\n\n**Memory is limited**  \nRunning on a single GPU? Limited cloud budget? DPO uses half the memory. That's the difference between \"this works\" and \"out of memory error.\"\n\n**You want stable, predictable training**  \nNo one likes debugging why their PPO training suddenly went haywire at 3 AM. DPO just works. The loss goes down. The model gets better. Sleep soundly.\n\n**You have high-quality preference data**  \nIf you've invested in collecting really good preference pairs ‚Äî diverse, well-labeled, comprehensive ‚Äî DPO will make excellent use of them.\n\n**You're doing academic research**  \nUnless you're specifically studying RLHF, DPO is usually sufficient for proving your point and it's easier to reproduce.\n\n### Choose RLHF if:\n\n**You need to iterate on the reward function**  \nBuilding a production system where requirements evolve? The ability to swap out reward models without retraining everything is powerful.\n\n**You have many prompts but limited preferences**  \nCan easily collect thousands of prompts but getting human preferences is expensive? RLHF lets you leverage that unlabeled data through generation.\n\n**You need maximum control over alignment**  \nMultiple objectives to balance? Complex safety requirements? RLHF's modularity gives you fine-grained control.\n\n**You're scaling to very large models**  \nThe major labs (OpenAI, Anthropic, etc.) still use RLHF variants for frontier models. There are reasons for this ‚Äî mostly around fine-grained control and continuous iteration.\n\n**You have the infrastructure**  \nBig compute budget, experienced ML engineers, robust training infrastructure? RLHF's complexity becomes manageable, and its flexibility becomes valuable.\n\n### The Honest Truth\n\nFor most people reading this notebook, DPO is probably the right choice.\n\nIt's simpler, cheaper, more stable, and easier to debug. The flexibility you lose is flexibility you probably don't need yet. Start with DPO, understand it deeply, and graduate to RLHF if and when you hit its limitations.\n\n(This is why you've seen DPO explode in popularity in the open-source community. It democratizes preference learning.)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## What's Next?\n\nYou've now seen both DPO and RLHF in action, and you understand the tradeoffs between them.\n\nBut here's the thing: the field keeps moving. Researchers keep publishing new variants, new improvements, new ways to get the best of both worlds. Some recent developments worth knowing about:\n\n- **RLAIF** (Reinforcement Learning from AI Feedback): Use a strong AI model instead of humans to generate preferences. Cheaper, faster, but you inherit the AI's biases.\n\n- **IPO** (Identity Preference Optimization): A DPO variant with different loss function properties. Sometimes works better than vanilla DPO.\n\n- **KTO** (Kahneman-Tversky Optimization): Uses binary feedback (good/bad) instead of pairwise preferences. Easier to collect data for.\n\n- **Hybrid approaches**: Some folks train a reward model but use it differently than traditional RLHF. Best of both worlds?\n\nThe key insight: **DPO proved that you don't need complex RL to do preference learning.**\n\nThat was the breakthrough. Everything else is refinement.\n\nSo start with DPO. Understand it deeply. Then explore the variants as needed. The fundamentals you've learned in these notebooks will carry over to whatever comes next.\n\nHappy training!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}