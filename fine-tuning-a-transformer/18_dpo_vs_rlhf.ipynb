{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# DPO vs RLHF\n",
    "\n",
    "**A comprehensive comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Training Pipeline Comparison\n",
    "\n",
    "### RLHF Pipeline\n",
    "```\n",
    "1. Supervised Fine-Tuning (SFT)\n",
    "2. Train Reward Model on preferences\n",
    "3. PPO with reward model feedback\n",
    "```\n",
    "\n",
    "### DPO Pipeline\n",
    "```\n",
    "1. Supervised Fine-Tuning (SFT)\n",
    "2. DPO on preference pairs\n",
    "```\n",
    "\n",
    "DPO eliminates the entire reward model stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Memory Requirements\n",
    "\n",
    "| Method | Models in Memory | Memory Factor |\n",
    "|--------|-----------------|---------------|\n",
    "| **RLHF** | Policy, Value, Reward, Reference | ~4x |\n",
    "| **DPO** | Policy, Reference | ~2x |\n",
    "\n",
    "DPO uses **half the memory** of RLHF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "methods = ['RLHF', 'DPO']\n",
    "models = {\n",
    "    'RLHF': ['Policy', 'Value', 'Reward', 'Reference'],\n",
    "    'DPO': ['Policy', 'Reference']\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# RLHF\n",
    "axes[0].bar(range(4), [1, 1, 1, 1], color=['blue', 'green', 'orange', 'gray'])\n",
    "axes[0].set_xticks(range(4))\n",
    "axes[0].set_xticklabels(['Policy\\n(trainable)', 'Value\\n(trainable)', 'Reward\\n(frozen)', 'Reference\\n(frozen)'])\n",
    "axes[0].set_ylabel('Relative Model Size')\n",
    "axes[0].set_title('RLHF: 4 Models')\n",
    "\n",
    "# DPO\n",
    "axes[1].bar(range(2), [1, 1], color=['blue', 'gray'])\n",
    "axes[1].set_xticks(range(2))\n",
    "axes[1].set_xticklabels(['Policy\\n(trainable)', 'Reference\\n(frozen)'])\n",
    "axes[1].set_ylabel('Relative Model Size')\n",
    "axes[1].set_title('DPO: 2 Models')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Stability Comparison\n",
    "\n",
    "| Issue | RLHF | DPO |\n",
    "|-------|------|-----|\n",
    "| **Reward hacking** | Common | Rare |\n",
    "| **Mode collapse** | Possible | Rare |\n",
    "| **Training instability** | Common | Rare |\n",
    "| **Hyperparameter sensitivity** | High | Low |\n",
    "\n",
    "DPO is significantly more stable because it's just supervised learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Sample Efficiency\n",
    "\n",
    "| Aspect | RLHF | DPO |\n",
    "|--------|------|-----|\n",
    "| **Data used** | Prompts only (generates responses) | Full preference pairs |\n",
    "| **Requires** | Many prompts for rollouts | Preference pairs |\n",
    "| **Data efficiency** | Lower | Higher |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Flexibility\n",
    "\n",
    "| Aspect | RLHF | DPO |\n",
    "|--------|------|-----|\n",
    "| **Reward iteration** | Easy (just retrain RM) | Hard (retrain everything) |\n",
    "| **Multi-objective** | Easy (multiple RMs) | Complex |\n",
    "| **Online learning** | Natural (generate + score) | Difficult |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "comparison = {\n",
    "    'Criterion': ['Simplicity', 'Memory', 'Stability', 'Sample Efficiency', 'Flexibility', 'Training Time'],\n",
    "    'RLHF': ['Low', '4x', 'Moderate', 'Lower', 'High', 'Longer'],\n",
    "    'DPO': ['High', '2x', 'High', 'Higher', 'Moderate', 'Shorter'],\n",
    "    'Winner': ['DPO', 'DPO', 'DPO', 'DPO', 'RLHF', 'DPO']\n",
    "}\n",
    "\n",
    "print(\"Summary Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Criterion':<20} {'RLHF':<12} {'DPO':<12} {'Winner':<12}\")\n",
    "print(\"-\"*60)\n",
    "for i in range(len(comparison['Criterion'])):\n",
    "    print(f\"{comparison['Criterion'][i]:<20} {comparison['RLHF'][i]:<12} {comparison['DPO'][i]:<12} {comparison['Winner'][i]:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## When to Choose Each\n",
    "\n",
    "### Choose DPO if:\n",
    "- You're new to preference learning\n",
    "- Memory is limited\n",
    "- You want stable, predictable training\n",
    "- You have good quality preference data\n",
    "\n",
    "### Choose RLHF if:\n",
    "- You need to iterate on the reward function\n",
    "- You have many prompts but limited preferences\n",
    "- You need maximum control over alignment\n",
    "- You're scaling to very large models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Let's dive deeper into the DPO loss function and understand the mathematics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
