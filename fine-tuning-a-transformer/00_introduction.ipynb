{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Transformer\n",
    "\n",
    "**From pre-trained model to aligned assistant through post-training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is Post-Training?\n",
    "\n",
    "**Post-training** (also called fine-tuning or alignment) is the process of taking a pre-trained language model and teaching it to be helpful, harmless, and honest. While pre-training teaches a model to predict the next token, post-training teaches it to:\n",
    "\n",
    "- **Follow instructions** — Respond appropriately to user requests\n",
    "- **Align with human preferences** — Generate responses humans actually prefer\n",
    "- **Refuse harmful requests** — Decline to help with dangerous or unethical tasks\n",
    "- **Be truthful** — Acknowledge uncertainty and avoid making things up\n",
    "\n",
    "This is what transforms a base model (which just completes text) into an assistant (which helps users)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Post-Training Pipeline\n",
    "\n",
    "Modern AI assistants like GPT-4, Claude, and Llama go through a multi-stage training process:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        PRE-TRAINING                                  │\n",
    "│  Train on massive text corpus to learn language patterns            │\n",
    "│  Result: Base model that can complete text                          │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│              SUPERVISED FINE-TUNING (SFT)                           │\n",
    "│  Train on (instruction, response) pairs                             │\n",
    "│  Result: Model that follows instructions                            │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│              PREFERENCE ALIGNMENT                                    │\n",
    "│  RLHF: Train reward model, then optimize with PPO                   │\n",
    "│  DPO: Directly optimize on preference pairs                         │\n",
    "│  Result: Model aligned with human preferences                       │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Learning Path\n",
    "\n",
    "In this section, we'll implement the complete post-training pipeline:\n",
    "\n",
    "1. **Why Post-Training Matters** — Understand the gap between pre-trained and aligned models\n",
    "2. **Supervised Fine-Tuning (SFT)** — Train models to follow instructions\n",
    "3. **Reward Modeling** — Learn to predict human preferences\n",
    "4. **RLHF with PPO** — Optimize models using reinforcement learning\n",
    "5. **Direct Preference Optimization (DPO)** — A simpler alternative to RLHF\n",
    "6. **Advanced Topics** — Memory optimization, hyperparameters, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:28:43.990038Z",
     "iopub.status.busy": "2025-12-06T23:28:43.989952Z",
     "iopub.status.idle": "2025-12-06T23:28:45.592611Z",
     "shell.execute_reply": "2025-12-06T23:28:45.592292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251124+rocm7.1\n",
      "CUDA available: True\n",
      "CUDA device: Radeon RX 7900 XTX\n"
     ]
    }
   ],
   "source": [
    "# Check our environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"MPS (Apple Silicon) available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## The Three Pillars of Post-Training\n",
    "\n",
    "| Stage | Input Data | Objective | Output |\n",
    "|-------|-----------|-----------|--------|\n",
    "| **SFT** | (instruction, response) pairs | Maximize P(response \\| instruction) | Instruction-following model |\n",
    "| **Reward Model** | (prompt, chosen, rejected) triples | Rank chosen > rejected | Preference predictor |\n",
    "| **RLHF/DPO** | Prompts + reward signal | Maximize expected reward | Aligned model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "**Supervised Fine-Tuning (SFT):**\n",
    "Train the model to generate good responses by showing it examples. Simple and effective, but limited by the quality and diversity of demonstrations.\n",
    "\n",
    "**Reinforcement Learning from Human Feedback (RLHF):**\n",
    "First train a reward model to predict which responses humans prefer, then use reinforcement learning (PPO) to optimize the language model to maximize this reward.\n",
    "\n",
    "**Direct Preference Optimization (DPO):**\n",
    "Skip the reward model entirely! DPO reformulates RLHF as a simple classification loss on preference pairs, making training much simpler and more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:28:45.593726Z",
     "iopub.status.busy": "2025-12-06T23:28:45.593598Z",
     "iopub.status.idle": "2025-12-06T23:28:45.595650Z",
     "shell.execute_reply": "2025-12-06T23:28:45.595359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model (just completes text):\n",
      "  Input: What is the capital of France?\n",
      "  Output: What is the capital of France? The capital of Germany? \n",
      "The capital of Italy? Th...\n",
      "\n",
      "Instruction-Tuned Model (answers questions):\n",
      "  Input: What is the capital of France?\n",
      "  Output: The capital of France is Paris. It's located in the \n",
      "north-central part of the country along the Seine River.\n"
     ]
    }
   ],
   "source": [
    "# Let's see the difference between a base model and an instruction-tuned model\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Base model completion (simulated)\n",
    "base_completion = \"\"\"What is the capital of France? The capital of Germany? \n",
    "The capital of Italy? These are questions that many students...\"\"\"\n",
    "\n",
    "# Instruction-tuned model response (simulated)\n",
    "instruct_response = \"\"\"The capital of France is Paris. It's located in the \n",
    "north-central part of the country along the Seine River.\"\"\"\n",
    "\n",
    "print(\"Base Model (just completes text):\")\n",
    "print(f\"  Input: {prompt}\")\n",
    "print(f\"  Output: {base_completion[:80]}...\")\n",
    "print()\n",
    "print(\"Instruction-Tuned Model (answers questions):\")\n",
    "print(f\"  Input: {prompt}\")\n",
    "print(f\"  Output: {instruct_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Let's Begin!\n",
    "\n",
    "In the following notebooks, we'll implement each component of the post-training pipeline with executable code you can run and modify.\n",
    "\n",
    "Ready? Let's start by understanding why post-training matters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
