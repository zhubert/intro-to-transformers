{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Fine-Tuning a Transformer\n\n**Or: How to teach a parrot to be a helpful assistant**\n\nYou know how GPT-4 and Claude can follow your instructions, answer questions, and refuse to help you build a bomb? That's not magic. That's post-training.\n\nAnd we're going to build it from scratch."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## What is Post-Training, Really?\n\nImagine you've trained a really smart parrot to predict what word comes next in any sentence. You feed it the entire internet, and boom — it can complete any text you throw at it. Impressive!\n\nBut here's the thing: that parrot doesn't know it's supposed to *help* you. Ask it \"What's the capital of France?\" and it might just continue with \"What's the capital of Germany? What's the capital of Italy?\" Because that's what text on the internet looks like — lists of similar questions.\n\nThis is the problem with **pre-trained models**. They're brilliant at language, but they don't know they're supposed to be assistants.\n\n**Post-training** (also called fine-tuning or alignment) is how we fix this. It's the process of teaching a pre-trained language model to:\n\n- **Follow instructions** — When you ask a question, it should answer (not just complete your sentence)\n- **Align with human preferences** — Generate responses humans actually like\n- **Refuse harmful requests** — Say \"no\" to dangerous or unethical tasks\n- **Be truthful** — Admit when it doesn't know something\n\nThis is what transforms a base model (fancy autocomplete) into an assistant (actually helpful).\n\nThink of it like this: pre-training teaches you grammar and vocabulary by reading every book in the library. Post-training is like going to charm school to learn *how to have a conversation*."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## The Post-Training Pipeline\n\nModern AI assistants like GPT-4, Claude, and Llama all go through the same basic journey. It's a three-stage process, and each stage builds on the last:\n\n```\n┌─────────────────────────────────────────────────────────────────────┐\n│                        STAGE 1: PRE-TRAINING                        │\n│  Train on massive text corpus (like, the whole internet)            │\n│  Goal: Learn to predict the next word                               │\n│  Result: A really good autocomplete                                 │\n└─────────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│              STAGE 2: SUPERVISED FINE-TUNING (SFT)                  │\n│  Train on thousands of (instruction → response) examples            │\n│  Goal: Learn to follow instructions                                 │\n│  Result: A model that acts like an assistant                        │\n└─────────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│              STAGE 3: PREFERENCE ALIGNMENT                          │\n│  Option A — RLHF: Train reward model, optimize with PPO             │\n│  Option B — DPO: Directly learn from preference pairs               │\n│  Goal: Match what humans actually want                              │\n│  Result: A model that's helpful, harmless, and honest               │\n└─────────────────────────────────────────────────────────────────────┘\n```\n\nLet's break down what each stage actually does (we'll go deep on all of these later, don't worry)."
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## What You'll Learn\n\nWe're going to implement the complete post-training pipeline, from scratch, with real code you can run and modify. Here's the journey:\n\n1. **Why Post-Training Matters** — See the difference between base models and aligned models (it's dramatic)\n\n2. **Supervised Fine-Tuning (SFT)** — Train a model to follow instructions using example conversations\n\n3. **Reward Modeling** — Teach a model to predict which responses humans prefer\n\n4. **RLHF with PPO** — Use Reinforcement Learning from Human Feedback with Proximal Policy Optimization (yeah, it's a mouthful — we'll explain)\n\n5. **Direct Preference Optimization (DPO)** — A simpler, more stable alternative to RLHF (this is the hot new thing)\n\n6. **Advanced Topics** — Memory optimization, hyperparameter tuning, and how to actually evaluate these models\n\nBy the end, you'll understand exactly how models like GPT-4 and Claude are built. Not just conceptually — you'll have working code."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:28:43.990038Z",
     "iopub.status.busy": "2025-12-06T23:28:43.989952Z",
     "iopub.status.idle": "2025-12-06T23:28:45.592611Z",
     "shell.execute_reply": "2025-12-06T23:28:45.592292Z"
    }
   },
   "outputs": [],
   "source": "# First things first — let's check our environment\n# (Making sure we have the tools we need)\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(\"(Nice! We've got a GPU. Training will be much faster.)\")\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    print(\"MPS (Apple Silicon) available\")\n    print(\"(Apple Silicon! Also great for training.)\")\nelse:\n    print(\"(No GPU detected — training will be slower but still works!)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## The Three Stages, Side by Side\n\nHere's a quick reference for what we're building. Don't worry if the math looks scary — we'll explain every symbol when we get there.\n\n| Stage | Training Data | What We're Optimizing | What We Get |\n|-------|---------------|----------------------|-------------|\n| **SFT** | (instruction, response) pairs | Maximize P(response \\| instruction) | Model that follows instructions |\n| **Reward Model** | (prompt, chosen, rejected) triples | Predict: chosen > rejected | Model that scores responses |\n| **RLHF/DPO** | Prompts + preference data | Maximize expected reward | Model aligned with human values |\n\nA few notes on that table:\n\n- **P(response | instruction)** just means \"the probability of generating this response, given this instruction\" — in other words, we're teaching the model to imitate good examples\n\n- **\"chosen > rejected\"** means we're training the model to give higher scores to responses humans prefer\n\n- **\"expected reward\"** is the average score the model gets — we want responses that the reward model thinks are good (but we'll see why this gets tricky)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## The Three Training Methods (In Plain English)\n\nLet's talk about what each method actually *does*, without the jargon.\n\n### Supervised Fine-Tuning (SFT)\n\nThis is the simplest approach: show the model good examples and train it to imitate them.\n\nYou give it thousands of pairs like:\n- **Instruction:** \"What's the capital of France?\"\n- **Response:** \"The capital of France is Paris.\"\n\nAnd the model learns: \"Oh, when someone asks me a question, I should answer it directly.\" Simple! Effective! But limited by the quality and diversity of your examples.\n\nIt's like learning to cook by following recipes. You'll get good at the dishes you practiced, but you might struggle with variations.\n\n### Reinforcement Learning from Human Feedback (RLHF)\n\nThis is where things get interesting (and complicated).\n\nFirst, you train a **reward model** to predict which responses humans prefer. You show it pairs like:\n- **Prompt:** \"Explain quantum computing\"\n- **Response A:** \"Quantum computers use qubits...\" ✓ (humans prefer this)\n- **Response B:** \"Idk lol\" ✗ (humans reject this)\n\nThen you use **PPO** (Proximal Policy Optimization — a reinforcement learning algorithm) to train your language model to generate responses that score highly on the reward model.\n\nIt's like learning to cook by having a food critic taste everything and give you feedback. You experiment, get scored, and gradually learn what people like.\n\nThe downside? This is *complicated*. You need two models (language model + reward model), and PPO is notoriously finicky to tune.\n\n### Direct Preference Optimization (DPO)\n\nDPO is the new kid on the block, and it's elegant as hell.\n\nThe key insight: you can skip the reward model entirely! Instead of:\n1. Train reward model on preferences\n2. Use RL to optimize language model against reward model\n\nYou just:\n1. Directly optimize the language model on preference pairs\n\nIt reformulates the whole RLHF pipeline as a simple classification loss. Same results, way simpler, more stable training.\n\nIt's like learning to cook by comparing your dishes to reference examples: \"My version should taste more like the good example and less like the bad example.\" No critic needed — you learn directly from the comparisons.\n\n(We'll implement all three methods, so you can see the tradeoffs yourself.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:28:45.593726Z",
     "iopub.status.busy": "2025-12-06T23:28:45.593598Z",
     "iopub.status.idle": "2025-12-06T23:28:45.595650Z",
     "shell.execute_reply": "2025-12-06T23:28:45.595359Z"
    }
   },
   "outputs": [],
   "source": "# Let me show you the difference in action\n# (This is simulated, but it's exactly what you'd see with real models)\n\nprompt = \"What is the capital of France?\"\n\n# What a BASE MODEL does (just autocomplete)\nbase_completion = \"\"\"What is the capital of France? What is the capital of Germany? \nWhat is the capital of Italy? These are common geography questions that students \noften struggle with. Let's explore the capitals of European countries...\"\"\"\n\n# What an INSTRUCTION-TUNED MODEL does (actually helpful)\ninstruct_response = \"\"\"The capital of France is Paris. It's located in the \nnorth-central part of the country along the Seine River.\"\"\"\n\nprint(\"═\" * 70)\nprint(\"BASE MODEL (just completes text):\")\nprint(\"═\" * 70)\nprint(f\"Input: {prompt}\")\nprint(f\"\\nOutput: {base_completion}\")\nprint()\nprint(\"═\" * 70)\nprint(\"INSTRUCTION-TUNED MODEL (answers questions):\")\nprint(\"═\" * 70)\nprint(f\"Input: {prompt}\")\nprint(f\"\\nOutput: {instruct_response}\")\nprint()\nprint(\"See the difference? The base model treats your question like\")\nprint(\"the beginning of an article. The tuned model actually helps you.\")\nprint(\"That's the magic of post-training!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Let's Begin!\n\nIn the notebooks that follow, we'll implement each component of the post-training pipeline with real, runnable code. You'll see exactly how it works, not just in theory, but in practice.\n\nWe'll start small (fine-tuning a tiny model on a simple task) and build up to the full pipeline (SFT → Reward Model → RLHF/DPO). By the end, you'll understand how modern AI assistants are built, from first principles.\n\nReady? Let's go.\n\n**Next up:** Why post-training matters (with examples that'll make it click)."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}