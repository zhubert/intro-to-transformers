{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Why Post-Training Matters\n",
    "\n",
    "**Understanding the gap between pre-trained and aligned models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Pre-Training Gap\n",
    "\n",
    "A pre-trained language model is impressive—it has learned grammar, facts, reasoning patterns, and even some common sense from billions of words of text. But there's a fundamental gap between \"predicting the next word\" and \"being a helpful assistant.\"\n",
    "\n",
    "**Pre-training teaches:** Given text, predict what comes next.\n",
    "\n",
    "**Users want:** Given a question, provide a helpful answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## What Pre-Trained Models Actually Learn\n",
    "\n",
    "Pre-training optimizes:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{pretrain}} = -\\sum_{t} \\log P(x_t | x_{<t})$$\n",
    "\n",
    "This means the model learns to predict the **most likely** continuation, not the **most helpful** one.\n",
    "\n",
    "**Example:** If trained on internet text, the model learns that after \"How do I hack...\" often comes tutorials on hacking, not refusals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Base model behavior vs. aligned model behavior\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"prompt\": \"Write a poem about\",\n",
    "        \"base\": \"Write a poem about nature. The trees sway gently...\",  # Continues the text\n",
    "        \"aligned\": \"I'd be happy to write a poem for you! What topic would you like?\"  # Responds helpfully\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is 2+2?\",\n",
    "        \"base\": \"What is 2+2? What is 3+3? These basic math questions...\",  # Continues pattern\n",
    "        \"aligned\": \"2 + 2 = 4\"  # Answers directly\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Can you help me with my homework?\",\n",
    "        \"base\": \"Can you help me with my homework? asked Sarah nervously...\",  # Writes a story\n",
    "        \"aligned\": \"Of course! What subject is your homework in?\"  # Offers assistance\n",
    "    }\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    print(f\"Prompt: {ex['prompt']}\")\n",
    "    print(f\"  Base model: {ex['base'][:60]}...\")\n",
    "    print(f\"  Aligned model: {ex['aligned']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Three Problems Post-Training Solves\n",
    "\n",
    "### 1. Format Problem\n",
    "\n",
    "Pre-trained models don't know they should **respond** to questions rather than **continue** them.\n",
    "\n",
    "**SFT teaches:** When you see a question, generate an answer.\n",
    "\n",
    "### 2. Quality Problem  \n",
    "\n",
    "Many valid completions exist, but humans prefer some over others.\n",
    "\n",
    "**RLHF/DPO teaches:** Among valid responses, prefer ones humans rate highly.\n",
    "\n",
    "### 3. Safety Problem\n",
    "\n",
    "The internet contains harmful content. A model trained on it will happily generate harmful content.\n",
    "\n",
    "**Alignment teaches:** Refuse harmful requests, be honest about limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## The Alignment Tax\n",
    "\n",
    "Post-training isn't free. There's often a tradeoff:\n",
    "\n",
    "| Capability | Pre-training | Post-training | Net Effect |\n",
    "|-----------|-------------|---------------|------------|\n",
    "| Raw knowledge | +++++ | - | Slight decrease |\n",
    "| Following instructions | + | +++++ | Large increase |\n",
    "| Safety/Refusals | - | +++++ | Large increase |\n",
    "| Helpfulness | ++ | ++++ | Moderate increase |\n",
    "\n",
    "The \"alignment tax\" refers to the small decrease in raw capabilities that sometimes accompanies making models safer and more helpful. Modern techniques minimize this tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Evidence: InstructGPT Paper\n",
    "\n",
    "OpenAI's InstructGPT paper (2022) showed dramatic results:\n",
    "\n",
    "- **1.3B InstructGPT** outperformed **175B GPT-3** on human preference evaluations\n",
    "- Users preferred InstructGPT outputs 85% of the time\n",
    "- The model was more truthful and less toxic\n",
    "\n",
    "**Key insight:** A small model with good post-training beats a large model without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualization of the InstructGPT results\n",
    "models = ['GPT-3\\n(175B)', 'SFT\\n(1.3B)', 'InstructGPT\\n(1.3B)']\n",
    "human_preference = [15, 45, 85]  # Approximate win rates vs GPT-3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = ax.bar(models, human_preference, color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "ax.set_ylabel('Human Preference Win Rate (%)')\n",
    "ax.set_title('Post-Training Impact: Model Size vs Alignment')\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "for bar, val in zip(bars, human_preference):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "            f'{val}%', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: A 1.3B parameter model with RLHF beats a 175B model without it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## What We'll Build\n",
    "\n",
    "In this tutorial series, we'll implement:\n",
    "\n",
    "1. **SFT** — Supervised fine-tuning with instruction formatting and loss masking\n",
    "2. **Reward Models** — Train models to predict human preferences\n",
    "3. **RLHF** — Full PPO training loop with KL penalty\n",
    "4. **DPO** — Simpler alternative that skips the reward model\n",
    "\n",
    "By the end, you'll understand exactly how models like GPT-4 and Claude are trained after pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we understand why post-training matters, let's look at the project overview and then dive into Supervised Fine-Tuning (SFT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
