{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Why Post-Training Matters\n\n**Or: How we teach language models to be helpful instead of just...weird**\n\nYou've probably used ChatGPT or Claude. They answer questions, help with code, refuse to write malware. Pretty useful, right?\n\nBut here's the thing: the base model—the one that comes straight out of pre-training—doesn't do any of that naturally. It just predicts the next word based on internet text.\n\nThis notebook explains the gap between \"predicting text\" and \"being helpful,\" and why closing that gap matters so much."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The Pre-Training Gap\n\nHere's what's wild: a pre-trained language model has already learned an incredible amount. It knows grammar, facts about the world, how to write code, even how to reason through problems. It absorbed all of this from billions of words of text.\n\nBut—and this is crucial—it learned by playing a simple game: **predict the next word**.\n\nThat's it. Given some text, what word comes next?\n\nThis is powerful, but it creates a fundamental mismatch:\n\n**Pre-training teaches:** Given text, predict what comes next  \n**Users actually want:** Given a question, provide a helpful answer\n\nSee the problem? When you ask a base model \"What's the capital of France?\", it doesn't think \"oh, they want an answer.\" It thinks \"what text usually follows this pattern?\"\n\nSometimes that's a good answer. Sometimes it's...not."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## What Pre-Trained Models Actually Learn (The Math)\n\nLet's get specific. Pre-training optimizes this objective:\n\n$$\\mathcal{L}_{\\text{pretrain}} = -\\sum_{t} \\log P(x_t | x_{<t})$$\n\nOkay, what does that actually mean? Let's break it down symbol by symbol:\n\n- **$\\mathcal{L}_{\\text{pretrain}}$** — The pre-training loss (what we're trying to minimize). Lower is better.\n- **$\\sum_{t}$** — Sum over all positions $t$ in the text (every word/token)\n- **$x_t$** — The actual token at position $t$ (what the text really says)\n- **$x_{<t}$** — All the tokens before position $t$ (the context so far)\n- **$P(x_t | x_{<t})$** — The probability the model assigns to the correct next token, given the context\n- **$\\log$** — We take the log of that probability (math reasons—makes the optimization nicer)\n- **$-$** — We negate it, so minimizing loss = maximizing the probability of correct predictions\n\nIn plain English: **make the model assign high probability to whatever word actually comes next**.\n\nThis is brilliant for learning language. But notice what's missing: there's nothing here about being helpful, truthful, or safe. The model just learns to predict **likely** continuations, not **good** ones.\n\n**Example:** If the internet is full of text like \"How to hack into...\" followed by actual hacking tutorials, well, the model learns that pattern. It's just doing what it was trained to do—predict likely next words."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:28:47.647521Z",
     "iopub.status.busy": "2025-12-06T23:28:47.647406Z",
     "iopub.status.idle": "2025-12-06T23:28:47.650240Z",
     "shell.execute_reply": "2025-12-06T23:28:47.649956Z"
    }
   },
   "outputs": [],
   "source": "# Let's see this in action: Base model vs. aligned model behavior\n\n# These are simplified examples to illustrate the difference\nexamples = [\n    {\n        \"prompt\": \"Write a poem about\",\n        \"base\": \"Write a poem about nature. The trees sway gently in the breeze...\",\n        \"base_explanation\": \"Continues the text (found similar patterns in training data)\",\n        \"aligned\": \"I'd be happy to write a poem for you! What topic would you like?\",\n        \"aligned_explanation\": \"Recognizes this as a request and responds helpfully\"\n    },\n    {\n        \"prompt\": \"What is 2+2?\",\n        \"base\": \"What is 2+2? What is 3+3? These basic math questions...\",\n        \"base_explanation\": \"Continues the pattern (maybe from a quiz or forum post)\",\n        \"aligned\": \"2 + 2 = 4\",\n        \"aligned_explanation\": \"Understands this is a question and provides the answer\"\n    },\n    {\n        \"prompt\": \"Can you help me with my homework?\",\n        \"base\": \"Can you help me with my homework? asked Sarah nervously...\",\n        \"base_explanation\": \"Writes a story (saw similar narrative text in training)\",\n        \"aligned\": \"Of course! What subject is your homework in?\",\n        \"aligned_explanation\": \"Recognizes request and offers to assist\"\n    }\n]\n\nprint(\"=\" * 70)\nprint(\"BASE MODEL vs. ALIGNED MODEL BEHAVIOR\")\nprint(\"=\" * 70)\n\nfor i, ex in enumerate(examples, 1):\n    print(f\"\\n{i}. Prompt: \\\"{ex['prompt']}\\\"\")\n    print(f\"\\n   Base model says:\")\n    print(f\"   \\\"{ex['base']}\\\"\")\n    print(f\"   → {ex['base_explanation']}\")\n    \n    print(f\"\\n   Aligned model says:\")\n    print(f\"   \\\"{ex['aligned']}\\\"\")\n    print(f\"   → {ex['aligned_explanation']}\")\n    print()\n\nprint(\"=\" * 70)\nprint(\"Notice: Base models aren't broken—they're just optimized for a\")\nprint(\"different task. Post-training teaches them to be helpful assistants.\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## Three Problems Post-Training Solves\n\nPost-training isn't just one thing—it addresses three distinct problems. Let's break them down.\n\n### 1. The Format Problem\n\n**Issue:** Pre-trained models don't know they should **respond** to questions rather than **continue** them.\n\nWhen you type \"What's the weather like?\", a base model might continue with \"What's the weather like in London? What's the weather like in Paris?\" because that's a pattern it saw in training data (maybe a quiz or list).\n\n**Solution:** Supervised Fine-Tuning (SFT)\n\nSFT teaches the model the Q&A format using examples like:\n- **User:** \"What's the weather like?\"\n- **Assistant:** \"I don't have access to real-time weather data, but I can help you understand how to check it!\"\n\nAfter seeing thousands of these examples, the model learns to respond instead of continue.\n\n### 2. The Quality Problem  \n\n**Issue:** Many valid completions exist, but humans prefer some over others.\n\nAsk \"Explain quantum computing\" and you could get:\n- A helpful, clear explanation ✓\n- A technically correct but incomprehensible lecture ✗\n- A snarky dismissal ✗\n- \"I don't know\" (even though it clearly does) ✗\n\n**Solution:** Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO)\n\nThese techniques train models using human preferences. People rate different responses, and the model learns to produce outputs humans prefer.\n\n### 3. The Safety Problem\n\n**Issue:** The internet contains harmful content. Models trained on it will happily generate harmful content.\n\nThis isn't the model being malicious—it's just predicting text patterns. If \"How do I hack...\" is often followed by hacking tutorials in the training data, the model learns that pattern.\n\n**Solution:** Alignment training (part of RLHF/DPO)\n\nThrough post-training, models learn to:\n- Refuse harmful requests\n- Be honest about limitations\n- Avoid generating toxic, biased, or dangerous content\n\nNot perfectly—this is still an active area of research—but much better than base models."
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## The Alignment Tax\n\nOkay, here's the tradeoff nobody likes to talk about (but we should).\n\nPost-training isn't free. When you train a model to be helpful and safe, you sometimes sacrifice a bit of raw capability. This is called the \"alignment tax.\"\n\nThink of it like this: a base model will answer **anything**. No filters, no guardrails. Just pure next-token prediction. That includes answering questions it probably shouldn't, but it also means it never refuses to help with legitimate tasks.\n\nAn aligned model is more selective. It refuses harmful requests (good!), but sometimes it also refuses things it shouldn't, or gives overly cautious answers (less good).\n\nHere's roughly how it shakes out:\n\n| Capability | Pre-trained Model | After Post-Training | Net Effect |\n|-----------|------------------|---------------------|------------|\n| Raw knowledge | +++++ | ++++ | Slight decrease |\n| Following instructions | + | +++++ | Large increase |\n| Safety/Refusals | - | +++++ | Large increase |\n| Helpfulness | ++ | ++++ | Moderate increase |\n| Creativity (sometimes) | +++++ | +++ | Can decrease |\n\n**The good news:** Modern techniques (like DPO, which we'll implement) minimize this tradeoff. The InstructGPT paper showed that you can actually get **better** at almost everything with good post-training, not worse.\n\n**The key insight:** A model that refuses to help you isn't very useful, but neither is one that cheerfully explains how to build a bomb. The art is finding the right balance."
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## The Evidence: InstructGPT Changed Everything\n\nIn 2022, OpenAI published a paper that proved post-training really matters: InstructGPT.\n\nThe results were kind of shocking:\n\n- A **1.3 billion parameter** InstructGPT model outperformed the **175 billion parameter** GPT-3 base model\n- Users preferred InstructGPT outputs **85% of the time** (!)\n- The model was more truthful and less toxic\n\nLet that sink in: a model 130x smaller, with good post-training, beat the massive base model.\n\n**Why?** Because being helpful matters more than being huge.\n\nGPT-3 knew a ton—it had 175B parameters full of knowledge. But it didn't know how to be an assistant. It would:\n- Continue your question instead of answering it\n- Generate plausible-sounding nonsense\n- Happily produce toxic content\n\nInstructGPT, with way fewer parameters but good alignment training, learned to:\n- Actually answer questions\n- Admit when it didn't know something\n- Refuse harmful requests\n\nThis was a watershed moment. It showed that post-training isn't just a nice-to-have—it's essential for useful AI systems."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:28:47.651238Z",
     "iopub.status.busy": "2025-12-06T23:28:47.651165Z",
     "iopub.status.idle": "2025-12-06T23:28:47.889652Z",
     "shell.execute_reply": "2025-12-06T23:28:47.889251Z"
    }
   },
   "outputs": [],
   "source": "# Let's visualize the InstructGPT results\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# These are approximate numbers from the InstructGPT paper\nmodels = ['GPT-3\\n(175B params)', 'SFT Only\\n(1.3B params)', 'InstructGPT\\n(1.3B params)']\nhuman_preference = [15, 45, 85]  # Win rate vs GPT-3 baseline\nmodel_colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\n\nfig, ax = plt.subplots(figsize=(10, 6))\nbars = ax.bar(models, human_preference, color=model_colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n\nax.set_ylabel('Human Preference Win Rate (%)', fontsize=12, fontweight='bold')\nax.set_title('Post-Training Impact: Size Isn\\'t Everything', fontsize=14, fontweight='bold')\nax.set_ylim(0, 100)\nax.grid(axis='y', alpha=0.3, linestyle='--')\n\n# Add value labels on bars\nfor bar, val in zip(bars, human_preference):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n            f'{val}%', ha='center', fontsize=14, fontweight='bold')\n\n# Add annotations\nax.annotate('Base model:\\nJust next-token\\nprediction', \n            xy=(0, 15), xytext=(-0.3, 30),\n            arrowprops=dict(arrowstyle='->', color='gray', lw=1.5),\n            fontsize=9, ha='center')\n\nax.annotate('With RLHF:\\n130x smaller,\\nbut way better!', \n            xy=(2, 85), xytext=(2.3, 70),\n            arrowprops=dict(arrowstyle='->', color='gray', lw=1.5),\n            fontsize=9, ha='center', color='darkgreen', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"KEY INSIGHT\")\nprint(\"=\" * 70)\nprint(\"A 1.3B parameter model with RLHF beats a 175B parameter model without it.\")\nprint(\"\\nPost-training isn't optional—it's what makes models actually useful.\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## What We'll Build in This Tutorial Series\n\nAlright, enough theory. Let's talk about what you'll actually implement.\n\nWe're going to build the complete post-training pipeline—the same techniques used to create ChatGPT, Claude, and other aligned models. Here's the roadmap:\n\n### 1. Supervised Fine-Tuning (SFT)\n\nWe'll start with the basics: teaching a model the Q&A format.\n\nYou'll learn:\n- How to format instruction data (user/assistant pairs)\n- Loss masking (only train on assistant responses, not user prompts)\n- Why this alone isn't enough (but it's a critical first step)\n\n**What it does:** Turns \"text predictor\" into \"question answerer\"\n\n### 2. Reward Models\n\nBefore we can train with human feedback, we need to predict what humans prefer.\n\nYou'll learn:\n- How to train a model to score responses\n- Why we can't just use human ratings directly (spoiler: we need millions of comparisons)\n- How one reward model can guide training on billions of examples\n\n**What it does:** Learns to predict \"will humans like this response?\"\n\n### 3. Reinforcement Learning from Human Feedback (RLHF)\n\nThe full algorithm—PPO (Proximal Policy Optimization) with a KL (Kullback-Leibler divergence) penalty.\n\nYou'll learn:\n- How to use the reward model to guide training\n- Why we need a KL penalty (to prevent the model from \"hacking\" the reward)\n- The complete PPO training loop\n\n**What it does:** Optimizes for human preferences while staying close to the original model\n\n### 4. Direct Preference Optimization (DPO)\n\nA simpler, more elegant alternative to RLHF that skips the reward model entirely.\n\nYou'll learn:\n- How to train directly on preference pairs\n- Why this works as well as RLHF (or better!)\n- When to use DPO vs. RLHF\n\n**What it does:** Same goal as RLHF, but more efficient\n\n---\n\nBy the end, you'll understand exactly how models like GPT-4 and Claude go from \"impressive but weird\" to \"actually useful.\"\n\nLet's get started."
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## Next Steps\n\nNow that you understand **why** post-training matters, it's time to see **how** it works.\n\nHead to the next notebook to see the project overview, then we'll dive into Supervised Fine-Tuning (SFT)—where you'll train your first aligned model.\n\nSee you there!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}