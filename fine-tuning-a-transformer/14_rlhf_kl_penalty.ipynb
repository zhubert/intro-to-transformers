{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# KL Divergence: The Safety Belt for RLHF\n\n**Or: How we prevent our model from going completely off the rails**\n\nHere's the problem we're solving: you've trained a reward model, and now you're using it to make your language model better. But without constraints, your model might find creative (read: terrible) ways to hack the reward system.\n\nWe need a safety mechanism. Enter the KL divergence penalty.\n\n(Don't worry, we'll explain what KL divergence actually *is* in a moment. Promise.)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## What Goes Wrong Without a KL Penalty?\n\nImagine you're training a chatbot to be helpful. You've got a reward model that scores responses, and your policy model is learning to maximize those scores.\n\nSounds great, right?\n\nWell. Without constraints, here's what can happen:\n\n**Reward hacking** — Your model discovers that the reward model loves responses with lots of exclamation marks!!! So it starts adding them everywhere!!! Even when it makes no sense!!!\n\n**Mode collapse** — The model finds ONE response that scores really well and just... keeps generating variations of that same response. Over and over. It's like a song stuck on repeat.\n\n**Forgetting how to language** — In its quest for high rewards, the model might drift so far from natural language that it starts producing grammatically correct but semantically bizarre text. It technically speaks English, but it's... off.\n\nThink of it this way: you're giving a dog treats for doing tricks. Without any guidance, the dog might start doing weird, spastic movements because it got a treat once when it twitched. That's not what you wanted, but the dog is just optimizing for treats!\n\nWe need to keep the model tethered to something sensible. That's where KL divergence comes in."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## So What *Is* KL Divergence?\n\nKL stands for **Kullback-Leibler** (named after two mathematicians). But more importantly, let's talk about what it *measures*.\n\nKL divergence is a way to measure **how different two probability distributions are**.\n\n### What's a probability distribution?\n\nYour language model is basically a fancy probability machine. Given some context, it assigns probabilities to every possible next token. \"The cat sat on the ___\" might give high probability to \"mat\" and \"floor\", low probability to \"quantum\", and vanishingly small probability to \"xylophone\".\n\nThat collection of probabilities across all possible tokens? That's a probability distribution.\n\n### What's divergence?\n\n\"Divergence\" is just a fancy word for \"difference\" or \"distance\". When two distributions diverge, they're pulling apart from each other.\n\nIf I have two probability distributions and their KL divergence is:\n- **Low (near 0)** — They're very similar. Almost twins.\n- **High** — They're quite different. One assigns high probability where the other assigns low probability.\n\n### Here's the intuition\n\nImagine you have two weather forecasters:\n- **Forecaster A** says: 70% chance of rain, 30% chance of sun\n- **Forecaster B** says: 30% chance of rain, 70% chance of sun\n\nThese forecasts have high KL divergence — they're predicting very different things.\n\nBut if:\n- **Forecaster A** says: 70% chance of rain, 30% chance of sun\n- **Forecaster C** says: 65% chance of rain, 35% chance of sun\n\nThese have low KL divergence — they're pretty close.\n\nIn RLHF, we use KL divergence to measure: \"How much has my model's behavior changed from where it started?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:57.116046Z",
     "iopub.status.busy": "2025-12-06T23:29:57.115969Z",
     "iopub.status.idle": "2025-12-06T23:29:57.938664Z",
     "shell.execute_reply": "2025-12-06T23:29:57.938265Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef compute_kl_penalty(\n    logprobs: torch.Tensor,\n    ref_logprobs: torch.Tensor,\n    kl_coef: float = 0.1\n) -> torch.Tensor:\n    \"\"\"\n    Compute KL divergence penalty between policy and reference.\n    \n    The formula: KL(π || π_ref) ≈ E[log π - log π_ref]\n    \n    Translation: KL divergence is approximately the difference between\n    log probabilities under the current policy vs. the reference.\n    \n    Args:\n        logprobs: Log probabilities under current policy (the model we're training)\n        ref_logprobs: Log probabilities under reference model (the frozen copy)\n        kl_coef: How much we care about the penalty (β, typically 0.1)\n    \n    Returns:\n        KL penalty scaled by coefficient\n    \"\"\"\n    # The difference in log probabilities IS the KL divergence\n    # (well, an approximation of it for a single sample)\n    kl_divergence = logprobs - ref_logprobs\n    \n    # Scale by our penalty coefficient and average across the batch\n    kl_penalty = kl_coef * kl_divergence.mean()\n    \n    return kl_penalty\n\n# Let's try it out!\nbatch_size = 16\n# These are log probabilities, so they're negative numbers (log of values between 0 and 1)\nlogprobs = torch.randn(batch_size) - 3.0  # Current policy's log probs\nref_logprobs = torch.randn(batch_size) - 3.0  # Reference model's log probs\n\nkl_penalty = compute_kl_penalty(logprobs, ref_logprobs, kl_coef=0.1)\nprint(f\"KL Penalty: {kl_penalty.item():.4f}\")\nprint(f\"\\nWhat does this mean? If the penalty is negative, the current policy\")\nprint(f\"is actually assigning LOWER probability to these tokens than the reference.\")\nprint(f\"If positive, it's assigning HIGHER probability.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## The Complete Reward Formula\n\nOkay, now let's see how this all fits together. When we're training with RLHF, we don't just use the raw reward from the reward model. Instead, we adjust it:\n\n$$r_{\\text{total}}(x, y) = r(x, y) - \\beta \\cdot D_{\\text{KL}}(\\pi_\\theta(\\cdot|x) \\| \\pi_{\\text{ref}}(\\cdot|x))$$\n\nLet me translate each piece:\n\n- **$r_{\\text{total}}(x, y)$** — The final reward we'll use for training\n- **$r(x, y)$** — The raw score from our reward model (higher = better response)\n- **$\\beta$** — The KL coefficient (usually around 0.1). Think of this as the \"strength\" of our safety belt\n- **$D_{\\text{KL}}(...)$** — The KL divergence (how much the distributions differ)\n- **$\\pi_\\theta$** — Our current policy (the model we're actively training)\n- **$\\pi_{\\text{ref}}$** — The reference policy (a frozen copy of the model from when we started)\n- **$x$** — The input prompt\n- **$y$** — The generated response\n\n### The intuition\n\nWe take the reward model's score and **subtract a penalty** based on how far we've drifted from our starting point.\n\nHigh reward? Great! But if you had to change your behavior drastically to get there... that penalty brings you back down.\n\nIt's like a rubber band. You can stretch away from the reference model to get better rewards, but the further you stretch, the harder the band pulls you back. The parameter β controls how stiff that rubber band is."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:57.939669Z",
     "iopub.status.busy": "2025-12-06T23:29:57.939571Z",
     "iopub.status.idle": "2025-12-06T23:29:58.051154Z",
     "shell.execute_reply": "2025-12-06T23:29:58.050878Z"
    }
   },
   "outputs": [],
   "source": "# Let's visualize what different β values do to our total reward\nkl_values = np.linspace(0, 2, 100)\nbase_reward = 1.0\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Different β values — these control how \"stiff\" our rubber band is\nfor beta in [0.05, 0.1, 0.2, 0.5]:\n    total_reward = base_reward - beta * kl_values\n    axes[0].plot(kl_values, total_reward, label=f'β={beta}')\n\naxes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\naxes[0].set_xlabel('KL Divergence (how far we\\'ve drifted)')\naxes[0].set_ylabel('Total Reward')\naxes[0].set_title('Effect of β on Total Reward')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# What's the equilibrium point for different β values?\nbetas = np.linspace(0.01, 0.5, 50)\n# If the reward model gives us more reward as we drift (reward hacking!)\n# we want to find the point where reward gain = KL penalty\nreward_gain_per_kl = 0.8  # Made-up value for illustration\noptimal_kl = reward_gain_per_kl / betas  # Where the derivative = 0\noptimal_kl = np.clip(optimal_kl, 0, 5)\n\naxes[1].plot(betas, optimal_kl, 'b-', linewidth=2)\naxes[1].set_xlabel('β (KL coefficient)')\naxes[1].set_ylabel('Equilibrium KL')\naxes[1].set_title('Higher β → Policy Stays Closer to Reference')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nLeft plot: As KL increases (we drift further), the penalty cuts into our reward.\")\nprint(\"Steeper line = stronger penalty = less drift allowed.\")\nprint(\"\\nRight plot: Higher β means the model settles at a lower KL divergence.\")\nprint(\"It's forced to stay closer to the reference model.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## Putting It All Together: Computing Rewards with KL Penalty\n\nIn practice, here's how we apply the KL penalty during training. We compute it for each token in the generated sequence, then subtract it from the reward."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:58.052068Z",
     "iopub.status.busy": "2025-12-06T23:29:58.051991Z",
     "iopub.status.idle": "2025-12-06T23:29:58.054412Z",
     "shell.execute_reply": "2025-12-06T23:29:58.054089Z"
    }
   },
   "outputs": [],
   "source": "def compute_rewards_with_kl(\n    rewards: torch.Tensor,\n    policy_logprobs: torch.Tensor,\n    ref_logprobs: torch.Tensor,\n    kl_coef: float = 0.1\n) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute total rewards with KL penalty subtracted.\n    \n    Formula: total_reward = reward - β * KL\n    \n    This is what we actually use to train the policy. The KL penalty\n    prevents the model from drifting too far from the reference.\n    \n    Args:\n        rewards: Raw scores from the reward model\n        policy_logprobs: Log probs from current policy\n        ref_logprobs: Log probs from frozen reference model\n        kl_coef: Strength of the penalty (β)\n    \n    Returns:\n        (total_rewards, kl_divergence)\n    \"\"\"\n    # Per-token KL divergence\n    kl = policy_logprobs - ref_logprobs\n    \n    # Apply the penalty to the rewards\n    total_rewards = rewards - kl_coef * kl\n    \n    return total_rewards, kl\n\n# Example with some realistic-ish values\nrewards = torch.tensor([1.0, 0.5, -0.2, 0.8])\npolicy_logprobs = torch.tensor([-2.0, -2.5, -3.0, -2.2])\nref_logprobs = torch.tensor([-2.1, -2.3, -2.8, -2.5])\n\ntotal_rewards, kl = compute_rewards_with_kl(rewards, policy_logprobs, ref_logprobs)\n\nprint(\"Original rewards:  \", [f\"{r:.2f}\" for r in rewards.tolist()])\nprint(\"KL divergence:     \", [f\"{k:.2f}\" for k in kl.tolist()])\nprint(\"Total rewards:     \", [f\"{r:.2f}\" for r in total_rewards.tolist()])\nprint(\"\\nSee how the KL penalty adjusted our rewards?\")\nprint(\"Where KL is positive (policy assigns higher prob than ref), reward goes DOWN.\")\nprint(\"Where KL is negative (policy assigns lower prob than ref), reward goes UP slightly.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Tuning β: Finding the Sweet Spot\n\nThe KL coefficient β is a hyperparameter you need to tune. It's all about balance.\n\n| β Value | What Happens |\n|---------|--------------|\n| **Too low** (0.01) | Weak rubber band. Policy drifts far, reward hacking becomes possible. Model might find weird exploits. |\n| **Just right** (0.05-0.2) | Goldilocks zone. Model improves but stays grounded in sensible behavior. |\n| **Too high** (1.0) | Rubber band too stiff. Policy can't improve much — it's stuck near the reference model. Might as well not train at all! |\n\n### Why not just use a fixed value?\n\nWell, you *can*. Many implementations just use β = 0.1 and call it a day.\n\nBut here's the thing: early in training, you might want a looser constraint (lower β) to allow exploration. Later, once you've found good territory, you might want a tighter constraint (higher β) to prevent overfitting.\n\nThat's where adaptive KL control comes in..."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:58.055185Z",
     "iopub.status.busy": "2025-12-06T23:29:58.055112Z",
     "iopub.status.idle": "2025-12-06T23:29:58.057416Z",
     "shell.execute_reply": "2025-12-06T23:29:58.057180Z"
    }
   },
   "outputs": [],
   "source": "## Adaptive KL Control: A Self-Adjusting Safety Belt\n\nInstead of picking a fixed β, we can adjust it dynamically based on what we observe during training. It's like cruise control for your KL divergence.\n\nThe idea:\n- Set a **target KL** (say, 0.02)\n- If observed KL is too high → increase β (tighten the constraint)\n- If observed KL is too low → decrease β (loosen up a bit)\n\nThis way, the model naturally settles around your target KL value.\n\nclass AdaptiveKLController:\n    \"\"\"\n    Dynamically adjust β to maintain a target KL divergence.\n    \n    Think of this as a thermostat for KL divergence. You set a target\n    temperature (KL value), and it adjusts the heating/cooling (β) to\n    maintain that target.\n    \"\"\"\n    \n    def __init__(self, init_kl_coef=0.1, target_kl=0.02, horizon=10000):\n        self.kl_coef = init_kl_coef  # Current β value\n        self.target_kl = target_kl    # Our target KL divergence\n        self.horizon = horizon         # Training steps (not used in this simple version)\n    \n    def update(self, observed_kl):\n        \"\"\"\n        Update β based on observed KL divergence.\n        \n        Too high? Increase β to pull back harder.\n        Too low? Decrease β to allow more exploration.\n        \"\"\"\n        # How far are we from target?\n        error = observed_kl - self.target_kl\n        \n        # Proportional control (simple but effective!)\n        if observed_kl > 1.5 * self.target_kl:\n            # Way too high — tighten the constraint\n            self.kl_coef *= 1.5\n        elif observed_kl < 0.5 * self.target_kl:\n            # Too low — loosen up\n            self.kl_coef /= 1.5\n        # Otherwise, we're in the sweet spot, no change needed\n        \n        # Keep β in a reasonable range\n        self.kl_coef = max(0.01, min(1.0, self.kl_coef))\n        \n        return self.kl_coef\n\n# Let's see it in action!\ncontroller = AdaptiveKLController(init_kl_coef=0.1, target_kl=0.02)\n\nprint(\"Adaptive KL Control in Action:\")\nprint(\"=\" * 50)\n# Simulate a training run with varying KL divergence\nkl_observations = [0.01, 0.02, 0.05, 0.1, 0.03, 0.02]\nfor step, observed_kl in enumerate(kl_observations):\n    new_coef = controller.update(observed_kl)\n    status = \"✓\" if 0.01 <= observed_kl <= 0.03 else \"⚠\"\n    print(f\"Step {step}: KL = {observed_kl:.3f} → β = {new_coef:.4f} {status}\")\n\nprint(\"\\nSee how β adjusts to keep KL near the target (0.02)?\")\nprint(\"When KL spikes to 0.05 or 0.1, β increases to pull it back down.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## What to Watch During Training\n\nWhen you're running RLHF with KL penalties, keep an eye on these metrics:\n\n**Mean KL divergence** — The average KL across your batch. Healthy range: 0.01 to 0.1\n- Too low (< 0.01)? Your model isn't learning much, β might be too high\n- Too high (> 0.1)? You're drifting too far, risk of reward hacking\n\n**Max KL** — The highest KL in your batch. Watch for spikes!\n- Sudden spikes often indicate instability\n- The model might be finding an exploit in the reward function\n\n**KL over time** — Plot this across training steps\n- Should start near zero (you just copied the reference model)\n- Gradually increase as the model learns\n- Then stabilize at some equilibrium value\n- If it keeps climbing without bound... Houston, we have a problem\n\n**Mean reward** — Obviously, you want this going up\n- But not at the cost of skyrocketing KL!\n- The best training runs show reward increasing while KL stays bounded\n\nThink of it like driving a car: reward is your speed, KL is how far you're veering from your lane. You want to go fast (high reward) but stay on the road (low KL)."
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## The Big Picture\n\nLet's recap what we've learned:\n\n**KL divergence** measures how different two probability distributions are. In RLHF, it tells us how much our policy has changed from the reference model.\n\n**The KL penalty** prevents reward hacking by keeping the model tethered to sensible behavior. It's a safety mechanism that says \"you can improve, but don't go crazy.\"\n\n**The coefficient β** controls the strength of this constraint. Too low and you drift, too high and you can't learn. Adaptive control can help find the sweet spot.\n\n**Why this matters**: Without the KL penalty, RLHF would be unstable and prone to exploits. With it, we can safely improve our models using reinforcement learning while maintaining coherent, natural language generation.\n\nIt's one of those clever tricks that seems obvious in hindsight but was crucial to making RLHF actually work in practice.\n\n---\n\n## Next Steps\n\nNow that we understand the KL penalty, we're ready to see the complete RLHF training loop: generating rollouts, computing advantages with GAE, and putting it all together with PPO.\n\nThat's where things get *really* interesting."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}