{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# KL Divergence Penalty in RLHF\n",
    "\n",
    "**Preventing reward hacking and maintaining stability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why KL Penalty?\n",
    "\n",
    "Without constraints, the policy can **drift** far from its initialization, leading to:\n",
    "\n",
    "- **Reward hacking** — Exploiting reward model weaknesses\n",
    "- **Mode collapse** — Generating repetitive outputs\n",
    "- **Incoherent text** — Forgetting language patterns\n",
    "\n",
    "The **KL divergence penalty** keeps the policy close to a frozen reference model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Mathematical Formulation\n",
    "\n",
    "The total reward with KL penalty:\n",
    "\n",
    "$$r_{\\text{total}}(x, y) = r(x, y) - \\beta \\cdot D_{\\text{KL}}(\\pi_\\theta(\\cdot|x) \\| \\pi_{\\text{ref}}(\\cdot|x))$$\n",
    "\n",
    "where:\n",
    "- $r(x, y)$ = reward model score\n",
    "- $\\beta$ = KL coefficient (typically 0.1)\n",
    "- $\\pi_\\theta$ = current policy\n",
    "- $\\pi_{\\text{ref}}$ = frozen reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compute_kl_penalty(\n",
    "    logprobs: torch.Tensor,\n",
    "    ref_logprobs: torch.Tensor,\n",
    "    kl_coef: float = 0.1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute KL divergence penalty between policy and reference.\n",
    "    \n",
    "    KL(π || π_ref) ≈ E[log π - log π_ref]\n",
    "    \n",
    "    Args:\n",
    "        logprobs: Log probs under current policy\n",
    "        ref_logprobs: Log probs under reference model\n",
    "        kl_coef: KL penalty coefficient (β)\n",
    "    \n",
    "    Returns:\n",
    "        KL penalty scaled by coefficient\n",
    "    \"\"\"\n",
    "    kl_divergence = logprobs - ref_logprobs\n",
    "    kl_penalty = kl_coef * kl_divergence.mean()\n",
    "    \n",
    "    return kl_penalty\n",
    "\n",
    "# Example\n",
    "batch_size = 16\n",
    "logprobs = torch.randn(batch_size) - 3.0\n",
    "ref_logprobs = torch.randn(batch_size) - 3.0\n",
    "\n",
    "kl_penalty = compute_kl_penalty(logprobs, ref_logprobs, kl_coef=0.1)\n",
    "print(f\"KL Penalty: {kl_penalty.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Visualizing KL Penalty Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how KL penalty affects total reward\n",
    "kl_values = np.linspace(0, 2, 100)\n",
    "base_reward = 1.0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Different β values\n",
    "for beta in [0.05, 0.1, 0.2, 0.5]:\n",
    "    total_reward = base_reward - beta * kl_values\n",
    "    axes[0].plot(kl_values, total_reward, label=f'β={beta}')\n",
    "\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('KL Divergence')\n",
    "axes[0].set_ylabel('Total Reward')\n",
    "axes[0].set_title('Effect of β on Total Reward')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Optimal KL for different β\n",
    "betas = np.linspace(0.01, 0.5, 50)\n",
    "# Assuming policy wants to maximize reward - β*KL, \n",
    "# and reward increases with KL (reward hacking)\n",
    "reward_gain_per_kl = 0.8\n",
    "optimal_kl = reward_gain_per_kl / betas  # Where derivative = 0\n",
    "optimal_kl = np.clip(optimal_kl, 0, 5)\n",
    "\n",
    "axes[1].plot(betas, optimal_kl, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('β (KL coefficient)')\n",
    "axes[1].set_ylabel('Equilibrium KL')\n",
    "axes[1].set_title('Higher β → Policy Stays Closer to Reference')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Applying KL in PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rewards_with_kl(\n",
    "    rewards: torch.Tensor,\n",
    "    policy_logprobs: torch.Tensor,\n",
    "    ref_logprobs: torch.Tensor,\n",
    "    kl_coef: float = 0.1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute total rewards with KL penalty subtracted.\n",
    "    \n",
    "    total_reward = reward - β * KL\n",
    "    \"\"\"\n",
    "    # Per-token KL\n",
    "    kl = policy_logprobs - ref_logprobs\n",
    "    \n",
    "    # Apply penalty\n",
    "    total_rewards = rewards - kl_coef * kl\n",
    "    \n",
    "    return total_rewards, kl\n",
    "\n",
    "# Example\n",
    "rewards = torch.tensor([1.0, 0.5, -0.2, 0.8])\n",
    "policy_logprobs = torch.tensor([-2.0, -2.5, -3.0, -2.2])\n",
    "ref_logprobs = torch.tensor([-2.1, -2.3, -2.8, -2.5])\n",
    "\n",
    "total_rewards, kl = compute_rewards_with_kl(rewards, policy_logprobs, ref_logprobs)\n",
    "\n",
    "print(\"Original rewards:\", rewards.tolist())\n",
    "print(\"KL divergence:\", kl.tolist())\n",
    "print(\"Total rewards:\", total_rewards.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Tuning the KL Coefficient\n",
    "\n",
    "| β Value | Effect |\n",
    "|---------|--------|\n",
    "| **Too low** (0.01) | Policy drifts, reward hacking |\n",
    "| **Good** (0.1) | Balance between improvement and stability |\n",
    "| **Too high** (1.0) | Policy can't improve, stuck near reference |\n",
    "\n",
    "### Adaptive KL Control\n",
    "\n",
    "Some implementations adjust β dynamically:\n",
    "- If KL too high: increase β\n",
    "- If KL too low: decrease β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveKLController:\n",
    "    \"\"\"Adaptive KL coefficient controller.\"\"\"\n",
    "    \n",
    "    def __init__(self, init_kl_coef=0.1, target_kl=0.02, horizon=10000):\n",
    "        self.kl_coef = init_kl_coef\n",
    "        self.target_kl = target_kl\n",
    "        self.horizon = horizon\n",
    "    \n",
    "    def update(self, observed_kl):\n",
    "        \"\"\"Update KL coefficient based on observed KL.\"\"\"\n",
    "        # Proportional control\n",
    "        error = observed_kl - self.target_kl\n",
    "        \n",
    "        # Adjust coefficient\n",
    "        if observed_kl > 1.5 * self.target_kl:\n",
    "            self.kl_coef *= 1.5  # Increase penalty\n",
    "        elif observed_kl < 0.5 * self.target_kl:\n",
    "            self.kl_coef /= 1.5  # Decrease penalty\n",
    "        \n",
    "        # Clamp to reasonable range\n",
    "        self.kl_coef = max(0.01, min(1.0, self.kl_coef))\n",
    "        \n",
    "        return self.kl_coef\n",
    "\n",
    "# Example\n",
    "controller = AdaptiveKLController(init_kl_coef=0.1, target_kl=0.02)\n",
    "\n",
    "print(\"Adaptive KL Control:\")\n",
    "for observed_kl in [0.01, 0.02, 0.05, 0.1, 0.03, 0.02]:\n",
    "    new_coef = controller.update(observed_kl)\n",
    "    print(f\"  Observed KL: {observed_kl:.3f} → β = {new_coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Monitoring KL During Training\n",
    "\n",
    "Track these KL-related metrics:\n",
    "\n",
    "- **Mean KL**: Should stay in range [0.01, 0.1]\n",
    "- **Max KL**: Watch for spikes indicating instability\n",
    "- **KL over time**: Should stabilize, not diverge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now let's learn about the complete training dynamics — rollout generation, GAE, and the two-phase training loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
