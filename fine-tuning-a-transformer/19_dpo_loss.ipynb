{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Understanding the DPO Loss Function\n\n**Let's crack open the math and see what makes it tick**\n\nYou know how we've been talking about DPO (Direct Preference Optimization) as this clever way to train language models from human preferences? Well, now we're going to dig into the actual loss function that makes it work.\n\nFair warning: there's some math ahead. But don't worry â€” we'll break it down piece by piece until it feels obvious."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The DPO Loss Formula\n\nHere's the intimidating version you'll see in papers:\n\n$$\\mathcal{L}_{\\text{DPO}} = -\\mathbb{E} \\left[ \\log \\sigma \\left( \\beta \\left( \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right) \\right]$$\n\nOkay, what the heck does all that mean? Let's decode every symbol:\n\n- **Ï€_Î¸** (pi-theta) = your policy model (the one you're training)\n- **Ï€_ref** (pi-ref) = your reference model (the frozen starting point, usually the SFT model)\n- **y_w** (y-win) = the chosen/winning/preferred response\n- **y_l** (y-lose) = the rejected/losing/dispreferred response  \n- **x** = the prompt/input\n- **Î²** (beta) = temperature parameter that controls how much the policy can deviate from reference\n- **Ïƒ** (sigma) = the sigmoid function (squashes things to 0-1 range)\n- **ð”¼** (that fancy E) = expected value (just means \"average over all examples\")\n\nStill with me? Good.\n\n## The Intuition\n\nHere's what this loss is actually doing:\n\nThink of it like training a judge. You show the judge two responses to the same prompt. You tell them \"humans preferred response A over response B.\" The judge's job is to learn to predict that preference.\n\nBut here's the clever bit: instead of just learning raw preferences, we're learning **how much the new model's preferences differ from the reference model's preferences**. \n\nWhy? Because we want the new model to be better than the reference, but not go completely off the rails. The reference model is like a guardrail â€” we can stray from it, but not too far.\n\nLet's break down the formula step by step to see how this works in practice."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Step 1: Computing Log Ratios\n\nFirst, we compute something called a \"log ratio\" for each response. This tells us: **how much does the policy model like this response compared to the reference model?**\n\nFor the chosen (winning) response:\n$$\\text{logratio}_w = \\log \\pi_\\theta(y_w|x) - \\log \\pi_{\\text{ref}}(y_w|x)$$\n\nFor the rejected (losing) response:\n$$\\text{logratio}_l = \\log \\pi_\\theta(y_l|x) - \\log \\pi_{\\text{ref}}(y_l|x)$$\n\nNow, remember that log probabilities are negative numbers (typically -45, -50, etc.). When we subtract one negative from another, we get the log of their ratio. Math trick!\n\n$$\\log \\frac{A}{B} = \\log A - \\log B$$\n\nSo `logratio_w` tells us: is the policy model more or less enthusiastic about the chosen response than the reference model was?\n\n- **Positive log ratio** â†’ Policy likes it MORE than reference did\n- **Negative log ratio** â†’ Policy likes it LESS than reference did  \n- **Zero** â†’ Policy and reference agree\n\nLet's see this in code:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:11.761912Z",
     "iopub.status.busy": "2025-12-06T23:30:11.761837Z",
     "iopub.status.idle": "2025-12-06T23:30:12.578376Z",
     "shell.execute_reply": "2025-12-06T23:30:12.578050Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef compute_log_ratios(\n    policy_logps: torch.Tensor,\n    reference_logps: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    Compute log(Ï€_Î¸ / Ï€_ref) = log Ï€_Î¸ - log Ï€_ref\n    \n    This tells us how much more (or less) the policy model likes \n    this response compared to the reference model.\n    \"\"\"\n    return policy_logps - reference_logps\n\n# Let's create some example log probabilities\n# (Remember: these are negative because they're log probabilities)\npolicy_chosen_logps = torch.tensor([-45.0, -50.0, -48.0, -52.0])\npolicy_rejected_logps = torch.tensor([-48.0, -52.0, -46.0, -55.0])\nref_chosen_logps = torch.tensor([-46.0, -51.0, -49.0, -53.0])\nref_rejected_logps = torch.tensor([-47.0, -51.0, -47.0, -54.0])\n\n# Compute the log ratios\nchosen_logratios = compute_log_ratios(policy_chosen_logps, ref_chosen_logps)\nrejected_logratios = compute_log_ratios(policy_rejected_logps, ref_rejected_logps)\n\nprint(\"Log ratios for CHOSEN responses:\")\nprint(f\"  {chosen_logratios.tolist()}\")\nprint(\"\\nLog ratios for REJECTED responses:\")\nprint(f\"  {rejected_logratios.tolist()}\")\n\nprint(\"\\nWhat does this mean?\")\nprint(\"  - Chosen responses: all +1.0 â†’ policy likes them MORE than reference\")\nprint(\"  - Rejected responses: mostly -1.0 â†’ policy likes them LESS than reference\")\nprint(\"  - This is what we want! Policy is learning the right preferences.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## Step 2: Computing the Logits\n\nNow we combine those two log ratios into a single number that represents our \"confidence\" that the policy has learned the right preference:\n\n$$\\text{logits} = \\beta \\cdot (\\text{logratio}_w - \\text{logratio}_l)$$\n\nThink about what this means:\n- We subtract the rejected log ratio from the chosen log ratio\n- This gives us: **how much MORE does the policy prefer the chosen response over the rejected one, compared to the reference?**\n\nIf the policy is learning correctly:\n- It should like the chosen response MORE than reference did (positive `logratio_w`)\n- It should like the rejected response LESS than reference did (negative `logratio_l`)\n- So the difference should be POSITIVE (and large!)\n\nThat Î² (beta) parameter? It's like a volume knob. Higher Î² means we care more about the difference. Lower Î² means we're more conservative.\n\nLet's calculate these logits:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:12.579571Z",
     "iopub.status.busy": "2025-12-06T23:30:12.579454Z",
     "iopub.status.idle": "2025-12-06T23:30:12.581454Z",
     "shell.execute_reply": "2025-12-06T23:30:12.581194Z"
    }
   },
   "outputs": [],
   "source": "beta = 0.1  # Typical value - not too aggressive\n\n# Compute the logits: how much does policy prefer chosen over rejected?\nlogits = beta * (chosen_logratios - rejected_logratios)\n\nprint(f\"Logits (with Î²={beta}):\")\nprint(f\"  {logits.tolist()}\\n\")\n\n# Let's interpret each logit\nprint(\"Interpretation:\")\nfor i, logit in enumerate(logits):\n    if logit > 0:\n        print(f\"  Example {i}: {logit:.2f} â†’ âœ“ Policy correctly prefers chosen response\")\n    elif logit < 0:\n        print(f\"  Example {i}: {logit:.2f} â†’ âœ— Policy incorrectly prefers rejected response\")\n    else:\n        print(f\"  Example {i}: {logit:.2f} â†’ ? Policy is uncertain (equal preference)\")\n\n# Calculate accuracy\naccuracy = (logits > 0).float().mean()\nprint(f\"\\nAccuracy: {accuracy:.1%} of examples have positive logits\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## Step 3: Computing the Loss\n\nNow for the final step. We take those logits and turn them into a loss using this formula:\n\n$$\\mathcal{L} = -\\log \\sigma(\\text{logits})$$\n\nWhere Ïƒ (sigma) is the sigmoid function: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n\nWhy sigmoid? Because it squashes any number into the range (0, 1), which we can interpret as a probability.\n\nHere's the beautiful part:\n- **High positive logit** â†’ sigmoid â‰ˆ 1 â†’ log(1) = 0 â†’ **low loss** âœ“\n- **High negative logit** â†’ sigmoid â‰ˆ 0 â†’ log(0) = -âˆž â†’ **high loss** âœ—\n- **Logit = 0** â†’ sigmoid = 0.5 â†’ log(0.5) â‰ˆ -0.69 â†’ **medium loss**\n\nThis is actually just binary cross-entropy loss! We're treating the preference prediction as a binary classification problem where the target is always 1 (we always want the chosen response to be preferred).\n\nLet's compute the full DPO loss:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:12.582165Z",
     "iopub.status.busy": "2025-12-06T23:30:12.582094Z",
     "iopub.status.idle": "2025-12-06T23:30:12.585300Z",
     "shell.execute_reply": "2025-12-06T23:30:12.585048Z"
    }
   },
   "outputs": [],
   "source": "def compute_dpo_loss_detailed(\n    policy_chosen_logps: torch.Tensor,\n    policy_rejected_logps: torch.Tensor,\n    ref_chosen_logps: torch.Tensor,\n    ref_rejected_logps: torch.Tensor,\n    beta: float = 0.1\n) -> dict:\n    \"\"\"\n    Compute DPO loss with detailed metrics to understand what's happening.\n    \n    Returns a dictionary with loss and useful diagnostic metrics.\n    \"\"\"\n    \n    # Step 1: Compute log ratios (how much does policy differ from reference?)\n    chosen_logratios = policy_chosen_logps - ref_chosen_logps\n    rejected_logratios = policy_rejected_logps - ref_rejected_logps\n    \n    # Step 2: Compute logits (how much does policy prefer chosen over rejected?)\n    logits = beta * (chosen_logratios - rejected_logratios)\n    \n    # Step 3: Compute loss (penalize when policy doesn't prefer chosen)\n    # F.logsigmoid is more numerically stable than torch.log(torch.sigmoid())\n    loss = -F.logsigmoid(logits).mean()\n    \n    # Bonus: Calculate some useful metrics\n    accuracy = (logits > 0).float().mean()  # How often does policy prefer chosen?\n    margin = (chosen_logratios - rejected_logratios).mean()  # Average preference gap\n    \n    return {\n        'loss': loss,\n        'accuracy': accuracy,\n        'margin': margin,\n        'chosen_logratios_mean': chosen_logratios.mean(),\n        'rejected_logratios_mean': rejected_logratios.mean()\n    }\n\n# Compute all the metrics\nmetrics = compute_dpo_loss_detailed(\n    policy_chosen_logps, policy_rejected_logps,\n    ref_chosen_logps, ref_rejected_logps,\n    beta=0.1\n)\n\nprint(\"DPO Loss Metrics:\")\nprint(f\"  Loss: {metrics['loss'].item():.4f}\")\nprint(f\"  Accuracy: {metrics['accuracy'].item():.1%} (how often policy prefers chosen)\")\nprint(f\"  Margin: {metrics['margin'].item():.4f} (average preference gap)\")\nprint(f\"  Chosen log ratios: {metrics['chosen_logratios_mean'].item():.4f} (avg)\")\nprint(f\"  Rejected log ratios: {metrics['rejected_logratios_mean'].item():.4f} (avg)\")\n\nprint(\"\\nWhat this tells us:\")\nprint(\"  - Loss around 0.6 is decent (random would be 0.693)\")\nprint(\"  - 75% accuracy means policy is learning, but not perfect yet\")\nprint(\"  - Positive margin (1.5) is good - clear preference separation\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## The Î² (Beta) Parameter: Your Constraint Knob\n\nRemember that Î² parameter? It's actually super important. Let's understand what it does.\n\nÎ² controls **how much the policy is allowed to deviate from the reference model**.\n\nThink of it like a leash:\n- **Low Î² (e.g., 0.01)**: Short leash. Policy stays very close to reference. Safe but limited learning.\n- **Medium Î² (e.g., 0.1)**: Balanced leash. Typical choice. Good exploration without going wild.\n- **High Î² (e.g., 1.0)**: Long leash. Policy can deviate significantly. More learning but risk of instability.\n\nHere's the technical reason: Î² scales the logits, which affects how strongly we penalize deviation from the reference. Higher Î² means the loss changes more dramatically with small changes in the log ratios.\n\nLet's visualize this:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:12.586135Z",
     "iopub.status.busy": "2025-12-06T23:30:12.586057Z",
     "iopub.status.idle": "2025-12-06T23:30:12.645688Z",
     "shell.execute_reply": "2025-12-06T23:30:12.645395Z"
    }
   },
   "outputs": [],
   "source": "# Let's see how Î² affects the loss curve\nmargin_range = np.linspace(-5, 5, 100)\n\nplt.figure(figsize=(12, 6))\n\n# Plot loss curves for different Î² values\nfor beta_val in [0.01, 0.1, 0.5, 1.0]:\n    logits = beta_val * margin_range\n    # Compute -log(sigmoid(logits))\n    loss = -np.log(1 / (1 + np.exp(-logits)))\n    plt.plot(margin_range, loss, label=f'Î²={beta_val}', linewidth=2)\n\nplt.axvline(x=0, color='red', linestyle='--', alpha=0.3, label='No preference')\nplt.axhline(y=np.log(2), color='gray', linestyle='--', alpha=0.3, label='Random guess loss')\n\nplt.xlabel('Log Ratio Margin (chosen - rejected)', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('How Î² Affects DPO Loss Sensitivity', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.ylim([0, 3])\n\n# Add annotations\nplt.text(2, 2.5, 'Negative margin = wrong preference!\\nHigher Î² = steeper penalty', \n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=10)\nplt.text(-2, 0.5, 'Positive margin = correct preference\\nHigher Î² = faster learning', \n         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5), fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey insights:\")\nprint(\"  - Higher Î² = steeper curves = stronger gradients = faster learning\")\nprint(\"  - But too high Î² can cause instability (overfitting to preferences)\")\nprint(\"  - Î²=0.1 is the sweet spot for most cases\")\nprint(\"  - All curves pass through the same point at margin=0 (where loss â‰ˆ 0.693)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## Optional: Label Smoothing\n\nHere's a fun trick you sometimes see in DPO implementations: **label smoothing**.\n\nThe idea is simple. Instead of treating preferences as absolute (this response is 100% better), we add a bit of uncertainty:\n\n$$\\mathcal{L}_{\\text{smooth}} = (1-\\epsilon) \\cdot \\mathcal{L}_{\\text{chosen}} + \\epsilon \\cdot \\mathcal{L}_{\\text{rejected}}$$\n\nWhere:\n- **Îµ** (epsilon) is the smoothing factor (typically 0.1 or so)\n- **L_chosen** is the loss for preferring the chosen response\n- **L_rejected** is the loss for preferring the rejected response\n\nWhy would we do this? A few reasons:\n1. **Regularization**: Prevents overconfident predictions\n2. **Noise tolerance**: Human preferences aren't always consistent\n3. **Better calibration**: Model learns to be appropriately uncertain\n\nThink of it like hedging your bets. Instead of saying \"this response is definitely better,\" we say \"this response is probably better, but I'm not 100% sure.\"\n\nLet's implement it:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:12.646579Z",
     "iopub.status.busy": "2025-12-06T23:30:12.646508Z",
     "iopub.status.idle": "2025-12-06T23:30:12.649790Z",
     "shell.execute_reply": "2025-12-06T23:30:12.649472Z"
    }
   },
   "outputs": [],
   "source": "def compute_dpo_loss_with_smoothing(\n    policy_chosen_logps: torch.Tensor,\n    policy_rejected_logps: torch.Tensor,\n    ref_chosen_logps: torch.Tensor,\n    ref_rejected_logps: torch.Tensor,\n    beta: float = 0.1,\n    label_smoothing: float = 0.0\n) -> torch.Tensor:\n    \"\"\"\n    DPO loss with optional label smoothing.\n    \n    Label smoothing adds a bit of the \"wrong\" loss to regularize the model.\n    It's like saying \"I'm 90% sure the chosen response is better\" instead of 100%.\n    \"\"\"\n    \n    # Compute log ratios\n    chosen_logratios = policy_chosen_logps - ref_chosen_logps\n    rejected_logratios = policy_rejected_logps - ref_rejected_logps\n    \n    # Compute logits\n    logits = beta * (chosen_logratios - rejected_logratios)\n    \n    if label_smoothing > 0:\n        # Loss for preferring chosen (what we want)\n        loss_chosen = -F.logsigmoid(logits)\n        # Loss for preferring rejected (what we DON'T want, but add a tiny bit)\n        loss_rejected = -F.logsigmoid(-logits)\n        # Weighted combination\n        loss = (1 - label_smoothing) * loss_chosen + label_smoothing * loss_rejected\n    else:\n        # Standard DPO loss (no smoothing)\n        loss = -F.logsigmoid(logits)\n    \n    return loss.mean()\n\n# Let's compare with and without smoothing\nprint(\"Comparing standard DPO vs. label-smoothed DPO:\\n\")\n\nloss_no_smooth = compute_dpo_loss_with_smoothing(\n    policy_chosen_logps, policy_rejected_logps,\n    ref_chosen_logps, ref_rejected_logps,\n    beta=0.1, label_smoothing=0.0\n)\n\nloss_smooth = compute_dpo_loss_with_smoothing(\n    policy_chosen_logps, policy_rejected_logps,\n    ref_chosen_logps, ref_rejected_logps,\n    beta=0.1, label_smoothing=0.1\n)\n\nprint(f\"Loss without smoothing (Îµ=0.0): {loss_no_smooth.item():.4f}\")\nprint(f\"Loss with smoothing (Îµ=0.1):    {loss_smooth.item():.4f}\")\n\nprint(\"\\nNotice:\")\nprint(\"  - Smoothed loss is slightly higher (we're being less confident)\")\nprint(\"  - This can help prevent overfitting to noisy preference data\")\nprint(\"  - But most DPO implementations use Îµ=0 (no smoothing)\")\nprint(\"  - Only add smoothing if you suspect your preference labels are noisy\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "## Wrapping Up\n\nAlright, let's recap what we learned about the DPO loss function.\n\n**The big picture:**\n- DPO trains a model to prefer chosen responses over rejected ones\n- But it keeps the model anchored to a reference model (so it doesn't go crazy)\n- The loss is basically binary cross-entropy on preference predictions\n\n**The three steps:**\n1. **Log ratios**: How much does the policy differ from reference for each response?\n2. **Logits**: How much more does policy prefer chosen over rejected (scaled by Î²)?\n3. **Loss**: Penalize when the logits are negative (wrong preference)\n\n**Key parameters:**\n- **Î² (beta)**: Controls how much the policy can deviate from reference. Typical value: 0.1\n- **Îµ (epsilon)**: Optional label smoothing for regularization. Usually 0 (no smoothing)\n\n**Why this works:**\nThe genius of DPO is that it bypasses the need for a separate reward model (like in RLHF). Instead, it learns preferences directly by comparing how much the policy's predictions differ from the reference for chosen vs. rejected responses.\n\nPretty clever, right?\n\nNext up, we'll implement the full DPO training loop and actually train a model!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}