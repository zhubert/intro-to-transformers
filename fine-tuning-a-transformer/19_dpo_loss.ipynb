{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# DPO Loss Function\n",
    "\n",
    "**Deep dive into the mathematics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The DPO Loss\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}} = -\\mathbb{E} \\left[ \\log \\sigma \\left( \\beta \\left( \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right) \\right]$$\n",
    "\n",
    "Let's break this down step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Log Ratios\n",
    "\n",
    "For each response, compute the log ratio between policy and reference:\n",
    "\n",
    "$$\\text{logratio}_w = \\log \\pi_\\theta(y_w|x) - \\log \\pi_{\\text{ref}}(y_w|x)$$\n",
    "$$\\text{logratio}_l = \\log \\pi_\\theta(y_l|x) - \\log \\pi_{\\text{ref}}(y_l|x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compute_log_ratios(\n",
    "    policy_logps: torch.Tensor,\n",
    "    reference_logps: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute log(π_θ / π_ref) = log π_θ - log π_ref\n",
    "    \"\"\"\n",
    "    return policy_logps - reference_logps\n",
    "\n",
    "# Example\n",
    "policy_chosen_logps = torch.tensor([-45.0, -50.0, -48.0, -52.0])\n",
    "policy_rejected_logps = torch.tensor([-48.0, -52.0, -46.0, -55.0])\n",
    "ref_chosen_logps = torch.tensor([-46.0, -51.0, -49.0, -53.0])\n",
    "ref_rejected_logps = torch.tensor([-47.0, -51.0, -47.0, -54.0])\n",
    "\n",
    "chosen_logratios = compute_log_ratios(policy_chosen_logps, ref_chosen_logps)\n",
    "rejected_logratios = compute_log_ratios(policy_rejected_logps, ref_rejected_logps)\n",
    "\n",
    "print(f\"Chosen log ratios: {chosen_logratios.tolist()}\")\n",
    "print(f\"Rejected log ratios: {rejected_logratios.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Logits\n",
    "\n",
    "The \"logits\" for the preference prediction:\n",
    "\n",
    "$$\\text{logits} = \\beta \\cdot (\\text{logratio}_w - \\text{logratio}_l)$$\n",
    "\n",
    "This represents how much more the policy prefers the chosen response relative to reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.1\n",
    "\n",
    "logits = beta * (chosen_logratios - rejected_logratios)\n",
    "print(f\"Logits (β={beta}): {logits.tolist()}\")\n",
    "\n",
    "# Positive logits → policy prefers chosen (correct!)\n",
    "# Negative logits → policy prefers rejected (wrong!)\n",
    "print(f\"Policy prefers chosen: {(logits > 0).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Loss\n",
    "\n",
    "$$\\mathcal{L} = -\\log \\sigma(\\text{logits})$$\n",
    "\n",
    "This is binary cross-entropy loss where:\n",
    "- High logits → low loss (correct preference)\n",
    "- Low logits → high loss (wrong preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete DPO loss\n",
    "def compute_dpo_loss_detailed(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    ref_chosen_logps: torch.Tensor,\n",
    "    ref_rejected_logps: torch.Tensor,\n",
    "    beta: float = 0.1\n",
    ") -> dict:\n",
    "    \"\"\"Compute DPO loss with detailed metrics.\"\"\"\n",
    "    \n",
    "    # Step 1: Log ratios\n",
    "    chosen_logratios = policy_chosen_logps - ref_chosen_logps\n",
    "    rejected_logratios = policy_rejected_logps - ref_rejected_logps\n",
    "    \n",
    "    # Step 2: Logits\n",
    "    logits = beta * (chosen_logratios - rejected_logratios)\n",
    "    \n",
    "    # Step 3: Loss\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (logits > 0).float().mean()\n",
    "    margin = (chosen_logratios - rejected_logratios).mean()\n",
    "    \n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'margin': margin,\n",
    "        'chosen_logratios_mean': chosen_logratios.mean(),\n",
    "        'rejected_logratios_mean': rejected_logratios.mean()\n",
    "    }\n",
    "\n",
    "metrics = compute_dpo_loss_detailed(\n",
    "    policy_chosen_logps, policy_rejected_logps,\n",
    "    ref_chosen_logps, ref_rejected_logps,\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "print(\"DPO Loss Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## The β Parameter\n",
    "\n",
    "β controls how much the policy can deviate from reference:\n",
    "\n",
    "| β Value | Effect |\n",
    "|---------|--------|\n",
    "| **Low (0.01)** | Policy stays close to reference |\n",
    "| **Medium (0.1)** | Balanced (typical) |\n",
    "| **High (1.0)** | Policy can deviate more |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of β\n",
    "margin_range = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for beta in [0.01, 0.1, 0.5, 1.0]:\n",
    "    logits = beta * margin_range\n",
    "    loss = -np.log(1 / (1 + np.exp(-logits)))  # -log sigmoid\n",
    "    plt.plot(margin_range, loss, label=f'β={beta}')\n",
    "\n",
    "plt.xlabel('Log Ratio Margin (chosen - rejected)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('DPO Loss vs Margin for Different β')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Higher β → Steeper loss curve → Stronger preference signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Label Smoothing\n",
    "\n",
    "Optional: Add label smoothing for regularization:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{smooth}} = (1-\\epsilon) \\cdot \\mathcal{L}_{\\text{chosen}} + \\epsilon \\cdot \\mathcal{L}_{\\text{rejected}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss_with_smoothing(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    ref_chosen_logps: torch.Tensor,\n",
    "    ref_rejected_logps: torch.Tensor,\n",
    "    beta: float = 0.1,\n",
    "    label_smoothing: float = 0.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"DPO loss with optional label smoothing.\"\"\"\n",
    "    \n",
    "    chosen_logratios = policy_chosen_logps - ref_chosen_logps\n",
    "    rejected_logratios = policy_rejected_logps - ref_rejected_logps\n",
    "    \n",
    "    logits = beta * (chosen_logratios - rejected_logratios)\n",
    "    \n",
    "    if label_smoothing > 0:\n",
    "        # Soft targets\n",
    "        loss_chosen = -F.logsigmoid(logits)\n",
    "        loss_rejected = -F.logsigmoid(-logits)\n",
    "        loss = (1 - label_smoothing) * loss_chosen + label_smoothing * loss_rejected\n",
    "    else:\n",
    "        loss = -F.logsigmoid(logits)\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "# Compare with and without smoothing\n",
    "loss_no_smooth = compute_dpo_loss_with_smoothing(\n",
    "    policy_chosen_logps, policy_rejected_logps,\n",
    "    ref_chosen_logps, ref_rejected_logps,\n",
    "    beta=0.1, label_smoothing=0.0\n",
    ")\n",
    "\n",
    "loss_smooth = compute_dpo_loss_with_smoothing(\n",
    "    policy_chosen_logps, policy_rejected_logps,\n",
    "    ref_chosen_logps, ref_rejected_logps,\n",
    "    beta=0.1, label_smoothing=0.1\n",
    ")\n",
    "\n",
    "print(f\"Loss without smoothing: {loss_no_smooth.item():.4f}\")\n",
    "print(f\"Loss with smoothing (ε=0.1): {loss_smooth.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now let's implement the complete DPO training loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
