{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Loss Masking in SFT\n",
    "\n",
    "**Why we only compute loss on response tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "In SFT, our training data looks like:\n",
    "\n",
    "```\n",
    "[INSTRUCTION TOKENS] [RESPONSE TOKENS]\n",
    "```\n",
    "\n",
    "If we compute loss on ALL tokens, the model learns to predict the instruction too. But we don't want that! The instruction is given by the user â€” the model should only learn to generate good responses.\n",
    "\n",
    "**Loss masking** sets the loss to zero for instruction tokens, so the model only learns from response tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Mathematical Formulation\n",
    "\n",
    "Without masking:\n",
    "$$\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(x_t | x_{<t})$$\n",
    "\n",
    "With masking:\n",
    "$$\\mathcal{L} = -\\sum_{t=t_{\\text{response}}}^{T} \\log P(x_t | x_{<t})$$\n",
    "\n",
    "where $t_{\\text{response}}$ is the token position where the response begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_loss_mask(input_ids: torch.Tensor, response_start: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a mask that is 1 for response tokens and 0 for prompt tokens.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Token IDs, shape (batch_size, seq_len)\n",
    "        response_start: Token position where response begins\n",
    "    \n",
    "    Returns:\n",
    "        Mask tensor, shape (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mask = torch.zeros(batch_size, seq_len)\n",
    "    \n",
    "    # Set mask to 1 for response tokens (from response_start onwards)\n",
    "    mask[:, response_start:] = 1.0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Example\n",
    "batch_size, seq_len = 2, 10\n",
    "response_start = 6\n",
    "\n",
    "input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "mask = create_loss_mask(input_ids, response_start)\n",
    "\n",
    "print(\"Input shape:\", input_ids.shape)\n",
    "print(\"Mask:\")\n",
    "print(mask[0])\n",
    "print(\"\\nMask visualization (0=prompt, 1=response):\")\n",
    "print(\"Position:\", list(range(seq_len)))\n",
    "print(\"Mask:    \", mask[0].int().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sft_loss(\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    loss_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute SFT loss with masking.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model outputs, shape (batch_size, seq_len, vocab_size)\n",
    "        labels: Target token IDs, shape (batch_size, seq_len)\n",
    "        loss_mask: Mask for loss computation, shape (batch_size, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Shift for next-token prediction\n",
    "    # logits[:, :-1] predicts labels[:, 1:]\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    shift_mask = loss_mask[:, 1:].contiguous()\n",
    "    \n",
    "    # Flatten for cross-entropy\n",
    "    batch_size, seq_len, vocab_size = shift_logits.shape\n",
    "    flat_logits = shift_logits.view(-1, vocab_size)\n",
    "    flat_labels = shift_labels.view(-1)\n",
    "    flat_mask = shift_mask.view(-1)\n",
    "    \n",
    "    # Compute per-token loss\n",
    "    per_token_loss = F.cross_entropy(\n",
    "        flat_logits, \n",
    "        flat_labels, \n",
    "        reduction='none'\n",
    "    )\n",
    "    \n",
    "    # Apply mask and average\n",
    "    masked_loss = per_token_loss * flat_mask\n",
    "    loss = masked_loss.sum() / (flat_mask.sum() + 1e-8)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Example\n",
    "batch_size, seq_len, vocab_size = 2, 10, 1000\n",
    "response_start = 6\n",
    "\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "loss_mask = create_loss_mask(labels, response_start)\n",
    "\n",
    "loss = compute_sft_loss(logits, labels, loss_mask)\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Why Masking Matters\n",
    "\n",
    "Without loss masking:\n",
    "- Model learns to predict instructions (waste of capacity)\n",
    "- May memorize specific instruction phrasings\n",
    "- Gradient signal diluted across instruction tokens\n",
    "\n",
    "With loss masking:\n",
    "- Model focuses entirely on generating good responses\n",
    "- More efficient use of model capacity\n",
    "- Stronger gradient signal for response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Compare gradients with and without masking\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "seq_len = 20\n",
    "response_start = 12\n",
    "\n",
    "# Simulated gradient magnitudes\n",
    "no_mask_grads = np.random.uniform(0.5, 1.5, seq_len)  # All tokens contribute\n",
    "masked_grads = np.zeros(seq_len)\n",
    "masked_grads[response_start:] = np.random.uniform(0.5, 1.5, seq_len - response_start) * 2  # Response only, stronger\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Without masking\n",
    "colors = ['#ff6b6b' if i < response_start else '#4ecdc4' for i in range(seq_len)]\n",
    "axes[0].bar(range(seq_len), no_mask_grads, color=colors)\n",
    "axes[0].set_title('Without Loss Masking')\n",
    "axes[0].set_xlabel('Token Position')\n",
    "axes[0].set_ylabel('Gradient Magnitude')\n",
    "axes[0].axvline(x=response_start-0.5, color='black', linestyle='--', label='Response start')\n",
    "\n",
    "# With masking\n",
    "axes[1].bar(range(seq_len), masked_grads, color=colors)\n",
    "axes[1].set_title('With Loss Masking')\n",
    "axes[1].set_xlabel('Token Position')\n",
    "axes[1].set_ylabel('Gradient Magnitude')\n",
    "axes[1].axvline(x=response_start-0.5, color='black', linestyle='--', label='Response start')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Red = Prompt tokens, Teal = Response tokens\")\n",
    "print(\"With masking, all gradient signal goes to response tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Handling Variable-Length Batches\n",
    "\n",
    "In practice, each example has a different response start position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_mask_batch(\n",
    "    input_ids: torch.Tensor,\n",
    "    response_starts: list[int],\n",
    "    pad_token_id: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create loss mask for a batch with variable response start positions.\n",
    "    Also masks out padding tokens.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mask = torch.zeros(batch_size, seq_len)\n",
    "    \n",
    "    for i, start in enumerate(response_starts):\n",
    "        # Mask response tokens (1 = compute loss)\n",
    "        mask[i, start:] = 1.0\n",
    "        \n",
    "        # Mask out padding (0 = ignore)\n",
    "        padding_mask = (input_ids[i] == pad_token_id)\n",
    "        mask[i] = mask[i] * (~padding_mask).float()\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Example with variable response starts\n",
    "batch_size, seq_len = 3, 15\n",
    "pad_token_id = 0\n",
    "\n",
    "# Simulate input with padding\n",
    "input_ids = torch.randint(1, 1000, (batch_size, seq_len))\n",
    "input_ids[0, 12:] = pad_token_id  # First example padded at position 12\n",
    "input_ids[1, 14:] = pad_token_id  # Second example padded at position 14\n",
    "# Third example: no padding\n",
    "\n",
    "response_starts = [5, 7, 6]  # Different response start for each example\n",
    "\n",
    "mask = create_loss_mask_batch(input_ids, response_starts, pad_token_id)\n",
    "\n",
    "print(\"Loss masks for batch:\")\n",
    "for i in range(batch_size):\n",
    "    print(f\"Example {i}: {mask[i].int().tolist()}\")\n",
    "    print(f\"  Response starts at: {response_starts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Using HuggingFace's Built-in Support\n",
    "\n",
    "The `labels` field in HuggingFace models uses `-100` to indicate tokens that should be ignored in loss computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels_for_hf(\n",
    "    input_ids: torch.Tensor,\n",
    "    response_start: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Prepare labels for HuggingFace models.\n",
    "    Set prompt tokens to -100 (ignored in loss).\n",
    "    \"\"\"\n",
    "    labels = input_ids.clone()\n",
    "    labels[:, :response_start] = -100  # Ignore prompt tokens\n",
    "    return labels\n",
    "\n",
    "# Example\n",
    "input_ids = torch.tensor([[101, 102, 103, 104, 105, 201, 202, 203]])\n",
    "response_start = 5\n",
    "\n",
    "labels = prepare_labels_for_hf(input_ids, response_start)\n",
    "\n",
    "print(\"Input IDs:\", input_ids[0].tolist())\n",
    "print(\"Labels:   \", labels[0].tolist())\n",
    "print(\"\\n-100 means 'ignore this token in loss computation'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we understand loss masking, let's put it all together in a complete SFT training loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
