{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Loss Masking in SFT\n",
    "\n",
    "When you fine-tune a language model, if you're not careful, you end up training it to predict the _instructions_ you give it. Which isn't what we want.\n",
    "\n",
    "This notebook is about how to avoid that mistake. Short version: loss masking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Problem: Teaching a Model the Wrong Thing\n",
    "\n",
    "Picture this. You're training a model on instruction-following. Your data looks like this:\n",
    "\n",
    "```\n",
    "[User asks a question] [Model gives a great answer]\n",
    "```\n",
    "\n",
    "Or in token-speak:\n",
    "\n",
    "```\n",
    "[INSTRUCTION TOKENS] [RESPONSE TOKENS]\n",
    "```\n",
    "\n",
    "Now here's the trap. If you compute loss on _all_ the tokens (like you would during pretraining), the model learns two things:\n",
    "\n",
    "1. How to predict the instruction (bad! the user gives us that)\n",
    "2. How to predict the response (good! this is what we want)\n",
    "\n",
    "It's like training a chef by making them memorize both the customer's order _and_ how to cook the dish. We only care about #2!\n",
    "\n",
    "**Loss masking** is the solution. We set the loss to zero for instruction tokens, so the model only learns from response tokens. The model still _sees_ the instruction (it needs that context), but it doesn't get graded on predicting it.\n",
    "\n",
    "Think of it like a student taking a test where the questions are provided (not graded) and only the answers count toward their score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Math\n",
    "\n",
    "Let's write this out formally.\n",
    "\n",
    "**Without masking** (the naive approach):\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(x_t | x_{<t})$$\n",
    "\n",
    "Breaking this down:\n",
    "- $\\mathcal{L}$ is the **loss**: the number we're trying to minimize during training\n",
    "- $t$ is the **time step** (or position in the sequence). think of it like an index: token 1, token 2, token 3...\n",
    "- $T$ is the **total length** of the sequence. If we have 100 tokens, $T = 100$\n",
    "- $x_t$ is the **token at position** $t$. This is the actual token we're trying to predict\n",
    "- $x_{<t}$ means \"all tokens _before_ position $t$\". The context the model uses to predict $x_t$\n",
    "- $P(x_t | x_{<t})$ is the **probability** the model assigns to the correct token, given the context\n",
    "- $\\sum_{t=1}^{T}$ means \"sum over all positions from 1 to $T$\"\n",
    "\n",
    "So this formula says: \"Calculate the negative log probability for _every_ token in the sequence, then add them all up.\" The model gets penalized equally for every token it predicts wrong.\n",
    "\n",
    "**With masking** (the smart approach):\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{t=t_{\\text{response}}}^{T} \\log P(x_t | x_{<t})$$\n",
    "\n",
    "Same formula, but notice the sum now starts at $t_{\\text{response}}$ instead of $t=1$. \n",
    "\n",
    "- $t_{\\text{response}}$ is the **position where the response begins**: everything before this is the instruction\n",
    "\n",
    "Now we only sum over the response tokens. The instruction tokens (positions 1 through $t_{\\text{response}}-1$) contribute zero to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:57:52.015856Z",
     "iopub.status.busy": "2026-01-22T01:57:52.015856Z",
     "iopub.status.idle": "2026-01-22T01:57:53.301248Z",
     "shell.execute_reply": "2026-01-22T01:57:53.301248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10])\n",
      "\n",
      "Loss mask for first example:\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1.])\n",
      "\n",
      "Let's visualize this more clearly:\n",
      "Position: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Mask:     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "\n",
      "0 = instruction (ignored), 1 = response (graded)\n",
      "\n",
      "So positions 0-5 are the instruction, positions 6-9 are the response.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_loss_mask(input_ids: torch.Tensor, response_start: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a mask that is 1 for response tokens and 0 for prompt tokens.\n",
    "    \n",
    "    Think of this like a highlighting tool: we're marking which tokens\n",
    "    the model should be graded on (1 = grade this, 0 = ignore this).\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Token IDs, shape (batch_size, seq_len)\n",
    "        response_start: Token position where response begins\n",
    "    \n",
    "    Returns:\n",
    "        Mask tensor, shape (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mask = torch.zeros(batch_size, seq_len)  # Start with all zeros (ignore everything)\n",
    "    \n",
    "    # Set mask to 1 for response tokens (from response_start onwards)\n",
    "    mask[:, response_start:] = 1.0  # \"Grade these tokens!\"\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Let's see this in action\n",
    "batch_size, seq_len = 2, 10\n",
    "response_start = 6  # Response begins at position 6\n",
    "\n",
    "input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "mask = create_loss_mask(input_ids, response_start)\n",
    "\n",
    "print(\"Input shape:\", input_ids.shape)\n",
    "print(\"\\nLoss mask for first example:\")\n",
    "print(mask[0])\n",
    "print(\"\\nLet's visualize this more clearly:\")\n",
    "print(\"Position:\", list(range(seq_len)))\n",
    "print(\"Mask:    \", mask[0].int().tolist())\n",
    "print(\"\\n0 = instruction (ignored), 1 = response (graded)\")\n",
    "print(f\"\\nSo positions 0-5 are the instruction, positions 6-9 are the response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:57:53.301248Z",
     "iopub.status.busy": "2026-01-22T01:57:53.301248Z",
     "iopub.status.idle": "2026-01-22T01:57:53.308364Z",
     "shell.execute_reply": "2026-01-22T01:57:53.308364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked SFT Loss: 7.7122\n",
      "\n",
      "This is the loss computed only over the response tokens (positions 6 onwards).\n",
      "\n",
      "Loss WITHOUT masking: 7.7253\n",
      "Loss WITH masking:    7.7122\n",
      "\n",
      "They're different! The masked version only counts the response tokens.\n"
     ]
    }
   ],
   "source": [
    "def compute_sft_loss(\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    loss_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute SFT loss with masking.\n",
    "    \n",
    "    This is the key step. We calculate loss for every token,\n",
    "    but then multiply by the mask to zero out the instruction tokens.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model outputs, shape (batch_size, seq_len, vocab_size)\n",
    "                These are the raw predictions before softmax\n",
    "        labels: Target token IDs, shape (batch_size, seq_len)\n",
    "                The \"correct answers\" we want the model to predict\n",
    "        loss_mask: Mask for loss computation, shape (batch_size, seq_len)\n",
    "                   1 = include this token, 0 = ignore it\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss value (a single number to minimize)\n",
    "    \"\"\"\n",
    "    # Shift for next-token prediction\n",
    "    # At each position, we're predicting the NEXT token\n",
    "    # So logits[:, :-1] (positions 0 to T-1) predict labels[:, 1:] (positions 1 to T)\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    shift_mask = loss_mask[:, 1:].contiguous()\n",
    "    \n",
    "    # Flatten everything for cross-entropy\n",
    "    # PyTorch's cross_entropy expects: (batch*seq_len, vocab_size) and (batch*seq_len,)\n",
    "    batch_size, seq_len, vocab_size = shift_logits.shape\n",
    "    flat_logits = shift_logits.view(-1, vocab_size)\n",
    "    flat_labels = shift_labels.view(-1)\n",
    "    flat_mask = shift_mask.view(-1)\n",
    "    \n",
    "    # Compute per-token loss (one loss value per token)\n",
    "    per_token_loss = F.cross_entropy(\n",
    "        flat_logits, \n",
    "        flat_labels, \n",
    "        reduction='none'  # Don't average yet! We need to mask first\n",
    "    )\n",
    "    \n",
    "    # Apply mask: multiply each loss by its mask value\n",
    "    # Instruction tokens (mask=0) contribute 0 to the loss\n",
    "    # Response tokens (mask=1) contribute their full loss\n",
    "    masked_loss = per_token_loss * flat_mask\n",
    "    \n",
    "    # Average over the response tokens only\n",
    "    # We divide by the sum of the mask (number of response tokens)\n",
    "    # The +1e-8 prevents division by zero if mask is all zeros (shouldn't happen, but safety!)\n",
    "    loss = masked_loss.sum() / (flat_mask.sum() + 1e-8)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Let's test this with some fake data\n",
    "batch_size, seq_len, vocab_size = 2, 10, 1000\n",
    "response_start = 6\n",
    "\n",
    "# Create fake model outputs and labels\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)  # Random predictions\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))  # Random targets\n",
    "loss_mask = create_loss_mask(labels, response_start)\n",
    "\n",
    "# Compute the loss\n",
    "loss = compute_sft_loss(logits, labels, loss_mask)\n",
    "\n",
    "print(f\"Masked SFT Loss: {loss.item():.4f}\")\n",
    "print(f\"\\nThis is the loss computed only over the response tokens (positions {response_start} onwards).\")\n",
    "\n",
    "# For comparison, let's compute loss without masking\n",
    "no_mask = torch.ones_like(loss_mask)  # All ones = include everything\n",
    "loss_no_mask = compute_sft_loss(logits, labels, no_mask)\n",
    "\n",
    "print(f\"\\nLoss WITHOUT masking: {loss_no_mask.item():.4f}\")\n",
    "print(f\"Loss WITH masking:    {loss.item():.4f}\")\n",
    "print(f\"\\nThey're different! The masked version only counts the response tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Why This Actually Matters\n",
    "\n",
    "We've established that we don't want the model learning to predict instructions. But let's dig deeper into _why_ this matters:\n",
    "\n",
    "**Without loss masking:**\n",
    "- **Wasted capacity**: The model dedicates parameters to predicting instruction phrasings instead of response quality. It's like hiring a chef and making them memorize the menu instead of learning to cook.\n",
    "- **Memorization risk**: The model might start memorizing specific instruction templates. \"Oh, the user always asks 'What is...' so I'll just predict that pattern.\" Not helpful.\n",
    "- **Diluted gradients**: The gradient signal (the learning signal) gets spread across _all_ tokens. If your instruction is 50 tokens and your response is 50 tokens, half your gradient is wasted.\n",
    "\n",
    "**With loss masking:**\n",
    "- **Focused learning**: All the model's capacity goes toward generating good responses. This is what we actually care about.\n",
    "- **Stronger signal**: The gradient signal is concentrated on the response tokens. It's like turning up the volume on the parts that matter.\n",
    "- **Better generalization**: The model doesn't overfit to specific instruction phrasings. It learns the general skill of \"given a context, generate a good response.\"\n",
    "\n",
    "The difference is real and measurable. Models trained with proper loss masking work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:57:53.308364Z",
     "iopub.status.busy": "2026-01-22T01:57:53.308364Z",
     "iopub.status.idle": "2026-01-22T01:57:53.704690Z",
     "shell.execute_reply": "2026-01-22T01:57:53.704690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”´ Red bars = Instruction tokens\n",
      "ðŸŸ¢ Teal bars = Response tokens\n",
      "\n",
      "Notice the difference:\n",
      "- LEFT: Gradients spread across all tokens (wasted on instruction)\n",
      "- RIGHT: All gradient signal concentrated on response tokens\n",
      "\n",
      "This is why masking works so well. Same total gradient, focused where it matters.\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize what this looks like in practice\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "seq_len = 20\n",
    "response_start = 12\n",
    "\n",
    "# Simulate gradient magnitudes (these would come from backprop in real training)\n",
    "# Without masking: all tokens get gradients, roughly similar magnitude\n",
    "no_mask_grads = np.random.uniform(0.5, 1.5, seq_len)\n",
    "\n",
    "# With masking: only response tokens get gradients, and they're stronger\n",
    "# (because the same total gradient is concentrated on fewer tokens)\n",
    "masked_grads = np.zeros(seq_len)\n",
    "masked_grads[response_start:] = np.random.uniform(0.8, 2.0, seq_len - response_start)\n",
    "\n",
    "# Create the visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Color scheme: red for instruction, teal for response\n",
    "colors = ['#ff6b6b' if i < response_start else '#4ecdc4' for i in range(seq_len)]\n",
    "\n",
    "# Without masking\n",
    "axes[0].bar(range(seq_len), no_mask_grads, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_title('Without Loss Masking', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Token Position', fontsize=12)\n",
    "axes[0].set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "axes[0].axvline(x=response_start-0.5, color='black', linestyle='--', linewidth=2, label='Response starts here')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 2.5)\n",
    "\n",
    "# With masking\n",
    "axes[1].bar(range(seq_len), masked_grads, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "axes[1].set_title('With Loss Masking', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Token Position', fontsize=12)\n",
    "axes[1].set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "axes[1].axvline(x=response_start-0.5, color='black', linestyle='--', linewidth=2, label='Response starts here')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 2.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ”´ Red bars = Instruction tokens\")\n",
    "print(\"ðŸŸ¢ Teal bars = Response tokens\")\n",
    "print(\"\\nNotice the difference:\")\n",
    "print(\"- LEFT: Gradients spread across all tokens (wasted on instruction)\")\n",
    "print(\"- RIGHT: All gradient signal concentrated on response tokens\")\n",
    "print(\"\\nThis is why masking works so well. Same total gradient, focused where it matters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Real-World Complication: Variable-Length Sequences\n",
    "\n",
    "Okay, so far we've been pretending that every example has the instruction ending at the same position. But that's not how the real world works.\n",
    "\n",
    "In a real batch of training data:\n",
    "- Example 1 might have a 5-token instruction and a 20-token response\n",
    "- Example 2 might have a 15-token instruction and a 10-token response  \n",
    "- Example 3 might have a 50-token instruction and a 5-token response\n",
    "\n",
    "Plus, to make batching work, we pad shorter sequences to the same length. And we definitely don't want to compute loss on padding tokens!\n",
    "\n",
    "So our masking needs to handle two things:\n",
    "1. Different response start positions for each example\n",
    "2. Padding tokens (which should always be ignored)\n",
    "\n",
    "Let's build that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:57:53.704690Z",
     "iopub.status.busy": "2026-01-22T01:57:53.704690Z",
     "iopub.status.idle": "2026-01-22T01:57:53.710740Z",
     "shell.execute_reply": "2026-01-22T01:57:53.710740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's look at each example in the batch:\n",
      "\n",
      "Example 0:\n",
      "  Response starts at position: 5\n",
      "  Mask: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "  â†’ 5 instruction tokens (masked out)\n",
      "  â†’ 7 response tokens (counted in loss)\n",
      "  â†’ 3 padding tokens (masked out)\n",
      "\n",
      "Example 1:\n",
      "  Response starts at position: 7\n",
      "  Mask: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "  â†’ 7 instruction tokens (masked out)\n",
      "  â†’ 7 response tokens (counted in loss)\n",
      "  â†’ 1 padding tokens (masked out)\n",
      "\n",
      "Example 2:\n",
      "  Response starts at position: 6\n",
      "  Mask: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  â†’ 6 instruction tokens (masked out)\n",
      "  â†’ 9 response tokens (counted in loss)\n",
      "  â†’ 0 padding tokens (masked out)\n",
      "\n",
      "Notice: Each example can have different regions, and padding is always masked!\n"
     ]
    }
   ],
   "source": [
    "def create_loss_mask_batch(\n",
    "    input_ids: torch.Tensor,\n",
    "    response_starts: list[int],\n",
    "    pad_token_id: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create loss mask for a batch with variable response start positions.\n",
    "    Also masks out padding tokens (because we definitely don't want to learn to predict padding!).\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Token IDs, shape (batch_size, seq_len)\n",
    "        response_starts: List of response start positions, one per example\n",
    "        pad_token_id: The token ID used for padding (typically 0)\n",
    "    \n",
    "    Returns:\n",
    "        Mask tensor, shape (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mask = torch.zeros(batch_size, seq_len)\n",
    "    \n",
    "    for i, start in enumerate(response_starts):\n",
    "        # Step 1: Mark response tokens as 1\n",
    "        mask[i, start:] = 1.0\n",
    "        \n",
    "        # Step 2: Mask out any padding tokens (set back to 0)\n",
    "        # Even if they're in the \"response\" region\n",
    "        padding_mask = (input_ids[i] == pad_token_id)\n",
    "        mask[i] = mask[i] * (~padding_mask).float()\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Let's create a realistic example with variable-length sequences\n",
    "batch_size, seq_len = 3, 15\n",
    "pad_token_id = 0\n",
    "\n",
    "# Simulate input with different lengths (shorter sequences get padded)\n",
    "input_ids = torch.randint(1, 1000, (batch_size, seq_len))\n",
    "\n",
    "# Example 1: ends at position 12, rest is padding\n",
    "input_ids[0, 12:] = pad_token_id\n",
    "\n",
    "# Example 2: ends at position 14, just one padding token\n",
    "input_ids[1, 14:] = pad_token_id\n",
    "\n",
    "# Example 3: no padding (uses full sequence)\n",
    "# input_ids[2] stays as-is\n",
    "\n",
    "# Different response start for each example\n",
    "response_starts = [5, 7, 6]\n",
    "\n",
    "# Create the mask\n",
    "mask = create_loss_mask_batch(input_ids, response_starts, pad_token_id)\n",
    "\n",
    "print(\"Let's look at each example in the batch:\\n\")\n",
    "for i in range(batch_size):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"  Response starts at position: {response_starts[i]}\")\n",
    "    print(f\"  Mask: {mask[i].int().tolist()}\")\n",
    "    \n",
    "    # Count different regions\n",
    "    instruction_tokens = response_starts[i]\n",
    "    response_tokens = int(mask[i].sum())\n",
    "    padding_tokens = (input_ids[i] == pad_token_id).sum().item()\n",
    "    \n",
    "    print(f\"  â†’ {instruction_tokens} instruction tokens (masked out)\")\n",
    "    print(f\"  â†’ {response_tokens} response tokens (counted in loss)\")\n",
    "    print(f\"  â†’ {padding_tokens} padding tokens (masked out)\")\n",
    "    print()\n",
    "\n",
    "print(\"Notice: Each example can have different regions, and padding is always masked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## The HuggingFace Shortcut\n",
    "\n",
    "Now, if you're using HuggingFace Transformers (and let's be honest, you probably are), there's a built-in convention that makes this even easier.\n",
    "\n",
    "Instead of manually creating and applying masks, HuggingFace models use a special token ID: **-100**.\n",
    "\n",
    "When you pass labels to a HuggingFace model, any position with label=-100 gets automatically ignored in the loss computation. It's like a pre-built masking system.\n",
    "\n",
    "So instead of:\n",
    "```python\n",
    "labels = [101, 102, 103, 104, ...]  # All the tokens\n",
    "mask = [0, 0, 0, 1, ...]            # Separate mask tensor\n",
    "```\n",
    "\n",
    "You just do:\n",
    "```python\n",
    "labels = [-100, -100, -100, 104, ...]  # -100 for instruction, real IDs for response\n",
    "```\n",
    "\n",
    "Cleaner, right? Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:57:53.710740Z",
     "iopub.status.busy": "2026-01-22T01:57:53.710740Z",
     "iopub.status.idle": "2026-01-22T01:57:53.715671Z",
     "shell.execute_reply": "2026-01-22T01:57:53.715671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input_ids: [101, 102, 103, 104, 105, 201, 202, 203]\n",
      "Labels for HF:      [-100, -100, -100, -100, -100, 201, 202, 203]\n",
      "\n",
      "What this means:\n",
      "  Positions 0-4: -100 (instruction, ignored in loss)\n",
      "  Positions 5-7: actual token IDs (response, counted in loss)\n",
      "\n",
      "Loss computed with -100 labels: 5.4616\n",
      "\n",
      "PyTorch automatically ignores the -100 tokens.\n",
      "(That's why HuggingFace uses -100. It's PyTorch's default ignore value)\n"
     ]
    }
   ],
   "source": [
    "def prepare_labels_for_hf(\n",
    "    input_ids: torch.Tensor,\n",
    "    response_start: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Prepare labels for HuggingFace models using the -100 convention.\n",
    "    \n",
    "    This is the standard way to do loss masking in HuggingFace.\n",
    "    Set instruction tokens to -100 (ignored), keep response tokens as-is.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Token IDs, shape (batch_size, seq_len)\n",
    "        response_start: Position where response begins\n",
    "    \n",
    "    Returns:\n",
    "        Labels tensor with -100 for masked positions\n",
    "    \"\"\"\n",
    "    labels = input_ids.clone()  # Start with a copy\n",
    "    labels[:, :response_start] = -100  # Mask out instruction tokens\n",
    "    return labels\n",
    "\n",
    "# Example with a concrete sequence\n",
    "input_ids = torch.tensor([[101, 102, 103, 104, 105, 201, 202, 203]])\n",
    "response_start = 5\n",
    "\n",
    "labels = prepare_labels_for_hf(input_ids, response_start)\n",
    "\n",
    "print(\"Original input_ids:\", input_ids[0].tolist())\n",
    "print(\"Labels for HF:     \", labels[0].tolist())\n",
    "print(\"\\nWhat this means:\")\n",
    "print(\"  Positions 0-4: -100 (instruction, ignored in loss)\")\n",
    "print(\"  Positions 5-7: actual token IDs (response, counted in loss)\")\n",
    "\n",
    "# Let's verify this works with PyTorch's cross_entropy\n",
    "# (which is what HuggingFace uses under the hood)\n",
    "vocab_size = 300\n",
    "fake_logits = torch.randn(1, 8, vocab_size)\n",
    "\n",
    "# Compute loss - PyTorch's cross_entropy ignores -100 by default!\n",
    "shift_logits = fake_logits[:, :-1, :].contiguous().view(-1, vocab_size)\n",
    "shift_labels = labels[:, 1:].contiguous().view(-1)\n",
    "\n",
    "loss = F.cross_entropy(shift_logits, shift_labels)  # ignore_index=-100 by default!\n",
    "\n",
    "print(f\"\\nLoss computed with -100 labels: {loss.item():.4f}\")\n",
    "print(\"\\nPyTorch automatically ignores the -100 tokens.\")\n",
    "print(\"(That's why HuggingFace uses -100. It's PyTorch's default ignore value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "That's loss masking. Simple idea, big impact.\n",
    "\n",
    "The key insight: during supervised fine-tuning, we only want the model to learn to generate responses, not to predict the instructions we give it. Loss masking accomplishes this by zeroing out the loss for instruction tokens.\n",
    "\n",
    "**Two ways to implement it:**\n",
    "1. Manual masking: create a mask tensor (0 for instruction, 1 for response) and multiply it with your per-token loss\n",
    "2. HuggingFace convention: set instruction positions to -100 in your labels tensor\n",
    "\n",
    "Both work. The HuggingFace approach is simpler if you're using their libraries.\n",
    "\n",
    "**Why it matters:**\n",
    "- Focuses the model's learning on what actually matters (response quality)\n",
    "- Prevents wasting capacity on instruction memorization  \n",
    "- Concentrates gradient signal where it's useful\n",
    "- Results in better, more generalizable models\n",
    "\n",
    "Now that we understand loss masking, we're ready to put it all together in a complete SFT training loop."
   ]
  }
 ],
 "metadata": {
  "description": "Masks instruction tokens in loss computation so model only learns to predict assistant responses.",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
