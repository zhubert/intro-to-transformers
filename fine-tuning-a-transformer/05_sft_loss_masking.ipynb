{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Loss Masking in SFT\n\n**Or: Why we don't train the model on the question**\n\nHere's a weird thing that happens when you fine-tune a language model: if you're not careful, you end up training it to predict the _instructions_ you give it. Which is...not what we want.\n\nThis notebook is about how to avoid that mistake. Short version: loss masking. Let's dig in."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The Problem: Teaching a Model the Wrong Thing\n\nPicture this. You're training a model on instruction-following. Your data looks like this:\n\n```\n[User asks a question] [Model gives a great answer]\n```\n\nOr in token-speak:\n\n```\n[INSTRUCTION TOKENS] [RESPONSE TOKENS]\n```\n\nNow here's the trap. If you compute loss on _all_ the tokens (like you would during pretraining), the model learns two things:\n\n1. How to predict the instruction (bad! the user gives us that)\n2. How to predict the response (good! this is what we want)\n\nIt's like training a chef by making them memorize both the customer's order _and_ how to cook the dish. We only care about #2!\n\n**Loss masking** is the solution. We set the loss to zero for instruction tokens, so the model only learns from response tokens. The model still _sees_ the instruction (it needs that context), but it doesn't get graded on predicting it.\n\nThink of it like a student taking a test where the questions are provided (not graded) and only the answers count toward their score."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## The Math (Don't Worry, We'll Explain Each Symbol)\n\nLet's write this out formally. But instead of throwing symbols at you, we'll actually explain what each one means.\n\n**Without masking** (the naive approach):\n\n$$\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(x_t | x_{<t})$$\n\nBreaking this down:\n- $\\mathcal{L}$ is the **loss** â€” the number we're trying to minimize during training\n- $t$ is the **time step** (or position in the sequence) â€” think of it like an index: token 1, token 2, token 3...\n- $T$ is the **total length** of the sequence â€” if we have 100 tokens, $T = 100$\n- $x_t$ is the **token at position** $t$ â€” this is the actual token we're trying to predict\n- $x_{<t}$ means \"all tokens _before_ position $t$\" â€” the context the model uses to predict $x_t$\n- $P(x_t | x_{<t})$ is the **probability** the model assigns to the correct token, given the context\n- $\\sum_{t=1}^{T}$ means \"sum over all positions from 1 to $T$\"\n\nSo this formula says: \"Calculate the negative log probability for _every_ token in the sequence, then add them all up.\" The model gets penalized equally for every token it predicts wrong.\n\n**With masking** (the smart approach):\n\n$$\\mathcal{L} = -\\sum_{t=t_{\\text{response}}}^{T} \\log P(x_t | x_{<t})$$\n\nSame formula, but notice the sum now starts at $t_{\\text{response}}$ instead of $t=1$. \n\n- $t_{\\text{response}}$ is the **position where the response begins** â€” everything before this is the instruction\n\nNow we only sum over the response tokens! The instruction tokens (positions 1 through $t_{\\text{response}}-1$) contribute zero to the loss.\n\n(Yeah, the math is actually pretty simple once you know what the symbols mean.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:11.552271Z",
     "iopub.status.busy": "2025-12-06T23:29:11.552158Z",
     "iopub.status.idle": "2025-12-06T23:29:12.243420Z",
     "shell.execute_reply": "2025-12-06T23:29:12.243091Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport torch.nn.functional as F\n\ndef create_loss_mask(input_ids: torch.Tensor, response_start: int) -> torch.Tensor:\n    \"\"\"\n    Create a mask that is 1 for response tokens and 0 for prompt tokens.\n    \n    Think of this like a highlighting tool: we're marking which tokens\n    the model should be graded on (1 = grade this, 0 = ignore this).\n    \n    Args:\n        input_ids: Token IDs, shape (batch_size, seq_len)\n        response_start: Token position where response begins\n    \n    Returns:\n        Mask tensor, shape (batch_size, seq_len)\n    \"\"\"\n    batch_size, seq_len = input_ids.shape\n    mask = torch.zeros(batch_size, seq_len)  # Start with all zeros (ignore everything)\n    \n    # Set mask to 1 for response tokens (from response_start onwards)\n    mask[:, response_start:] = 1.0  # \"Grade these tokens!\"\n    \n    return mask\n\n# Let's see this in action\nbatch_size, seq_len = 2, 10\nresponse_start = 6  # Response begins at position 6\n\ninput_ids = torch.randint(0, 1000, (batch_size, seq_len))\nmask = create_loss_mask(input_ids, response_start)\n\nprint(\"Input shape:\", input_ids.shape)\nprint(\"\\nLoss mask for first example:\")\nprint(mask[0])\nprint(\"\\nLet's visualize this more clearly:\")\nprint(\"Position:\", list(range(seq_len)))\nprint(\"Mask:    \", mask[0].int().tolist())\nprint(\"\\n0 = instruction (ignored), 1 = response (graded)\")\nprint(f\"\\nSo positions 0-5 are the instruction, positions 6-9 are the response.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:12.244413Z",
     "iopub.status.busy": "2025-12-06T23:29:12.244311Z",
     "iopub.status.idle": "2025-12-06T23:29:12.249703Z",
     "shell.execute_reply": "2025-12-06T23:29:12.249422Z"
    }
   },
   "outputs": [],
   "source": "def compute_sft_loss(\n    logits: torch.Tensor,\n    labels: torch.Tensor,\n    loss_mask: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    Compute SFT loss with masking.\n    \n    This is where the magic happens. We calculate loss for every token,\n    but then multiply by the mask to zero out the instruction tokens.\n    \n    Args:\n        logits: Model outputs, shape (batch_size, seq_len, vocab_size)\n                These are the raw predictions before softmax\n        labels: Target token IDs, shape (batch_size, seq_len)\n                The \"correct answers\" we want the model to predict\n        loss_mask: Mask for loss computation, shape (batch_size, seq_len)\n                   1 = include this token, 0 = ignore it\n    \n    Returns:\n        Scalar loss value (a single number to minimize)\n    \"\"\"\n    # Shift for next-token prediction\n    # At each position, we're predicting the NEXT token\n    # So logits[:, :-1] (positions 0 to T-1) predict labels[:, 1:] (positions 1 to T)\n    shift_logits = logits[:, :-1, :].contiguous()\n    shift_labels = labels[:, 1:].contiguous()\n    shift_mask = loss_mask[:, 1:].contiguous()\n    \n    # Flatten everything for cross-entropy\n    # PyTorch's cross_entropy expects: (batch*seq_len, vocab_size) and (batch*seq_len,)\n    batch_size, seq_len, vocab_size = shift_logits.shape\n    flat_logits = shift_logits.view(-1, vocab_size)\n    flat_labels = shift_labels.view(-1)\n    flat_mask = shift_mask.view(-1)\n    \n    # Compute per-token loss (one loss value per token)\n    per_token_loss = F.cross_entropy(\n        flat_logits, \n        flat_labels, \n        reduction='none'  # Don't average yet! We need to mask first\n    )\n    \n    # Apply mask: multiply each loss by its mask value\n    # Instruction tokens (mask=0) contribute 0 to the loss\n    # Response tokens (mask=1) contribute their full loss\n    masked_loss = per_token_loss * flat_mask\n    \n    # Average over the response tokens only\n    # We divide by the sum of the mask (number of response tokens)\n    # The +1e-8 prevents division by zero if mask is all zeros (shouldn't happen, but safety!)\n    loss = masked_loss.sum() / (flat_mask.sum() + 1e-8)\n    \n    return loss\n\n# Let's test this with some fake data\nbatch_size, seq_len, vocab_size = 2, 10, 1000\nresponse_start = 6\n\n# Create fake model outputs and labels\nlogits = torch.randn(batch_size, seq_len, vocab_size)  # Random predictions\nlabels = torch.randint(0, vocab_size, (batch_size, seq_len))  # Random targets\nloss_mask = create_loss_mask(labels, response_start)\n\n# Compute the loss\nloss = compute_sft_loss(logits, labels, loss_mask)\n\nprint(f\"Masked SFT Loss: {loss.item():.4f}\")\nprint(f\"\\nThis is the loss computed only over the response tokens (positions {response_start} onwards).\")\n\n# For comparison, let's compute loss without masking\nno_mask = torch.ones_like(loss_mask)  # All ones = include everything\nloss_no_mask = compute_sft_loss(logits, labels, no_mask)\n\nprint(f\"\\nLoss WITHOUT masking: {loss_no_mask.item():.4f}\")\nprint(f\"Loss WITH masking:    {loss.item():.4f}\")\nprint(f\"\\nThey're different! The masked version only counts the response tokens.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## Why This Actually Matters (Beyond the Obvious)\n\nOkay, so we've established that we don't want the model learning to predict instructions. But let's dig deeper into _why_ this matters:\n\n**Without loss masking:**\n- **Wasted capacity**: The model dedicates parameters to predicting instruction phrasings instead of response quality. It's like hiring a chef and making them memorize the menu instead of learning to cook.\n- **Memorization risk**: The model might start memorizing specific instruction templates. \"Oh, the user always asks 'What is...' so I'll just predict that pattern.\" Not helpful.\n- **Diluted gradients**: The gradient signal (the learning signal) gets spread across _all_ tokens. If your instruction is 50 tokens and your response is 50 tokens, half your gradient is wasted.\n\n**With loss masking:**\n- **Focused learning**: All the model's capacity goes toward generating good responses. This is what we actually care about.\n- **Stronger signal**: The gradient signal is concentrated on the response tokens. It's like turning up the volume on the parts that matter.\n- **Better generalization**: The model doesn't overfit to specific instruction phrasings. It learns the general skill of \"given a context, generate a good response.\"\n\nThe difference? Real. Measurable. The models trained with proper loss masking just work better.\n\n(This is one of those things that seems like a small implementation detail but actually has a huge impact on final model quality.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:12.250457Z",
     "iopub.status.busy": "2025-12-06T23:29:12.250384Z",
     "iopub.status.idle": "2025-12-06T23:29:12.512077Z",
     "shell.execute_reply": "2025-12-06T23:29:12.511779Z"
    }
   },
   "outputs": [],
   "source": "# Let's visualize what this looks like in practice\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseq_len = 20\nresponse_start = 12\n\n# Simulate gradient magnitudes (these would come from backprop in real training)\n# Without masking: all tokens get gradients, roughly similar magnitude\nno_mask_grads = np.random.uniform(0.5, 1.5, seq_len)\n\n# With masking: only response tokens get gradients, and they're stronger\n# (because the same total gradient is concentrated on fewer tokens)\nmasked_grads = np.zeros(seq_len)\nmasked_grads[response_start:] = np.random.uniform(0.8, 2.0, seq_len - response_start)\n\n# Create the visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\n# Color scheme: red for instruction, teal for response\ncolors = ['#ff6b6b' if i < response_start else '#4ecdc4' for i in range(seq_len)]\n\n# Without masking\naxes[0].bar(range(seq_len), no_mask_grads, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\naxes[0].set_title('Without Loss Masking', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Token Position', fontsize=12)\naxes[0].set_ylabel('Gradient Magnitude', fontsize=12)\naxes[0].axvline(x=response_start-0.5, color='black', linestyle='--', linewidth=2, label='Response starts here')\naxes[0].legend()\naxes[0].set_ylim(0, 2.5)\n\n# With masking\naxes[1].bar(range(seq_len), masked_grads, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\naxes[1].set_title('With Loss Masking', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Token Position', fontsize=12)\naxes[1].set_ylabel('Gradient Magnitude', fontsize=12)\naxes[1].axvline(x=response_start-0.5, color='black', linestyle='--', linewidth=2, label='Response starts here')\naxes[1].legend()\naxes[1].set_ylim(0, 2.5)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"ðŸ”´ Red bars = Instruction tokens\")\nprint(\"ðŸŸ¢ Teal bars = Response tokens\")\nprint(\"\\nNotice the difference:\")\nprint(\"- LEFT: Gradients spread across all tokens (wasted on instruction)\")\nprint(\"- RIGHT: All gradient signal concentrated on response tokens\")\nprint(\"\\nThis is why masking works so well. Same total gradient, focused where it matters.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Real-World Complication: Variable-Length Sequences\n\nOkay, so far we've been pretending that every example has the instruction ending at the same position. But that's not how the real world works.\n\nIn a real batch of training data:\n- Example 1 might have a 5-token instruction and a 20-token response\n- Example 2 might have a 15-token instruction and a 10-token response  \n- Example 3 might have a 50-token instruction and a 5-token response\n\nPlus, to make batching work, we pad shorter sequences to the same length. And we definitely don't want to compute loss on padding tokens!\n\nSo our masking needs to handle two things:\n1. Different response start positions for each example\n2. Padding tokens (which should always be ignored)\n\nLet's build that."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:12.512986Z",
     "iopub.status.busy": "2025-12-06T23:29:12.512873Z",
     "iopub.status.idle": "2025-12-06T23:29:12.516164Z",
     "shell.execute_reply": "2025-12-06T23:29:12.515870Z"
    }
   },
   "outputs": [],
   "source": "def create_loss_mask_batch(\n    input_ids: torch.Tensor,\n    response_starts: list[int],\n    pad_token_id: int\n) -> torch.Tensor:\n    \"\"\"\n    Create loss mask for a batch with variable response start positions.\n    Also masks out padding tokens (because we definitely don't want to learn to predict padding!).\n    \n    Args:\n        input_ids: Token IDs, shape (batch_size, seq_len)\n        response_starts: List of response start positions, one per example\n        pad_token_id: The token ID used for padding (typically 0)\n    \n    Returns:\n        Mask tensor, shape (batch_size, seq_len)\n    \"\"\"\n    batch_size, seq_len = input_ids.shape\n    mask = torch.zeros(batch_size, seq_len)\n    \n    for i, start in enumerate(response_starts):\n        # Step 1: Mark response tokens as 1\n        mask[i, start:] = 1.0\n        \n        # Step 2: Mask out any padding tokens (set back to 0)\n        # Even if they're in the \"response\" region\n        padding_mask = (input_ids[i] == pad_token_id)\n        mask[i] = mask[i] * (~padding_mask).float()\n    \n    return mask\n\n# Let's create a realistic example with variable-length sequences\nbatch_size, seq_len = 3, 15\npad_token_id = 0\n\n# Simulate input with different lengths (shorter sequences get padded)\ninput_ids = torch.randint(1, 1000, (batch_size, seq_len))\n\n# Example 1: ends at position 12, rest is padding\ninput_ids[0, 12:] = pad_token_id\n\n# Example 2: ends at position 14, just one padding token\ninput_ids[1, 14:] = pad_token_id\n\n# Example 3: no padding (uses full sequence)\n# input_ids[2] stays as-is\n\n# Different response start for each example\nresponse_starts = [5, 7, 6]\n\n# Create the mask\nmask = create_loss_mask_batch(input_ids, response_starts, pad_token_id)\n\nprint(\"Let's look at each example in the batch:\\n\")\nfor i in range(batch_size):\n    print(f\"Example {i}:\")\n    print(f\"  Response starts at position: {response_starts[i]}\")\n    print(f\"  Mask: {mask[i].int().tolist()}\")\n    \n    # Count different regions\n    instruction_tokens = response_starts[i]\n    response_tokens = int(mask[i].sum())\n    padding_tokens = (input_ids[i] == pad_token_id).sum().item()\n    \n    print(f\"  â†’ {instruction_tokens} instruction tokens (masked out)\")\n    print(f\"  â†’ {response_tokens} response tokens (counted in loss)\")\n    print(f\"  â†’ {padding_tokens} padding tokens (masked out)\")\n    print()\n\nprint(\"Notice: Each example can have different regions, and padding is always masked!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## The HuggingFace Shortcut\n\nNow, if you're using HuggingFace Transformers (and let's be honest, you probably are), there's a built-in convention that makes this even easier.\n\nInstead of manually creating and applying masks, HuggingFace models use a special token ID: **-100**.\n\nHere's the deal: when you pass labels to a HuggingFace model, any position with label=-100 gets automatically ignored in the loss computation. It's like a pre-built masking system.\n\nSo instead of:\n```python\nlabels = [101, 102, 103, 104, ...]  # All the tokens\nmask = [0, 0, 0, 1, ...]            # Separate mask tensor\n```\n\nYou just do:\n```python\nlabels = [-100, -100, -100, 104, ...]  # -100 for instruction, real IDs for response\n```\n\nCleaner, right? Let's see this in action."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:12.516874Z",
     "iopub.status.busy": "2025-12-06T23:29:12.516802Z",
     "iopub.status.idle": "2025-12-06T23:29:12.518901Z",
     "shell.execute_reply": "2025-12-06T23:29:12.518634Z"
    }
   },
   "outputs": [],
   "source": "def prepare_labels_for_hf(\n    input_ids: torch.Tensor,\n    response_start: int\n) -> torch.Tensor:\n    \"\"\"\n    Prepare labels for HuggingFace models using the -100 convention.\n    \n    This is the standard way to do loss masking in HuggingFace.\n    Set instruction tokens to -100 (ignored), keep response tokens as-is.\n    \n    Args:\n        input_ids: Token IDs, shape (batch_size, seq_len)\n        response_start: Position where response begins\n    \n    Returns:\n        Labels tensor with -100 for masked positions\n    \"\"\"\n    labels = input_ids.clone()  # Start with a copy\n    labels[:, :response_start] = -100  # Mask out instruction tokens\n    return labels\n\n# Example with a concrete sequence\ninput_ids = torch.tensor([[101, 102, 103, 104, 105, 201, 202, 203]])\nresponse_start = 5\n\nlabels = prepare_labels_for_hf(input_ids, response_start)\n\nprint(\"Original input_ids:\", input_ids[0].tolist())\nprint(\"Labels for HF:     \", labels[0].tolist())\nprint(\"\\nWhat this means:\")\nprint(\"  Positions 0-4: -100 (instruction, ignored in loss)\")\nprint(\"  Positions 5-7: actual token IDs (response, counted in loss)\")\n\n# Let's verify this works with PyTorch's cross_entropy\n# (which is what HuggingFace uses under the hood)\nvocab_size = 300\nfake_logits = torch.randn(1, 8, vocab_size)\n\n# Compute loss - PyTorch's cross_entropy ignores -100 by default!\nshift_logits = fake_logits[:, :-1, :].contiguous().view(-1, vocab_size)\nshift_labels = labels[:, 1:].contiguous().view(-1)\n\nloss = F.cross_entropy(shift_logits, shift_labels)  # ignore_index=-100 by default!\n\nprint(f\"\\nLoss computed with -100 labels: {loss.item():.4f}\")\nprint(\"\\nPyTorch automatically ignores the -100 tokens. Nice!\")\nprint(\"(That's why HuggingFace uses -100 â€” it's PyTorch's default ignore value)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Wrapping Up\n\nSo that's loss masking. Simple idea, big impact.\n\nThe key insight: during supervised fine-tuning, we only want the model to learn to generate responses, not to predict the instructions we give it. Loss masking accomplishes this by zeroing out the loss for instruction tokens.\n\n**Two ways to implement it:**\n1. Manual masking: create a mask tensor (0 for instruction, 1 for response) and multiply it with your per-token loss\n2. HuggingFace convention: set instruction positions to -100 in your labels tensor\n\nBoth work. The HuggingFace approach is simpler if you're using their libraries (which you probably are).\n\n**Why it matters:**\n- Focuses the model's learning on what actually matters (response quality)\n- Prevents wasting capacity on instruction memorization  \n- Concentrates gradient signal where it's useful\n- Results in better, more generalizable models\n\nNow that we understand loss masking, we're ready to put it all together in a complete SFT training loop. Onward!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}