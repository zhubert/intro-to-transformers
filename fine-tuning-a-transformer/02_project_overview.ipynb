{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Project Overview\n\n**What we're building, why it matters, and how it all fits together**"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Complete Pipeline\n",
    "\n",
    "Alright, let's talk about what happens after you train a base model.\n",
    "\n",
    "You've got a language model that can complete text. Great! But it doesn't follow instructions. It doesn't know what you actually *want* when you ask it a question. It's like a really smart parrot that just continues whatever pattern you started.\n",
    "\n",
    "So how do we go from \"autocomplete machine\" to \"helpful assistant\"?\n",
    "\n",
    "Three stages. Each builds on the last:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                     SUPERVISED FINE-TUNING (SFT)                    │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│ Teaching the model to follow instructions                           │\n",
    "│                                                                      │\n",
    "│ • Show it examples: \"When asked X, respond with Y\"                  │\n",
    "│ • Use chat templates to format conversations                        │\n",
    "│ • Only train on the responses (not the questions)                   │\n",
    "│ • LoRA keeps this efficient (we'll explain later)                   │\n",
    "│                                                                      │\n",
    "│ Analogy: Teaching someone the *format* of good answers              │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                      REWARD MODELING                                │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│ Teaching the model what \"good\" means                                │\n",
    "│                                                                      │\n",
    "│ • Show it pairs: \"This answer is better than that one\"              │\n",
    "│ • Train it to score responses (higher = better)                     │\n",
    "│ • Bradley-Terry loss (fancy ranking math)                           │\n",
    "│ • Evaluation: does it rank things the way humans would?             │\n",
    "│                                                                      │\n",
    "│ Analogy: Teaching a judge to score gymnastics routines              │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "              ┌───────────────┴───────────────┐\n",
    "              ▼                               ▼\n",
    "┌─────────────────────────┐     ┌─────────────────────────┐\n",
    "│         RLHF            │     │          DPO            │\n",
    "├─────────────────────────┤     ├─────────────────────────┤\n",
    "│ Two-stage approach      │     │ One-stage shortcut      │\n",
    "│                         │     │                         │\n",
    "│ • Train reward model    │     │ • Skip reward model     │\n",
    "│ • Use it to train       │     │ • Optimize preferences  │\n",
    "│   policy with PPO       │     │   directly              │\n",
    "│ • Complex but powerful  │     │ • Simpler, faster       │\n",
    "│ • Needs 4 models (!)    │     │ • Only needs 2 models   │\n",
    "│                         │     │                         │\n",
    "│ Classic method          │     │ Modern alternative      │\n",
    "└─────────────────────────┘     └─────────────────────────┘\n",
    "```\n",
    "\n",
    "**RLHF** = Reinforcement Learning from Human Feedback  \n",
    "**DPO** = Direct Preference Optimization  \n",
    "**PPO** = Proximal Policy Optimization (the RL algorithm RLHF uses)\n",
    "\n",
    "The end goal? A model that doesn't just follow instructions, but follows them *well*. Helpful, harmless, and honest (as the saying goes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Module Organization\n",
    "\n",
    "Our code is split into clean modules. Each one handles a different stage of the pipeline.\n",
    "\n",
    "| Module | Purpose | Key Functions |\n",
    "|--------|---------|---------------|\n",
    "| `sft/` | Supervised Fine-Tuning | `SFTTrainer`, `format_instruction` |\n",
    "| `reward/` | Reward Model Training | `RewardModel`, `RewardModelTrainer` |\n",
    "| `rlhf/` | RLHF with PPO | `PPOTrainer`, `ValueNetwork`, `RolloutBuffer` |\n",
    "| `dpo/` | Direct Preference Optimization | `DPOTrainer`, `compute_dpo_loss` |\n",
    "| `utils/` | Shared Utilities | `load_model_and_tokenizer`, `setup_device` |\n",
    "\n",
    "Think of these as separate kitchens in a restaurant. Each one specializes in a different course of the meal. You don't make dessert where you're grilling steaks (though I suppose you could...probably shouldn't)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Data Formats\n",
    "\n",
    "Here's the thing about training pipelines: each stage speaks a different language. Not literally, but in terms of what data format it expects.\n",
    "\n",
    "Let's break them down.\n",
    "\n",
    "### SFT (Supervised Fine-Tuning) Data\n",
    "\n",
    "Simple input-output pairs. Question and answer. Instruction and response.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"What is the capital of France?\",\n",
    "    \"response\": \"The capital of France is Paris.\"\n",
    "}\n",
    "```\n",
    "\n",
    "Dead simple. The model learns \"when you see this format of question, generate this format of answer.\"\n",
    "\n",
    "### Preference Data (for Reward Model & DPO)\n",
    "\n",
    "Now it gets spicy. Instead of just one answer, we show the model two answers to the same prompt. One good, one bad.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"Explain quantum computing simply.\",\n",
    "    \"chosen\": \"Imagine a coin spinning in the air—it's both heads and tails until it lands. Quantum computers work with information in that 'spinning' state, processing multiple possibilities simultaneously.\",\n",
    "    \"rejected\": \"Quantum computers use qubits which leverage quantum superposition and entanglement to perform computations exponentially faster than classical computers by exploiting quantum mechanical phenomena.\"\n",
    "}\n",
    "```\n",
    "\n",
    "See the difference? The \"chosen\" response is simple, clear, uses an analogy. The \"rejected\" one? Technically accurate but sounds like it swallowed a physics textbook.\n",
    "\n",
    "The model learns: \"When comparing these two, rank the first one higher.\"\n",
    "\n",
    "### Prompt Data (for RLHF)\n",
    "\n",
    "Once we have a reward model, we can just give it prompts and let it generate responses, then score them.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"Write a haiku about programming.\"\n",
    "}\n",
    "```\n",
    "\n",
    "The model generates completions, the reward model scores them, and we use those scores to improve the policy. Rinse and repeat.\n",
    "\n",
    "(We'll see this in action much later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:18:07.778896Z",
     "iopub.status.busy": "2025-12-10T21:18:07.778824Z",
     "iopub.status.idle": "2025-12-10T21:18:07.781715Z",
     "shell.execute_reply": "2025-12-10T21:18:07.781445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT Data Format:\n",
      "  Keys: ['instruction', 'response']\n",
      "  Example instruction: \"What is Python?\"\n",
      "\n",
      "Preference Data Format:\n",
      "  Keys: ['prompt', 'chosen', 'rejected']\n",
      "  Chosen response length: 139 chars\n",
      "  Rejected response length: 3 chars\n",
      "\n",
      "(Notice how the rejected response is way shorter? Sometimes bad answers are just...lazy.)\n"
     ]
    }
   ],
   "source": [
    "# Let's make these concrete with actual Python data structures\n",
    "\n",
    "# SFT data: instruction-response pairs\n",
    "sft_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is Python?\", \n",
    "        \"response\": \"Python is a high-level programming language known for its readability and simplicity. It's great for beginners and powerful enough for experts.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Translate 'hello' to French\", \n",
    "        \"response\": \"'Hello' in French is 'Bonjour'.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preference data: one prompt, two competing responses\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain artificial intelligence briefly.\",\n",
    "        \"chosen\": \"AI is technology that enables machines to simulate human intelligence—learning from experience, recognizing patterns, and making decisions.\",\n",
    "        \"rejected\": \"AI.\"  # Technically correct but...useless\n",
    "    }\n",
    "]\n",
    "\n",
    "# Let's look at what these actually contain\n",
    "print(\"SFT Data Format:\")\n",
    "print(f\"  Keys: {list(sft_data[0].keys())}\")\n",
    "print(f\"  Example instruction: \\\"{sft_data[0]['instruction']}\\\"\")\n",
    "print()\n",
    "\n",
    "print(\"Preference Data Format:\")\n",
    "print(f\"  Keys: {list(preference_data[0].keys())}\")\n",
    "print(f\"  Chosen response length: {len(preference_data[0]['chosen'])} chars\")\n",
    "print(f\"  Rejected response length: {len(preference_data[0]['rejected'])} chars\")\n",
    "print()\n",
    "print(\"(Notice how the rejected response is way shorter? Sometimes bad answers are just...lazy.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Training Progression\n",
    "\n",
    "So you've got a base model. Now what?\n",
    "\n",
    "Here's the typical path from \"raw autocomplete\" to \"helpful assistant\":\n",
    "\n",
    "| Step | Input Model | Output Model | What Happens | Training Time* |\n",
    "|------|-------------|--------------|--------------|----------------|\n",
    "| **1. SFT** | Base (GPT-2) | SFT Model | Learn to follow instructions | ~1 hour |\n",
    "| **2. Reward** | SFT Model | Reward Model | Learn to judge quality | ~30 min |\n",
    "| **3a. RLHF** | SFT Model + RM | RLHF Model | Optimize for high rewards | ~2 hours |\n",
    "| **3b. DPO** | SFT Model | DPO Model | Optimize preferences directly | ~1 hour |\n",
    "\n",
    "*Approximate times for GPT-2 (124M params) on a single GPU. Your mileage may vary.\n",
    "\n",
    "Notice step 3 splits? That's the fork in the road. You can either:\n",
    "\n",
    "- Go the **RLHF route**: Train a reward model first, then use reinforcement learning (PPO) to optimize your policy against it. More complex, more moving parts, but this is what OpenAI used for GPT-4.\n",
    "\n",
    "- Go the **DPO route**: Skip the reward model entirely and optimize preferences directly. Simpler, faster, and honestly? Often just as good. This is the new hotness.\n",
    "\n",
    "We'll implement both so you understand the tradeoffs. (Because understanding > blindly following trends.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Key Hyperparameters\n",
    "\n",
    "Hyperparameters are the dials you turn to make training work. Each stage has different sweet spots.\n",
    "\n",
    "Let me explain the reasoning behind these numbers (instead of just throwing them at you).\n",
    "\n",
    "### SFT (Supervised Fine-Tuning)\n",
    "- **Learning rate:** 2e-4 (that's 0.0002)\n",
    "  - Higher than pre-training! We're making bigger updates because we're teaching a new skill\n",
    "  - But not *too* high or we'll destroy what the model already knows\n",
    "- **Batch size:** 4-8\n",
    "  - Small because we're fine-tuning, not pre-training\n",
    "  - Larger batches = more stable but more memory\n",
    "- **Epochs:** 3-5\n",
    "  - A few passes through the data is usually enough\n",
    "  - Too many and you overfit (model memorizes instead of generalizes)\n",
    "\n",
    "### Reward Model\n",
    "- **Learning rate:** 1e-5 (that's 0.00001)\n",
    "  - Much lower! Reward models are sensitive\n",
    "  - We want careful, stable learning of the preference ranking\n",
    "- **Batch size:** 4 (but each sample has 2 sequences)\n",
    "  - We're comparing pairs, so effective batch size is 8 sequences\n",
    "- **Epochs:** 1\n",
    "  - Just one pass! Reward models overfit easily\n",
    "  - If you train too long, they memorize specific preferences instead of learning general quality\n",
    "\n",
    "### RLHF (with PPO)\n",
    "- **Learning rate:** 1e-6 (that's 0.000001)\n",
    "  - Tiny! RL is unstable, we need baby steps\n",
    "  - Too high and training collapses (you'll see divergence, mode collapse, gibberish)\n",
    "- **KL coefficient:** 0.1\n",
    "  - This keeps the model close to the original SFT model\n",
    "  - Prevents it from going off the rails chasing reward\n",
    "- **PPO epochs:** 4\n",
    "  - How many times we update on each batch of rollouts\n",
    "  - Classic PPO sweet spot\n",
    "\n",
    "### DPO (Direct Preference Optimization)\n",
    "- **Learning rate:** 1e-6\n",
    "  - Same as RLHF—we're doing preference learning, gotta be gentle\n",
    "- **Beta (β):** 0.1\n",
    "  - Controls how strongly we optimize preferences\n",
    "  - Higher = more aggressive, lower = more conservative\n",
    "- **Epochs:** 1-3\n",
    "  - DPO is more stable than PPO, can train a bit longer\n",
    "  - But still, don't overdo it\n",
    "\n",
    "The pattern? As we get further from standard supervised learning, we get more conservative. RL is temperamental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:18:07.782474Z",
     "iopub.status.busy": "2025-12-10T21:18:07.782408Z",
     "iopub.status.idle": "2025-12-10T21:18:07.784575Z",
     "shell.execute_reply": "2025-12-10T21:18:07.784338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration summary:\n",
      "  SFT learning rate:    0.000200\n",
      "  Reward learning rate: 0.000010\n",
      "  PPO learning rate:    0.000001\n",
      "  DPO learning rate:    0.000001\n",
      "\n",
      "Notice the pattern? Learning rates get smaller as training gets trickier.\n"
     ]
    }
   ],
   "source": [
    "# Here are those configurations in code\n",
    "# (so you can see them all in one place)\n",
    "\n",
    "sft_config = {\n",
    "    \"learning_rate\": 2e-4,      # 0.0002 - higher for teaching new skills\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"max_length\": 512,          # truncate long sequences here\n",
    "    \"warmup_steps\": 100,        # gradually increase LR at start\n",
    "}\n",
    "\n",
    "reward_config = {\n",
    "    \"learning_rate\": 1e-5,      # 0.00001 - much lower, reward models are sensitive\n",
    "    \"batch_size\": 4,            # but remember: 2 sequences per sample!\n",
    "    \"num_epochs\": 1,            # just one pass to avoid overfitting\n",
    "    \"max_length\": 512,\n",
    "}\n",
    "\n",
    "ppo_config = {\n",
    "    \"learning_rate\": 1e-6,      # 0.000001 - tiny! RL is unstable\n",
    "    \"batch_size\": 4,\n",
    "    \"ppo_epochs\": 4,            # how many times to update per rollout batch\n",
    "    \"kl_coef\": 0.1,             # keeps us close to reference model\n",
    "    \"clip_ratio\": 0.2,          # PPO clipping (prevents huge updates)\n",
    "}\n",
    "\n",
    "dpo_config = {\n",
    "    \"learning_rate\": 1e-6,      # same as PPO\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 1,            # conservative - can go up to 3 if needed\n",
    "    \"beta\": 0.1,                # preference optimization strength\n",
    "}\n",
    "\n",
    "print(\"Configuration summary:\")\n",
    "print(f\"  SFT learning rate:    {sft_config['learning_rate']:.6f}\")\n",
    "print(f\"  Reward learning rate: {reward_config['learning_rate']:.6f}\")\n",
    "print(f\"  PPO learning rate:    {ppo_config['learning_rate']:.6f}\")\n",
    "print(f\"  DPO learning rate:    {dpo_config['learning_rate']:.6f}\")\n",
    "print()\n",
    "print(\"Notice the pattern? Learning rates get smaller as training gets trickier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Memory Considerations\n",
    "\n",
    "Here's the dirty secret about post-training: it's *expensive*.\n",
    "\n",
    "Not money expensive (well, also that), but memory expensive. Let me break down why.\n",
    "\n",
    "| Stage | Models in Memory | Memory Factor | What's Loaded |\n",
    "|-------|-----------------|---------------|---------------|\n",
    "| **SFT** | 1 model | 1x | Just the model we're training |\n",
    "| **Reward** | 1 model | 1x | Just the reward model (but 2 sequences/batch) |\n",
    "| **RLHF** | 4 models | 4x | Policy, value net, reward model, reference model |\n",
    "| **DPO** | 2 models | 2x | Policy, reference model |\n",
    "\n",
    "See why RLHF is so painful? Four models in memory at once:\n",
    "\n",
    "1. **Policy model** - the one we're actually training\n",
    "2. **Value network** - estimates expected future reward (RL thing)\n",
    "3. **Reward model** - scores our generations\n",
    "4. **Reference model** - the original SFT model we're trying not to drift too far from\n",
    "\n",
    "DPO cuts this in half by skipping the reward model and value network. Just policy + reference.\n",
    "\n",
    "### How to fit this on a single GPU?\n",
    "\n",
    "We've got tricks:\n",
    "\n",
    "- **LoRA** (Low-Rank Adaptation)\n",
    "  - Instead of updating all parameters, we add small trainable adapters\n",
    "  - Massively reduces memory for gradients and optimizer states\n",
    "  - Like teaching someone by giving them a cheat sheet instead of rewriting their brain\n",
    "  \n",
    "- **Gradient checkpointing**\n",
    "  - Trade computation for memory\n",
    "  - Recompute activations during backward pass instead of storing them\n",
    "  - Slower but fits in VRAM\n",
    "\n",
    "- **Mixed precision** (fp16/bf16)\n",
    "  - Use 16-bit floats instead of 32-bit\n",
    "  - Cuts memory in half (roughly)\n",
    "  - Modern GPUs are built for this\n",
    "\n",
    "- **Gradient accumulation**\n",
    "  - Simulate larger batches by accumulating gradients over multiple steps\n",
    "  - Doesn't reduce peak memory but improves training stability\n",
    "  - \"I can't lift 100 pounds at once, but I can make four trips with 25 pounds each\"\n",
    "\n",
    "Without these tricks? You'd need a data center. With them? You can do this on a consumer GPU.\n",
    "\n",
    "(Well, for GPT-2 scale. If you want to fine-tune Llama-70B...get your credit card ready.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Alright, you've got the bird's eye view of the entire pipeline.\n",
    "\n",
    "We're going from base model → instruction-following → preference-aligned. Three stages (or four, if you count the RLHF/DPO fork).\n",
    "\n",
    "Now let's get our hands dirty.\n",
    "\n",
    "Next up: **Supervised Fine-Tuning** (SFT). We'll teach our model to follow instructions, format responses properly, and actually be useful.\n",
    "\n",
    "It's the foundation everything else builds on. Get this right, and the rest flows naturally. Get it wrong, and...well, you'll be debugging reward models for a week.\n",
    "\n",
    "Let's go."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}