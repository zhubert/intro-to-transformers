{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "**Architecture, components, and how everything fits together**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Complete Pipeline\n",
    "\n",
    "This section covers the full post-training pipeline:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                     SUPERVISED FINE-TUNING                          │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│ • Instruction formatting (chat templates)                           │\n",
    "│ • Loss masking (only train on responses)                           │\n",
    "│ • LoRA for efficient training                                       │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                      REWARD MODELING                                │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│ • Preference data format                                            │\n",
    "│ • Bradley-Terry ranking loss                                        │\n",
    "│ • Evaluation metrics                                                │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "              ┌───────────────┴───────────────┐\n",
    "              ▼                               ▼\n",
    "┌─────────────────────────┐     ┌─────────────────────────┐\n",
    "│         RLHF            │     │          DPO            │\n",
    "├─────────────────────────┤     ├─────────────────────────┤\n",
    "│ • PPO algorithm         │     │ • Direct optimization   │\n",
    "│ • KL divergence         │     │ • No reward model       │\n",
    "│ • Value network         │     │ • Simpler training      │\n",
    "│ • Reference model       │     │ • Reference model       │\n",
    "└─────────────────────────┘     └─────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Module Organization\n",
    "\n",
    "The codebase is organized into clear modules:\n",
    "\n",
    "| Module | Purpose | Key Functions |\n",
    "|--------|---------|---------------|\n",
    "| `sft/` | Supervised Fine-Tuning | `SFTTrainer`, `format_instruction` |\n",
    "| `reward/` | Reward Model Training | `RewardModel`, `RewardModelTrainer` |\n",
    "| `rlhf/` | RLHF with PPO | `PPOTrainer`, `ValueNetwork`, `RolloutBuffer` |\n",
    "| `dpo/` | Direct Preference Optimization | `DPOTrainer`, `compute_dpo_loss` |\n",
    "| `utils/` | Shared Utilities | `load_model_and_tokenizer`, `setup_device` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Data Formats\n",
    "\n",
    "Each stage uses a specific data format:\n",
    "\n",
    "### SFT Data\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"What is the capital of France?\",\n",
    "    \"response\": \"The capital of France is Paris.\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Preference Data (Reward Model & DPO)\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"Explain quantum computing simply.\",\n",
    "    \"chosen\": \"Imagine a coin spinning in the air...\",\n",
    "    \"rejected\": \"Quantum computers use qubits which leverage...\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Prompt Data (RLHF)\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"Write a haiku about programming.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:28:49.641661Z",
     "iopub.status.busy": "2025-12-06T23:28:49.641576Z",
     "iopub.status.idle": "2025-12-06T23:28:50.670465Z",
     "shell.execute_reply": "2025-12-06T23:28:50.670095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT Data Format:\n",
      "  Keys: ['instruction', 'response']\n",
      "\n",
      "Preference Data Format:\n",
      "  Keys: ['prompt', 'chosen', 'rejected']\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading different data formats\n",
    "from datasets import Dataset\n",
    "\n",
    "# SFT data example\n",
    "sft_data = [\n",
    "    {\"instruction\": \"What is Python?\", \"response\": \"Python is a programming language.\"},\n",
    "    {\"instruction\": \"Translate 'hello' to French\", \"response\": \"'Hello' in French is 'Bonjour'.\"}\n",
    "]\n",
    "\n",
    "# Preference data example  \n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain AI briefly.\",\n",
    "        \"chosen\": \"AI is technology that enables machines to simulate human intelligence.\",\n",
    "        \"rejected\": \"AI.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"SFT Data Format:\")\n",
    "print(f\"  Keys: {list(sft_data[0].keys())}\")\n",
    "print()\n",
    "print(\"Preference Data Format:\")\n",
    "print(f\"  Keys: {list(preference_data[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Training Progression\n",
    "\n",
    "The typical training flow:\n",
    "\n",
    "| Step | Input Model | Output Model | Training Time* |\n",
    "|------|-------------|--------------|----------------|\n",
    "| 1. SFT | Base (GPT-2) | SFT Model | ~1 hour |\n",
    "| 2. Reward | SFT Model | Reward Model | ~30 min |\n",
    "| 3a. RLHF | SFT Model + RM | RLHF Model | ~2 hours |\n",
    "| 3b. DPO | SFT Model | DPO Model | ~1 hour |\n",
    "\n",
    "*Approximate times for GPT-2 scale on a single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Key Hyperparameters\n",
    "\n",
    "Each stage has critical hyperparameters:\n",
    "\n",
    "### SFT\n",
    "- **Learning rate:** 2e-4 (higher than pre-training)\n",
    "- **Batch size:** 4-8\n",
    "- **Epochs:** 3-5\n",
    "\n",
    "### Reward Model\n",
    "- **Learning rate:** 1e-5 (lower than SFT)\n",
    "- **Batch size:** 4 (2 sequences per sample)\n",
    "- **Epochs:** 1 (avoid overfitting)\n",
    "\n",
    "### RLHF\n",
    "- **Learning rate:** 1e-6 (very low)\n",
    "- **KL coefficient:** 0.1\n",
    "- **PPO epochs:** 4\n",
    "\n",
    "### DPO  \n",
    "- **Learning rate:** 1e-6\n",
    "- **Beta (β):** 0.1\n",
    "- **Epochs:** 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:28:50.671607Z",
     "iopub.status.busy": "2025-12-06T23:28:50.671481Z",
     "iopub.status.idle": "2025-12-06T23:28:50.673397Z",
     "shell.execute_reply": "2025-12-06T23:28:50.673143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations loaded for all training stages!\n"
     ]
    }
   ],
   "source": [
    "# Configuration examples for each stage\n",
    "\n",
    "sft_config = {\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"max_length\": 512,\n",
    "    \"warmup_steps\": 100,\n",
    "}\n",
    "\n",
    "reward_config = {\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"max_length\": 512,\n",
    "}\n",
    "\n",
    "ppo_config = {\n",
    "    \"learning_rate\": 1e-6,\n",
    "    \"batch_size\": 4,\n",
    "    \"ppo_epochs\": 4,\n",
    "    \"kl_coef\": 0.1,\n",
    "    \"clip_ratio\": 0.2,\n",
    "}\n",
    "\n",
    "dpo_config = {\n",
    "    \"learning_rate\": 1e-6,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"beta\": 0.1,\n",
    "}\n",
    "\n",
    "print(\"Configurations loaded for all training stages!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Memory Considerations\n",
    "\n",
    "Post-training can be memory-intensive:\n",
    "\n",
    "| Stage | Models in Memory | Memory Factor |\n",
    "|-------|-----------------|---------------|\n",
    "| SFT | 1 model | 1x |\n",
    "| Reward | 1 model | 1x (but 2 sequences/batch) |\n",
    "| RLHF | 4 models (policy, value, reward, reference) | 4x |\n",
    "| DPO | 2 models (policy, reference) | 2x |\n",
    "\n",
    "**Solutions:**\n",
    "- LoRA for efficient fine-tuning\n",
    "- Gradient checkpointing\n",
    "- Mixed precision (fp16/bf16)\n",
    "- Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have an overview of the complete pipeline, let's dive into the first stage: Supervised Fine-Tuning (SFT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
