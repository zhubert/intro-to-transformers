{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# The Reference Model: Your AI's Anchor to Reality\n",
    "\n",
    "We've trained a reward model. We've got our policy ready to learn. \n",
    "\n",
    "But there's one more piece to this RLHF puzzle. And it's weirdly important.\n",
    "\n",
    "We need to keep a copy of our model *exactly as it is right now* — before we start letting the reward model push it around. A frozen snapshot. An anchor point.\n",
    "\n",
    "This is the **reference model**.\n",
    "\n",
    "(And yes, it does mean we'll have two copies of the same giant neural network sitting in memory. We'll deal with that headache in a minute.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Drift Problem (Or: Why Models Go Crazy)\n",
    "\n",
    "What happens without a reference model.\n",
    "\n",
    "You start RLHF training. Your policy generates some text. The reward model scores it. The policy adjusts to get higher scores.\n",
    "\n",
    "Sounds great, right?\n",
    "\n",
    "Except... the policy starts to *drift*.\n",
    "\n",
    "At first it's subtle. Maybe it discovers that the reward model really likes certain phrases. So it uses them more often. Then it discovers that repeating the same word over and over scores well (because the reward model has some weird quirk). So it does that. A lot.\n",
    "\n",
    "Eventually, you end up with a model that outputs complete gibberish but somehow scores incredibly high rewards.\n",
    "\n",
    "This is called **reward hacking**. The policy has learned to exploit the reward model instead of actually getting better at the task.\n",
    "\n",
    "Think of it like this: You're trying to teach a dog to fetch. But instead of learning to bring back the ball, the dog realizes that if it just sits on your lap and licks your face, you'll give it treats anyway. Technically high reward. Completely missing the point.\n",
    "\n",
    "## Enter the Reference Model\n",
    "\n",
    "The reference model is our solution. It's a frozen copy of the policy *before* RLHF training starts.\n",
    "\n",
    "During training, we don't just maximize reward. We maximize reward *while staying close to the reference model*.\n",
    "\n",
    "Here's the actual objective function we optimize:\n",
    "\n",
    "**reward(response) - β × KL(policy || reference)**\n",
    "\n",
    "Let me break that down in English:\n",
    "- **reward(response)**: How much the reward model likes our output\n",
    "- **β** (beta): A hyperparameter controlling how much we care about staying close (typically 0.01 to 0.1)\n",
    "- **KL(policy || reference)**: The KL divergence — a measure of how different the policy's probability distribution is from the reference's\n",
    "\n",
    "The KL divergence is the key. It penalizes the policy for assigning very different probabilities to words compared to the reference. If the policy starts drifting into weird territory, the KL penalty pulls it back.\n",
    "\n",
    "It's like training that dog on a leash. Sure, explore a bit. Get creative. But don't wander so far that you forget what a normal dog is supposed to do.\n",
    "\n",
    "```\n",
    "SFT Model (our starting point)\n",
    "    │\n",
    "    ├──> Policy Model (trainable, learns from rewards)\n",
    "    │\n",
    "    └──> Reference Model (frozen, stays exactly as it was)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## What Can Go Wrong Without a Reference?\n\nLet's be specific about the failure modes:\n\n**1. Reward Hacking**\nThe policy finds shortcuts to high reward that completely miss the point. Like discovering the reward model gives high scores to responses that end with \"I hope this helps!\" — so the policy starts ending *every response* with that phrase, even when it makes no sense.\n\n**2. Mode Collapse**\nThe policy collapses to generating the same response over and over. Why? Because it found *one* response that scores well, and without the KL penalty, there's no incentive to explore. You end up with a model that outputs \"That's a great question! I'm happy to help.\" to literally everything.\n\n**3. Language Degradation**\nThe policy forgets how to speak coherently. It drifts so far from natural language that it's generating statistically high-reward token sequences that look like alphabet soup to humans.\n\nThe reference model prevents all three. It keeps the policy grounded in the language patterns it learned during pretraining and SFT."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:40.345851Z",
     "iopub.status.busy": "2025-12-10T21:19:40.345780Z",
     "iopub.status.idle": "2025-12-10T21:19:42.610137Z",
     "shell.execute_reply": "2025-12-10T21:19:42.609781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 as our policy model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating frozen reference model...\n",
      "\n",
      "Policy model: 124,439,808 trainable parameters\n",
      "Reference model: 0 trainable parameters\n",
      "\n",
      "The reference is completely frozen - exactly what we want!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def create_reference_model(policy_model):\n",
    "    \"\"\"\n",
    "    Create a frozen reference model from the policy.\n",
    "    \n",
    "    This is literally just a deep copy with all the parameters frozen.\n",
    "    We're making a snapshot of the model at this exact moment in time.\n",
    "    \"\"\"\n",
    "    # Deep copy the entire model (this duplicates all weights in memory)\n",
    "    reference_model = copy.deepcopy(policy_model)\n",
    "    \n",
    "    # Freeze every single parameter - no gradients, no updates\n",
    "    for param in reference_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Set to evaluation mode (disables dropout, etc.)\n",
    "    reference_model.eval()\n",
    "    \n",
    "    return reference_model\n",
    "\n",
    "# Let's create a reference model from GPT-2\n",
    "print(\"Loading GPT-2 as our policy model...\")\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"Creating frozen reference model...\")\n",
    "reference_model = create_reference_model(policy_model)\n",
    "\n",
    "# Verify that the policy can be trained but the reference cannot\n",
    "policy_trainable = sum(p.numel() for p in policy_model.parameters() if p.requires_grad)\n",
    "ref_trainable = sum(p.numel() for p in reference_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nPolicy model: {policy_trainable:,} trainable parameters\")\n",
    "print(f\"Reference model: {ref_trainable:,} trainable parameters\")\n",
    "print(f\"\\nThe reference is completely frozen - exactly what we want!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## The Memory Problem\n\nOkay, so we just made a complete copy of our model.\n\nIf you're training GPT-2 (124M parameters), that's annoying but manageable. If you're training Llama 2 7B... well, now you've got 14B parameters sitting in memory. If you're training Llama 2 70B, you might want to start crying now.\n\nTwo identical massive models. Double the memory usage. All so we can compute a penalty term.\n\nThere are a few ways to deal with this headache:\n\n**Option 1: Half Precision Reference**\nKeep the reference in FP16 (half precision) instead of FP32. Cuts memory in half. The reference doesn't need to be super precise — we're just using it to compute KL divergence.\n\n**Option 2: CPU Reference**\nMove the reference to CPU. Frees up GPU memory for the policy. But now every time you need to compute KL, you're shuffling data between CPU and GPU. Slower, but sometimes necessary.\n\n**Option 3: Periodic KL Computation**\nOnly compute the KL penalty every N steps instead of every step. It's an approximation, but it can work if you're willing to accept slightly less stable training.\n\n**Option 4: No Reference (DPO)**\nJust... don't use a reference model at all. This is actually what DPO (Direct Preference Optimization) does. It's a completely different algorithm that doesn't need reward models *or* reference models. We'll look at that in the next notebook.\n\nLet's implement options 1 and 2:"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:42.611101Z",
     "iopub.status.busy": "2025-12-10T21:19:42.610942Z",
     "iopub.status.idle": "2025-12-10T21:19:42.678019Z",
     "shell.execute_reply": "2025-12-10T21:19:42.677683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FP16 reference model...\n",
      "\n",
      "Memory usage comparison:\n",
      "  FP32 reference: ~474.7 MB\n",
      "  FP16 reference: ~237.4 MB\n",
      "  Savings: ~237.4 MB (50% reduction)\n",
      "\n",
      "For a 7B parameter model, that's saving ~14GB of memory. Not bad!\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Keep reference in half precision\n",
    "def create_reference_model_fp16(policy_model):\n",
    "    \"\"\"\n",
    "    Create reference model in half precision to save memory.\n",
    "    \n",
    "    FP16 (float16) uses 16 bits per parameter instead of 32.\n",
    "    Cuts memory usage in half. The reference doesn't need full precision\n",
    "    since we're just using it for KL divergence calculations.\n",
    "    \"\"\"\n",
    "    reference_model = copy.deepcopy(policy_model)\n",
    "    reference_model = reference_model.half()  # Convert to FP16\n",
    "    \n",
    "    for param in reference_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    reference_model.eval()\n",
    "    return reference_model\n",
    "\n",
    "# Option 2: Move reference to CPU (slower but saves GPU memory)\n",
    "def create_reference_model_cpu(policy_model):\n",
    "    \"\"\"\n",
    "    Create reference model on CPU to save GPU memory.\n",
    "    \n",
    "    This moves the reference entirely to CPU. Frees up GPU memory\n",
    "    for the policy, but you'll pay a speed penalty when computing\n",
    "    KL divergence (have to move data between CPU and GPU).\n",
    "    \"\"\"\n",
    "    reference_model = copy.deepcopy(policy_model)\n",
    "    reference_model = reference_model.cpu()\n",
    "    \n",
    "    for param in reference_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    reference_model.eval()\n",
    "    return reference_model\n",
    "\n",
    "# Let's try the FP16 version\n",
    "print(\"Creating FP16 reference model...\")\n",
    "reference_model_fp16 = create_reference_model_fp16(policy_model)\n",
    "\n",
    "# Check memory savings (approximate)\n",
    "fp32_params = sum(p.numel() for p in reference_model.parameters())\n",
    "fp16_params = sum(p.numel() for p in reference_model_fp16.parameters())\n",
    "\n",
    "fp32_memory_mb = (fp32_params * 4) / (1024 * 1024)  # 4 bytes per FP32\n",
    "fp16_memory_mb = (fp16_params * 2) / (1024 * 1024)  # 2 bytes per FP16\n",
    "\n",
    "print(f\"\\nMemory usage comparison:\")\n",
    "print(f\"  FP32 reference: ~{fp32_memory_mb:.1f} MB\")\n",
    "print(f\"  FP16 reference: ~{fp16_memory_mb:.1f} MB\")\n",
    "print(f\"  Savings: ~{fp32_memory_mb - fp16_memory_mb:.1f} MB ({((fp32_memory_mb - fp16_memory_mb) / fp32_memory_mb * 100):.0f}% reduction)\")\n",
    "print(f\"\\nFor a 7B parameter model, that's saving ~14GB of memory. Not bad!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Computing KL Divergence: The Actual Math\n",
    "\n",
    "Alright, time to get into the weeds a bit.\n",
    "\n",
    "We need to compute the KL divergence between the policy and reference. But what does that actually mean in code?\n",
    "\n",
    "Remember: both models are probability distributions over the next token. Given the same input, they both output a probability for every possible next token.\n",
    "\n",
    "The KL divergence measures how different those probability distributions are.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "**KL(policy || reference) = Σ p(token) × log(p(token) / r(token))**\n",
    "\n",
    "In English:\n",
    "- **p(token)**: Probability the policy assigns to this token\n",
    "- **r(token)**: Probability the reference assigns to this token\n",
    "- **log(p/r)**: The log ratio (becomes 0 when they're equal, positive when policy prefers this token more, negative when less)\n",
    "- **Σ**: Sum over all tokens in the vocabulary\n",
    "\n",
    "The trick: we don't actually compute this sum over the entire vocabulary (that would be 50,000+ tokens for GPT-2). \n",
    "\n",
    "Instead, we compute it only for the tokens that were *actually generated*. This is called the **per-token KL divergence** and it's what you'll see in practice.\n",
    "\n",
    "For each token in the generated response:\n",
    "1. Get the policy's log probability for that token\n",
    "2. Get the reference's log probability for that token  \n",
    "3. Take the difference\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:42.678975Z",
     "iopub.status.busy": "2025-12-10T21:19:42.678896Z",
     "iopub.status.idle": "2025-12-10T21:19:43.040291Z",
     "shell.execute_reply": "2025-12-10T21:19:43.039957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: 'Hello, how are you doing today?'\n",
      "Tokens: [15496, 11, 703, 389, 345, 1804, 1909, 30]\n",
      "\n",
      "Log probability shapes: torch.Size([1, 7])\n",
      "(One log prob for each of the 7 tokens we're predicting)\n",
      "\n",
      "Per-token KL divergences: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Mean KL divergence: 0.0000\n",
      "\n",
      "Since we haven't trained the policy yet, it's identical to the reference.\n",
      "That's why KL divergence is basically zero. After training, this would be > 0.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_log_probs(model, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Get log probabilities for tokens under a model.\n",
    "    \n",
    "    This runs the model forward and extracts the log probability\n",
    "    it assigned to each token that actually appeared in the sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # No gradients for the reference\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # Raw scores for each token in vocab\n",
    "    \n",
    "    # Shift for next-token prediction\n",
    "    # The model predicts token i+1 from tokens 0...i\n",
    "    # So logits[:, 0] predicts token 1, logits[:, 1] predicts token 2, etc.\n",
    "    shift_logits = logits[:, :-1, :]  # Drop last position (no next token)\n",
    "    shift_labels = input_ids[:, 1:]    # Drop first token (not predicted)\n",
    "    \n",
    "    # Convert logits to log probabilities via log softmax\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "    \n",
    "    # Extract just the log probs for the actual tokens that appeared\n",
    "    # This gathers log_probs[i, shift_labels[i]] for each position i\n",
    "    token_log_probs = torch.gather(\n",
    "        log_probs,\n",
    "        dim=-1,\n",
    "        index=shift_labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    \n",
    "    return token_log_probs\n",
    "\n",
    "# Let's test this with a real sentence\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text = \"Hello, how are you doing today?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Input text: '{text}'\")\n",
    "print(f\"Tokens: {inputs['input_ids'].tolist()[0]}\")\n",
    "\n",
    "# Get log probs from both models\n",
    "policy_logprobs = get_log_probs(policy_model, inputs['input_ids'], inputs['attention_mask'])\n",
    "ref_logprobs = get_log_probs(reference_model, inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print(f\"\\nLog probability shapes: {policy_logprobs.shape}\")\n",
    "print(f\"(One log prob for each of the {policy_logprobs.shape[1]} tokens we're predicting)\")\n",
    "\n",
    "# Compute per-token KL divergence\n",
    "kl_per_token = policy_logprobs - ref_logprobs\n",
    "kl_mean = kl_per_token.mean()\n",
    "\n",
    "print(f\"\\nPer-token KL divergences: {kl_per_token[0].tolist()}\")\n",
    "print(f\"Mean KL divergence: {kl_mean.item():.4f}\")\n",
    "\n",
    "print(f\"\\nSince we haven't trained the policy yet, it's identical to the reference.\")\n",
    "print(f\"That's why KL divergence is basically zero. After training, this would be > 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Sanity Check: Is the Reference Actually Frozen?\n",
    "\n",
    "Before we finish, let's verify our reference model is actually frozen. You'd be surprised how easy it is to mess this up (and then spend hours debugging why your training is behaving weirdly).\n",
    "\n",
    "We want to check two things:\n",
    "1. **No parameters require gradients** — the reference should not update during training\n",
    "2. **Weights stay constant** — after policy training, policy and reference should be different\n",
    "\n",
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:43.041248Z",
     "iopub.status.busy": "2025-12-10T21:19:43.041166Z",
     "iopub.status.idle": "2025-12-10T21:19:43.190756Z",
     "shell.execute_reply": "2025-12-10T21:19:43.190407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Model Verification:\n",
      "  Any parameter requires grad? False\n",
      "    ✓ Should be False (frozen)\n",
      "\n",
      "  Weights equal to policy? True\n",
      "    ✓ Should be True initially (same starting point)\n",
      "\n",
      "  In eval mode? True\n",
      "    ✓ Should be True (no dropout, etc.)\n",
      "\n",
      "✓ Reference model is properly frozen!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verify_reference_frozen(policy_model, reference_model):\n",
    "    \"\"\"\n",
    "    Verify that reference model is properly frozen.\n",
    "    \n",
    "    Returns True if everything looks good, False if something's wrong.\n",
    "    \"\"\"\n",
    "    # Check 1: No parameters should require gradients\n",
    "    ref_requires_grad = any(p.requires_grad for p in reference_model.parameters())\n",
    "    \n",
    "    # Check 2: Weights should be identical to policy initially\n",
    "    # (They'll diverge after policy training)\n",
    "    first_policy_param = next(policy_model.parameters())\n",
    "    first_ref_param = next(reference_model.parameters())\n",
    "    weights_equal = torch.allclose(first_policy_param, first_ref_param, atol=1e-6)\n",
    "    \n",
    "    print(\"Reference Model Verification:\")\n",
    "    print(f\"  Any parameter requires grad? {ref_requires_grad}\")\n",
    "    print(f\"    ✓ Should be False (frozen)\" if not ref_requires_grad else \"    ✗ PROBLEM: Should be False!\")\n",
    "    \n",
    "    print(f\"\\n  Weights equal to policy? {weights_equal}\")\n",
    "    print(f\"    ✓ Should be True initially (same starting point)\" if weights_equal else \"    ✓ Different after training\")\n",
    "    \n",
    "    print(f\"\\n  In eval mode? {not reference_model.training}\")\n",
    "    print(f\"    ✓ Should be True (no dropout, etc.)\" if not reference_model.training else \"    ✗ PROBLEM: Should be True!\")\n",
    "    \n",
    "    all_good = (not ref_requires_grad) and (not reference_model.training)\n",
    "    \n",
    "    if all_good:\n",
    "        print(\"\\n✓ Reference model is properly frozen!\")\n",
    "    else:\n",
    "        print(\"\\n✗ Something's wrong with the reference model setup!\")\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "# Test it\n",
    "verify_reference_frozen(policy_model, reference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Monitoring Drift During Training\n",
    "\n",
    "One more useful tool: tracking how far the policy has drifted from the reference.\n",
    "\n",
    "During RLHF training, you want to keep an eye on this. If the policy drifts too far, you might need to increase β (the KL penalty weight). If it barely drifts at all, you might be able to decrease β and let it explore more.\n",
    "\n",
    "Here's a simple way to measure drift in weight space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:43.191542Z",
     "iopub.status.busy": "2025-12-10T21:19:43.191457Z",
     "iopub.status.idle": "2025-12-10T21:19:43.255171Z",
     "shell.execute_reply": "2025-12-10T21:19:43.254856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight divergence from reference:\n",
      "  Absolute: 0.000000\n",
      "  Relative: 0.000000\n",
      "\n",
      "Should be zero initially. After training, this will increase.\n",
      "If it increases too much, you're drifting far from the reference!\n",
      "\n",
      "Typical values during RLHF:\n",
      "  Early training: 0.01 - 0.05\n",
      "  Mid training: 0.05 - 0.15\n",
      "  Late training: 0.15 - 0.30\n",
      "  Too much drift: > 0.50 (might be reward hacking!)\n"
     ]
    }
   ],
   "source": [
    "def compute_weight_divergence(policy_model, reference_model):\n",
    "    \"\"\"\n",
    "    Compute how far policy weights have diverged from reference.\n",
    "    \n",
    "    This gives you a single number you can track during training.\n",
    "    Useful for detecting if the policy is drifting too far too fast.\n",
    "    \"\"\"\n",
    "    total_diff = 0.0\n",
    "    total_norm = 0.0\n",
    "    \n",
    "    # Sum up the norm of differences across all parameters\n",
    "    for (name, p_param), (_, r_param) in zip(\n",
    "        policy_model.named_parameters(),\n",
    "        reference_model.named_parameters()\n",
    "    ):\n",
    "        diff = (p_param - r_param).norm().item()  # L2 norm of difference\n",
    "        norm = r_param.norm().item()               # L2 norm of reference\n",
    "        total_diff += diff\n",
    "        total_norm += norm\n",
    "    \n",
    "    # Relative divergence normalizes by reference magnitude\n",
    "    relative_divergence = total_diff / (total_norm + 1e-8)\n",
    "    \n",
    "    return {\n",
    "        'absolute_divergence': total_diff,\n",
    "        'relative_divergence': relative_divergence\n",
    "    }\n",
    "\n",
    "# Check initial divergence (should be ~0 since we haven't trained yet)\n",
    "divergence = compute_weight_divergence(policy_model, reference_model)\n",
    "\n",
    "print(f\"Weight divergence from reference:\")\n",
    "print(f\"  Absolute: {divergence['absolute_divergence']:.6f}\")\n",
    "print(f\"  Relative: {divergence['relative_divergence']:.6f}\")\n",
    "print(f\"\\nShould be zero initially. After training, this will increase.\")\n",
    "print(f\"If it increases too much, you're drifting far from the reference!\")\n",
    "print(f\"\\nTypical values during RLHF:\")\n",
    "print(f\"  Early training: 0.01 - 0.05\")\n",
    "print(f\"  Mid training: 0.05 - 0.15\")\n",
    "print(f\"  Late training: 0.15 - 0.30\")\n",
    "print(f\"  Too much drift: > 0.50 (might be reward hacking!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Wrapping Up: The Complete Picture\n",
    "\n",
    "Let's recap what we've learned about reference models:\n",
    "\n",
    "**What it is:** A frozen copy of your policy model at the start of RLHF training.\n",
    "\n",
    "**Why you need it:** Without it, the policy drifts into reward hacking, mode collapse, or language degradation. The KL penalty keeps it grounded.\n",
    "\n",
    "**The cost:** Double the memory usage (though we can mitigate with FP16 or CPU offloading).\n",
    "\n",
    "**The math:** We compute KL divergence as the difference in log probabilities between policy and reference, then penalize the policy for drifting too far.\n",
    "\n",
    "**How to use it:** Create it once at the start of training, freeze it completely, and use it to compute the KL penalty at every training step.\n",
    "\n",
    "---\n",
    "\n",
    "Now we've covered the complete RLHF pipeline:\n",
    "1. ✓ Supervised fine-tuning (SFT) to teach the model to follow instructions\n",
    "2. ✓ Reward model training to learn human preferences\n",
    "3. ✓ Reference model creation to prevent drift\n",
    "4. ✓ (Next up: putting it all together with PPO)\n",
    "\n",
    "But there's actually a simpler way to do all of this. It's called **Direct Preference Optimization (DPO)**, and it skips the reward model *and* the reference model entirely.\n",
    "\n",
    "Let's look at that next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Uses frozen reference model for KL penalty computation and explores reward-KL tradeoff."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}