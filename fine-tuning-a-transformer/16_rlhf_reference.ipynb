{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Reference Models in RLHF\n",
    "\n",
    "**Creating and managing frozen reference models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is a Reference Model?\n",
    "\n",
    "The **reference model** is a frozen copy of the policy at the start of RLHF training. It serves as an anchor to prevent the policy from drifting too far.\n",
    "\n",
    "```\n",
    "SFT Model\n",
    "    │\n",
    "    ├── → Policy Model (trainable)\n",
    "    │\n",
    "    └── → Reference Model (frozen)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Why Reference Models Matter\n",
    "\n",
    "Without a reference model, the policy can:\n",
    "\n",
    "1. **Reward hack** — Find degenerate high-reward outputs\n",
    "2. **Mode collapse** — Generate repetitive responses\n",
    "3. **Forget language** — Lose coherent generation ability\n",
    "\n",
    "The KL penalty against the reference prevents these failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def create_reference_model(policy_model):\n",
    "    \"\"\"\n",
    "    Create a frozen reference model from the policy.\n",
    "    \n",
    "    The reference model is a deep copy with all parameters frozen.\n",
    "    \"\"\"\n",
    "    # Deep copy the model\n",
    "    reference_model = copy.deepcopy(policy_model)\n",
    "    \n",
    "    # Freeze all parameters\n",
    "    for param in reference_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    reference_model.eval()\n",
    "    \n",
    "    return reference_model\n",
    "\n",
    "# Example\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "reference_model = create_reference_model(policy_model)\n",
    "\n",
    "# Verify\n",
    "policy_trainable = sum(p.numel() for p in policy_model.parameters() if p.requires_grad)\n",
    "ref_trainable = sum(p.numel() for p in reference_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Policy trainable params: {policy_trainable:,}\")\n",
    "print(f\"Reference trainable params: {ref_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Memory Optimization\n",
    "\n",
    "Having both policy and reference in memory doubles memory usage. Solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Keep reference in half precision\n",
    "def create_reference_model_fp16(policy_model):\n",
    "    \"\"\"Create reference model in half precision to save memory.\"\"\"\n",
    "    reference_model = copy.deepcopy(policy_model)\n",
    "    reference_model = reference_model.half()  # Convert to FP16\n",
    "    \n",
    "    for param in reference_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    reference_model.eval()\n",
    "    return reference_model\n",
    "\n",
    "# Option 2: Move reference to CPU (slower but saves GPU memory)\n",
    "def create_reference_model_cpu(policy_model):\n",
    "    \"\"\"Create reference model on CPU to save GPU memory.\"\"\"\n",
    "    reference_model = copy.deepcopy(policy_model)\n",
    "    reference_model = reference_model.cpu()\n",
    "    \n",
    "    for param in reference_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    reference_model.eval()\n",
    "    return reference_model\n",
    "\n",
    "print(\"Memory optimization strategies:\")\n",
    "print(\"  1. FP16 reference: ~50% memory reduction\")\n",
    "print(\"  2. CPU reference: Full GPU memory for policy (slower)\")\n",
    "print(\"  3. Compute KL only periodically (approximation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Computing Reference Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_log_probs(model, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Get log probabilities for tokens under a model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Shift for next-token prediction\n",
    "    shift_logits = logits[:, :-1, :]\n",
    "    shift_labels = input_ids[:, 1:]\n",
    "    \n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "    \n",
    "    # Gather log probs for actual tokens\n",
    "    token_log_probs = torch.gather(\n",
    "        log_probs,\n",
    "        dim=-1,\n",
    "        index=shift_labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    \n",
    "    return token_log_probs\n",
    "\n",
    "# Example usage\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text = \"Hello, how are you doing today?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "policy_logprobs = get_log_probs(policy_model, inputs['input_ids'], inputs['attention_mask'])\n",
    "ref_logprobs = get_log_probs(reference_model, inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print(f\"Policy log probs shape: {policy_logprobs.shape}\")\n",
    "print(f\"Reference log probs shape: {ref_logprobs.shape}\")\n",
    "\n",
    "kl = (policy_logprobs - ref_logprobs).mean()\n",
    "print(f\"KL divergence: {kl.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Verifying Reference is Frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_reference_frozen(policy_model, reference_model):\n",
    "    \"\"\"\n",
    "    Verify that reference model is properly frozen.\n",
    "    \"\"\"\n",
    "    # Check no gradients\n",
    "    ref_requires_grad = any(p.requires_grad for p in reference_model.parameters())\n",
    "    \n",
    "    # Check weights are different from policy (after training)\n",
    "    # Initially they should be the same\n",
    "    first_policy_param = next(policy_model.parameters())\n",
    "    first_ref_param = next(reference_model.parameters())\n",
    "    weights_equal = torch.allclose(first_policy_param, first_ref_param)\n",
    "    \n",
    "    print(\"Reference Model Verification:\")\n",
    "    print(f\"  Requires grad: {ref_requires_grad} (should be False)\")\n",
    "    print(f\"  Weights equal to policy: {weights_equal} (True initially, False after training)\")\n",
    "    \n",
    "    return not ref_requires_grad\n",
    "\n",
    "verify_reference_frozen(policy_model, reference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Monitoring Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight_divergence(policy_model, reference_model):\n",
    "    \"\"\"\n",
    "    Compute how far policy weights have diverged from reference.\n",
    "    \"\"\"\n",
    "    total_diff = 0.0\n",
    "    total_norm = 0.0\n",
    "    \n",
    "    for (name, p_param), (_, r_param) in zip(\n",
    "        policy_model.named_parameters(),\n",
    "        reference_model.named_parameters()\n",
    "    ):\n",
    "        diff = (p_param - r_param).norm().item()\n",
    "        norm = r_param.norm().item()\n",
    "        total_diff += diff\n",
    "        total_norm += norm\n",
    "    \n",
    "    relative_divergence = total_diff / (total_norm + 1e-8)\n",
    "    \n",
    "    return {\n",
    "        'absolute_divergence': total_diff,\n",
    "        'relative_divergence': relative_divergence\n",
    "    }\n",
    "\n",
    "# Initially should be ~0\n",
    "divergence = compute_weight_divergence(policy_model, reference_model)\n",
    "print(f\"Weight divergence:\")\n",
    "print(f\"  Absolute: {divergence['absolute_divergence']:.6f}\")\n",
    "print(f\"  Relative: {divergence['relative_divergence']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we understand the complete RLHF pipeline, let's explore DPO — a simpler alternative that doesn't require a reward model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
