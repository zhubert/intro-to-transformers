{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Introduction to Reward Models"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is a Reward Model?\n",
    "\n",
    "Your SFT model can follow instructions. But it doesn't know which of its responses are *actually good*.\n",
    "\n",
    "After Supervised Fine-Tuning, the model learned to mimic the training examples. But what if there are multiple valid responses? What if some are helpful while others are *really* helpful? The model has no way to distinguish them.\n",
    "\n",
    "**Reward models** solve this.\n",
    "\n",
    "A reward model is a neural network that predicts human preferences. You give it a prompt and a response, and it outputs a number (a \"reward score\") that indicates how good that response is.\n",
    "\n",
    "The math:\n",
    "\n",
    "$$r_\\theta(x, y) \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the **prompt** (the user's question or instruction)\n",
    "- $y$ is the **response** (what the model generated)\n",
    "- $r_\\theta$ is the **reward model** (parameterized by weights $\\theta$)\n",
    "- $\\rightarrow \\mathbb{R}$ means it outputs a real number (the reward score)\n",
    "\n",
    "Higher score = better response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Why Do We Need Reward Models?\n",
    "\n",
    "Here's a thought experiment. Rate this essay on a scale of 1 to 10.\n",
    "\n",
    "Hard. Is it a 7? Maybe an 8? What's the difference between a 7 and an 8 anyway?\n",
    "\n",
    "Now: I give you two essays and ask which one is better.\n",
    "\n",
    "Much easier. You can just compare them directly.\n",
    "\n",
    "**Humans are better at comparisons than absolute ratings.** This insight is the foundation of reward modeling.\n",
    "\n",
    "After SFT, your model can follow instructions. But it doesn't know:\n",
    "\n",
    "- Which of two valid responses is **better**\n",
    "- How to balance competing objectives (should I be maximally helpful, or play it safe?)\n",
    "- What makes a response **exceptional** vs just acceptable\n",
    "\n",
    "We teach the model human preferences by showing it comparisons:\n",
    "\n",
    "- \"This response is better than that one\"\n",
    "- \"This response is better than that one\"  \n",
    "- \"This response is better than that one\"\n",
    "\n",
    "The reward model learns from these comparisons. Eventually it can predict. for any prompt and response. How much a human would like it.\n",
    "\n",
    "Then we use that reward model to further train the base model. But that's RLHF, which comes later.\n",
    "\n",
    "First: building the reward model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The Bradley-Terry Model\n",
    "\n",
    "We need a way to convert reward scores into preference probabilities. Enter the **Bradley-Terry model**.\n",
    "\n",
    "This is a classic model from the 1950s developed by Ralph Bradley and Milton Terry for ranking things when you only have pairwise comparisons. Think chess rankings, or comparing sports teams.\n",
    "\n",
    "The idea: if I show you two responses. call them \"winner\" and \"loser\" based on human preference. the probability that a human prefers the winner is:\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$$\n",
    "\n",
    "Breaking this down:\n",
    "\n",
    "- $P(...)$ means \"probability of...\"\n",
    "- $y_w$ is the **winning response** (the one humans preferred)\n",
    "- $y_l$ is the **losing response** (the one humans rejected)  \n",
    "- $y_w \\succ y_l$ reads as \"$y_w$ is preferred to $y_l$\"\n",
    "- $| x$ means \"given prompt $x$\"\n",
    "- $\\sigma$ is the **sigmoid function**\n",
    "- $r_\\theta(x, y_w)$ is the reward score for the winning response\n",
    "- $r_\\theta(x, y_l)$ is the reward score for the losing response\n",
    "\n",
    "So: \"The probability that the winner is preferred over the loser equals the sigmoid of the difference in their reward scores.\"\n",
    "\n",
    "Why sigmoid? Because we need to convert a difference (which could be any real number) into a probability (which must be between 0 and 1).\n",
    "\n",
    "How it works:\n",
    "- If the reward difference is **large and positive** (winner scored much higher), sigmoid outputs close to 1.0\n",
    "- If the reward difference is **zero** (both scored the same), sigmoid outputs 0.5 (50-50)\n",
    "- If the reward difference is **large and negative** (loser scored higher. the model got it wrong), sigmoid outputs close to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:07.120521Z",
     "iopub.status.busy": "2025-12-10T21:19:07.120429Z",
     "iopub.status.idle": "2025-12-10T21:19:08.044640Z",
     "shell.execute_reply": "2025-12-10T21:19:08.044303Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Visualize the Bradley-Terry model\nreward_diff = np.linspace(-5, 5, 100)\nprob_prefer_chosen = 1 / (1 + np.exp(-reward_diff))  # This is the sigmoid function\n\nplt.figure(figsize=(10, 6))\nplt.plot(reward_diff, prob_prefer_chosen, linewidth=2.5, color='#2E86AB')\nplt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50-50 preference')\nplt.axvline(x=0, color='gray', linestyle='--', alpha=0.5, label='Equal rewards')\nplt.xlabel('Reward Difference (r_chosen - r_rejected)', fontsize=12)\nplt.ylabel('Probability of Preferring Chosen Response', fontsize=12)\nplt.title('The Bradley-Terry Model: How Reward Differences → Preference Probabilities', fontsize=14, pad=20)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(\"Understanding the curve:\")\nprint(\"=\" * 60)\nprint(f\"When reward difference = 0:  P(prefer chosen) = {1/(1+np.exp(0)):.2f}\")\nprint(\"  → Both responses equally good, 50-50\")\nprint()\nprint(f\"When reward difference = +2: P(prefer chosen) = {1/(1+np.exp(-2)):.2f}\")  \nprint(\"  → Chosen response scored 2 points higher, 88% confident\")\nprint()\nprint(f\"When reward difference = -2: P(prefer chosen) = {1/(1+np.exp(2)):.2f}\")\nprint(\"  → Chosen response scored LOWER. Only 12% confident.\")\nprint(\"  → The reward model is making a mistake here.\")\nprint()\nprint(\"The sigmoid squashes any difference into a probability.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## Reward Model Architecture\n\nHow do we actually *build* this thing?\n\nA reward model is a language model with one addition: a **value head**.\n\nHere's the architecture:\n\n```\nInput: [prompt] [response]  ← Concatenate these together\n       ↓\n┌─────────────────────────┐\n│   Language Model        │  ← Start with a pre-trained model\n│   (GPT, LLaMA, etc.)    │     (often the same one you used for SFT)\n└───────────┬─────────────┘\n            │\n    Get the hidden state of the last token\n    (this vector \"summarizes\" the whole sequence)\n            │\n            ↓\n┌─────────────────────────┐\n│     Value Head          │  ← A simple linear layer\n│  (Linear → Scalar)      │     (this is the only new part)\n└───────────┬─────────────┘\n            │\n            ↓\n       Reward Score\n```\n\nThe language model reads the prompt + response and builds up a rich understanding of what's happening. Then the value head (usually a single linear layer) converts that understanding into a single number: the reward.\n\nYou can either:\n1. **Freeze the base model** (only train the value head): faster, but less expressive\n2. **Train everything**: slower, but the base model can learn to extract features specifically useful for predicting preferences\n\nMost people train everything."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:08.045684Z",
     "iopub.status.busy": "2025-12-10T21:19:08.045582Z",
     "iopub.status.idle": "2025-12-10T21:19:09.699602Z",
     "shell.execute_reply": "2025-12-10T21:19:09.699284Z"
    }
   },
   "outputs": [],
   "source": "from transformers import AutoModel, AutoTokenizer\n\nclass RewardModel(nn.Module):\n    \"\"\"\n    A reward model for predicting human preferences.\n    \n    Takes a prompt + response, outputs a scalar reward score.\n    \"\"\"\n    \n    def __init__(self, base_model, hidden_size, freeze_base=False):\n        super().__init__()\n        self.base_model = base_model\n        \n        # Optionally freeze the base model (only train the value head)\n        if freeze_base:\n            for param in self.base_model.parameters():\n                param.requires_grad = False\n        \n        # Value head: converts hidden state → scalar reward\n        # (Just a linear layer with dropout for regularization)\n        self.value_head = nn.Sequential(\n            nn.Dropout(0.1),  # Prevent overfitting\n            nn.Linear(hidden_size, 1)  # hidden_size → 1 number\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Compute reward for an input sequence.\n        \n        Args:\n            input_ids: Token IDs for [prompt] [response]\n            attention_mask: 1 for real tokens, 0 for padding\n            \n        Returns:\n            reward: Scalar score for this prompt-response pair\n        \"\"\"\n        # Step 1: Run the base model to get hidden states\n        outputs = self.base_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True\n        )\n        \n        # Step 2: Get the last hidden state\n        # Shape: (batch_size, sequence_length, hidden_size)\n        hidden_states = outputs.last_hidden_state\n        \n        # Step 3: Extract the hidden state at the LAST non-padding token\n        # Why the last token? It's \"seen\" the entire prompt + response,\n        # so it has all the context needed to judge quality.\n        \n        # Find the position of the last real token for each sequence\n        seq_lengths = attention_mask.sum(dim=1) - 1  # -1 for 0-indexing\n        \n        # Index into the hidden states to grab that last position\n        batch_size = hidden_states.shape[0]\n        last_hidden = hidden_states[\n            torch.arange(batch_size),\n            seq_lengths.long()\n        ]\n        \n        # Step 4: Pass through value head to get scalar reward\n        reward = self.value_head(last_hidden).squeeze(-1)\n        \n        return reward\n\n\n# Create a reward model\nprint(\"Building a reward model from GPT-2...\")\nprint()\n\nmodel_name = \"gpt2\"\nbase_model = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default\n\nreward_model = RewardModel(\n    base_model,\n    hidden_size=base_model.config.hidden_size,\n    freeze_base=False  # Train everything\n)\n\nprint(f\"Reward model created.\")\nprint()\nprint(f\"Base model parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\nprint(f\"Value head parameters: {sum(p.numel() for p in reward_model.value_head.parameters()):,}\")\nprint()\nprint(\"That value head is tiny: just 769 parameters.\")\nprint(\"(It's literally: 768-dimensional vector → 1 number)\")\nprint(\"But it's enough to learn human preferences when combined with the base model.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Testing the Reward Model\n\nLet's test our (untrained) reward model.\n\nWe'll give it two responses to \"What is 2+2?\":\n1. A correct answer\n2. An \"I don't know\" response\n\nBefore training, the rewards will be random. The model hasn't learned anything about preferences yet."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:09.700598Z",
     "iopub.status.busy": "2025-12-10T21:19:09.700448Z",
     "iopub.status.idle": "2025-12-10T21:19:09.743484Z",
     "shell.execute_reply": "2025-12-10T21:19:09.743171Z"
    }
   },
   "outputs": [],
   "source": "# Test forward pass with two responses\ntest_texts = [\n    \"What is 2+2? The answer is 4.\",\n    \"What is 2+2? I don't know.\"\n]\n\n# Tokenize both responses\ninputs = tokenizer(\n    test_texts,\n    padding=True,  # Pad to same length\n    return_tensors=\"pt\"  # Return PyTorch tensors\n)\n\n# Run through the reward model\nwith torch.no_grad():  # Don't compute gradients (we're just testing)\n    rewards = reward_model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"]\n    )\n\nprint(\"Reward scores (before training):\")\nprint(\"=\" * 60)\nfor text, reward in zip(test_texts, rewards):\n    print(f\"  '{text}' → {reward.item():.4f}\")\n\nprint()\nprint(\"The rewards are random.\")\nprint(\"The model has no idea that the first response is better.\")\nprint()\nprint(\"After training on preference data, we'd expect:\")\nprint(\"  - First response (correct answer): HIGH reward\")\nprint(\"  - Second response (unhelpful): LOW reward\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## The Training Objective\n\nWe've got the architecture. Now: how do we train it?\n\nWe have preference data: lots of examples where humans said \"Response A is better than Response B\" for some prompt.\n\nOur goal: teach the model to assign higher rewards to preferred responses.\n\nWe do this with the **ranking loss** (also called the \"preference loss\"):\n\n$$\\mathcal{L} = -\\mathbb{E}_{(x, y_w, y_l)} \\left[ \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right]$$\n\nBreaking it down:\n\n- $\\mathcal{L}$ is the **loss** we're minimizing\n- $\\mathbb{E}_{(...)}$ means \"expected value over...\" (in practice: average over all training examples)\n- $(x, y_w, y_l)$ is one training example: a prompt $x$, winning response $y_w$, and losing response $y_l$\n- $\\log$ is the natural logarithm\n- $\\sigma(...)$ is sigmoid\n- $r_\\theta(x, y_w) - r_\\theta(x, y_l)$ is the difference in rewards\n\nSo: \"The loss is the negative log probability that we assign the correct preference.\"\n\nWe negate it because we *minimize* loss. Maximizing log probability = minimizing negative log probability.\n\n**Intuitively:**\n- If $r_\\theta(x, y_w) > r_\\theta(x, y_l)$ (we correctly ranked the winner higher), the loss is LOW\n- If $r_\\theta(x, y_w) < r_\\theta(x, y_l)$ (we got it backwards), the loss is HIGH. The gradient will push $r(y_w)$ up and $r(y_l)$ down.\n- If $r_\\theta(x, y_w) \\approx r_\\theta(x, y_l)$ (we're not sure), the loss is medium. We'll adjust the rewards to be more confident.\n\nThe clean thing about this loss: it doesn't care about the *absolute* values of rewards, only the *differences*. The model can scale its rewards however it wants, as long as the rankings are correct."
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We've covered the theory. Now we make it real.\n",
    "\n",
    "In the following notebooks:\n",
    "\n",
    "1. **Preference Data**: Where does this data come from? What does it look like? How do we format it?\n",
    "2. **Training**: Complete implementation of the training loop. We'll train a reward model.\n",
    "3. **Evaluation**: How do you know if your reward model is good? (Accuracy alone isn't enough. we need to watch for reward hacking.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Introduces reward modeling for learning human preferences from comparison data."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}