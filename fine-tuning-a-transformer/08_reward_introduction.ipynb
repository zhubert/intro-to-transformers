{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Introduction to Reward Models\n\n**Teaching AI to know good from great**\n\n(Spoiler: it's all about preferences)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## What is a Reward Model?\n\nSo you've got your model fine-tuned. It can follow instructions. It's polite. It's helpful. Great!\n\nBut here's the thing: it doesn't know which of its responses are *actually good*.\n\nThink about it. After SFT (Supervised Fine-Tuning), your model learned to mimic the training examples. But what if there are multiple valid responses? What if some are helpful while others are *really* helpful? The model has no idea.\n\nThat's where **reward models** come in.\n\nA reward model is just a neural network that learned to predict human preferences. You give it a prompt and a response, and it spits out a number—a \"reward score\"—that says how good that response is.\n\nThe math looks like this:\n\n$$r_\\theta(x, y) \\rightarrow \\mathbb{R}$$\n\nLet me break down what these symbols mean:\n- $x$ is the **prompt** (the user's question or instruction)\n- $y$ is the **response** (what the model generated)\n- $r_\\theta$ is the **reward model** itself (that subscript $\\theta$ just means \"parameterized by theta\"—it's machine learning speak for \"the model has weights we can train\")\n- $\\rightarrow \\mathbb{R}$ means \"produces a real number\" (that fancy R is mathematician notation for \"any number on the number line\")\n\nSo the whole thing reads: \"The reward model takes a prompt and response, and outputs a scalar score.\"\n\nHigher score = better response. Simple as that."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Why Do We Need Reward Models?\n\nHere's a thought experiment. I ask you to rate this essay on a scale of 1 to 10.\n\nHard, right? Is it a 7? Maybe an 8? What's the difference between a 7 and an 8 anyway?\n\nBut now I give you two essays and ask: \"Which one is better?\"\n\nWay easier! You can just compare them directly.\n\nTurns out, **humans are much better at comparisons than absolute ratings**. And that insight is the whole foundation of reward modeling.\n\nAfter SFT, your model can follow instructions, sure. But it doesn't know:\n\n- Which of two valid responses is **better**\n- How to balance competing objectives (should I be maximally helpful, or play it safe?)\n- What makes a response **exceptional** vs just acceptable\n\nWe need to teach the model human preferences. And we do that by showing it tons of comparisons:\n\n- \"This response is better than that one\"\n- \"This response is better than that one\"  \n- \"This response is better than that one\"\n\nThe reward model learns from all those comparisons, and eventually it can predict—for any prompt and response—how much a human would like it.\n\nThen (and this is the clever part) we can use that reward model to further train the base model. But that's RLHF (Reinforcement Learning from Human Feedback), which we'll get to later.\n\nFirst, let's build the reward model itself."
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## The Bradley-Terry Model\n\nTime for some probability theory. (Don't worry, I'll explain everything.)\n\nWe need a way to convert reward scores into preference probabilities. Enter the **Bradley-Terry model**.\n\nThis is a classic model from the 1950s (yes, really!) developed by two statisticians—Ralph Bradley and Milton Terry—who were trying to figure out how to rank things when you only have pairwise comparisons. Think chess rankings, or which sports team is better.\n\nThe idea is beautifully simple. If I show you two responses—let's call them \"winner\" and \"loser\" based on human preference—the probability that a human prefers the winner is:\n\n$$P(y_w \\succ y_l | x) = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$$\n\nOkay, let's decode this formula piece by piece:\n\n- $P(...)$ means \"probability of...\"\n- $y_w$ is the **winning response** (the one humans preferred)\n- $y_l$ is the **losing response** (the one humans rejected)  \n- $y_w \\succ y_l$ reads as \"$y_w$ is preferred to $y_l$\" (that $\\succ$ symbol means \"is preferred to\" or \"is better than\")\n- $| x$ means \"given prompt $x$\" (that vertical bar means \"conditional on\" or \"given\")\n- $\\sigma$ is the **sigmoid function** (more on this in a second)\n- $r_\\theta(x, y_w)$ is the reward score for the winning response\n- $r_\\theta(x, y_l)$ is the reward score for the losing response\n\nSo the whole thing reads: \"The probability that the winner is preferred over the loser, given this prompt, equals the sigmoid of the difference in their reward scores.\"\n\nWhy sigmoid? Because we need to convert a difference (which could be any number from negative infinity to positive infinity) into a probability (which has to be between 0 and 1).\n\nThe sigmoid function does exactly that:\n- If the reward difference is **large and positive** (winner scored way higher), sigmoid outputs close to 1.0 (very confident in the preference)\n- If the reward difference is **zero** (both scored the same), sigmoid outputs 0.5 (50-50 coin flip)  \n- If the reward difference is **large and negative** (loser somehow scored higher—uh oh), sigmoid outputs close to 0.0 (very confident in the *opposite* preference)\n\nLet's visualize this."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Visualize the Bradley-Terry model\nreward_diff = np.linspace(-5, 5, 100)\nprob_prefer_chosen = 1 / (1 + np.exp(-reward_diff))  # This is the sigmoid function\n\nplt.figure(figsize=(10, 6))\nplt.plot(reward_diff, prob_prefer_chosen, linewidth=2.5, color='#2E86AB')\nplt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50-50 preference')\nplt.axvline(x=0, color='gray', linestyle='--', alpha=0.5, label='Equal rewards')\nplt.xlabel('Reward Difference (r_chosen - r_rejected)', fontsize=12)\nplt.ylabel('Probability of Preferring Chosen Response', fontsize=12)\nplt.title('The Bradley-Terry Model: How Reward Differences → Preference Probabilities', fontsize=14, pad=20)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(\"Understanding the curve:\")\nprint(\"=\" * 60)\nprint(f\"When reward difference = 0:  P(prefer chosen) = {1/(1+np.exp(0)):.2f}\")\nprint(\"  → Both responses equally good, 50-50 coin flip\")\nprint()\nprint(f\"When reward difference = +2: P(prefer chosen) = {1/(1+np.exp(-2)):.2f}\")  \nprint(\"  → Chosen response scored 2 points higher, 88% confident\")\nprint()\nprint(f\"When reward difference = -2: P(prefer chosen) = {1/(1+np.exp(2)):.2f}\")\nprint(\"  → Uh oh! Chosen response scored LOWER. Only 12% confident.\")\nprint(\"  → This means our reward model is making a mistake!\")\nprint()\nprint(\"The sigmoid squashes any difference into a nice probability.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## Reward Model Architecture\n\nAlright, so how do we actually *build* this thing?\n\nTurns out, a reward model is just a language model with one small addition: a **value head**.\n\nHere's the architecture:\n\n```\nInput: [prompt] [response]  ← Concatenate these together\n       ↓\n┌─────────────────────────┐\n│   Language Model        │  ← Start with a pre-trained model\n│   (GPT, LLaMA, etc.)    │     (often the same one you used for SFT)\n└───────────┬─────────────┘\n            │\n    Get the hidden state of the last token\n    (this vector \"summarizes\" the whole sequence)\n            │\n            ↓\n┌─────────────────────────┐\n│     Value Head          │  ← A simple linear layer\n│  (Linear → Scalar)      │     (this is the only new part!)\n└───────────┬─────────────┘\n            │\n            ↓\n       Reward Score\n```\n\nThe idea: the language model reads the prompt + response and builds up a rich understanding of what's happening. Then the value head (just a tiny neural network—usually a single linear layer) converts that understanding into a single number: the reward.\n\nYou can either:\n1. **Freeze the base model** (only train the value head)—faster, but less expressive\n2. **Train everything**—slower, but the base model can learn to extract features specifically useful for predicting preferences\n\nMost people train everything. You've already got the base model anyway, so why not use its full capacity?\n\nLet's implement this."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModel, AutoTokenizer\n\nclass RewardModel(nn.Module):\n    \"\"\"\n    A reward model for predicting human preferences.\n    \n    Takes a prompt + response, outputs a scalar reward score.\n    \"\"\"\n    \n    def __init__(self, base_model, hidden_size, freeze_base=False):\n        super().__init__()\n        self.base_model = base_model\n        \n        # Optionally freeze the base model (only train the value head)\n        if freeze_base:\n            for param in self.base_model.parameters():\n                param.requires_grad = False\n        \n        # Value head: converts hidden state → scalar reward\n        # (Just a linear layer with dropout for regularization)\n        self.value_head = nn.Sequential(\n            nn.Dropout(0.1),  # Prevent overfitting\n            nn.Linear(hidden_size, 1)  # hidden_size → 1 number\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Compute reward for an input sequence.\n        \n        Args:\n            input_ids: Token IDs for [prompt] [response]\n            attention_mask: 1 for real tokens, 0 for padding\n            \n        Returns:\n            reward: Scalar score for this prompt-response pair\n        \"\"\"\n        # Step 1: Run the base model to get hidden states\n        outputs = self.base_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True\n        )\n        \n        # Step 2: Get the last hidden state\n        # Shape: (batch_size, sequence_length, hidden_size)\n        hidden_states = outputs.last_hidden_state\n        \n        # Step 3: Extract the hidden state at the LAST non-padding token\n        # Why the last token? It's \"seen\" the entire prompt + response,\n        # so it has all the context needed to judge quality.\n        \n        # Find the position of the last real token for each sequence\n        seq_lengths = attention_mask.sum(dim=1) - 1  # -1 for 0-indexing\n        \n        # Index into the hidden states to grab that last position\n        batch_size = hidden_states.shape[0]\n        last_hidden = hidden_states[\n            torch.arange(batch_size),\n            seq_lengths.long()\n        ]\n        \n        # Step 4: Pass through value head to get scalar reward\n        reward = self.value_head(last_hidden).squeeze(-1)\n        \n        return reward\n\n\n# Let's create a reward model!\nprint(\"Building a reward model from GPT-2...\")\nprint()\n\nmodel_name = \"gpt2\"\nbase_model = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default\n\nreward_model = RewardModel(\n    base_model,\n    hidden_size=base_model.config.hidden_size,\n    freeze_base=False  # Train everything!\n)\n\nprint(f\"✓ Reward model created!\")\nprint()\nprint(f\"Base model parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\nprint(f\"Value head parameters: {sum(p.numel() for p in reward_model.value_head.parameters()):,}\")\nprint()\nprint(\"That value head is tiny! Just 769 parameters.\")\nprint(\"(It's literally just: 768-dimensional vector → 1 number)\")\nprint(\"But it's enough to learn human preferences when combined with the base model.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Testing the Reward Model\n\nLet's take our brand new (untrained) reward model for a spin.\n\nWe'll give it two responses to the question \"What is 2+2?\":\n1. A correct answer\n2. An \"I don't know\" response\n\nBefore training, the rewards should be basically random. The model hasn't learned anything about preferences yet!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Test forward pass with two responses\ntest_texts = [\n    \"What is 2+2? The answer is 4.\",\n    \"What is 2+2? I don't know.\"\n]\n\n# Tokenize both responses\ninputs = tokenizer(\n    test_texts,\n    padding=True,  # Pad to same length\n    return_tensors=\"pt\"  # Return PyTorch tensors\n)\n\n# Run through the reward model\nwith torch.no_grad():  # Don't compute gradients (we're just testing)\n    rewards = reward_model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"]\n    )\n\nprint(\"Reward scores (before training):\")\nprint(\"=\" * 60)\nfor text, reward in zip(test_texts, rewards):\n    print(f\"  '{text}' → {reward.item():.4f}\")\n\nprint()\nprint(\"Notice: the rewards are essentially random!\")\nprint(\"The model has no idea that the first response is better.\")\nprint()\nprint(\"After training on preference data, we'd expect:\")\nprint(\"  - First response (correct answer): HIGH reward\")\nprint(\"  - Second response (unhelpful): LOW reward\")\nprint()\nprint(\"Let's learn how to train this thing...\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## The Training Objective\n\nAlright, we've got our model architecture. Now we need to train it.\n\nRemember, we have preference data: lots of examples where humans said \"Response A is better than Response B\" for some prompt.\n\nOur goal: teach the model to assign higher rewards to preferred responses.\n\nWe do this with the **ranking loss** (also called the \"preference loss\"):\n\n$$\\mathcal{L} = -\\mathbb{E}_{(x, y_w, y_l)} \\left[ \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right]$$\n\nOkay, this looks scary. Let me break it down:\n\n- $\\mathcal{L}$ is the **loss** we're trying to minimize (the \"badness\" of our model's predictions)\n- $\\mathbb{E}_{(...)}$ means \"expected value over...\" (in practice, this means \"average over all examples in your dataset\")\n- $(x, y_w, y_l)$ is one training example: a prompt $x$, winning response $y_w$, and losing response $y_l$\n- $\\log$ is the natural logarithm\n- $\\sigma(...)$ is sigmoid (remember from Bradley-Terry!)\n- $r_\\theta(x, y_w) - r_\\theta(x, y_l)$ is the difference in rewards\n\nSo the whole thing reads: \"The loss is the negative log probability that we assign the correct preference.\"\n\nWhy negative? Because we're *minimizing* the loss. Maximizing log probability = minimizing negative log probability. (Classic machine learning trick.)\n\n**Intuitively:**\n- If $r_\\theta(x, y_w) > r_\\theta(x, y_l)$ (we correctly ranked the winner higher), the loss is LOW. Good!\n- If $r_\\theta(x, y_w) < r_\\theta(x, y_l)$ (we got it backwards—uh oh), the loss is HIGH. The gradient will push $r(y_w)$ up and $r(y_l)$ down.\n- If $r_\\theta(x, y_w) \\approx r_\\theta(x, y_l)$ (we're not sure), the loss is medium. We'll adjust the rewards to be more confident.\n\nThe beautiful thing about this loss: it doesn't care about the *absolute* values of rewards, only the *differences*. That means the model can scale its rewards however it wants, as long as the rankings are correct.\n\n(This turns out to be important later when we use the reward model for RLHF—but one thing at a time!)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## Next Steps\n\nWe've covered the theory. Now let's make it real.\n\nIn the following notebooks, we'll cover:\n\n1. **Preference Data** — Where does this data come from? What does it look like? How do we format it?\n2. **Training** — Complete implementation of the training loop. We'll actually train a reward model!\n3. **Evaluation** — How do you know if your reward model is good? (Spoiler: accuracy isn't enough. We need to watch for \"reward hacking.\")\n\nLet's dive in."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}