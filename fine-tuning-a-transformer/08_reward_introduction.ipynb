{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Introduction to Reward Models\n",
    "\n",
    "**Learning to predict human preferences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is a Reward Model?\n",
    "\n",
    "A **reward model** is a neural network that learns to predict which responses humans prefer. Given a prompt and response, it outputs a scalar score indicating quality.\n",
    "\n",
    "$$r_\\theta(x, y) \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "where:\n",
    "- $x$ is the prompt\n",
    "- $y$ is the response\n",
    "- $r_\\theta$ is the reward model\n",
    "- Output is a scalar reward score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Why Do We Need Reward Models?\n",
    "\n",
    "After SFT, models can follow instructions, but they don't know:\n",
    "\n",
    "- Which of two valid responses is **better**\n",
    "- How to balance competing objectives (helpful vs safe)\n",
    "- What makes a response **high quality** vs just acceptable\n",
    "\n",
    "**Key insight:** Humans are much better at **comparing** responses than **rating** them.\n",
    "\n",
    "- \"Which is better, A or B?\" → Easy for humans\n",
    "- \"Rate this response 1-10\" → Hard and inconsistent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The Bradley-Terry Model\n",
    "\n",
    "Reward models use the **Bradley-Terry** model for pairwise comparisons:\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$$\n",
    "\n",
    "where:\n",
    "- $y_w$ is the preferred (winning) response\n",
    "- $y_l$ is the rejected (losing) response\n",
    "- $\\sigma$ is the sigmoid function\n",
    "\n",
    "The probability of preferring $y_w$ over $y_l$ depends on the **difference** in their rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualize the Bradley-Terry model\n",
    "reward_diff = np.linspace(-5, 5, 100)\n",
    "prob_prefer_chosen = 1 / (1 + np.exp(-reward_diff))  # Sigmoid\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_diff, prob_prefer_chosen, linewidth=2)\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Reward Difference (chosen - rejected)', fontsize=12)\n",
    "plt.ylabel('P(prefer chosen)', fontsize=12)\n",
    "plt.title('Bradley-Terry Model: Preference Probability', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"When reward_diff = 0: P(prefer chosen) = 0.5 (equal preference)\")\n",
    "print(\"When reward_diff = 2: P(prefer chosen) = 0.88\")\n",
    "print(\"When reward_diff = -2: P(prefer chosen) = 0.12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Reward Model Architecture\n",
    "\n",
    "A reward model is typically a language model with a **value head**:\n",
    "\n",
    "```\n",
    "Input: [prompt] [response]\n",
    "       ↓\n",
    "┌─────────────────────────┐\n",
    "│   Language Model        │  ← Pre-trained (often from SFT)\n",
    "│   (frozen or trainable) │\n",
    "└───────────┬─────────────┘\n",
    "            │\n",
    "    Hidden state of last token\n",
    "            │\n",
    "            ↓\n",
    "┌─────────────────────────┐\n",
    "│     Value Head          │  ← Trainable\n",
    "│  (Linear → Scalar)      │\n",
    "└───────────┬─────────────┘\n",
    "            │\n",
    "            ↓\n",
    "       Reward Score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward model for predicting human preferences.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, hidden_size, freeze_base=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        # Optionally freeze base model\n",
    "        if freeze_base:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Value head: projects hidden state to scalar\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Compute reward for input sequence.\n",
    "        \n",
    "        Returns reward at the last non-padding token.\n",
    "        \"\"\"\n",
    "        # Get hidden states from base model\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Get last hidden state\n",
    "        hidden_states = outputs.last_hidden_state  # (batch, seq, hidden)\n",
    "        \n",
    "        # Find position of last real token (before padding)\n",
    "        # Sum attention mask to get sequence lengths\n",
    "        seq_lengths = attention_mask.sum(dim=1) - 1  # -1 for 0-indexing\n",
    "        \n",
    "        # Extract hidden state at last position\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        last_hidden = hidden_states[\n",
    "            torch.arange(batch_size),\n",
    "            seq_lengths.long()\n",
    "        ]\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = self.value_head(last_hidden).squeeze(-1)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "# Example\n",
    "model_name = \"gpt2\"\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "reward_model = RewardModel(\n",
    "    base_model,\n",
    "    hidden_size=base_model.config.hidden_size,\n",
    "    freeze_base=False\n",
    ")\n",
    "\n",
    "print(f\"Reward model created!\")\n",
    "print(f\"Base model parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
    "print(f\"Value head parameters: {sum(p.numel() for p in reward_model.value_head.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Testing the Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_texts = [\n",
    "    \"What is 2+2? The answer is 4.\",\n",
    "    \"What is 2+2? I don't know.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    test_texts,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    rewards = reward_model(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "print(\"Reward scores (before training):\")\n",
    "for text, reward in zip(test_texts, rewards):\n",
    "    print(f\"  '{text[:40]}...' → {reward.item():.4f}\")\n",
    "\n",
    "print(\"\\nNote: Before training, rewards are essentially random!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## The Training Objective\n",
    "\n",
    "Train the reward model to predict human preferences using the **ranking loss**:\n",
    "\n",
    "$$\\mathcal{L} = -\\mathbb{E}_{(x, y_w, y_l)} \\left[ \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right]$$\n",
    "\n",
    "This loss:\n",
    "- Is minimized when $r(y_w) > r(y_l)$ (correct ranking)\n",
    "- Pushes chosen reward higher and rejected reward lower\n",
    "- Uses log-sigmoid for numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the following notebooks, we'll cover:\n",
    "\n",
    "1. **Preference Data** — Format, sources, and quality considerations\n",
    "2. **Training** — Complete training loop implementation\n",
    "3. **Evaluation** — Metrics and debugging reward hacking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
