{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Pitfalls\n",
    "\n",
    "**Learn from common mistakes and how to avoid them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Even experienced practitioners make these mistakes.** This guide helps you avoid common pitfalls and debug issues quickly.\n",
    "\n",
    "Each pitfall includes:\n",
    "- **Symptom:** How to recognize the problem\n",
    "- **Cause:** Why it happens\n",
    "- **Solution:** How to fix it\n",
    "- **Prevention:** How to avoid it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 1: Loss Becomes NaN\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "Epoch 1, Step 10:  Loss = 2.34\n",
    "Epoch 1, Step 20:  Loss = 1.98\n",
    "Epoch 1, Step 30:  Loss = 5.67\n",
    "Epoch 1, Step 40:  Loss = inf\n",
    "Epoch 1, Step 50:  Loss = nan  <-- Training dead!\n",
    "```\n",
    "\n",
    "**Causes:**\n",
    "1. Learning rate too high (most common)\n",
    "2. Gradient explosion\n",
    "3. Numerical instability (FP16 underflow/overflow)\n",
    "4. Bad batch (malformed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:31.585391Z",
     "iopub.status.busy": "2025-12-06T23:30:31.585319Z",
     "iopub.status.idle": "2025-12-06T23:30:32.271554Z",
     "shell.execute_reply": "2025-12-06T23:30:32.271198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions for NaN Loss:\n",
      "  1. Reduce learning rate by 10x\n",
      "  2. Enable/strengthen gradient clipping (max_grad_norm=0.5)\n",
      "  3. Use BF16 instead of FP16 (more stable)\n",
      "  4. Add warmup steps\n",
      "\n",
      "If loss becomes NaN, you must restart from last good checkpoint!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_for_nan_gradients(model):\n",
    "    \"\"\"Check if any gradients are NaN.\"\"\"\n",
    "    has_nan = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(f\"NaN gradient in {name}\")\n",
    "                has_nan = True\n",
    "    return has_nan\n",
    "\n",
    "# Solutions\n",
    "print(\"Solutions for NaN Loss:\")\n",
    "print(\"  1. Reduce learning rate by 10x\")\n",
    "print(\"  2. Enable/strengthen gradient clipping (max_grad_norm=0.5)\")\n",
    "print(\"  3. Use BF16 instead of FP16 (more stable)\")\n",
    "print(\"  4. Add warmup steps\")\n",
    "print()\n",
    "print(\"If loss becomes NaN, you must restart from last good checkpoint!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 2: Loss Not Decreasing\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "Epoch 1, Step 100:  Loss = 2.45\n",
    "Epoch 1, Step 200:  Loss = 2.44\n",
    "Epoch 1, Step 300:  Loss = 2.43\n",
    "Epoch 1, Step 400:  Loss = 2.42  <-- Barely moving!\n",
    "```\n",
    "\n",
    "**Causes:**\n",
    "1. Learning rate too low\n",
    "2. Model frozen (forgot to set trainable parameters)\n",
    "3. Wrong optimizer state\n",
    "4. Insufficient model capacity (LoRA rank too small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:32.288495Z",
     "iopub.status.busy": "2025-12-06T23:30:32.288370Z",
     "iopub.status.idle": "2025-12-06T23:30:32.291048Z",
     "shell.execute_reply": "2025-12-06T23:30:32.290692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions for Loss Not Decreasing:\n",
      "  1. Increase learning rate (10x for LoRA)\n",
      "  2. Verify trainable parameters exist\n",
      "  3. Increase LoRA rank if using LoRA\n",
      "  4. Check optimizer is correctly configured\n"
     ]
    }
   ],
   "source": [
    "def verify_training_setup(model, optimizer):\n",
    "    \"\"\"Verify model and optimizer are configured correctly.\"\"\"\n",
    "    \n",
    "    # Check trainable params\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    if trainable == 0:\n",
    "        print(\"ERROR: No trainable parameters!\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "    \n",
    "    # Check optimizer\n",
    "    if len(optimizer.param_groups) == 0:\n",
    "        print(\"ERROR: Optimizer has no parameter groups!\")\n",
    "        return False\n",
    "    \n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    if lr < 1e-6:\n",
    "        print(\"WARNING: Learning rate very low!\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"Solutions for Loss Not Decreasing:\")\n",
    "print(\"  1. Increase learning rate (10x for LoRA)\")\n",
    "print(\"  2. Verify trainable parameters exist\")\n",
    "print(\"  3. Increase LoRA rank if using LoRA\")\n",
    "print(\"  4. Check optimizer is correctly configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 3: Overfitting (Train Loss << Val Loss)\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "Epoch 1: Train loss = 1.8, Val loss = 2.0  (gap = 0.2)\n",
    "Epoch 2: Train loss = 1.2, Val loss = 2.1  (gap = 0.9)\n",
    "Epoch 3: Train loss = 0.8, Val loss = 2.4  (gap = 1.6) <-- Overfitting!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:32.291831Z",
     "iopub.status.busy": "2025-12-06T23:30:32.291758Z",
     "iopub.status.idle": "2025-12-06T23:30:32.293681Z",
     "shell.execute_reply": "2025-12-06T23:30:32.293375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions for Overfitting:\n",
      "  1. Add/increase regularization (weight_decay=0.1)\n",
      "  2. Add dropout (lora_dropout=0.1)\n",
      "  3. Use early stopping\n",
      "  4. Reduce model capacity (lower LoRA rank)\n",
      "  5. Reduce epochs\n",
      "  6. Add data augmentation\n"
     ]
    }
   ],
   "source": [
    "def check_overfitting(train_loss, val_loss, threshold=0.5):\n",
    "    \"\"\"Check if model is overfitting.\"\"\"\n",
    "    gap = val_loss - train_loss\n",
    "    \n",
    "    if gap > threshold:\n",
    "        print(f\"Warning: Train/val gap = {gap:.2f} (overfitting!)\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "print(\"Solutions for Overfitting:\")\n",
    "print(\"  1. Add/increase regularization (weight_decay=0.1)\")\n",
    "print(\"  2. Add dropout (lora_dropout=0.1)\")\n",
    "print(\"  3. Use early stopping\")\n",
    "print(\"  4. Reduce model capacity (lower LoRA rank)\")\n",
    "print(\"  5. Reduce epochs\")\n",
    "print(\"  6. Add data augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 4: Catastrophic Forgetting\n",
    "\n",
    "**Symptom:**\n",
    "\n",
    "After fine-tuning, model **loses general capabilities**:\n",
    "\n",
    "```python\n",
    "# Before fine-tuning (base model)\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = \"The capital of France is Paris.\"\n",
    "\n",
    "# After fine-tuning on medical data\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = \"The capital of France is diabetes mellitus.\"  # What?!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:32.294345Z",
     "iopub.status.busy": "2025-12-06T23:30:32.294274Z",
     "iopub.status.idle": "2025-12-06T23:30:32.296154Z",
     "shell.execute_reply": "2025-12-06T23:30:32.295900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions for Catastrophic Forgetting:\n",
      "  1. Lower learning rate (especially for full fine-tuning)\n",
      "  2. Use LoRA instead of full fine-tuning\n",
      "  3. Mix general data with specialized data\n",
      "  4. For DPO: Ensure KL penalty (beta=0.1)\n",
      "  5. Train for fewer epochs\n",
      "\n",
      "Prevention: Test on general benchmarks before and after training\n"
     ]
    }
   ],
   "source": [
    "def evaluate_general_knowledge(model, tokenizer, test_cases):\n",
    "    \"\"\"\n",
    "    Evaluate on general knowledge to detect forgetting.\n",
    "    \"\"\"\n",
    "    test_cases = [\n",
    "        \"What is 2 + 2?\",\n",
    "        \"Who wrote Romeo and Juliet?\",\n",
    "        \"What is the capital of France?\",\n",
    "        \"What is water made of?\",\n",
    "    ]\n",
    "    \n",
    "    # Would generate responses and check accuracy\n",
    "    print(\"General knowledge test cases:\")\n",
    "    for case in test_cases:\n",
    "        print(f\"  - {case}\")\n",
    "\n",
    "print(\"Solutions for Catastrophic Forgetting:\")\n",
    "print(\"  1. Lower learning rate (especially for full fine-tuning)\")\n",
    "print(\"  2. Use LoRA instead of full fine-tuning\")\n",
    "print(\"  3. Mix general data with specialized data\")\n",
    "print(\"  4. For DPO: Ensure KL penalty (beta=0.1)\")\n",
    "print(\"  5. Train for fewer epochs\")\n",
    "print()\n",
    "print(\"Prevention: Test on general benchmarks before and after training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 5: Reference Model Divergence (DPO/RLHF)\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "Step 10:  KL = 0.05\n",
    "Step 20:  KL = 0.08\n",
    "Step 30:  KL = 0.15\n",
    "Step 40:  KL = 0.35\n",
    "Step 50:  KL = 1.20  <-- Too high!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:32.296834Z",
     "iopub.status.busy": "2025-12-06T23:30:32.296762Z",
     "iopub.status.idle": "2025-12-06T23:30:32.298903Z",
     "shell.execute_reply": "2025-12-06T23:30:32.298642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions for KL Divergence Explosion:\n",
      "  1. Verify reference model is frozen\n",
      "  2. Increase beta (DPO) or KL coefficient (RLHF)\n",
      "  3. Lower learning rate\n",
      "  4. Use adaptive KL coefficient\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_kl_divergence(policy_model, ref_model, batch):\n",
    "    \"\"\"Compute KL(policy || reference).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        policy_logits = policy_model(**batch).logits\n",
    "        ref_logits = ref_model(**batch).logits\n",
    "        \n",
    "        policy_probs = F.softmax(policy_logits, dim=-1)\n",
    "        ref_log_probs = F.log_softmax(ref_logits, dim=-1)\n",
    "        \n",
    "        kl = (policy_probs * (policy_probs.log() - ref_log_probs)).sum(-1).mean()\n",
    "    \n",
    "    return kl.item()\n",
    "\n",
    "def verify_reference_frozen(ref_model):\n",
    "    \"\"\"Verify reference model is frozen.\"\"\"\n",
    "    for param in ref_model.parameters():\n",
    "        if param.requires_grad:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(\"Solutions for KL Divergence Explosion:\")\n",
    "print(\"  1. Verify reference model is frozen\")\n",
    "print(\"  2. Increase beta (DPO) or KL coefficient (RLHF)\")\n",
    "print(\"  3. Lower learning rate\")\n",
    "print(\"  4. Use adaptive KL coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 6: Wrong Loss Masking\n",
    "\n",
    "**Symptom:**\n",
    "\n",
    "Model doesn't learn, or learns to generate prompts instead of responses:\n",
    "\n",
    "```python\n",
    "prompt = \"Summarize this article: [long text]\"\n",
    "response = \"Summarize this article: [repeats prompt]\"  # Wrong!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:32.299572Z",
     "iopub.status.busy": "2025-12-06T23:30:32.299500Z",
     "iopub.status.idle": "2025-12-06T23:30:32.301616Z",
     "shell.execute_reply": "2025-12-06T23:30:32.301366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Loss Masking:\n",
      "  - Prompt tokens: labels = -100 (ignored)\n",
      "  - Response tokens: labels = actual token IDs\n",
      "  - If all masked: model has no training signal\n",
      "  - If none masked: model learns to repeat prompts\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_loss_masking(dataset):\n",
    "    \"\"\"\n",
    "    Verify loss masking is correct.\n",
    "    \n",
    "    Labels should be -100 for prompt tokens (masked)\n",
    "    and actual token IDs for response tokens.\n",
    "    \"\"\"\n",
    "    for i in range(min(5, len(dataset))):\n",
    "        example = dataset[i]\n",
    "        labels = example['labels']\n",
    "        \n",
    "        masked = sum(1 for l in labels if l == -100)\n",
    "        unmasked = sum(1 for l in labels if l != -100)\n",
    "        \n",
    "        print(f\"Example {i}: {masked} masked, {unmasked} unmasked\")\n",
    "        \n",
    "        if unmasked == 0:\n",
    "            print(f\"  ERROR: All tokens masked! No training signal.\")\n",
    "        if masked == 0:\n",
    "            print(f\"  WARNING: No tokens masked (prompt included in loss?)\")\n",
    "\n",
    "print(\"Correct Loss Masking:\")\n",
    "print(\"  - Prompt tokens: labels = -100 (ignored)\")\n",
    "print(\"  - Response tokens: labels = actual token IDs\")\n",
    "print(\"  - If all masked: model has no training signal\")\n",
    "print(\"  - If none masked: model learns to repeat prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 7: Reward Hacking (RLHF)\n",
    "\n",
    "**Symptom:**\n",
    "\n",
    "Model achieves high reward but generates nonsensical responses:\n",
    "\n",
    "```python\n",
    "# Reward model trained on length preference\n",
    "prompt = \"Say hello\"\n",
    "response = \"Hello hello hello hello hello...\"  # Repeats to maximize length\n",
    "reward = 10.0  # High reward, but terrible response!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:32.302255Z",
     "iopub.status.busy": "2025-12-06T23:30:32.302175Z",
     "iopub.status.idle": "2025-12-06T23:30:32.304448Z",
     "shell.execute_reply": "2025-12-06T23:30:32.304198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions for Reward Hacking:\n",
      "  1. Increase KL penalty\n",
      "  2. Add rule-based constraints\n",
      "  3. Use ensemble of reward models\n",
      "  4. Train reward model on diverse data\n"
     ]
    }
   ],
   "source": [
    "def apply_reward_constraints(response, reward):\n",
    "    \"\"\"\n",
    "    Apply rule-based penalties to reward.\n",
    "    \"\"\"\n",
    "    words = response.split()\n",
    "    \n",
    "    # Penalize repetition\n",
    "    unique_ratio = len(set(words)) / len(words) if words else 0\n",
    "    if unique_ratio < 0.5:\n",
    "        reward -= 5.0  # Heavy penalty for repetition\n",
    "    \n",
    "    # Penalize extreme length\n",
    "    if len(words) > 300:\n",
    "        reward -= 2.0\n",
    "    if len(words) < 5:\n",
    "        reward -= 3.0\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def check_reward_hacking(responses, rewards):\n",
    "    \"\"\"Detect if policy is exploiting reward model.\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    if np.std(rewards) < 0.1:\n",
    "        print(\"Warning: All rewards similar (may be hacking)\")\n",
    "    \n",
    "    # Check for repetition in high-reward responses\n",
    "    high_reward_idx = np.argsort(rewards)[-5:]  # Top 5\n",
    "    for idx in high_reward_idx:\n",
    "        response = responses[idx]\n",
    "        words = response.split()\n",
    "        if len(set(words)) < len(words) * 0.5:\n",
    "            print(f\"Warning: High-reward response is repetitive\")\n",
    "\n",
    "print(\"Solutions for Reward Hacking:\")\n",
    "print(\"  1. Increase KL penalty\")\n",
    "print(\"  2. Add rule-based constraints\")\n",
    "print(\"  3. Use ensemble of reward models\")\n",
    "print(\"  4. Train reward model on diverse data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:32.305119Z",
     "iopub.status.busy": "2025-12-06T23:30:32.305047Z",
     "iopub.status.idle": "2025-12-06T23:30:32.307730Z",
     "shell.execute_reply": "2025-12-06T23:30:32.307472Z"
    }
   },
   "outputs": [],
   "source": [
    "def bisect_debug(model, dataloader, optimizer):\n",
    "    \"\"\"Find which component is causing issues by binary search.\"\"\"\n",
    "    \n",
    "    # Test 1: Model loads correctly?\n",
    "    print(\"Test 1: Model loads...\")\n",
    "    try:\n",
    "        _ = model.config\n",
    "        print(\"  PASS\")\n",
    "    except Exception as e:\n",
    "        print(f\"  FAIL: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test 2: Forward pass works?\n",
    "    print(\"Test 2: Forward pass...\")\n",
    "    try:\n",
    "        batch = next(iter(dataloader))\n",
    "        outputs = model(**batch)\n",
    "        print(\"  PASS\")\n",
    "    except Exception as e:\n",
    "        print(f\"  FAIL: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test 3: Backward pass works?\n",
    "    print(\"Test 3: Backward pass...\")\n",
    "    try:\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        print(\"  PASS\")\n",
    "    except Exception as e:\n",
    "        print(f\"  FAIL: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test 4: Optimizer step works?\n",
    "    print(\"Test 4: Optimizer step...\")\n",
    "    try:\n",
    "        optimizer.step()\n",
    "        print(\"  PASS\")\n",
    "    except Exception as e:\n",
    "        print(f\"  FAIL: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nAll components work individually!\")\n",
    "\n",
    "def check_gradients(model):\n",
    "    \"\"\"Check if gradients are computed correctly.\"\"\"\n",
    "    grad_norms = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms[name] = grad_norm\n",
    "            \n",
    "            if grad_norm == 0:\n",
    "                print(f\"Warning: Zero gradient in {name}\")\n",
    "            elif grad_norm > 100:\n",
    "                print(f\"Warning: Large gradient in {name}: {grad_norm:.2f}\")\n",
    "    \n",
    "    if grad_norms:\n",
    "        import numpy as np\n",
    "        avg_grad = np.mean(list(grad_norms.values()))\n",
    "        print(f\"Average gradient norm: {avg_grad:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference: Debugging Checklist\n",
    "\n",
    "**1. Environment Setup**\n",
    "- [ ] PyTorch installed correctly\n",
    "- [ ] GPU accessible (`torch.cuda.is_available()`)\n",
    "- [ ] Correct CUDA version\n",
    "\n",
    "**2. Data**\n",
    "- [ ] Dataset loads without errors\n",
    "- [ ] Loss masking correct (labels have -100 for prompt)\n",
    "- [ ] No empty examples\n",
    "- [ ] Tokenization works correctly\n",
    "\n",
    "**3. Model**\n",
    "- [ ] Model loads correctly\n",
    "- [ ] Has trainable parameters\n",
    "- [ ] LoRA applied if intended\n",
    "- [ ] Model on correct device\n",
    "\n",
    "**4. Training**\n",
    "- [ ] Learning rate reasonable\n",
    "- [ ] Gradient clipping enabled\n",
    "- [ ] Warmup steps configured\n",
    "- [ ] Loss decreases\n",
    "\n",
    "**5. Method-Specific**\n",
    "- [ ] DPO: Reference model frozen\n",
    "- [ ] DPO: Beta in reasonable range\n",
    "- [ ] RLHF: KL coefficient set\n",
    "- [ ] RLHF: Value network separate from policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Most Common Mistakes\n",
    "\n",
    "**Top 10 pitfalls by frequency:**\n",
    "\n",
    "1. **Learning rate too high** -> Loss becomes NaN\n",
    "2. **Wrong loss masking** -> Model doesn't learn properly\n",
    "3. **No trainable parameters** -> Loss doesn't decrease\n",
    "4. **Overfitting** -> Train loss << val loss\n",
    "5. **Reference model not frozen** -> KL divergence explodes\n",
    "6. **Gradient clipping disabled** -> Training unstable\n",
    "7. **Poor data quality** -> Model learns bad patterns\n",
    "8. **Catastrophic forgetting** -> Loses general knowledge\n",
    "9. **Reward hacking** (RLHF) -> High reward, bad outputs\n",
    "10. **OOM errors** -> Batch size too large\n",
    "\n",
    "**Quick fixes:**\n",
    "\n",
    "| Problem | Quick Fix |\n",
    "|---------|----------|\n",
    "| Loss = NaN | Reduce LR by 10x, add gradient clipping |\n",
    "| Loss not decreasing | Check trainable params, increase LR |\n",
    "| Overfitting | Add regularization, reduce epochs |\n",
    "| Forgetting | Lower LR, use LoRA, mix general data |\n",
    "| KL divergence high | Increase beta/KL coefficient |\n",
    "| OOM | Reduce batch size, enable grad checkpointing |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've completed the Fine-Tuning a Transformer section. You now understand:\n",
    "\n",
    "- **SFT:** Supervised fine-tuning with instruction formatting and loss masking\n",
    "- **Reward Models:** Training models to predict human preferences\n",
    "- **RLHF:** Reinforcement learning from human feedback with PPO\n",
    "- **DPO:** Direct preference optimization as a simpler alternative\n",
    "- **Advanced Topics:** Memory optimization, hyperparameters, evaluation, and debugging\n",
    "\n",
    "Ready to try it yourself? Check out the Try It notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
