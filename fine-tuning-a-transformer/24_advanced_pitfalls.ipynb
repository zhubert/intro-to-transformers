{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# When Everything Goes Wrong\n\n**A field guide to debugging disasters**\n\nYou're going to break things. We all do.\n\nThe question is: can you fix them quickly, or will you spend three days hunting a bug that turns out to be a single misplaced -100?\n\n(Spoiler: I've done the latter. Multiple times. Let's save you from that.)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Pattern\n\nHere's what always happens:\n\nYou start training. Everything looks fine. Loss is going down. You grab coffee.\n\nYou come back. Loss is `nan`. Or infinity. Or stuck at 2.45 for 800 steps. Or worse — it's going down beautifully, but your model now thinks Paris is the capital of diabetes.\n\nThis notebook is your debugging playbook. Each pitfall follows a pattern:\n\n**The Story:** What went wrong (because context matters)  \n**The Symptoms:** How to recognize it's happening  \n**The Cause:** Why it actually happens  \n**The Fix:** What to do about it\n\nReady? Let's break some models.\n\n(And then fix them.)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pitfall 1: The NaN Death Spiral\n\n**The Story:**\n\nIt's 2am. You've been training for three hours. Loss started at 2.5, dropped to 1.2, everything's beautiful.\n\nThen:\n```\nStep 1840: Loss = 1.18\nStep 1841: Loss = 1.15\nStep 1842: Loss = 3.47   <- uh oh\nStep 1843: Loss = inf    <- UH OH\nStep 1844: Loss = nan    <- dead\nStep 1845: Loss = nan    <- still dead\n```\n\nYour model is toast. Can't recover. Have to restart from the last checkpoint.\n\n(If you saved checkpoints. You did save checkpoints, right?)\n\n**What Happened:**\n\nSomething caused a gradient to explode. Maybe one batch had some weird tokens. Maybe the learning rate was too aggressive. Maybe you're using FP16 and hit numerical limits.\n\nDoesn't matter. Once you get a NaN gradient, it infects everything it touches. Like a zombie virus for tensors.\n\n**How to Spot It:**\n\nThe pattern is always the same: loss starts normal, maybe even improving, then suddenly jumps to infinity, then NaN. Sometimes you get warning signs (loss spiking but recovering), sometimes it just dies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:51:40.098473Z",
     "iopub.status.busy": "2025-12-07T18:51:40.098385Z",
     "iopub.status.idle": "2025-12-07T18:51:40.830974Z",
     "shell.execute_reply": "2025-12-07T18:51:40.830615Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\n\n# Here's how to check for NaN gradients before they kill your training\ndef check_for_nan_gradients(model):\n    \"\"\"Find which parameters have NaN gradients (if any).\"\"\"\n    has_nan = False\n    nan_params = []\n    \n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            if torch.isnan(param.grad).any():\n                nan_params.append(name)\n                has_nan = True\n    \n    return has_nan, nan_params\n\n# Let's demonstrate this with a simple model\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(10, 5)\n        self.linear2 = nn.Linear(5, 1)\n    \n    def forward(self, x):\n        return self.linear2(torch.relu(self.linear1(x)))\n\nmodel = SimpleModel()\n\nprint(\"Testing NaN Detection\")\nprint(\"=\" * 60)\n\n# Test 1: Normal healthy gradients\nx = torch.randn(4, 10)\ny = model(x)\ny.sum().backward()\n\nhas_nan, nan_params = check_for_nan_gradients(model)\nprint(f\"\\nHealthy gradients:\")\nprint(f\"  Has NaN? {has_nan}\")\nprint(f\"  Which params? {nan_params if nan_params else 'None - all good!'}\")\n\n# Test 2: Now let's inject a NaN and see it get caught\nmodel.zero_grad()\ny = model(x)\ny.sum().backward()\nmodel.linear1.weight.grad[0, 0] = float('nan')  # Simulate NaN\n\nhas_nan, nan_params = check_for_nan_gradients(model)\nprint(f\"\\nAfter injecting NaN:\")\nprint(f\"  Has NaN? {has_nan}\")\nprint(f\"  Which params? {nan_params}\")\nprint(f\"  ^ This is what you'd see right before your training dies\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"How to Fix NaN Loss:\")\nprint()\nprint(\"  1. Reduce learning rate (try 10x smaller)\")\nprint(\"  2. Add gradient clipping: max_grad_norm=1.0\")\nprint(\"  3. Switch from FP16 to BF16 (more stable)\")\nprint(\"  4. Add warmup (gradual LR increase)\")\nprint()\nprint(\"Important: Once you hit NaN, you MUST restart from\")\nprint(\"the last checkpoint. NaN is terminal. No recovery.\")\nprint()\nprint(\"(This is why you checkpoint frequently.)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pitfall 2: The Frozen Model Mystery\n\n**The Story:**\n\nYour training loop runs. No errors. Loss is being logged. Everything looks fine.\n\nExcept... the loss isn't moving. At all.\n\n```\nStep 100: Loss = 2.4532\nStep 200: Loss = 2.4531\nStep 300: Loss = 2.4529\nStep 400: Loss = 2.4528\n```\n\nThat's not learning. That's rounding error.\n\nYou check your learning rate: `1e-4`. Seems fine.  \nYou check your data: looks good.  \nYou check your sanity: questionable, but unrelated.\n\nThen you finally check: `sum(p.numel() for p in model.parameters() if p.requires_grad)`\n\nReturns: **0**\n\nOh.\n\n**What Happened:**\n\nSomewhere in your setup, you froze the model. Maybe you loaded a pretrained model and forgot to unfreeze it. Maybe you disabled gradients for inference and never re-enabled them. Maybe you applied LoRA but something went wrong.\n\nDoesn't matter. If `requires_grad=False` for all parameters, you're not training anything. You're just... running a very expensive random number generator.\n\n**How to Spot It:**\n\nLoss that barely moves (or moves identically every epoch). Model outputs that never change. That sinking feeling when you realize you've been \"training\" for six hours."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:51:40.848407Z",
     "iopub.status.busy": "2025-12-07T18:51:40.848253Z",
     "iopub.status.idle": "2025-12-07T18:51:41.265458Z",
     "shell.execute_reply": "2025-12-07T18:51:41.265117Z"
    }
   },
   "outputs": [],
   "source": "def verify_training_setup(model, optimizer):\n    \"\"\"Check if your model is actually set up to train.\"\"\"\n    issues = []\n    \n    # Count trainable vs frozen parameters\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    \n    if trainable == 0:\n        issues.append(\"CRITICAL: No trainable parameters! Model is completely frozen.\")\n    \n    # Check optimizer configuration\n    if len(optimizer.param_groups) == 0:\n        issues.append(\"CRITICAL: Optimizer has no parameter groups!\")\n    else:\n        lr = optimizer.param_groups[0]['lr']\n        if lr < 1e-6:\n            issues.append(f\"WARNING: Learning rate very low: {lr}\")\n        if lr > 1e-2:\n            issues.append(f\"WARNING: Learning rate very high: {lr} (may cause NaN)\")\n    \n    return {\n        'trainable_params': trainable,\n        'total_params': total,\n        'trainable_pct': 100 * trainable / total if total > 0 else 0,\n        'learning_rate': optimizer.param_groups[0]['lr'] if optimizer.param_groups else None,\n        'issues': issues,\n        'ok': len(issues) == 0\n    }\n\nprint(\"Training Setup Verification\")\nprint(\"=\" * 60)\n\n# Good setup: model is trainable\nmodel = SimpleModel()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\nresult = verify_training_setup(model, optimizer)\nprint(f\"\\nSetup 1: Normal configuration\")\nprint(f\"  Trainable params: {result['trainable_params']:,} ({result['trainable_pct']:.1f}%)\")\nprint(f\"  Learning rate: {result['learning_rate']}\")\nprint(f\"  Status: {'✓ Good to go!' if result['ok'] else 'Problems detected'}\")\nif result['issues']:\n    for issue in result['issues']:\n        print(f\"    - {issue}\")\n\n# Bad setup: accidentally froze everything\nfrozen_model = SimpleModel()\nfor param in frozen_model.parameters():\n    param.requires_grad = False\n\nresult = verify_training_setup(frozen_model, optimizer)\nprint(f\"\\nSetup 2: Frozen model (common mistake)\")\nprint(f\"  Trainable params: {result['trainable_params']:,} ({result['trainable_pct']:.1f}%)\")\nprint(f\"  Status: {'✓ Good to go!' if result['ok'] else '✗ Problems detected'}\")\nif result['issues']:\n    for issue in result['issues']:\n        print(f\"    - {issue}\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"How to Fix Frozen Model:\")\nprint()\nprint(\"  1. Check: model.parameters() should have requires_grad=True\")\nprint(\"  2. For LoRA: verify LoRA adapter was applied correctly\")\nprint(\"  3. For full fine-tuning: don't freeze anything\")\nprint(\"  4. If using PEFT: call prepare_model_for_kbit_training()\")\nprint()\nprint(\"Pro tip: Always run this check before training starts.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pitfall 3: The Overfitting Trap\n\n**The Story:**\n\nTraining is going great! Look at that loss curve:\n\n```\nEpoch 1: Train = 2.1, Val = 2.2  (gap: 0.1)\nEpoch 2: Train = 1.5, Val = 1.7  (gap: 0.2)\nEpoch 3: Train = 1.0, Val = 1.5  (gap: 0.5)\nEpoch 4: Train = 0.6, Val = 1.8  (gap: 1.2)  <- Houston...\nEpoch 5: Train = 0.3, Val = 2.2  (gap: 1.9)  <- We have a problem\n```\n\nYour model has memorized the training set. It can recite training examples perfectly. But show it something new? Crashes and burns.\n\nThis is like a student who memorized all the practice problems but doesn't actually understand math. They ace the homework, bomb the test.\n\n**What Happened:**\n\nYour model has more capacity than you need, and it used that capacity to memorize rather than generalize. Maybe you trained too long. Maybe your training set is too small. Maybe your LoRA rank is too high.\n\nThe telltale sign: training loss keeps improving while validation loss gets *worse*. That's the model saying \"I'm getting better at this specific thing!\" while actually getting worse at the general thing.\n\n**How to Spot It:**\n\nWatch the gap between train and validation loss. A small gap (0.1-0.3) is normal. A growing gap is overfitting. If val loss starts *increasing* while train loss decreases, stop immediately."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:51:41.266306Z",
     "iopub.status.busy": "2025-12-07T18:51:41.266199Z",
     "iopub.status.idle": "2025-12-07T18:51:41.270085Z",
     "shell.execute_reply": "2025-12-07T18:51:41.269748Z"
    }
   },
   "outputs": [],
   "source": "def check_overfitting(train_losses, val_losses, threshold=0.5):\n    \"\"\"\n    Analyze train/val loss to detect overfitting.\n    \n    Think of this as your \"stop training now\" alarm.\n    \"\"\"\n    gaps = [val - train for train, val in zip(train_losses, val_losses)]\n    \n    # Is the gap growing beyond healthy range?\n    is_overfitting = len(gaps) > 1 and gaps[-1] > gaps[0] + threshold\n    \n    # Classic overfitting pattern: val up, train down\n    val_increasing = len(val_losses) > 1 and val_losses[-1] > val_losses[-2]\n    train_decreasing = len(train_losses) > 1 and train_losses[-1] < train_losses[-2]\n    classic_overfit = val_increasing and train_decreasing\n    \n    return {\n        'gaps': gaps,\n        'final_gap': gaps[-1] if gaps else 0,\n        'is_overfitting': is_overfitting,\n        'classic_pattern': classic_overfit,\n        'recommendation': 'STOP TRAINING!' if is_overfitting else 'Keep going'\n    }\n\nprint(\"Overfitting Detection\")\nprint(\"=\" * 60)\n\n# Scenario 1: Healthy training\nprint(\"\\nScenario 1: Healthy training\")\nprint(\"(Both train and val improving together)\")\nhealthy_train = [2.5, 2.0, 1.6, 1.3, 1.1]\nhealthy_val =   [2.6, 2.1, 1.7, 1.4, 1.2]\n\nresult = check_overfitting(healthy_train, healthy_val)\nprint(f\"  Train: {healthy_train}\")\nprint(f\"  Val:   {healthy_val}\")\nprint(f\"  Gaps:  {[f'{g:.1f}' for g in result['gaps']]}\")\nprint(f\"  Overfitting? {result['is_overfitting']}\")\nprint(f\"  → {result['recommendation']}\")\n\n# Scenario 2: Overfitting disaster\nprint(\"\\nScenario 2: Overfitting (train improving, val getting worse)\")\noverfit_train = [2.5, 1.8, 1.2, 0.8, 0.5]\noverfit_val =   [2.6, 2.0, 2.0, 2.2, 2.5]\n\nresult = check_overfitting(overfit_train, overfit_val)\nprint(f\"  Train: {overfit_train}\")\nprint(f\"  Val:   {overfit_val}\")\nprint(f\"  Gaps:  {[f'{g:.1f}' for g in result['gaps']]}\")\nprint(f\"  Overfitting? {result['is_overfitting']}\")\nprint(f\"  Classic pattern? {result['classic_pattern']}\")\nprint(f\"  → {result['recommendation']}\")\nprint()\nprint(\"  ^ See how the gap keeps growing? Model is memorizing,\")\nprint(\"    not learning. Should have stopped at epoch 2.\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"How to Fix Overfitting:\")\nprint()\nprint(\"  Prevention (do these first):\")\nprint(\"    • Add regularization (weight_decay=0.1)\")\nprint(\"    • Use dropout (lora_dropout=0.1)\")\nprint(\"    • Lower LoRA rank\")\nprint(\"    • Get more training data\")\nprint()\nprint(\"  Reaction (when it happens):\")\nprint(\"    • Stop training immediately\")\nprint(\"    • Use checkpoint from before overfitting started\")\nprint(\"    • Reduce number of epochs for next run\")\nprint()\nprint(\"Pro tip: Always monitor both train AND val loss.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pitfall 4: Catastrophic Forgetting\n\n**The Story:**\n\nYou fine-tuned a model on medical data. It's amazing at medical questions now!\n\nYou test it:\n\n```python\nQ: \"What are the symptoms of pneumonia?\"\nA: \"Cough, fever, difficulty breathing...\"  # Perfect!\n```\n\nGreat! Ship it!\n\nWait, let's just check one more thing:\n\n```python\nQ: \"What is the capital of France?\"\nA: \"The capital of France is a common symptom of...\"  # WHAT\n```\n\nYour model has forgotten how to... be a normal language model. It only speaks medical now.\n\nThis is catastrophic forgetting. The model's weights got so optimized for medical text that it lost its general knowledge. It's like someone who went to medical school and forgot what a cat is.\n\n**What Happened:**\n\nFine-tuning modifies the weights. If you're too aggressive (high learning rate, full fine-tuning, too many epochs), you overwrite the general knowledge that was in the pretrained model.\n\nThe model doesn't \"know\" it should preserve general capabilities. You have to tell it (via lower learning rates, LoRA instead of full fine-tuning, or mixing in general data).\n\n**How to Spot It:**\n\nTest your fine-tuned model on simple general knowledge questions. If it fails at \"what is 2+2\" or \"who wrote Romeo and Juliet\", you've forgotten too much."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:51:41.270831Z",
     "iopub.status.busy": "2025-12-07T18:51:41.270739Z",
     "iopub.status.idle": "2025-12-07T18:51:41.273961Z",
     "shell.execute_reply": "2025-12-07T18:51:41.273670Z"
    }
   },
   "outputs": [],
   "source": "def evaluate_general_knowledge(model, tokenizer, test_cases):\n    \"\"\"\n    Check if model still has basic general knowledge.\n    \n    You'd run this before and after fine-tuning to detect forgetting.\n    (Here we just demonstrate the evaluation logic.)\n    \"\"\"\n    results = []\n    \n    for prompt, expected_keywords in test_cases:\n        # In real life: response = generate_from_model(model, tokenizer, prompt)\n        # Here we simulate to show the concept\n        response = f\"[Would generate response for: {prompt}]\"\n        \n        # Check if response contains expected answer keywords\n        passed = any(kw.lower() in response.lower() for kw in expected_keywords)\n        results.append({\n            'prompt': prompt,\n            'expected': expected_keywords,\n            'passed': passed\n        })\n    \n    accuracy = sum(r['passed'] for r in results) / len(results) if results else 0\n    return accuracy, results\n\nprint(\"Catastrophic Forgetting Detection\")\nprint(\"=\" * 60)\n\n# These are questions any language model should be able to answer\ngeneral_knowledge_tests = [\n    (\"What is 2 + 2?\", [\"4\", \"four\"]),\n    (\"Who wrote Romeo and Juliet?\", [\"Shakespeare\", \"William\"]),\n    (\"What is the capital of France?\", [\"Paris\"]),\n    (\"What is water made of?\", [\"H2O\", \"hydrogen\", \"oxygen\"]),\n    (\"What year did World War 2 end?\", [\"1945\"]),\n]\n\nprint(\"\\nGeneral knowledge sanity checks:\")\nfor i, (prompt, expected) in enumerate(general_knowledge_tests, 1):\n    print(f\"  {i}. {prompt}\")\n    print(f\"     Expected: {', '.join(expected)}\")\n\nprint(f\"\\n\" + \"-\" * 60)\nprint(\"Example: Medical model that forgot everything else\")\nprint()\nprint(\"Before fine-tuning:\")\nprint(\"  Q: What is the capital of France?\")\nprint(\"  A: The capital of France is Paris.\")\nprint(\"  ✓ Correct\")\nprint()\nprint(\"After aggressive fine-tuning on medical data:\")\nprint(\"  Q: What is the capital of France?\")\nprint(\"  A: The capital of France is a common symptom associated\")\nprint(\"     with acute respiratory distress syndrome...\")\nprint(\"  ✗ Model only speaks medical now\")\nprint()\nprint(\"After fine-tuning with LoRA (less aggressive):\")\nprint(\"  Q: What is the capital of France?\")\nprint(\"  A: The capital of France is Paris.\")\nprint(\"  ✓ Preserved general knowledge!\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"How to Prevent Catastrophic Forgetting:\")\nprint()\nprint(\"  1. Use LoRA instead of full fine-tuning\")\nprint(\"     (Only modifies small adapters, not whole model)\")\nprint()\nprint(\"  2. Use lower learning rates\")\nprint(\"     (5e-5 instead of 1e-4 for full fine-tuning)\")\nprint()\nprint(\"  3. Mix general data with specialized data\")\nprint(\"     (10-20% general examples in training set)\")\nprint()\nprint(\"  4. Train for fewer epochs\")\nprint(\"     (Stop when specialized performance plateaus)\")\nprint()\nprint(\"  5. For DPO/RLHF: Use KL penalty\")\nprint(\"     (Keeps model close to reference)\")\nprint()\nprint(\"Always test: Run these checks before AND after training!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pitfall 5: Reference Model Drift (DPO/RLHF)\n\n**The Story:**\n\nYou're doing DPO. Your policy model should stay reasonably close to the reference model (that's the whole point of the KL penalty).\n\nBut you check the KL divergence:\n\n```\nStep 10:  KL = 0.05  (normal)\nStep 20:  KL = 0.08  (fine)\nStep 30:  KL = 0.15  (getting high...)\nStep 40:  KL = 0.35  (uh oh)\nStep 50:  KL = 1.20  (completely diverged)\n```\n\nYour policy model has wandered off into the wilderness. It's now generating text that's completely different from what the reference model would generate.\n\nSometimes this is fine! If it's generating *better* text, great!\n\nBut often it means your model has found some weird local optimum. Maybe it learned that short responses get high rewards, so now it only outputs \"Yes\" to everything. Maybe it discovered some pathological pattern that games your reward model.\n\n**What Happened:**\n\nMost common cause: you forgot to freeze the reference model. So it's getting updated alongside the policy model. Which means your KL penalty (which measures distance from reference) is measuring distance from a moving target.\n\nOther causes: learning rate too high, KL penalty (beta) too low, or the reward signal is so strong it's worth diverging despite the penalty.\n\n**How to Spot It:**\n\nMonitor KL divergence during training. Should start low (< 0.1) and stay relatively low (< 0.5). If it's growing steadily, something's wrong."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:51:41.274627Z",
     "iopub.status.busy": "2025-12-07T18:51:41.274551Z",
     "iopub.status.idle": "2025-12-07T18:51:41.289849Z",
     "shell.execute_reply": "2025-12-07T18:51:41.289569Z"
    }
   },
   "outputs": [],
   "source": "import torch.nn.functional as F\n\ndef compute_kl_divergence(policy_logits, ref_logits):\n    \"\"\"\n    Compute KL(policy || reference).\n    \n    This measures how different the policy model's predictions are\n    from the reference model. High KL = big difference.\n    \"\"\"\n    policy_probs = F.softmax(policy_logits, dim=-1)\n    ref_log_probs = F.log_softmax(ref_logits, dim=-1)\n    policy_log_probs = F.log_softmax(policy_logits, dim=-1)\n    \n    # KL divergence: sum of p * (log p - log q)\n    kl = (policy_probs * (policy_log_probs - ref_log_probs)).sum(-1).mean()\n    \n    return kl.item()\n\ndef verify_reference_frozen(ref_model):\n    \"\"\"Check that reference model is actually frozen.\"\"\"\n    trainable = sum(1 for p in ref_model.parameters() if p.requires_grad)\n    total = sum(1 for _ in ref_model.parameters())\n    \n    return {\n        'is_frozen': trainable == 0,\n        'trainable_params': trainable,\n        'total_params': total\n    }\n\nprint(\"KL Divergence Monitoring\")\nprint(\"=\" * 60)\n\n# Simulate some model outputs\nvocab_size = 1000\nbatch_size = 4\nseq_len = 10\n\ntorch.manual_seed(42)  # For reproducibility\nref_logits = torch.randn(batch_size, seq_len, vocab_size)\n\nprint(\"\\nKL Divergence Examples:\")\nprint(\"(Lower KL = models are similar, Higher KL = models diverged)\")\n\n# Case 1: Models are identical\npolicy_identical = ref_logits.clone()\nkl = compute_kl_divergence(policy_identical, ref_logits)\nprint(f\"\\n  1. Policy = Reference: KL = {kl:.6f}\")\nprint(f\"     ^ This is what you'd see at the very start of training\")\n\n# Case 2: Small difference (healthy)\npolicy_small_diff = ref_logits + 0.1 * torch.randn_like(ref_logits)\nkl = compute_kl_divergence(policy_small_diff, ref_logits)\nprint(f\"\\n  2. Small divergence: KL = {kl:.6f}\")\nprint(f\"     ^ This is healthy - model is learning but staying close\")\n\n# Case 3: Large difference (problem!)\npolicy_large_diff = ref_logits + 2.0 * torch.randn_like(ref_logits)\nkl = compute_kl_divergence(policy_large_diff, ref_logits)\nprint(f\"\\n  3. Large divergence: KL = {kl:.6f}\")\nprint(f\"     ^ This is bad - model has drifted too far\")\n\nprint(f\"\\n\" + \"-\" * 60)\nprint(\"Checking if Reference is Frozen:\")\n\n# Test 1: Properly frozen\nfrozen_model = SimpleModel()\nfor param in frozen_model.parameters():\n    param.requires_grad = False\n\nresult = verify_reference_frozen(frozen_model)\nprint(f\"\\n  Correctly frozen reference:\")\nprint(f\"    Trainable: {result['trainable_params']}/{result['total_params']}\")\nprint(f\"    Status: {'✓ Good!' if result['is_frozen'] else '✗ Bug!'}\")\n\n# Test 2: Accidentally not frozen (common bug!)\nunfrozen_model = SimpleModel()  # Oops, forgot to freeze\n\nresult = verify_reference_frozen(unfrozen_model)\nprint(f\"\\n  Accidentally unfrozen reference:\")\nprint(f\"    Trainable: {result['trainable_params']}/{result['total_params']}\")\nprint(f\"    Status: {'✓ Good!' if result['is_frozen'] else '✗ BUG - reference is being updated!'}\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"How to Fix KL Divergence Problems:\")\nprint()\nprint(\"  1. Freeze the reference model:\")\nprint(\"     for param in ref_model.parameters():\")\nprint(\"         param.requires_grad = False\")\nprint()\nprint(\"  2. Increase beta (KL penalty strength) in DPO\")\nprint(\"     (Try 0.1 → 0.5)\")\nprint()\nprint(\"  3. Lower learning rate\")\nprint(\"     (Try 1e-6 for DPO instead of 1e-5)\")\nprint()\nprint(\"  4. Use gradient clipping\")\nprint()\nprint(\"Remember: Some KL divergence is good (means learning).\")\nprint(\"But too much means the policy has gone rogue.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pitfall 6: The Loss Masking Bug\n\n**The Story:**\n\nYour model isn't learning. At all. Loss is doing something weird.\n\nYou debug everything. Learning rate? Fine. Gradients? Fine. Data? Fine.\n\nThen you print out your labels:\n\n```python\nprint(labels)\n# Output: tensor([-100, -100, -100, -100, ...])  # All -100!\n```\n\nOh.\n\nSee, in causal language modeling, we use `-100` as the label for tokens we want to ignore in the loss. Typically the prompt tokens. We only compute loss on the response tokens.\n\nBut if ALL your labels are -100, you're not computing loss on anything. The model has no training signal.\n\nOr worse: maybe NONE of your labels are -100. So you're training the model to predict the prompt tokens too. Which means it learns to generate prompts, not responses.\n\n**What Happened:**\n\nYour data processing pipeline messed up the loss masking. Maybe you:\n- Used the wrong tokenizer function\n- Forgot to set labels at all (defaults to -100)\n- Set labels incorrectly (no -100 where there should be)\n- Had an off-by-one error in where to start masking\n\nThis bug is silent and deadly. No error messages. Training runs fine. Model just doesn't learn anything useful.\n\n**How to Spot It:**\n\nModel not learning? First thing to check: print out a few examples from your dataloader and verify the labels are partially -100 (for prompt) and partially real token IDs (for response)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:51:41.290636Z",
     "iopub.status.busy": "2025-12-07T18:51:41.290554Z",
     "iopub.status.idle": "2025-12-07T18:51:41.293681Z",
     "shell.execute_reply": "2025-12-07T18:51:41.293379Z"
    }
   },
   "outputs": [],
   "source": "def test_loss_masking(labels_list):\n    \"\"\"\n    Verify that loss masking is set up correctly.\n    \n    Correct: Some -100 (prompt), some real IDs (response)\n    Wrong: All -100 (no training signal) or no -100 (learns prompts)\n    \"\"\"\n    results = []\n    \n    for i, labels in enumerate(labels_list):\n        masked = sum(1 for l in labels if l == -100)\n        unmasked = sum(1 for l in labels if l != -100)\n        total = len(labels)\n        \n        # Diagnose issues\n        issue = None\n        if unmasked == 0:\n            issue = \"All masked - no training signal!\"\n        elif masked == 0:\n            issue = \"Nothing masked - will learn to repeat prompts\"\n        elif unmasked < 5:\n            issue = \"Very few response tokens - weak signal\"\n        elif masked < 3:\n            issue = \"Very few prompt tokens - might learn wrong pattern\"\n        \n        results.append({\n            'example': i,\n            'masked': masked,\n            'unmasked': unmasked,\n            'total': total,\n            'issue': issue\n        })\n    \n    return results\n\nprint(\"Loss Masking Verification\")\nprint(\"=\" * 60)\n\n# Remember: -100 = ignore in loss, other values = compute loss\nprint(\"\\nWhat labels should look like:\")\nprint(\"  [-100, -100, -100, 42, 17, 89, ...]\")\nprint(\"   ^^^^^^^^^^^^^      ^^^^^^^^^^^^\")\nprint(\"   prompt (masked)    response (unmasked)\")\n\n# Test different scenarios\nscenarios = {\n    \"Correct\": [-100, -100, -100, -100, -100, 42, 17, 89, 33, 55],\n    \"All masked (bug!)\": [-100] * 10,\n    \"Nothing masked (bug!)\": [42, 17, 89, 33, 55, 12, 78, 34, 91, 23],\n    \"Too few response tokens\": [-100] * 8 + [42, 17],\n}\n\nprint(f\"\\n\" + \"-\" * 60)\nprint(\"Testing different masking patterns:\")\n\nfor name, labels in scenarios.items():\n    results = test_loss_masking([labels])\n    r = results[0]\n    \n    print(f\"\\n  {name}:\")\n    print(f\"    Labels: {labels}\")\n    print(f\"    Masked: {r['masked']}, Unmasked: {r['unmasked']}\")\n    \n    if r['issue']:\n        print(f\"    ✗ ISSUE: {r['issue']}\")\n    else:\n        print(f\"    ✓ Looks good\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"How to Fix Loss Masking:\")\nprint()\nprint(\"  Correct pattern:\")\nprint(\"    1. Tokenize prompt → set labels to -100\")\nprint(\"    2. Tokenize response → set labels to token IDs\")\nprint(\"    3. Concatenate both\")\nprint()\nprint(\"  Example:\")\nprint(\"    prompt_tokens = [1, 2, 3, 4]\")\nprint(\"    response_tokens = [5, 6, 7, 8]\")\nprint(\"    \")\nprint(\"    input_ids = [1, 2, 3, 4, 5, 6, 7, 8]\")\nprint(\"    labels = [-100, -100, -100, -100, 5, 6, 7, 8]\")\nprint(\"              ^^^ prompt ^^^  ^^^ response ^^^\")\nprint()\nprint(\"Pro tip: Always print a few examples from your dataloader\")\nprint(\"to verify masking is correct before training!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pitfall 7: Reward Hacking\n\n**The Story:**\n\nYou're doing RLHF. Your reward model prefers longer, more detailed responses.\n\nYou train your policy model. The rewards are going up! Success!\n\nYou check the outputs:\n\n```python\nPrompt: \"What is the capital of France?\"\n\nResponse: \"The capital of France is Paris Paris Paris Paris Paris \nParis Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris\nParis Paris Paris Paris Paris Paris Paris Paris Paris Paris...\"\n\nReward: 9.8/10  # High reward!\n```\n\nYour model discovered that the reward model likes long responses. So it just... repeats things. Forever. Gets great rewards. Completely useless.\n\nThis is reward hacking. The model found a loophole in your reward function and exploited it.\n\nIt's like when you tell a kid to clean their room, and they shove everything under the bed. Technically clean! Reward achieved! Completely missing the point.\n\n**What Happened:**\n\nReward models are imperfect. They capture some aspects of what makes a good response, but not all. And RL algorithms are very good at finding and exploiting edge cases.\n\nIf your reward model gives high rewards for length, the policy will maximize length (regardless of quality).  \nIf it rewards confidence, you get overconfident nonsense.  \nIf it rewards using specific words, you get word salad containing those words.\n\nThe policy is just optimizing for reward. It doesn't \"know\" what you actually wanted.\n\n**How to Spot It:**\n\nHigh rewards, terrible outputs. Or outputs that are obviously exploiting some pattern (all the same length, same structure, repetitive, etc.)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:51:41.294293Z",
     "iopub.status.busy": "2025-12-07T18:51:41.294219Z",
     "iopub.status.idle": "2025-12-07T18:51:41.298680Z",
     "shell.execute_reply": "2025-12-07T18:51:41.298426Z"
    }
   },
   "outputs": [],
   "source": "import numpy as np\n\ndef apply_reward_constraints(response, base_reward):\n    \"\"\"\n    Add rule-based penalties to catch reward hacking.\n    \n    Think of this as guardrails that prevent obvious exploits.\n    \"\"\"\n    words = response.split()\n    penalties = []\n    reward = base_reward\n    \n    # Penalize repetition\n    if words:\n        unique_words = len(set(words))\n        total_words = len(words)\n        unique_ratio = unique_words / total_words\n        \n        if unique_ratio < 0.5:  # More than half are repeats\n            penalty = 5.0\n            reward -= penalty\n            penalties.append(f\"Repetition penalty: -{penalty:.1f} (only {unique_ratio:.0%} unique)\")\n    \n    # Penalize extreme lengths\n    if len(words) > 300:\n        penalty = 2.0\n        reward -= penalty\n        penalties.append(f\"Too verbose: -{penalty:.1f} ({len(words)} words)\")\n    \n    if len(words) < 5:\n        penalty = 3.0\n        reward -= penalty\n        penalties.append(f\"Too short: -{penalty:.1f} ({len(words)} words)\")\n    \n    return reward, penalties\n\ndef check_reward_hacking(responses, rewards):\n    \"\"\"Detect if the policy is gaming the reward model.\"\"\"\n    warnings = []\n    \n    # Check for suspiciously uniform rewards\n    if len(rewards) > 1 and np.std(rewards) < 0.1:\n        warnings.append(\"All rewards very similar - possible exploitation\")\n    \n    # Check high-reward responses for obvious hacking\n    if rewards:\n        high_reward_idx = np.argsort(rewards)[-min(3, len(rewards)):]\n        \n        for idx in high_reward_idx:\n            words = responses[idx].split()\n            if words:\n                unique_ratio = len(set(words)) / len(words)\n                if unique_ratio < 0.5:\n                    warnings.append(\n                        f\"Response {idx} (reward={rewards[idx]:.1f}) is {unique_ratio:.0%} repetitive\"\n                    )\n    \n    return warnings\n\nprint(\"Reward Hacking Detection\")\nprint(\"=\" * 60)\n\n# Simulate different types of responses\nresponses = [\n    \"Here is a helpful and informative response to your question.\",\n    \"Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris.\",  # Repetitive hack!\n    \"Yes\",  # Too short\n    \"The capital of France is Paris, a beautiful city known for its culture and history.\",\n]\n\nbase_rewards = [7.5, 9.0, 2.0, 8.0]  # Note: repetitive one got high reward!\n\nprint(\"\\nApplying Reward Constraints:\")\nprint(\"(Catching exploits with rule-based penalties)\")\n\nfor i, (response, base_reward) in enumerate(zip(responses, base_rewards)):\n    print(f\"\\n  Response {i}: \\\"{response[:60]}{'...' if len(response) > 60 else ''}\\\"\")\n    \n    adjusted, penalties = apply_reward_constraints(response, base_reward)\n    \n    print(f\"    Base reward: {base_reward:.1f}\")\n    print(f\"    Adjusted reward: {adjusted:.1f}\")\n    \n    if penalties:\n        print(f\"    Penalties applied:\")\n        for p in penalties:\n            print(f\"      • {p}\")\n\nprint(f\"\\n\" + \"-\" * 60)\nprint(\"Checking for Systematic Hacking:\")\n\nwarnings = check_reward_hacking(responses, base_rewards)\nif warnings:\n    print(\"  ⚠ Warning signs detected:\")\n    for w in warnings:\n        print(f\"    • {w}\")\nelse:\n    print(\"  ✓ No obvious hacking detected\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"How to Prevent Reward Hacking:\")\nprint()\nprint(\"  1. Increase KL penalty (beta parameter)\")\nprint(\"     → Keeps model close to reference, prevents exploitation\")\nprint()\nprint(\"  2. Add rule-based constraints (as shown above)\")\nprint(\"     → Catches obvious patterns like repetition\")\nprint()\nprint(\"  3. Use ensemble of reward models\")\nprint(\"     → Harder to hack multiple models at once\")\nprint()\nprint(\"  4. Train reward model on diverse, adversarial examples\")\nprint(\"     → Include examples of hacking in training data\")\nprint()\nprint(\"  5. Manual review of high-reward outputs\")\nprint(\"     → Human-in-the-loop catches what automated checks miss\")\nprint()\nprint(\"Remember: If rewards are going up but outputs are getting\")\nprint(\"worse, you're being hacked!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Debugging Strategies\n\n**When something breaks (and it will), here's how to find the problem:**\n\nThink of debugging like a doctor diagnosing a patient. You don't just guess. You run tests, narrow down possibilities, find the root cause.\n\nHere are two debugging patterns I use constantly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:51:41.299332Z",
     "iopub.status.busy": "2025-12-07T18:51:41.299257Z",
     "iopub.status.idle": "2025-12-07T18:51:41.306681Z",
     "shell.execute_reply": "2025-12-07T18:51:41.306389Z"
    }
   },
   "outputs": [],
   "source": "def bisect_debug(model, sample_batch, optimizer):\n    \"\"\"\n    Find which component is broken by testing each step.\n    \n    This is like checking each domino in a chain to find which one\n    is broken. Start at the beginning, test each piece.\n    \"\"\"\n    results = {}\n    \n    # Step 1: Can we access the model?\n    try:\n        _ = sum(1 for _ in model.parameters())\n        results['model_accessible'] = {'passed': True, 'error': None}\n    except Exception as e:\n        results['model_accessible'] = {'passed': False, 'error': str(e)}\n        return results  # Can't continue without model\n    \n    # Step 2: Can we run a forward pass?\n    try:\n        outputs = model(sample_batch)\n        results['forward_pass'] = {'passed': True, 'error': None}\n    except Exception as e:\n        results['forward_pass'] = {'passed': False, 'error': str(e)}\n        return results  # Can't continue without forward pass\n    \n    # Step 3: Can we compute gradients?\n    try:\n        loss = outputs.sum()  # Simple loss for testing\n        loss.backward()\n        results['backward_pass'] = {'passed': True, 'error': None}\n    except Exception as e:\n        results['backward_pass'] = {'passed': False, 'error': str(e)}\n        return results  # Can't continue without gradients\n    \n    # Step 4: Can we update weights?\n    try:\n        optimizer.step()\n        results['optimizer_step'] = {'passed': True, 'error': None}\n    except Exception as e:\n        results['optimizer_step'] = {'passed': False, 'error': str(e)}\n    \n    return results\n\ndef check_gradients(model):\n    \"\"\"\n    Check gradient health across all parameters.\n    \n    Gradients should be: not zero, not NaN, not too large.\n    \"\"\"\n    grad_stats = {\n        'zero_grads': [],      # Parameters with zero gradient\n        'large_grads': [],     # Parameters with suspiciously large gradients\n        'nan_grads': [],       # Parameters with NaN gradients\n        'normal_grads': 0      # Parameters with normal gradients\n    }\n    \n    grad_norms = []\n    \n    for name, param in model.named_parameters():\n        if param.requires_grad and param.grad is not None:\n            grad_norm = param.grad.norm().item()\n            grad_norms.append(grad_norm)\n            \n            if torch.isnan(param.grad).any():\n                grad_stats['nan_grads'].append(name)\n            elif grad_norm == 0:\n                grad_stats['zero_grads'].append(name)\n            elif grad_norm > 100:\n                grad_stats['large_grads'].append((name, f\"{grad_norm:.2f}\"))\n            else:\n                grad_stats['normal_grads'] += 1\n    \n    grad_stats['avg_norm'] = np.mean(grad_norms) if grad_norms else 0\n    grad_stats['max_norm'] = max(grad_norms) if grad_norms else 0\n    \n    return grad_stats\n\nprint(\"Debugging Utilities\")\nprint(\"=\" * 60)\n\n# Demo 1: Bisect debugging\nprint(\"\\n1. Bisect Debugging\")\nprint(\"   (Find which component is failing)\")\n\nmodel = SimpleModel()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nsample_input = torch.randn(4, 10)\n\nresults = bisect_debug(model, sample_input, optimizer)\n\nprint()\nfor test_name, result in results.items():\n    status = \"✓ PASS\" if result['passed'] else f\"✗ FAIL\"\n    print(f\"  {test_name:20s} {status}\")\n    if result['error']:\n        print(f\"    Error: {result['error']}\")\n\nprint()\nprint(\"  → All steps passed! Model and optimizer working correctly.\")\n\n# Demo 2: Gradient checking\nprint(f\"\\n\" + \"-\" * 60)\nprint(\"2. Gradient Health Check\")\nprint(\"   (Make sure gradients are reasonable)\")\n\n# Reset and compute gradients\nmodel = SimpleModel()\nx = torch.randn(4, 10)\ny = model(x)\ny.sum().backward()\n\ngrad_stats = check_gradients(model)\n\nprint()\nprint(f\"  Normal gradients:  {grad_stats['normal_grads']}\")\nprint(f\"  Zero gradients:    {len(grad_stats['zero_grads'])}\")\nprint(f\"  NaN gradients:     {len(grad_stats['nan_grads'])}\")\nprint(f\"  Large gradients:   {len(grad_stats['large_grads'])}\")\nprint()\nprint(f\"  Average magnitude: {grad_stats['avg_norm']:.4f}\")\nprint(f\"  Max magnitude:     {grad_stats['max_norm']:.4f}\")\nprint()\nprint(\"  → Gradients look healthy!\")\n\n# Demo 3: Detecting a problem\nprint(f\"\\n\" + \"-\" * 60)\nprint(\"3. Detecting Gradient Problems\")\n\n# Inject an issue\nmodel.linear1.weight.grad = torch.zeros_like(model.linear1.weight.grad)\ngrad_stats = check_gradients(model)\n\nprint()\nprint(f\"  After zeroing linear1.weight gradient:\")\nprint(f\"    Zero gradients detected: {grad_stats['zero_grads']}\")\nprint()\nprint(\"  ^ This would indicate linear1.weight isn't being trained!\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"When to Use These Tools:\")\nprint()\nprint(\"  Bisect debugging:\")\nprint(\"    • Training crashes with cryptic error\")\nprint(\"    • Not sure which component is broken\")\nprint(\"    • Want to isolate the problem\")\nprint()\nprint(\"  Gradient checking:\")\nprint(\"    • Loss not decreasing\")\nprint(\"    • Suspicious training behavior\")\nprint(\"    • After making architecture changes\")\nprint()\nprint(\"Pro tip: Add these checks to your training loop during\")\nprint(\"development. Remove them once everything is working.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Pre-Flight Checklist\n\n**Before you start training, check these:**\n\nThink of this like a pilot's pre-flight checklist. Takes two minutes. Catches 90% of problems before they waste hours of training time.\n\n### Environment\n- [ ] PyTorch installed and importable\n- [ ] GPU accessible (`torch.cuda.is_available()` returns True)\n- [ ] Enough GPU memory for your batch size\n- [ ] Correct CUDA/ROCm version\n\n### Data\n- [ ] Dataset loads without errors\n- [ ] Loss masking is correct (some -100, some token IDs)\n- [ ] No empty examples in your data\n- [ ] Tokenization produces reasonable-looking tensors\n- [ ] Batch shapes are what you expect\n\n### Model\n- [ ] Model loads successfully\n- [ ] Has trainable parameters (> 0)\n- [ ] LoRA adapters applied if you intended to use them\n- [ ] Model moved to GPU\n- [ ] Forward pass works on sample batch\n\n### Optimizer\n- [ ] Learning rate in reasonable range (1e-6 to 1e-4)\n- [ ] Optimizer has the parameters you think it does\n- [ ] Gradient clipping enabled (max_grad_norm=1.0)\n- [ ] Warmup configured if needed\n\n### Training Loop\n- [ ] Loss is computed correctly\n- [ ] Gradients are being calculated\n- [ ] Weights are being updated\n- [ ] Logging is working\n\n### Method-Specific Checks\n\n**For DPO:**\n- [ ] Reference model is frozen\n- [ ] Beta (KL penalty) is set (typical: 0.1)\n- [ ] Both policy and reference on same device\n\n**For RLHF:**\n- [ ] Reward model is frozen during policy training\n- [ ] KL coefficient set appropriately\n- [ ] Value network separate from policy\n\nRun through this list. Find bugs before they waste hours.\n\n(Trust me. I've wasted the hours so you don't have to.)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Hall of Shame\n\n**Most common mistakes, ranked by how much time they waste:**\n\n### 1. Learning Rate Too High\n**Symptom:** Loss becomes NaN  \n**Time wasted:** 3+ hours before you notice  \n**Fix:** Reduce LR by 10x, add gradient clipping  \n**Prevention:** Start conservative (1e-5), increase if needed\n\n### 2. Wrong Loss Masking\n**Symptom:** Model doesn't learn anything useful  \n**Time wasted:** Could be days before you realize  \n**Fix:** Print your labels, verify -100 placement  \n**Prevention:** Always inspect first batch before training\n\n### 3. Frozen Model\n**Symptom:** Loss barely moves  \n**Time wasted:** However long you wait before checking  \n**Fix:** Check `requires_grad`, enable if needed  \n**Prevention:** Print trainable parameter count at startup\n\n### 4. Overfitting\n**Symptom:** Train loss goes down, val loss goes up  \n**Time wasted:** All epochs past the sweet spot  \n**Fix:** Use earlier checkpoint, reduce epochs  \n**Prevention:** Monitor both train and val loss\n\n### 5. Reference Not Frozen (DPO/RLHF)\n**Symptom:** KL divergence explodes  \n**Time wasted:** Full training run before you notice  \n**Fix:** Freeze reference model, restart  \n**Prevention:** Check `requires_grad` on reference\n\n### 6. No Gradient Clipping\n**Symptom:** Training unstable, occasional NaN  \n**Time wasted:** Multiple failed runs  \n**Fix:** Add `max_grad_norm=1.0`  \n**Prevention:** Always enable gradient clipping\n\n### 7. Catastrophic Forgetting\n**Symptom:** Model only speaks your domain language  \n**Time wasted:** Only noticed during final evaluation  \n**Fix:** Start over with LoRA or lower LR  \n**Prevention:** Test general knowledge before and after\n\n### 8. Reward Hacking\n**Symptom:** High rewards, terrible outputs  \n**Time wasted:** Full RLHF training run  \n**Fix:** Increase KL penalty, add constraints  \n**Prevention:** Manually check high-reward samples\n\n### 9. Bad Data Quality\n**Symptom:** Model learns nonsense patterns  \n**Time wasted:** Could be forever if you don't realize  \n**Fix:** Clean your data  \n**Prevention:** Manually inspect training examples\n\n### 10. Batch Size Too Large\n**Symptom:** CUDA out of memory  \n**Time wasted:** 5 minutes per crash  \n**Fix:** Reduce batch size, enable gradient checkpointing  \n**Prevention:** Start small, increase until OOM, then back off\n\n## Quick Reference Table\n\n| Problem | Symptom | Quick Fix |\n|---------|---------|-----------|\n| Loss = NaN | Sudden infinity→NaN | LR ÷ 10, add grad clipping |\n| Loss stuck | Barely changing | Check trainable params |\n| Train << Val | Growing gap | Stop early, add regularization |\n| Model speaks only domain | Failed general knowledge | Use LoRA, lower LR |\n| KL too high | Divergence > 1.0 | Increase beta, lower LR |\n| OOM | CUDA memory error | Reduce batch size |\n\nPrint this table. Tape it to your monitor. Thank me later."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## You Made It!\n\n**Congratulations.** You now know how to break and fix transformer training.\n\nMore importantly, you know how to *debug* it. Because that's the real skill.\n\nAnyone can copy a training script and run it. The question is: what do you do when it breaks?\n\nNow you know:\n- How to recognize the seven deadliest pitfalls\n- How to diagnose what's actually wrong\n- How to fix it quickly instead of wasting days\n- How to prevent the problem next time\n\n### What You've Learned (The Whole Series)\n\nLooking back at this entire fine-tuning section:\n\n**SFT:** You learned how to teach a model new behaviors through examples, with proper instruction formatting and loss masking.\n\n**Reward Models:** You learned how to capture human preferences in a model that scores responses.\n\n**RLHF:** You learned how to use reinforcement learning (PPO) to optimize for those preferences, with all its complexity.\n\n**DPO:** You learned a simpler approach that skips RL entirely and optimizes preferences directly.\n\n**Advanced Topics:** You learned about memory optimization, hyperparameter tuning, and evaluation metrics.\n\n**Debugging:** (This notebook) You learned what goes wrong and how to fix it.\n\nThat's the full pipeline. From raw model to fine-tuned, preference-aligned, debugged system.\n\n### What's Next?\n\nGo try it. Pick a model. Pick a task. Fine-tune something.\n\nYou'll break things. That's fine. You now know how to fix them.\n\nAnd when you inevitably spend three hours debugging, only to discover you forgot to set `requires_grad=True`?\n\nYou'll laugh. Print out this notebook. Tape it to your wall.\n\nWelcome to the club.\n\n---\n\n*Check out the Try It notebook if you want hands-on practice with these debugging techniques!*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}