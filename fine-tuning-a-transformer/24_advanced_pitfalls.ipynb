{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# When Everything Goes Wrong\n\nYou're going to break things. We all do.\n\nThe question is: can you fix them quickly, or will you spend three days hunting a bug that turns out to be a single misplaced -100?\n\nThis notebook is your debugging playbook."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pattern\n",
    "\n",
    "Here's what always happens:\n",
    "\n",
    "You start training. Everything looks fine. Loss is going down. You grab coffee.\n",
    "\n",
    "You come back. Loss is `nan`. Or infinity. Or stuck at 2.45 for 800 steps. Or worse — it's going down beautifully, but your model now thinks Paris is the capital of diabetes.\n",
    "\n",
    "This notebook is your debugging playbook. Each pitfall follows a pattern:\n",
    "\n",
    "**The Story:** What went wrong (because context matters)  \n",
    "**The Symptoms:** How to recognize it's happening  \n",
    "**The Cause:** Why it actually happens  \n",
    "**The Fix:** What to do about it\n",
    "\n",
    "Ready? Let's break some models.\n",
    "\n",
    "(And then fix them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pitfall 1: The NaN Death Spiral\n\n**The Story:**\n\nIt's 2am. You've been training for three hours. Loss started at 2.5, dropped to 1.2, everything's beautiful.\n\nThen:\n```\nStep 1840: Loss = 1.18\nStep 1841: Loss = 1.15\nStep 1842: Loss = 3.47   <- uh oh\nStep 1843: Loss = inf    <- UH OH\nStep 1844: Loss = nan    <- dead\nStep 1845: Loss = nan    <- still dead\n```\n\nYour model is toast. Can't recover. Have to restart from the last checkpoint.\n\n(If you saved checkpoints. You did save checkpoints, right?)\n\n**What Happened:**\n\nSomething caused a gradient to explode. Maybe one batch had some weird tokens. Maybe the learning rate was too aggressive. Maybe you're using FP16 and hit numerical limits.\n\nDoesn't matter. Once you get a NaN gradient, it infects everything it touches. Like a zombie virus for tensors.\n\n**How to Spot It:**\n\nThe pattern is always the same: loss starts normal, maybe even improving, then suddenly jumps to infinity, then NaN. Sometimes you get warning signs (loss spiking but recovering), sometimes it just dies."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NaN Detection\n",
      "============================================================\n",
      "\n",
      "Healthy gradients:\n",
      "  Has NaN? False\n",
      "  Which params? None - all good!\n",
      "\n",
      "After injecting NaN:\n",
      "  Has NaN? True\n",
      "  Which params? ['linear1.weight']\n",
      "  ^ This is what you'd see right before your training dies\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Here's how to check for NaN gradients before they kill your training\n",
    "def check_for_nan_gradients(model):\n",
    "    \"\"\"Find which parameters have NaN gradients (if any).\"\"\"\n",
    "    has_nan = False\n",
    "    nan_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                nan_params.append(name)\n",
    "                has_nan = True\n",
    "    \n",
    "    return has_nan, nan_params\n",
    "\n",
    "# Let's demonstrate this with a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 5)\n",
    "        self.linear2 = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(torch.relu(self.linear1(x)))\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "print(\"Testing NaN Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Normal healthy gradients\n",
    "x = torch.randn(4, 10)\n",
    "y = model(x)\n",
    "y.sum().backward()\n",
    "\n",
    "has_nan, nan_params = check_for_nan_gradients(model)\n",
    "print(f\"\\nHealthy gradients:\")\n",
    "print(f\"  Has NaN? {has_nan}\")\n",
    "print(f\"  Which params? {nan_params if nan_params else 'None - all good!'}\")\n",
    "\n",
    "# Test 2: Now let's inject a NaN and see it get caught\n",
    "model.zero_grad()\n",
    "y = model(x)\n",
    "y.sum().backward()\n",
    "model.linear1.weight.grad[0, 0] = float('nan')  # Simulate NaN\n",
    "\n",
    "has_nan, nan_params = check_for_nan_gradients(model)\n",
    "print(f\"\\nAfter injecting NaN:\")\n",
    "print(f\"  Has NaN? {has_nan}\")\n",
    "print(f\"  Which params? {nan_params}\")\n",
    "print(f\"  ^ This is what you'd see right before your training dies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Fix NaN Loss:**\n",
    "\n",
    "1. Reduce learning rate (try 10x smaller)\n",
    "2. Add gradient clipping: `max_grad_norm=1.0`\n",
    "3. Switch from FP16 to BF16 (more stable)\n",
    "4. Add warmup (gradual LR increase)\n",
    "\n",
    "**Important:** Once you hit NaN, you MUST restart from the last checkpoint. NaN is terminal. No recovery.\n",
    "\n",
    "(This is why you checkpoint frequently.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 2: The Frozen Model Mystery\n",
    "\n",
    "**The Story:**\n",
    "\n",
    "Your training loop runs. No errors. Loss is being logged. Everything looks fine.\n",
    "\n",
    "Except... the loss isn't moving. At all.\n",
    "\n",
    "```\n",
    "Step 100: Loss = 2.4532\n",
    "Step 200: Loss = 2.4531\n",
    "Step 300: Loss = 2.4529\n",
    "Step 400: Loss = 2.4528\n",
    "```\n",
    "\n",
    "That's not learning. That's rounding error.\n",
    "\n",
    "You check your learning rate: `1e-4`. Seems fine.  \n",
    "You check your data: looks good.  \n",
    "You check your sanity: questionable, but unrelated.\n",
    "\n",
    "Then you finally check: `sum(p.numel() for p in model.parameters() if p.requires_grad)`\n",
    "\n",
    "Returns: **0**\n",
    "\n",
    "Oh.\n",
    "\n",
    "**What Happened:**\n",
    "\n",
    "Somewhere in your setup, you froze the model. Maybe you loaded a pretrained model and forgot to unfreeze it. Maybe you disabled gradients for inference and never re-enabled them. Maybe you applied LoRA but something went wrong.\n",
    "\n",
    "Doesn't matter. If `requires_grad=False` for all parameters, you're not training anything. You're just... running a very expensive random number generator.\n",
    "\n",
    "**How to Spot It:**\n",
    "\n",
    "Loss that barely moves (or moves identically every epoch). Model outputs that never change. That sinking feeling when you realize you've been \"training\" for six hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Setup Verification\n",
      "============================================================\n",
      "\n",
      "Setup 1: Normal configuration\n",
      "  Trainable params: 61 (100.0%)\n",
      "  Learning rate: 0.0001\n",
      "  Status: ✓ Good to go!\n",
      "\n",
      "Setup 2: Frozen model (common mistake)\n",
      "  Trainable params: 0 (0.0%)\n",
      "  Status: ✗ Problems detected\n",
      "    - CRITICAL: No trainable parameters! Model is completely frozen.\n"
     ]
    }
   ],
   "source": [
    "def verify_training_setup(model, optimizer):\n",
    "    \"\"\"Check if your model is actually set up to train.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Count trainable vs frozen parameters\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    if trainable == 0:\n",
    "        issues.append(\"CRITICAL: No trainable parameters! Model is completely frozen.\")\n",
    "    \n",
    "    # Check optimizer configuration\n",
    "    if len(optimizer.param_groups) == 0:\n",
    "        issues.append(\"CRITICAL: Optimizer has no parameter groups!\")\n",
    "    else:\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if lr < 1e-6:\n",
    "            issues.append(f\"WARNING: Learning rate very low: {lr}\")\n",
    "        if lr > 1e-2:\n",
    "            issues.append(f\"WARNING: Learning rate very high: {lr} (may cause NaN)\")\n",
    "    \n",
    "    return {\n",
    "        'trainable_params': trainable,\n",
    "        'total_params': total,\n",
    "        'trainable_pct': 100 * trainable / total if total > 0 else 0,\n",
    "        'learning_rate': optimizer.param_groups[0]['lr'] if optimizer.param_groups else None,\n",
    "        'issues': issues,\n",
    "        'ok': len(issues) == 0\n",
    "    }\n",
    "\n",
    "print(\"Training Setup Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Good setup: model is trainable\n",
    "model = SimpleModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "result = verify_training_setup(model, optimizer)\n",
    "print(f\"\\nSetup 1: Normal configuration\")\n",
    "print(f\"  Trainable params: {result['trainable_params']:,} ({result['trainable_pct']:.1f}%)\")\n",
    "print(f\"  Learning rate: {result['learning_rate']}\")\n",
    "print(f\"  Status: {'✓ Good to go!' if result['ok'] else 'Problems detected'}\")\n",
    "if result['issues']:\n",
    "    for issue in result['issues']:\n",
    "        print(f\"    - {issue}\")\n",
    "\n",
    "# Bad setup: accidentally froze everything\n",
    "frozen_model = SimpleModel()\n",
    "for param in frozen_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "result = verify_training_setup(frozen_model, optimizer)\n",
    "print(f\"\\nSetup 2: Frozen model (common mistake)\")\n",
    "print(f\"  Trainable params: {result['trainable_params']:,} ({result['trainable_pct']:.1f}%)\")\n",
    "print(f\"  Status: {'✓ Good to go!' if result['ok'] else '✗ Problems detected'}\")\n",
    "if result['issues']:\n",
    "    for issue in result['issues']:\n",
    "        print(f\"    - {issue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Fix Frozen Model:**\n",
    "\n",
    "1. Check: `model.parameters()` should have `requires_grad=True`\n",
    "2. For LoRA: verify LoRA adapter was applied correctly\n",
    "3. For full fine-tuning: don't freeze anything\n",
    "4. If using PEFT: call `prepare_model_for_kbit_training()`\n",
    "\n",
    "Always run this check before training starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def check_overfitting(train_losses, val_losses, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Analyze train/val loss to detect overfitting.\n",
    "    \n",
    "    Think of this as your \"stop training now\" alarm.\n",
    "    \"\"\"\n",
    "    gaps = [val - train for train, val in zip(train_losses, val_losses)]\n",
    "    \n",
    "    # Is the gap growing beyond healthy range?\n",
    "    is_overfitting = len(gaps) > 1 and gaps[-1] > gaps[0] + threshold\n",
    "    \n",
    "    # Classic overfitting pattern: val up, train down\n",
    "    val_increasing = len(val_losses) > 1 and val_losses[-1] > val_losses[-2]\n",
    "    train_decreasing = len(train_losses) > 1 and train_losses[-1] < train_losses[-2]\n",
    "    classic_overfit = val_increasing and train_decreasing\n",
    "    \n",
    "    return {\n",
    "        'gaps': gaps,\n",
    "        'final_gap': gaps[-1] if gaps else 0,\n",
    "        'is_overfitting': is_overfitting,\n",
    "        'classic_pattern': classic_overfit,\n",
    "        'recommendation': 'STOP TRAINING!' if is_overfitting else 'Keep going'\n",
    "    }\n",
    "\n",
    "print(\"Overfitting Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario 1: Healthy training\n",
    "print(\"\\nScenario 1: Healthy training\")\n",
    "print(\"(Both train and val improving together)\")\n",
    "healthy_train = [2.5, 2.0, 1.6, 1.3, 1.1]\n",
    "healthy_val =   [2.6, 2.1, 1.7, 1.4, 1.2]\n",
    "\n",
    "result = check_overfitting(healthy_train, healthy_val)\n",
    "print(f\"  Train: {healthy_train}\")\n",
    "print(f\"  Val:   {healthy_val}\")\n",
    "print(f\"  Gaps:  {[f'{g:.1f}' for g in result['gaps']]}\")\n",
    "print(f\"  Overfitting? {result['is_overfitting']}\")\n",
    "print(f\"  → {result['recommendation']}\")\n",
    "\n",
    "# Scenario 2: Overfitting disaster\n",
    "print(\"\\nScenario 2: Overfitting (train improving, val getting worse)\")\n",
    "overfit_train = [2.5, 1.8, 1.2, 0.8, 0.5]\n",
    "overfit_val =   [2.6, 2.0, 2.0, 2.2, 2.5]\n",
    "\n",
    "result = check_overfitting(overfit_train, overfit_val)\n",
    "print(f\"  Train: {overfit_train}\")\n",
    "print(f\"  Val:   {overfit_val}\")\n",
    "print(f\"  Gaps:  {[f'{g:.1f}' for g in result['gaps']]}\")\n",
    "print(f\"  Overfitting? {result['is_overfitting']}\")\n",
    "print(f\"  Classic pattern? {result['classic_pattern']}\")\n",
    "print(f\"  → {result['recommendation']}\")\n",
    "print()\n",
    "print(\"  ^ See how the gap keeps growing? Model is memorizing,\")\n",
    "print(\"    not learning. Should have stopped at epoch 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Fix Overfitting:**\n",
    "\n",
    "**Prevention (do these first):**\n",
    "- Add regularization (`weight_decay=0.1`)\n",
    "- Use dropout (`lora_dropout=0.1`)\n",
    "- Lower LoRA rank\n",
    "- Get more training data\n",
    "\n",
    "**Reaction (when it happens):**\n",
    "- Stop training immediately\n",
    "- Use checkpoint from before overfitting started\n",
    "- Reduce number of epochs for next run\n",
    "\n",
    "Always monitor both train AND val loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catastrophic Forgetting Detection\n",
      "============================================================\n",
      "\n",
      "General knowledge sanity checks:\n",
      "  1. What is 2 + 2?\n",
      "     Expected: 4, four\n",
      "  2. Who wrote Romeo and Juliet?\n",
      "     Expected: Shakespeare, William\n",
      "  3. What is the capital of France?\n",
      "     Expected: Paris\n",
      "  4. What is water made of?\n",
      "     Expected: H2O, hydrogen, oxygen\n",
      "  5. What year did World War 2 end?\n",
      "     Expected: 1945\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example: Medical model that forgot everything else\n",
      "\n",
      "Before fine-tuning:\n",
      "  Q: What is the capital of France?\n",
      "  A: The capital of France is Paris.\n",
      "  ✓ Correct\n",
      "\n",
      "After aggressive fine-tuning on medical data:\n",
      "  Q: What is the capital of France?\n",
      "  A: The capital of France is a common symptom associated\n",
      "     with acute respiratory distress syndrome...\n",
      "  ✗ Model only speaks medical now\n",
      "\n",
      "After fine-tuning with LoRA (less aggressive):\n",
      "  Q: What is the capital of France?\n",
      "  A: The capital of France is Paris.\n",
      "  ✓ Preserved general knowledge!\n"
     ]
    }
   ],
   "source": [
    "def evaluate_general_knowledge(model, tokenizer, test_cases):\n",
    "    \"\"\"\n",
    "    Check if model still has basic general knowledge.\n",
    "    \n",
    "    You'd run this before and after fine-tuning to detect forgetting.\n",
    "    (Here we just demonstrate the evaluation logic.)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt, expected_keywords in test_cases:\n",
    "        # In real life: response = generate_from_model(model, tokenizer, prompt)\n",
    "        # Here we simulate to show the concept\n",
    "        response = f\"[Would generate response for: {prompt}]\"\n",
    "        \n",
    "        # Check if response contains expected answer keywords\n",
    "        passed = any(kw.lower() in response.lower() for kw in expected_keywords)\n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'expected': expected_keywords,\n",
    "            'passed': passed\n",
    "        })\n",
    "    \n",
    "    accuracy = sum(r['passed'] for r in results) / len(results) if results else 0\n",
    "    return accuracy, results\n",
    "\n",
    "print(\"Catastrophic Forgetting Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# These are questions any language model should be able to answer\n",
    "general_knowledge_tests = [\n",
    "    (\"What is 2 + 2?\", [\"4\", \"four\"]),\n",
    "    (\"Who wrote Romeo and Juliet?\", [\"Shakespeare\", \"William\"]),\n",
    "    (\"What is the capital of France?\", [\"Paris\"]),\n",
    "    (\"What is water made of?\", [\"H2O\", \"hydrogen\", \"oxygen\"]),\n",
    "    (\"What year did World War 2 end?\", [\"1945\"]),\n",
    "]\n",
    "\n",
    "print(\"\\nGeneral knowledge sanity checks:\")\n",
    "for i, (prompt, expected) in enumerate(general_knowledge_tests, 1):\n",
    "    print(f\"  {i}. {prompt}\")\n",
    "    print(f\"     Expected: {', '.join(expected)}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"Example: Medical model that forgot everything else\")\n",
    "print()\n",
    "print(\"Before fine-tuning:\")\n",
    "print(\"  Q: What is the capital of France?\")\n",
    "print(\"  A: The capital of France is Paris.\")\n",
    "print(\"  ✓ Correct\")\n",
    "print()\n",
    "print(\"After aggressive fine-tuning on medical data:\")\n",
    "print(\"  Q: What is the capital of France?\")\n",
    "print(\"  A: The capital of France is a common symptom associated\")\n",
    "print(\"     with acute respiratory distress syndrome...\")\n",
    "print(\"  ✗ Model only speaks medical now\")\n",
    "print()\n",
    "print(\"After fine-tuning with LoRA (less aggressive):\")\n",
    "print(\"  Q: What is the capital of France?\")\n",
    "print(\"  A: The capital of France is Paris.\")\n",
    "print(\"  ✓ Preserved general knowledge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Prevent Catastrophic Forgetting:**\n",
    "\n",
    "1. Use LoRA instead of full fine-tuning (only modifies small adapters, not whole model)\n",
    "2. Use lower learning rates (5e-5 instead of 1e-4 for full fine-tuning)\n",
    "3. Mix general data with specialized data (10-20% general examples in training set)\n",
    "4. Train for fewer epochs (stop when specialized performance plateaus)\n",
    "5. For DPO/RLHF: Use KL penalty (keeps model close to reference)\n",
    "\n",
    "Always test: Run these checks before AND after training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_kl_divergence(policy_logits, ref_logits):\n",
    "    \"\"\"\n",
    "    Compute KL(policy || reference).\n",
    "    \n",
    "    This measures how different the policy model's predictions are\n",
    "    from the reference model. High KL = big difference.\n",
    "    \"\"\"\n",
    "    policy_probs = F.softmax(policy_logits, dim=-1)\n",
    "    ref_log_probs = F.log_softmax(ref_logits, dim=-1)\n",
    "    policy_log_probs = F.log_softmax(policy_logits, dim=-1)\n",
    "    \n",
    "    # KL divergence: sum of p * (log p - log q)\n",
    "    kl = (policy_probs * (policy_log_probs - ref_log_probs)).sum(-1).mean()\n",
    "    \n",
    "    return kl.item()\n",
    "\n",
    "def verify_reference_frozen(ref_model):\n",
    "    \"\"\"Check that reference model is actually frozen.\"\"\"\n",
    "    trainable = sum(1 for p in ref_model.parameters() if p.requires_grad)\n",
    "    total = sum(1 for _ in ref_model.parameters())\n",
    "    \n",
    "    return {\n",
    "        'is_frozen': trainable == 0,\n",
    "        'trainable_params': trainable,\n",
    "        'total_params': total\n",
    "    }\n",
    "\n",
    "print(\"KL Divergence Monitoring\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate some model outputs\n",
    "vocab_size = 1000\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "ref_logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "\n",
    "print(\"\\nKL Divergence Examples:\")\n",
    "print(\"(Lower KL = models are similar, Higher KL = models diverged)\")\n",
    "\n",
    "# Case 1: Models are identical\n",
    "policy_identical = ref_logits.clone()\n",
    "kl = compute_kl_divergence(policy_identical, ref_logits)\n",
    "print(f\"\\n  1. Policy = Reference: KL = {kl:.6f}\")\n",
    "print(f\"     ^ This is what you'd see at the very start of training\")\n",
    "\n",
    "# Case 2: Small difference (healthy)\n",
    "policy_small_diff = ref_logits + 0.1 * torch.randn_like(ref_logits)\n",
    "kl = compute_kl_divergence(policy_small_diff, ref_logits)\n",
    "print(f\"\\n  2. Small divergence: KL = {kl:.6f}\")\n",
    "print(f\"     ^ This is healthy - model is learning but staying close\")\n",
    "\n",
    "# Case 3: Large difference (problem!)\n",
    "policy_large_diff = ref_logits + 2.0 * torch.randn_like(ref_logits)\n",
    "kl = compute_kl_divergence(policy_large_diff, ref_logits)\n",
    "print(f\"\\n  3. Large divergence: KL = {kl:.6f}\")\n",
    "print(f\"     ^ This is bad - model has drifted too far\")\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"Checking if Reference is Frozen:\")\n",
    "\n",
    "# Test 1: Properly frozen\n",
    "frozen_model = SimpleModel()\n",
    "for param in frozen_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "result = verify_reference_frozen(frozen_model)\n",
    "print(f\"\\n  Correctly frozen reference:\")\n",
    "print(f\"    Trainable: {result['trainable_params']}/{result['total_params']}\")\n",
    "print(f\"    Status: {'✓ Good!' if result['is_frozen'] else '✗ Bug!'}\")\n",
    "\n",
    "# Test 2: Accidentally not frozen (common bug!)\n",
    "unfrozen_model = SimpleModel()  # Oops, forgot to freeze\n",
    "\n",
    "result = verify_reference_frozen(unfrozen_model)\n",
    "print(f\"\\n  Accidentally unfrozen reference:\")\n",
    "print(f\"    Trainable: {result['trainable_params']}/{result['total_params']}\")\n",
    "print(f\"    Status: {'✓ Good!' if result['is_frozen'] else '✗ BUG - reference is being updated!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Fix KL Divergence Problems:**\n",
    "\n",
    "1. Freeze the reference model:\n",
    "   ```python\n",
    "   for param in ref_model.parameters():\n",
    "       param.requires_grad = False\n",
    "   ```\n",
    "\n",
    "2. Increase beta (KL penalty strength) in DPO (try 0.1 → 0.5)\n",
    "3. Lower learning rate (try 1e-6 for DPO instead of 1e-5)\n",
    "4. Use gradient clipping\n",
    "\n",
    "Remember: Some KL divergence is good (means learning). But too much means the policy has gone rogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Masking Verification\n",
      "============================================================\n",
      "\n",
      "What labels should look like:\n",
      "  [-100, -100, -100, 42, 17, 89, ...]\n",
      "   ^^^^^^^^^^^^^      ^^^^^^^^^^^^\n",
      "   prompt (masked)    response (unmasked)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing different masking patterns:\n",
      "\n",
      "  Correct:\n",
      "    Labels: [-100, -100, -100, -100, -100, 42, 17, 89, 33, 55]\n",
      "    Masked: 5, Unmasked: 5\n",
      "    ✓ Looks good\n",
      "\n",
      "  All masked (bug!):\n",
      "    Labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "    Masked: 10, Unmasked: 0\n",
      "    ✗ ISSUE: All masked - no training signal!\n",
      "\n",
      "  Nothing masked (bug!):\n",
      "    Labels: [42, 17, 89, 33, 55, 12, 78, 34, 91, 23]\n",
      "    Masked: 0, Unmasked: 10\n",
      "    ✗ ISSUE: Nothing masked - will learn to repeat prompts\n",
      "\n",
      "  Too few response tokens:\n",
      "    Labels: [-100, -100, -100, -100, -100, -100, -100, -100, 42, 17]\n",
      "    Masked: 8, Unmasked: 2\n",
      "    ✗ ISSUE: Very few response tokens - weak signal\n"
     ]
    }
   ],
   "source": [
    "def test_loss_masking(labels_list):\n",
    "    \"\"\"\n",
    "    Verify that loss masking is set up correctly.\n",
    "    \n",
    "    Correct: Some -100 (prompt), some real IDs (response)\n",
    "    Wrong: All -100 (no training signal) or no -100 (learns prompts)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, labels in enumerate(labels_list):\n",
    "        masked = sum(1 for l in labels if l == -100)\n",
    "        unmasked = sum(1 for l in labels if l != -100)\n",
    "        total = len(labels)\n",
    "        \n",
    "        # Diagnose issues\n",
    "        issue = None\n",
    "        if unmasked == 0:\n",
    "            issue = \"All masked - no training signal!\"\n",
    "        elif masked == 0:\n",
    "            issue = \"Nothing masked - will learn to repeat prompts\"\n",
    "        elif unmasked < 5:\n",
    "            issue = \"Very few response tokens - weak signal\"\n",
    "        elif masked < 3:\n",
    "            issue = \"Very few prompt tokens - might learn wrong pattern\"\n",
    "        \n",
    "        results.append({\n",
    "            'example': i,\n",
    "            'masked': masked,\n",
    "            'unmasked': unmasked,\n",
    "            'total': total,\n",
    "            'issue': issue\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Loss Masking Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Remember: -100 = ignore in loss, other values = compute loss\n",
    "print(\"\\nWhat labels should look like:\")\n",
    "print(\"  [-100, -100, -100, 42, 17, 89, ...]\")\n",
    "print(\"   ^^^^^^^^^^^^^      ^^^^^^^^^^^^\")\n",
    "print(\"   prompt (masked)    response (unmasked)\")\n",
    "\n",
    "# Test different scenarios\n",
    "scenarios = {\n",
    "    \"Correct\": [-100, -100, -100, -100, -100, 42, 17, 89, 33, 55],\n",
    "    \"All masked (bug!)\": [-100] * 10,\n",
    "    \"Nothing masked (bug!)\": [42, 17, 89, 33, 55, 12, 78, 34, 91, 23],\n",
    "    \"Too few response tokens\": [-100] * 8 + [42, 17],\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing different masking patterns:\")\n",
    "\n",
    "for name, labels in scenarios.items():\n",
    "    results = test_loss_masking([labels])\n",
    "    r = results[0]\n",
    "    \n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Labels: {labels}\")\n",
    "    print(f\"    Masked: {r['masked']}, Unmasked: {r['unmasked']}\")\n",
    "    \n",
    "    if r['issue']:\n",
    "        print(f\"    ✗ ISSUE: {r['issue']}\")\n",
    "    else:\n",
    "        print(f\"    ✓ Looks good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Fix Loss Masking:**\n",
    "\n",
    "Correct pattern:\n",
    "1. Tokenize prompt → set labels to -100\n",
    "2. Tokenize response → set labels to token IDs\n",
    "3. Concatenate both\n",
    "\n",
    "Example:\n",
    "```python\n",
    "prompt_tokens = [1, 2, 3, 4]\n",
    "response_tokens = [5, 6, 7, 8]\n",
    "\n",
    "input_ids = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "labels = [-100, -100, -100, -100, 5, 6, 7, 8]\n",
    "          # ^^^ prompt ^^^  ^^^ response ^^^\n",
    "```\n",
    "\n",
    "Always print a few examples from your dataloader to verify masking is correct before training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_reward_constraints(response, base_reward):\n",
    "    \"\"\"\n",
    "    Add rule-based penalties to catch reward hacking.\n",
    "    \n",
    "    Think of this as guardrails that prevent obvious exploits.\n",
    "    \"\"\"\n",
    "    words = response.split()\n",
    "    penalties = []\n",
    "    reward = base_reward\n",
    "    \n",
    "    # Penalize repetition\n",
    "    if words:\n",
    "        unique_words = len(set(words))\n",
    "        total_words = len(words)\n",
    "        unique_ratio = unique_words / total_words\n",
    "        \n",
    "        if unique_ratio < 0.5:  # More than half are repeats\n",
    "            penalty = 5.0\n",
    "            reward -= penalty\n",
    "            penalties.append(f\"Repetition penalty: -{penalty:.1f} (only {unique_ratio:.0%} unique)\")\n",
    "    \n",
    "    # Penalize extreme lengths\n",
    "    if len(words) > 300:\n",
    "        penalty = 2.0\n",
    "        reward -= penalty\n",
    "        penalties.append(f\"Too verbose: -{penalty:.1f} ({len(words)} words)\")\n",
    "    \n",
    "    if len(words) < 5:\n",
    "        penalty = 3.0\n",
    "        reward -= penalty\n",
    "        penalties.append(f\"Too short: -{penalty:.1f} ({len(words)} words)\")\n",
    "    \n",
    "    return reward, penalties\n",
    "\n",
    "def check_reward_hacking(responses, rewards):\n",
    "    \"\"\"Detect if the policy is gaming the reward model.\"\"\"\n",
    "    warnings = []\n",
    "    \n",
    "    # Check for suspiciously uniform rewards\n",
    "    if len(rewards) > 1 and np.std(rewards) < 0.1:\n",
    "        warnings.append(\"All rewards very similar - possible exploitation\")\n",
    "    \n",
    "    # Check high-reward responses for obvious hacking\n",
    "    if rewards:\n",
    "        high_reward_idx = np.argsort(rewards)[-min(3, len(rewards)):]\n",
    "        \n",
    "        for idx in high_reward_idx:\n",
    "            words = responses[idx].split()\n",
    "            if words:\n",
    "                unique_ratio = len(set(words)) / len(words)\n",
    "                if unique_ratio < 0.5:\n",
    "                    warnings.append(\n",
    "                        f\"Response {idx} (reward={rewards[idx]:.1f}) is {unique_ratio:.0%} repetitive\"\n",
    "                    )\n",
    "    \n",
    "    return warnings\n",
    "\n",
    "print(\"Reward Hacking Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate different types of responses\n",
    "responses = [\n",
    "    \"Here is a helpful and informative response to your question.\",\n",
    "    \"Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris.\",  # Repetitive hack!\n",
    "    \"Yes\",  # Too short\n",
    "    \"The capital of France is Paris, a beautiful city known for its culture and history.\",\n",
    "]\n",
    "\n",
    "base_rewards = [7.5, 9.0, 2.0, 8.0]  # Note: repetitive one got high reward!\n",
    "\n",
    "print(\"\\nApplying Reward Constraints:\")\n",
    "print(\"(Catching exploits with rule-based penalties)\")\n",
    "\n",
    "for i, (response, base_reward) in enumerate(zip(responses, base_rewards)):\n",
    "    print(f\"\\n  Response {i}: \\\"{response[:60]}{'...' if len(response) > 60 else ''}\\\"\")\n",
    "    \n",
    "    adjusted, penalties = apply_reward_constraints(response, base_reward)\n",
    "    \n",
    "    print(f\"    Base reward: {base_reward:.1f}\")\n",
    "    print(f\"    Adjusted reward: {adjusted:.1f}\")\n",
    "    \n",
    "    if penalties:\n",
    "        print(f\"    Penalties applied:\")\n",
    "        for p in penalties:\n",
    "            print(f\"      • {p}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"Checking for Systematic Hacking:\")\n",
    "\n",
    "warnings = check_reward_hacking(responses, base_rewards)\n",
    "if warnings:\n",
    "    print(\"  ⚠ Warning signs detected:\")\n",
    "    for w in warnings:\n",
    "        print(f\"    • {w}\")\n",
    "else:\n",
    "    print(\"  ✓ No obvious hacking detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Prevent Reward Hacking:**\n",
    "\n",
    "1. Increase KL penalty (beta parameter) → Keeps model close to reference, prevents exploitation\n",
    "2. Add rule-based constraints (as shown above) → Catches obvious patterns like repetition\n",
    "3. Use ensemble of reward models → Harder to hack multiple models at once\n",
    "4. Train reward model on diverse, adversarial examples → Include examples of hacking in training data\n",
    "5. Manual review of high-reward outputs → Human-in-the-loop catches what automated checks miss\n",
    "\n",
    "Remember: If rewards are going up but outputs are getting worse, you're being hacked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging Utilities\n",
      "============================================================\n",
      "\n",
      "1. Bisect Debugging\n",
      "   (Find which component is failing)\n",
      "\n",
      "  model_accessible     ✓ PASS\n",
      "  forward_pass         ✓ PASS\n",
      "  backward_pass        ✓ PASS\n",
      "  optimizer_step       ✓ PASS\n",
      "\n",
      "  → All steps passed! Model and optimizer working correctly.\n",
      "\n",
      "------------------------------------------------------------\n",
      "2. Gradient Health Check\n",
      "   (Make sure gradients are reasonable)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    109\u001b[39m y = model(x)\n\u001b[32m    110\u001b[39m y.sum().backward()\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m grad_stats = \u001b[43mcheck_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m    115\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Normal gradients:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad_stats[\u001b[33m'\u001b[39m\u001b[33mnormal_grads\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mcheck_gradients\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     71\u001b[39m             grad_stats[\u001b[33m'\u001b[39m\u001b[33mnormal_grads\u001b[39m\u001b[33m'\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m grad_stats[\u001b[33m'\u001b[39m\u001b[33mavg_norm\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mnp\u001b[49m.mean(grad_norms) \u001b[38;5;28;01mif\u001b[39;00m grad_norms \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     74\u001b[39m grad_stats[\u001b[33m'\u001b[39m\u001b[33mmax_norm\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mmax\u001b[39m(grad_norms) \u001b[38;5;28;01mif\u001b[39;00m grad_norms \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grad_stats\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def bisect_debug(model, sample_batch, optimizer):\n",
    "    \"\"\"\n",
    "    Find which component is broken by testing each step.\n",
    "    \n",
    "    This is like checking each domino in a chain to find which one\n",
    "    is broken. Start at the beginning, test each piece.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Step 1: Can we access the model?\n",
    "    try:\n",
    "        _ = sum(1 for _ in model.parameters())\n",
    "        results['model_accessible'] = {'passed': True, 'error': None}\n",
    "    except Exception as e:\n",
    "        results['model_accessible'] = {'passed': False, 'error': str(e)}\n",
    "        return results  # Can't continue without model\n",
    "    \n",
    "    # Step 2: Can we run a forward pass?\n",
    "    try:\n",
    "        outputs = model(sample_batch)\n",
    "        results['forward_pass'] = {'passed': True, 'error': None}\n",
    "    except Exception as e:\n",
    "        results['forward_pass'] = {'passed': False, 'error': str(e)}\n",
    "        return results  # Can't continue without forward pass\n",
    "    \n",
    "    # Step 3: Can we compute gradients?\n",
    "    try:\n",
    "        loss = outputs.sum()  # Simple loss for testing\n",
    "        loss.backward()\n",
    "        results['backward_pass'] = {'passed': True, 'error': None}\n",
    "    except Exception as e:\n",
    "        results['backward_pass'] = {'passed': False, 'error': str(e)}\n",
    "        return results  # Can't continue without gradients\n",
    "    \n",
    "    # Step 4: Can we update weights?\n",
    "    try:\n",
    "        optimizer.step()\n",
    "        results['optimizer_step'] = {'passed': True, 'error': None}\n",
    "    except Exception as e:\n",
    "        results['optimizer_step'] = {'passed': False, 'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_gradients(model):\n",
    "    \"\"\"\n",
    "    Check gradient health across all parameters.\n",
    "    \n",
    "    Gradients should be: not zero, not NaN, not too large.\n",
    "    \"\"\"\n",
    "    grad_stats = {\n",
    "        'zero_grads': [],      # Parameters with zero gradient\n",
    "        'large_grads': [],     # Parameters with suspiciously large gradients\n",
    "        'nan_grads': [],       # Parameters with NaN gradients\n",
    "        'normal_grads': 0      # Parameters with normal gradients\n",
    "    }\n",
    "    \n",
    "    grad_norms = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms.append(grad_norm)\n",
    "            \n",
    "            if torch.isnan(param.grad).any():\n",
    "                grad_stats['nan_grads'].append(name)\n",
    "            elif grad_norm == 0:\n",
    "                grad_stats['zero_grads'].append(name)\n",
    "            elif grad_norm > 100:\n",
    "                grad_stats['large_grads'].append((name, f\"{grad_norm:.2f}\"))\n",
    "            else:\n",
    "                grad_stats['normal_grads'] += 1\n",
    "    \n",
    "    grad_stats['avg_norm'] = np.mean(grad_norms) if grad_norms else 0\n",
    "    grad_stats['max_norm'] = max(grad_norms) if grad_norms else 0\n",
    "    \n",
    "    return grad_stats\n",
    "\n",
    "print(\"Debugging Utilities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Demo 1: Bisect debugging\n",
    "print(\"\\n1. Bisect Debugging\")\n",
    "print(\"   (Find which component is failing)\")\n",
    "\n",
    "model = SimpleModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "sample_input = torch.randn(4, 10)\n",
    "\n",
    "results = bisect_debug(model, sample_input, optimizer)\n",
    "\n",
    "print()\n",
    "for test_name, result in results.items():\n",
    "    status = \"✓ PASS\" if result['passed'] else f\"✗ FAIL\"\n",
    "    print(f\"  {test_name:20s} {status}\")\n",
    "    if result['error']:\n",
    "        print(f\"    Error: {result['error']}\")\n",
    "\n",
    "print()\n",
    "print(\"  → All steps passed! Model and optimizer working correctly.\")\n",
    "\n",
    "# Demo 2: Gradient checking\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"2. Gradient Health Check\")\n",
    "print(\"   (Make sure gradients are reasonable)\")\n",
    "\n",
    "# Reset and compute gradients\n",
    "model = SimpleModel()\n",
    "x = torch.randn(4, 10)\n",
    "y = model(x)\n",
    "y.sum().backward()\n",
    "\n",
    "grad_stats = check_gradients(model)\n",
    "\n",
    "print()\n",
    "print(f\"  Normal gradients:  {grad_stats['normal_grads']}\")\n",
    "print(f\"  Zero gradients:    {len(grad_stats['zero_grads'])}\")\n",
    "print(f\"  NaN gradients:     {len(grad_stats['nan_grads'])}\")\n",
    "print(f\"  Large gradients:   {len(grad_stats['large_grads'])}\")\n",
    "print()\n",
    "print(f\"  Average magnitude: {grad_stats['avg_norm']:.4f}\")\n",
    "print(f\"  Max magnitude:     {grad_stats['max_norm']:.4f}\")\n",
    "print()\n",
    "print(\"  → Gradients look healthy!\")\n",
    "\n",
    "# Demo 3: Detecting a problem\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"3. Detecting Gradient Problems\")\n",
    "\n",
    "# Inject an issue\n",
    "model.linear1.weight.grad = torch.zeros_like(model.linear1.weight.grad)\n",
    "grad_stats = check_gradients(model)\n",
    "\n",
    "print()\n",
    "print(f\"  After zeroing linear1.weight gradient:\")\n",
    "print(f\"    Zero gradients detected: {grad_stats['zero_grads']}\")\n",
    "print()\n",
    "print(\"  ^ This would indicate linear1.weight isn't being trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use These Tools:**\n",
    "\n",
    "**Bisect debugging:**\n",
    "- Training crashes with cryptic error\n",
    "- Not sure which component is broken\n",
    "- Want to isolate the problem\n",
    "\n",
    "**Gradient checking:**\n",
    "- Loss not decreasing\n",
    "- Suspicious training behavior\n",
    "- After making architecture changes\n",
    "\n",
    "Add these checks to your training loop during development. Remove them once everything is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 6: The Loss Masking Bug\n",
    "\n",
    "**The Story:**\n",
    "\n",
    "Your model isn't learning. At all. Loss is doing something weird.\n",
    "\n",
    "You debug everything. Learning rate? Fine. Gradients? Fine. Data? Fine.\n",
    "\n",
    "Then you print out your labels:\n",
    "\n",
    "```python\n",
    "print(labels)\n",
    "# Output: tensor([-100, -100, -100, -100, ...])  # All -100!\n",
    "```\n",
    "\n",
    "Oh.\n",
    "\n",
    "See, in causal language modeling, we use `-100` as the label for tokens we want to ignore in the loss. Typically the prompt tokens. We only compute loss on the response tokens.\n",
    "\n",
    "But if ALL your labels are -100, you're not computing loss on anything. The model has no training signal.\n",
    "\n",
    "Or worse: maybe NONE of your labels are -100. So you're training the model to predict the prompt tokens too. Which means it learns to generate prompts, not responses.\n",
    "\n",
    "**What Happened:**\n",
    "\n",
    "Your data processing pipeline messed up the loss masking. Maybe you:\n",
    "- Used the wrong tokenizer function\n",
    "- Forgot to set labels at all (defaults to -100)\n",
    "- Set labels incorrectly (no -100 where there should be)\n",
    "- Had an off-by-one error in where to start masking\n",
    "\n",
    "This bug is silent and deadly. No error messages. Training runs fine. Model just doesn't learn anything useful.\n",
    "\n",
    "**How to Spot It:**\n",
    "\n",
    "Model not learning? First thing to check: print out a few examples from your dataloader and verify the labels are partially -100 (for prompt) and partially real token IDs (for response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_masking(labels_list):\n",
    "    \"\"\"\n",
    "    Verify that loss masking is set up correctly.\n",
    "    \n",
    "    Correct: Some -100 (prompt), some real IDs (response)\n",
    "    Wrong: All -100 (no training signal) or no -100 (learns prompts)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, labels in enumerate(labels_list):\n",
    "        masked = sum(1 for l in labels if l == -100)\n",
    "        unmasked = sum(1 for l in labels if l != -100)\n",
    "        total = len(labels)\n",
    "        \n",
    "        # Diagnose issues\n",
    "        issue = None\n",
    "        if unmasked == 0:\n",
    "            issue = \"All masked - no training signal!\"\n",
    "        elif masked == 0:\n",
    "            issue = \"Nothing masked - will learn to repeat prompts\"\n",
    "        elif unmasked < 5:\n",
    "            issue = \"Very few response tokens - weak signal\"\n",
    "        elif masked < 3:\n",
    "            issue = \"Very few prompt tokens - might learn wrong pattern\"\n",
    "        \n",
    "        results.append({\n",
    "            'example': i,\n",
    "            'masked': masked,\n",
    "            'unmasked': unmasked,\n",
    "            'total': total,\n",
    "            'issue': issue\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Loss Masking Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Remember: -100 = ignore in loss, other values = compute loss\n",
    "print(\"\\nWhat labels should look like:\")\n",
    "print(\"  [-100, -100, -100, 42, 17, 89, ...]\")\n",
    "print(\"   ^^^^^^^^^^^^^      ^^^^^^^^^^^^\")\n",
    "print(\"   prompt (masked)    response (unmasked)\")\n",
    "\n",
    "# Test different scenarios\n",
    "scenarios = {\n",
    "    \"Correct\": [-100, -100, -100, -100, -100, 42, 17, 89, 33, 55],\n",
    "    \"All masked (bug!)\": [-100] * 10,\n",
    "    \"Nothing masked (bug!)\": [42, 17, 89, 33, 55, 12, 78, 34, 91, 23],\n",
    "    \"Too few response tokens\": [-100] * 8 + [42, 17],\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"Testing different masking patterns:\")\n",
    "\n",
    "for name, labels in scenarios.items():\n",
    "    results = test_loss_masking([labels])\n",
    "    r = results[0]\n",
    "    \n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Labels: {labels}\")\n",
    "    print(f\"    Masked: {r['masked']}, Unmasked: {r['unmasked']}\")\n",
    "    \n",
    "    if r['issue']:\n",
    "        print(f\"    ✗ ISSUE: {r['issue']}\")\n",
    "    else:\n",
    "        print(f\"    ✓ Looks good\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"How to Fix Loss Masking:\")\n",
    "print()\n",
    "print(\"  Correct pattern:\")\n",
    "print(\"    1. Tokenize prompt → set labels to -100\")\n",
    "print(\"    2. Tokenize response → set labels to token IDs\")\n",
    "print(\"    3. Concatenate both\")\n",
    "print()\n",
    "print(\"  Example:\")\n",
    "print(\"    prompt_tokens = [1, 2, 3, 4]\")\n",
    "print(\"    response_tokens = [5, 6, 7, 8]\")\n",
    "print(\"    \")\n",
    "print(\"    input_ids = [1, 2, 3, 4, 5, 6, 7, 8]\")\n",
    "print(\"    labels = [-100, -100, -100, -100, 5, 6, 7, 8]\")\n",
    "print(\"              ^^^ prompt ^^^  ^^^ response ^^^\")\n",
    "print()\n",
    "print(\"Always print a few examples from your dataloader\")\n",
    "print(\"to verify masking is correct before training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall 7: Reward Hacking\n",
    "\n",
    "**The Story:**\n",
    "\n",
    "You're doing RLHF. Your reward model prefers longer, more detailed responses.\n",
    "\n",
    "You train your policy model. The rewards are going up! Success!\n",
    "\n",
    "You check the outputs:\n",
    "\n",
    "```python\n",
    "Prompt: \"What is the capital of France?\"\n",
    "\n",
    "Response: \"The capital of France is Paris Paris Paris Paris Paris \n",
    "Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris\n",
    "Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris...\"\n",
    "\n",
    "Reward: 9.8/10  # High reward!\n",
    "```\n",
    "\n",
    "Your model discovered that the reward model likes long responses. So it just... repeats things. Forever. Gets great rewards. Completely useless.\n",
    "\n",
    "This is reward hacking. The model found a loophole in your reward function and exploited it.\n",
    "\n",
    "It's like when you tell a kid to clean their room, and they shove everything under the bed. Technically clean! Reward achieved! Completely missing the point.\n",
    "\n",
    "**What Happened:**\n",
    "\n",
    "Reward models are imperfect. They capture some aspects of what makes a good response, but not all. And RL algorithms are very good at finding and exploiting edge cases.\n",
    "\n",
    "If your reward model gives high rewards for length, the policy will maximize length (regardless of quality).  \n",
    "If it rewards confidence, you get overconfident nonsense.  \n",
    "If it rewards using specific words, you get word salad containing those words.\n",
    "\n",
    "The policy is just optimizing for reward. It doesn't \"know\" what you actually wanted.\n",
    "\n",
    "**How to Spot It:**\n",
    "\n",
    "High rewards, terrible outputs. Or outputs that are obviously exploiting some pattern (all the same length, same structure, repetitive, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_reward_constraints(response, base_reward):\n",
    "    \"\"\"\n",
    "    Add rule-based penalties to catch reward hacking.\n",
    "    \n",
    "    Think of this as guardrails that prevent obvious exploits.\n",
    "    \"\"\"\n",
    "    words = response.split()\n",
    "    penalties = []\n",
    "    reward = base_reward\n",
    "    \n",
    "    # Penalize repetition\n",
    "    if words:\n",
    "        unique_words = len(set(words))\n",
    "        total_words = len(words)\n",
    "        unique_ratio = unique_words / total_words\n",
    "        \n",
    "        if unique_ratio < 0.5:  # More than half are repeats\n",
    "            penalty = 5.0\n",
    "            reward -= penalty\n",
    "            penalties.append(f\"Repetition penalty: -{penalty:.1f} (only {unique_ratio:.0%} unique)\")\n",
    "    \n",
    "    # Penalize extreme lengths\n",
    "    if len(words) > 300:\n",
    "        penalty = 2.0\n",
    "        reward -= penalty\n",
    "        penalties.append(f\"Too verbose: -{penalty:.1f} ({len(words)} words)\")\n",
    "    \n",
    "    if len(words) < 5:\n",
    "        penalty = 3.0\n",
    "        reward -= penalty\n",
    "        penalties.append(f\"Too short: -{penalty:.1f} ({len(words)} words)\")\n",
    "    \n",
    "    return reward, penalties\n",
    "\n",
    "def check_reward_hacking(responses, rewards):\n",
    "    \"\"\"Detect if the policy is gaming the reward model.\"\"\"\n",
    "    warnings = []\n",
    "    \n",
    "    # Check for suspiciously uniform rewards\n",
    "    if len(rewards) > 1 and np.std(rewards) < 0.1:\n",
    "        warnings.append(\"All rewards very similar - possible exploitation\")\n",
    "    \n",
    "    # Check high-reward responses for obvious hacking\n",
    "    if rewards:\n",
    "        high_reward_idx = np.argsort(rewards)[-min(3, len(rewards)):]\n",
    "        \n",
    "        for idx in high_reward_idx:\n",
    "            words = responses[idx].split()\n",
    "            if words:\n",
    "                unique_ratio = len(set(words)) / len(words)\n",
    "                if unique_ratio < 0.5:\n",
    "                    warnings.append(\n",
    "                        f\"Response {idx} (reward={rewards[idx]:.1f}) is {unique_ratio:.0%} repetitive\"\n",
    "                    )\n",
    "    \n",
    "    return warnings\n",
    "\n",
    "print(\"Reward Hacking Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate different types of responses\n",
    "responses = [\n",
    "    \"Here is a helpful and informative response to your question.\",\n",
    "    \"Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris Paris.\",  # Repetitive hack!\n",
    "    \"Yes\",  # Too short\n",
    "    \"The capital of France is Paris, a beautiful city known for its culture and history.\",\n",
    "]\n",
    "\n",
    "base_rewards = [7.5, 9.0, 2.0, 8.0]  # Note: repetitive one got high reward!\n",
    "\n",
    "print(\"\\nApplying Reward Constraints:\")\n",
    "print(\"(Catching exploits with rule-based penalties)\")\n",
    "\n",
    "for i, (response, base_reward) in enumerate(zip(responses, base_rewards)):\n",
    "    print(f\"\\n  Response {i}: \\\"{response[:60]}{'...' if len(response) > 60 else ''}\\\"\")\n",
    "    \n",
    "    adjusted, penalties = apply_reward_constraints(response, base_reward)\n",
    "    \n",
    "    print(f\"    Base reward: {base_reward:.1f}\")\n",
    "    print(f\"    Adjusted reward: {adjusted:.1f}\")\n",
    "    \n",
    "    if penalties:\n",
    "        print(f\"    Penalties applied:\")\n",
    "        for p in penalties:\n",
    "            print(f\"      • {p}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"Checking for Systematic Hacking:\")\n",
    "\n",
    "warnings = check_reward_hacking(responses, base_rewards)\n",
    "if warnings:\n",
    "    print(\"  ⚠ Warning signs detected:\")\n",
    "    for w in warnings:\n",
    "        print(f\"    • {w}\")\n",
    "else:\n",
    "    print(\"  ✓ No obvious hacking detected\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"How to Prevent Reward Hacking:\")\n",
    "print()\n",
    "print(\"  1. Increase KL penalty (beta parameter)\")\n",
    "print(\"     → Keeps model close to reference, prevents exploitation\")\n",
    "print()\n",
    "print(\"  2. Add rule-based constraints (as shown above)\")\n",
    "print(\"     → Catches obvious patterns like repetition\")\n",
    "print()\n",
    "print(\"  3. Use ensemble of reward models\")\n",
    "print(\"     → Harder to hack multiple models at once\")\n",
    "print()\n",
    "print(\"  4. Train reward model on diverse, adversarial examples\")\n",
    "print(\"     → Include examples of hacking in training data\")\n",
    "print()\n",
    "print(\"  5. Manual review of high-reward outputs\")\n",
    "print(\"     → Human-in-the-loop catches what automated checks miss\")\n",
    "print()\n",
    "print(\"Remember: If rewards are going up but outputs are getting\")\n",
    "print(\"worse, you're being hacked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Strategies\n",
    "\n",
    "**When something breaks (and it will), here's how to find the problem:**\n",
    "\n",
    "Think of debugging like a doctor diagnosing a patient. You don't just guess. You run tests, narrow down possibilities, find the root cause.\n",
    "\n",
    "Here are two debugging patterns I use constantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisect_debug(model, sample_batch, optimizer):\n",
    "    \"\"\"\n",
    "    Find which component is broken by testing each step.\n",
    "    \n",
    "    This is like checking each domino in a chain to find which one\n",
    "    is broken. Start at the beginning, test each piece.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Step 1: Can we access the model?\n",
    "    try:\n",
    "        _ = sum(1 for _ in model.parameters())\n",
    "        results['model_accessible'] = {'passed': True, 'error': None}\n",
    "    except Exception as e:\n",
    "        results['model_accessible'] = {'passed': False, 'error': str(e)}\n",
    "        return results  # Can't continue without model\n",
    "    \n",
    "    # Step 2: Can we run a forward pass?\n",
    "    try:\n",
    "        outputs = model(sample_batch)\n",
    "        results['forward_pass'] = {'passed': True, 'error': None}\n",
    "    except Exception as e:\n",
    "        results['forward_pass'] = {'passed': False, 'error': str(e)}\n",
    "        return results  # Can't continue without forward pass\n",
    "    \n",
    "    # Step 3: Can we compute gradients?\n",
    "    try:\n",
    "        loss = outputs.sum()  # Simple loss for testing\n",
    "        loss.backward()\n",
    "        results['backward_pass'] = {'passed': True, 'error': None}\n",
    "    except Exception as e:\n",
    "        results['backward_pass'] = {'passed': False, 'error': str(e)}\n",
    "        return results  # Can't continue without gradients\n",
    "    \n",
    "    # Step 4: Can we update weights?\n",
    "    try:\n",
    "        optimizer.step()\n",
    "        results['optimizer_step'] = {'passed': True, 'error': None}\n",
    "    except Exception as e:\n",
    "        results['optimizer_step'] = {'passed': False, 'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_gradients(model):\n",
    "    \"\"\"\n",
    "    Check gradient health across all parameters.\n",
    "    \n",
    "    Gradients should be: not zero, not NaN, not too large.\n",
    "    \"\"\"\n",
    "    grad_stats = {\n",
    "        'zero_grads': [],      # Parameters with zero gradient\n",
    "        'large_grads': [],     # Parameters with suspiciously large gradients\n",
    "        'nan_grads': [],       # Parameters with NaN gradients\n",
    "        'normal_grads': 0      # Parameters with normal gradients\n",
    "    }\n",
    "    \n",
    "    grad_norms = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms.append(grad_norm)\n",
    "            \n",
    "            if torch.isnan(param.grad).any():\n",
    "                grad_stats['nan_grads'].append(name)\n",
    "            elif grad_norm == 0:\n",
    "                grad_stats['zero_grads'].append(name)\n",
    "            elif grad_norm > 100:\n",
    "                grad_stats['large_grads'].append((name, f\"{grad_norm:.2f}\"))\n",
    "            else:\n",
    "                grad_stats['normal_grads'] += 1\n",
    "    \n",
    "    grad_stats['avg_norm'] = np.mean(grad_norms) if grad_norms else 0\n",
    "    grad_stats['max_norm'] = max(grad_norms) if grad_norms else 0\n",
    "    \n",
    "    return grad_stats\n",
    "\n",
    "print(\"Debugging Utilities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Demo 1: Bisect debugging\n",
    "print(\"\\n1. Bisect Debugging\")\n",
    "print(\"   (Find which component is failing)\")\n",
    "\n",
    "model = SimpleModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "sample_input = torch.randn(4, 10)\n",
    "\n",
    "results = bisect_debug(model, sample_input, optimizer)\n",
    "\n",
    "print()\n",
    "for test_name, result in results.items():\n",
    "    status = \"✓ PASS\" if result['passed'] else f\"✗ FAIL\"\n",
    "    print(f\"  {test_name:20s} {status}\")\n",
    "    if result['error']:\n",
    "        print(f\"    Error: {result['error']}\")\n",
    "\n",
    "print()\n",
    "print(\"  → All steps passed! Model and optimizer working correctly.\")\n",
    "\n",
    "# Demo 2: Gradient checking\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"2. Gradient Health Check\")\n",
    "print(\"   (Make sure gradients are reasonable)\")\n",
    "\n",
    "# Reset and compute gradients\n",
    "model = SimpleModel()\n",
    "x = torch.randn(4, 10)\n",
    "y = model(x)\n",
    "y.sum().backward()\n",
    "\n",
    "grad_stats = check_gradients(model)\n",
    "\n",
    "print()\n",
    "print(f\"  Normal gradients:  {grad_stats['normal_grads']}\")\n",
    "print(f\"  Zero gradients:    {len(grad_stats['zero_grads'])}\")\n",
    "print(f\"  NaN gradients:     {len(grad_stats['nan_grads'])}\")\n",
    "print(f\"  Large gradients:   {len(grad_stats['large_grads'])}\")\n",
    "print()\n",
    "print(f\"  Average magnitude: {grad_stats['avg_norm']:.4f}\")\n",
    "print(f\"  Max magnitude:     {grad_stats['max_norm']:.4f}\")\n",
    "print()\n",
    "print(\"  → Gradients look healthy!\")\n",
    "\n",
    "# Demo 3: Detecting a problem\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"3. Detecting Gradient Problems\")\n",
    "\n",
    "# Inject an issue\n",
    "model.linear1.weight.grad = torch.zeros_like(model.linear1.weight.grad)\n",
    "grad_stats = check_gradients(model)\n",
    "\n",
    "print()\n",
    "print(f\"  After zeroing linear1.weight gradient:\")\n",
    "print(f\"    Zero gradients detected: {grad_stats['zero_grads']}\")\n",
    "print()\n",
    "print(\"  ^ This would indicate linear1.weight isn't being trained!\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"When to Use These Tools:\")\n",
    "print()\n",
    "print(\"  Bisect debugging:\")\n",
    "print(\"    • Training crashes with cryptic error\")\n",
    "print(\"    • Not sure which component is broken\")\n",
    "print(\"    • Want to isolate the problem\")\n",
    "print()\n",
    "print(\"  Gradient checking:\")\n",
    "print(\"    • Loss not decreasing\")\n",
    "print(\"    • Suspicious training behavior\")\n",
    "print(\"    • After making architecture changes\")\n",
    "print()\n",
    "print(\"Add these checks to your training loop during\")\n",
    "print(\"development. Remove them once everything is working.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pre-Flight Checklist\n",
    "\n",
    "**Before you start training, check these:**\n",
    "\n",
    "Think of this like a pilot's pre-flight checklist. Takes two minutes. Catches 90% of problems before they waste hours of training time.\n",
    "\n",
    "### Environment\n",
    "- [ ] PyTorch installed and importable\n",
    "- [ ] GPU accessible (`torch.cuda.is_available()` returns True)\n",
    "- [ ] Enough GPU memory for your batch size\n",
    "- [ ] Correct CUDA/ROCm version\n",
    "\n",
    "### Data\n",
    "- [ ] Dataset loads without errors\n",
    "- [ ] Loss masking is correct (some -100, some token IDs)\n",
    "- [ ] No empty examples in your data\n",
    "- [ ] Tokenization produces reasonable-looking tensors\n",
    "- [ ] Batch shapes are what you expect\n",
    "\n",
    "### Model\n",
    "- [ ] Model loads successfully\n",
    "- [ ] Has trainable parameters (> 0)\n",
    "- [ ] LoRA adapters applied if you intended to use them\n",
    "- [ ] Model moved to GPU\n",
    "- [ ] Forward pass works on sample batch\n",
    "\n",
    "### Optimizer\n",
    "- [ ] Learning rate in reasonable range (1e-6 to 1e-4)\n",
    "- [ ] Optimizer has the parameters you think it does\n",
    "- [ ] Gradient clipping enabled (max_grad_norm=1.0)\n",
    "- [ ] Warmup configured if needed\n",
    "\n",
    "### Training Loop\n",
    "- [ ] Loss is computed correctly\n",
    "- [ ] Gradients are being calculated\n",
    "- [ ] Weights are being updated\n",
    "- [ ] Logging is working\n",
    "\n",
    "### Method-Specific Checks\n",
    "\n",
    "**For DPO:**\n",
    "- [ ] Reference model is frozen\n",
    "- [ ] Beta (KL penalty) is set (typical: 0.1)\n",
    "- [ ] Both policy and reference on same device\n",
    "\n",
    "**For RLHF:**\n",
    "- [ ] Reward model is frozen during policy training\n",
    "- [ ] KL coefficient set appropriately\n",
    "- [ ] Value network separate from policy\n",
    "\n",
    "Run through this list. Find bugs before they waste hours.\n",
    "\n",
    "(I've wasted the hours so you don't have to.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hall of Shame\n",
    "\n",
    "**Most common mistakes, ranked by how much time they waste:**\n",
    "\n",
    "### 1. Learning Rate Too High\n",
    "**Symptom:** Loss becomes NaN  \n",
    "**Time wasted:** 3+ hours before you notice  \n",
    "**Fix:** Reduce LR by 10x, add gradient clipping  \n",
    "**Prevention:** Start conservative (1e-5), increase if needed\n",
    "\n",
    "### 2. Wrong Loss Masking\n",
    "**Symptom:** Model doesn't learn anything useful  \n",
    "**Time wasted:** Could be days before you realize  \n",
    "**Fix:** Print your labels, verify -100 placement  \n",
    "**Prevention:** Always inspect first batch before training\n",
    "\n",
    "### 3. Frozen Model\n",
    "**Symptom:** Loss barely moves  \n",
    "**Time wasted:** However long you wait before checking  \n",
    "**Fix:** Check `requires_grad`, enable if needed  \n",
    "**Prevention:** Print trainable parameter count at startup\n",
    "\n",
    "### 4. Overfitting\n",
    "**Symptom:** Train loss goes down, val loss goes up  \n",
    "**Time wasted:** All epochs past the sweet spot  \n",
    "**Fix:** Use earlier checkpoint, reduce epochs  \n",
    "**Prevention:** Monitor both train and val loss\n",
    "\n",
    "### 5. Reference Not Frozen (DPO/RLHF)\n",
    "**Symptom:** KL divergence explodes  \n",
    "**Time wasted:** Full training run before you notice  \n",
    "**Fix:** Freeze reference model, restart  \n",
    "**Prevention:** Check `requires_grad` on reference\n",
    "\n",
    "### 6. No Gradient Clipping\n",
    "**Symptom:** Training unstable, occasional NaN  \n",
    "**Time wasted:** Multiple failed runs  \n",
    "**Fix:** Add `max_grad_norm=1.0`  \n",
    "**Prevention:** Always enable gradient clipping\n",
    "\n",
    "### 7. Catastrophic Forgetting\n",
    "**Symptom:** Model only speaks your domain language  \n",
    "**Time wasted:** Only noticed during final evaluation  \n",
    "**Fix:** Start over with LoRA or lower LR  \n",
    "**Prevention:** Test general knowledge before and after\n",
    "\n",
    "### 8. Reward Hacking\n",
    "**Symptom:** High rewards, terrible outputs  \n",
    "**Time wasted:** Full RLHF training run  \n",
    "**Fix:** Increase KL penalty, add constraints  \n",
    "**Prevention:** Manually check high-reward samples\n",
    "\n",
    "### 9. Bad Data Quality\n",
    "**Symptom:** Model learns nonsense patterns  \n",
    "**Time wasted:** Could be forever if you don't realize  \n",
    "**Fix:** Clean your data  \n",
    "**Prevention:** Manually inspect training examples\n",
    "\n",
    "### 10. Batch Size Too Large\n",
    "**Symptom:** CUDA out of memory  \n",
    "**Time wasted:** 5 minutes per crash  \n",
    "**Fix:** Reduce batch size, enable gradient checkpointing  \n",
    "**Prevention:** Start small, increase until OOM, then back off\n",
    "\n",
    "## Quick Reference Table\n",
    "\n",
    "| Problem | Symptom | Quick Fix |\n",
    "|---------|---------|-----------|\n",
    "| Loss = NaN | Sudden infinity→NaN | LR ÷ 10, add grad clipping |\n",
    "| Loss stuck | Barely changing | Check trainable params |\n",
    "| Train << Val | Growing gap | Stop early, add regularization |\n",
    "| Model speaks only domain | Failed general knowledge | Use LoRA, lower LR |\n",
    "| KL too high | Divergence > 1.0 | Increase beta, lower LR |\n",
    "| OOM | CUDA memory error | Reduce batch size |\n",
    "\n",
    "Print this table. Tape it to your monitor. Thank me later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Made It!\n",
    "\n",
    "**Congratulations.** You now know how to break and fix transformer training.\n",
    "\n",
    "More importantly, you know how to *debug* it. Because that's the real skill.\n",
    "\n",
    "Anyone can copy a training script and run it. The question is: what do you do when it breaks?\n",
    "\n",
    "Now you know:\n",
    "- How to recognize the seven deadliest pitfalls\n",
    "- How to diagnose what's actually wrong\n",
    "- How to fix it quickly instead of wasting days\n",
    "- How to prevent the problem next time\n",
    "\n",
    "### What You've Learned (The Whole Series)\n",
    "\n",
    "Looking back at this entire fine-tuning section:\n",
    "\n",
    "**SFT:** You learned how to teach a model new behaviors through examples, with proper instruction formatting and loss masking.\n",
    "\n",
    "**Reward Models:** You learned how to capture human preferences in a model that scores responses.\n",
    "\n",
    "**RLHF:** You learned how to use reinforcement learning (PPO) to optimize for those preferences, with all its complexity.\n",
    "\n",
    "**DPO:** You learned a simpler approach that skips RL entirely and optimizes preferences directly.\n",
    "\n",
    "**Advanced Topics:** You learned about memory optimization, hyperparameter tuning, and evaluation metrics.\n",
    "\n",
    "**Debugging:** (This notebook) You learned what goes wrong and how to fix it.\n",
    "\n",
    "That's the full pipeline. From raw model to fine-tuned, preference-aligned, debugged system.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Go try it. Pick a model. Pick a task. Fine-tune something.\n",
    "\n",
    "You'll break things. That's fine. You now know how to fix them.\n",
    "\n",
    "And when you inevitably spend three hours debugging, only to discover you forgot to set `requires_grad=True`?\n",
    "\n",
    "You'll laugh. Print out this notebook. Tape it to your wall.\n",
    "\n",
    "Welcome to the club.\n",
    "\n",
    "---\n",
    "\n",
    "*Check out the Try It notebook if you want hands-on practice with these debugging techniques!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Common pitfalls in post-training: reward hacking, distribution shift, overfitting, and mitigation strategies."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}