{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "**Comprehensive guide to tuning hyperparameters for SFT, DPO, and RLHF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Hyperparameters Matter\n",
    "\n",
    "Hyperparameters can make the difference between a model that:\n",
    "\n",
    "- **Succeeds:** Converges smoothly, achieves strong performance, generalizes well\n",
    "- **Fails:** Diverges, gets stuck, overfits, or forgets pre-training knowledge\n",
    "\n",
    "```\n",
    "Bad hyperparameters:  Loss: 2.5 -> 2.3 -> 2.4 -> 2.6 -> NaN\n",
    "Good hyperparameters: Loss: 2.5 -> 1.8 -> 1.4 -> 1.2 -> 1.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "**The most important hyperparameter.** Too high -> instability. Too low -> slow convergence.\n",
    "\n",
    "### Method-Specific Recommendations\n",
    "\n",
    "**SFT (Supervised Fine-Tuning):**\n",
    "\n",
    "| Setup | Learning Rate | Rationale |\n",
    "|-------|--------------|----------|\n",
    "| Full fine-tuning | 2e-5 to 5e-5 | Model already trained, small updates |\n",
    "| LoRA (r=8-16) | 1e-4 to 5e-4 | LoRA params randomly initialized |\n",
    "| LoRA (r=32-64) | 5e-5 to 2e-4 | Higher capacity, more stable |\n",
    "\n",
    "**DPO (Direct Preference Optimization):**\n",
    "\n",
    "| Setup | Learning Rate | Rationale |\n",
    "|-------|--------------|----------|\n",
    "| Full fine-tuning | 5e-6 to 2e-5 | Very sensitive to preference signal |\n",
    "| LoRA (r=8-16) | 5e-5 to 2e-4 | Stronger updates for random init |\n",
    "\n",
    "**RLHF (PPO):**\n",
    "\n",
    "| Component | Learning Rate | Rationale |\n",
    "|-----------|--------------|----------|\n",
    "| Policy (full FT) | 1e-6 to 5e-6 | Extremely sensitive (RL) |\n",
    "| Policy (LoRA) | 1e-5 to 5e-5 | LoRA more stable |\n",
    "| Value network | 3e-5 to 1e-4 | Separate network, can train faster |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def lr_range_test(model, dataloader, min_lr=1e-6, max_lr=1e-3, steps=100):\n",
    "    \"\"\"\n",
    "    Run learning rate range test to find optimal LR.\n",
    "    \n",
    "    Tests exponentially increasing learning rates and plots loss.\n",
    "    Optimal LR is typically where loss decreases fastest.\n",
    "    \"\"\"\n",
    "    lrs = []\n",
    "    losses = []\n",
    "    \n",
    "    # Exponentially increase LR\n",
    "    lr_mult = (max_lr / min_lr) ** (1 / steps)\n",
    "    lr = min_lr\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= steps:\n",
    "            break\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = model(**batch).loss\n",
    "        \n",
    "        # Record\n",
    "        lrs.append(lr)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Increase learning rate\n",
    "        lr *= lr_mult\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    # Find LR with steepest gradient\n",
    "    gradients = np.gradient(losses)\n",
    "    best_idx = np.argmin(gradients)\n",
    "    optimal_lr = lrs[best_idx]\n",
    "    \n",
    "    print(f\"Suggested learning rate: {optimal_lr:.2e}\")\n",
    "    return optimal_lr, lrs, losses\n",
    "\n",
    "print(\"Learning Rate Range Test:\")\n",
    "print(\"  Run this to find optimal LR for your setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_constant_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Cosine decay (recommended)\n",
    "# scheduler = get_cosine_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=100,\n",
    "#     num_training_steps=1000,\n",
    "# )\n",
    "\n",
    "print(\"Learning Rate Schedule Options:\")\n",
    "print()\n",
    "print(\"1. Cosine with Warmup (Recommended):\")\n",
    "print(\"   0-100 steps:    Linear warmup (0 -> peak_lr)\")\n",
    "print(\"   100-1000 steps: Cosine decay (peak_lr -> 0.1 * peak_lr)\")\n",
    "print()\n",
    "print(\"2. Linear Decay:\")\n",
    "print(\"   0-100 steps:    Linear warmup\")\n",
    "print(\"   100-1000 steps: Linear decay to 0\")\n",
    "print()\n",
    "print(\"Warmup Steps:\")\n",
    "print(\"  Rule of thumb: 5-10% of total training steps\")\n",
    "print(\"  Short training (1000 steps):  warmup = 50-100\")\n",
    "print(\"  Long training (100,000 steps): warmup = 2000-5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size\n",
    "\n",
    "**Effective batch size = batch_size x gradient_accumulation_steps x num_gpus**\n",
    "\n",
    "| Method | Recommended Effective Batch | Typical Physical Batch |\n",
    "|--------|---------------------------|----------------------|\n",
    "| SFT | 32-128 | 8-16 per GPU |\n",
    "| DPO | 16-64 | 4-8 per GPU |\n",
    "| RLHF | 16-64 (rollout), 64-256 (PPO) | 4-8 per GPU |\n",
    "| Reward Model | 32-64 | 8-16 per GPU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size scaling with learning rate\n",
    "def scale_learning_rate(base_lr, base_batch, new_batch, method='sqrt'):\n",
    "    \"\"\"\n",
    "    Scale learning rate when changing batch size.\n",
    "    \n",
    "    Methods:\n",
    "    - 'linear': LR scales proportionally with batch size\n",
    "    - 'sqrt': LR scales with sqrt of batch size (more conservative)\n",
    "    \"\"\"\n",
    "    if method == 'linear':\n",
    "        return base_lr * (new_batch / base_batch)\n",
    "    elif method == 'sqrt':\n",
    "        return base_lr * np.sqrt(new_batch / base_batch)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "# Example\n",
    "base_lr = 3e-4\n",
    "base_batch = 32\n",
    "new_batch = 64\n",
    "\n",
    "new_lr = scale_learning_rate(base_lr, base_batch, new_batch, method='sqrt')\n",
    "print(f\"Base: batch={base_batch}, lr={base_lr}\")\n",
    "print(f\"Scaled: batch={new_batch}, lr={new_lr:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO: Beta Parameter\n",
    "\n",
    "**The key DPO hyperparameter.** Controls how strongly the model deviates from reference.\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma\\left(\\beta \\left[\\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right]\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualize effect of beta\n",
    "margin_range = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for beta in [0.01, 0.1, 0.3, 1.0]:\n",
    "    logits = beta * margin_range\n",
    "    loss = -np.log(1 / (1 + np.exp(-logits)))  # -log sigmoid\n",
    "    plt.plot(margin_range, loss, label=f'beta={beta}')\n",
    "\n",
    "plt.xlabel('Log Ratio Margin (chosen - rejected)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('DPO Loss vs Margin for Different Beta')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Beta Recommendations:\")\n",
    "print(\"  beta=0.01:  Very weak, model changes little\")\n",
    "print(\"  beta=0.1:   Standard (recommended)\")\n",
    "print(\"  beta=0.3:   Strong preference signal\")\n",
    "print(\"  beta=1.0:   Very strong, may overfit\")\n",
    "print()\n",
    "print(\"Dataset Quality -> Beta:\")\n",
    "print(\"  High (human annotated): 0.1-0.3\")\n",
    "print(\"  Medium (AI labeled):    0.05-0.1\")\n",
    "print(\"  Low (heuristic):        0.01-0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF: KL Coefficient\n",
    "\n",
    "**Controls how much the policy can deviate from reference model.**\n",
    "\n",
    "$$\\mathcal{L}_{\\text{RLHF}} = \\mathbb{E}\\left[r(x,y) - \\lambda_{\\text{KL}} \\cdot \\text{KL}(\\pi_\\theta || \\pi_{\\text{ref}})\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveKLController:\n",
    "    \"\"\"Adaptive KL coefficient for RLHF.\"\"\"\n",
    "    \n",
    "    def __init__(self, init_kl_coef=0.1, target_kl=0.01):\n",
    "        self.kl_coef = init_kl_coef\n",
    "        self.target_kl = target_kl\n",
    "    \n",
    "    def update(self, current_kl):\n",
    "        \"\"\"Adjust KL coefficient based on current KL.\"\"\"\n",
    "        if current_kl > 2 * self.target_kl:\n",
    "            # KL too high, increase coefficient\n",
    "            self.kl_coef *= 1.5\n",
    "            print(f\"Increasing KL coef to {self.kl_coef:.3f}\")\n",
    "        elif current_kl < 0.5 * self.target_kl:\n",
    "            # KL too low, decrease coefficient\n",
    "            self.kl_coef *= 0.8\n",
    "            print(f\"Decreasing KL coef to {self.kl_coef:.3f}\")\n",
    "        \n",
    "        return self.kl_coef\n",
    "\n",
    "print(\"KL Coefficient Recommendations:\")\n",
    "print(\"  kl_coef=0.01:  Policy can deviate significantly\")\n",
    "print(\"  kl_coef=0.1:   Balanced (recommended)\")\n",
    "print(\"  kl_coef=0.5:   Strong constraint\")\n",
    "print(\"  kl_coef=2.0:   Very constrained, minimal change\")\n",
    "print()\n",
    "print(\"PPO Hyperparameters:\")\n",
    "print(\"  clip_epsilon=0.2:  Standard value\")\n",
    "print(\"  ppo_epochs=4:      Balanced sample efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA: Rank and Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration guide\n",
    "print(\"LoRA Rank Selection:\")\n",
    "print()\n",
    "print(\"| Model Size | Task Complexity | Rank | Alpha |\")\n",
    "print(\"|------------|-----------------|------|-------|\")\n",
    "print(\"| <1B        | Simple          | 4-8  | 8-16  |\")\n",
    "print(\"| <1B        | Complex         | 8-16 | 16-32 |\")\n",
    "print(\"| 1B-7B      | Simple          | 8-16 | 16-32 |\")\n",
    "print(\"| 1B-7B      | Complex         | 16-32| 32-64 |\")\n",
    "print(\"| 7B+        | Complex         | 32-64| 64-128|\")\n",
    "print()\n",
    "print(\"Standard relationship: alpha = 2 * rank\")\n",
    "print()\n",
    "print(\"Effect of rank:\")\n",
    "print(\"  r=4:   ~0.1% trainable params, may underfit\")\n",
    "print(\"  r=8:   ~0.2% trainable params, good for simple tasks\")\n",
    "print(\"  r=16:  ~0.4% trainable params, standard choice\")\n",
    "print(\"  r=32:  ~0.8% trainable params, complex tasks\")\n",
    "print(\"  r=64:  ~1.6% trainable params, very complex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference configurations\n",
    "print(\"=\"*60)\n",
    "print(\"SFT (Supervised Fine-Tuning) - LoRA\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "SFTConfig(\n",
    "    learning_rate=3e-4,              # Key parameter\n",
    "    batch_size=8,                    # Fit to memory\n",
    "    gradient_accumulation_steps=4,   # Effective batch = 32\n",
    "    num_epochs=3,                    # 3-5 for most datasets\n",
    "    warmup_steps=100,                # 5-10% of training\n",
    "    max_grad_norm=1.0,               # Standard clipping\n",
    "    weight_decay=0.01,               # Standard regularization\n",
    ")\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DPO (Direct Preference Optimization) - LoRA\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "DPOConfig(\n",
    "    learning_rate=5e-5,              # Lower than SFT!\n",
    "    beta=0.1,                        # DPO temperature\n",
    "    batch_size=4,                    # Smaller (chosen+rejected)\n",
    "    gradient_accumulation_steps=8,   # Effective batch = 32\n",
    "    num_epochs=1,                    # Often sufficient\n",
    "    warmup_steps=50,                 # Short warmup\n",
    "    max_grad_norm=1.0,\n",
    ")\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RLHF (PPO) - LoRA\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "RLHFConfig(\n",
    "    policy_lr=1e-5,                  # Very low for stability\n",
    "    value_lr=5e-5,                   # Can be higher\n",
    "    kl_coef=0.1,                     # KL penalty strength\n",
    "    clip_epsilon=0.2,                # PPO clipping\n",
    "    ppo_epochs=4,                    # Optimization epochs\n",
    "    batch_size=4,                    # Small batches\n",
    "    max_grad_norm=0.5,               # Stricter clipping\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Hyperparameter Mistakes\n",
    "\n",
    "### Mistake 1: Using Same LR for All Methods\n",
    "```python\n",
    "# Bad: Same LR everywhere\n",
    "sft_config = SFTConfig(learning_rate=3e-4)      # OK\n",
    "dpo_config = DPOConfig(learning_rate=3e-4)      # Too high!\n",
    "rlhf_config = RLHFConfig(policy_lr=3e-4)        # Way too high!\n",
    "\n",
    "# Good: Method-specific LRs\n",
    "sft_config = SFTConfig(learning_rate=3e-4)\n",
    "dpo_config = DPOConfig(learning_rate=5e-5)\n",
    "rlhf_config = RLHFConfig(policy_lr=1e-5)\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Scaling LR for LoRA\n",
    "```python\n",
    "# Bad: Using full fine-tuning LR with LoRA\n",
    "config = SFTConfig(learning_rate=3e-5)  # Too low for LoRA!\n",
    "\n",
    "# Good: Higher LR for LoRA\n",
    "config = SFTConfig(learning_rate=3e-4)  # 10x higher\n",
    "```\n",
    "\n",
    "### Mistake 3: No Warmup\n",
    "```python\n",
    "# Bad: No warmup (unstable start)\n",
    "config = SFTConfig(warmup_steps=0)\n",
    "\n",
    "# Good: 5-10% warmup\n",
    "config = SFTConfig(warmup_steps=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Quick Start Hyperparameters:**\n",
    "\n",
    "| Method | LR | Batch | Beta/KL | Epochs |\n",
    "|--------|-----|-------|---------|--------|\n",
    "| SFT (LoRA) | 3e-4 | 8x4 | - | 3 |\n",
    "| DPO (LoRA) | 5e-5 | 4x8 | beta=0.1 | 1 |\n",
    "| RLHF (LoRA) | 1e-5 | 4 | KL=0.1 | - |\n",
    "\n",
    "**Tuning Priority:**\n",
    "1. **Learning rate** (biggest impact)\n",
    "2. **Batch size** (stability and speed)\n",
    "3. **Method-specific** (beta, KL coefficient)\n",
    "4. **Secondary** (warmup, weight decay, clipping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now let's explore comprehensive evaluation metrics beyond loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
