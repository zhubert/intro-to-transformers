{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Advanced Hyperparameter Tuning\n\n**Or: How to stop your model from exploding (and other useful tricks)**\n\nYou know what's frustrating? Training a model for 12 hours, coming back with coffee in hand, and seeing \"Loss: NaN\" on your screen.\n\n(Ask me how I know.)\n\nHyperparameters matter. Like, *really* matter. The difference between a hyperparameter configuration that works and one that doesn't isn't subtle—it's the difference between \"my model is learning!\" and \"why is everything on fire?\"\n\nThis notebook is your guide to tuning hyperparameters for SFT, DPO, and RLHF. We'll explain what each knob does, why it matters, and how to avoid the common mistakes that make models sad."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Stakes: Why Hyperparameters Matter\n\nLet's start with a reality check. Here's what can happen with different hyperparameter choices:\n\n**Bad hyperparameters:**\n```\nStep 100:  Loss = 2.5\nStep 200:  Loss = 2.3  (okay, moving in the right direction...)\nStep 300:  Loss = 2.4  (wait, it went up?)\nStep 400:  Loss = 2.6  (this is bad)\nStep 500:  Loss = NaN  (everything is on fire)\n```\n\n**Good hyperparameters:**\n```\nStep 100:  Loss = 2.5\nStep 200:  Loss = 1.8  (nice!)\nStep 300:  Loss = 1.4  (getting better)\nStep 400:  Loss = 1.2  (smooth sailing)\nStep 500:  Loss = 1.1  (we have a winner)\n```\n\nSame model. Same data. Different hyperparameters.\n\nThe good news? You don't need to guess randomly. There are patterns, heuristics, and rules of thumb that work across different models and tasks. That's what we're here to learn."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Learning Rate: The King of Hyperparameters\n\nIf you only tune one hyperparameter, make it the learning rate.\n\n**What is it?** The learning rate controls how big of a step your optimizer takes when updating model weights. Think of it like this: you're hiking down a mountain in the fog (the fog is the loss landscape, stay with me). The learning rate is how big your steps are.\n\n- **Too high:** You take huge steps, overshoot the valley, and bounce around like a pinball. Eventually you might bounce right off the mountain (Loss = NaN).\n- **Too low:** You take tiny, shuffling steps. You're *technically* making progress, but you'll die of old age before reaching the bottom.\n- **Just right:** Nice, confident steps that make steady progress toward the minimum.\n\nHere's the tricky part: the \"right\" learning rate depends on what you're doing.\n\n### Learning Rates for SFT (Supervised Fine-Tuning)\n\nWhen you're doing supervised fine-tuning, your model is already pre-trained. The weights are already pretty good! You just need to nudge them in the right direction. Small, gentle nudges.\n\n| Setup | Learning Rate | Why? |\n|-------|--------------|------|\n| Full fine-tuning | 2e-5 to 5e-5 | Model already knows language, small updates preserve that knowledge |\n| LoRA (r=8-16) | 1e-4 to 5e-4 | LoRA parameters start random, need stronger updates to learn anything |\n| LoRA (r=32-64) | 5e-5 to 2e-4 | Higher capacity = more parameters = more stable training |\n\nNotice something? LoRA uses a *higher* learning rate than full fine-tuning. This trips people up constantly. \"Wait, I thought lower learning rates were safer?\" They are! But LoRA's low-rank matrices start from random initialization. You need a bigger learning rate to get them moving.\n\n### Learning Rates for DPO (Direct Preference Optimization)\n\nDPO is... sensitive. Like, *really* sensitive. You're not teaching the model new tasks, you're adjusting its preferences between similar outputs. This requires a delicate touch.\n\n| Setup | Learning Rate | Why? |\n|-------|--------------|------|\n| Full fine-tuning | 5e-6 to 2e-5 | Tiny changes to preferences, don't want to break the model |\n| LoRA (r=8-16) | 5e-5 to 2e-4 | Random init again, need stronger signal |\n\nNotice DPO's learning rates are roughly 5-10x lower than SFT for the same setup. That's because the preference signal is subtle—if you use a huge learning rate, you'll overfit to your preference data and make the model weird.\n\n### Learning Rates for RLHF (Reinforcement Learning from Human Feedback)\n\nRLHF with PPO is the most sensitive of all. You're doing reinforcement learning now, which is inherently unstable (thanks, policy gradients). Lower learning rates = better sleep at night.\n\n| Component | Learning Rate | Why? |\n|-----------|--------------|------|\n| Policy (full FT) | 1e-6 to 5e-6 | RL is unstable, tiny steps keep things smooth |\n| Policy (LoRA) | 1e-5 to 5e-5 | LoRA more stable, can go a bit higher |\n| Value network | 3e-5 to 1e-4 | Separate network, doesn't affect generation, can train faster |\n\nThe value network gets a higher learning rate because it's a separate network that just predicts values—it doesn't directly affect what your model says, so you can be a bit more aggressive with training it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:25.689209Z",
     "iopub.status.busy": "2025-12-06T23:30:25.689127Z",
     "iopub.status.idle": "2025-12-06T23:30:26.365087Z",
     "shell.execute_reply": "2025-12-06T23:30:26.364700Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport numpy as np\n\ndef lr_range_test(model, dataloader, min_lr=1e-6, max_lr=1e-3, steps=100):\n    \"\"\"\n    Run learning rate range test to find optimal LR.\n    \n    The idea: gradually increase the learning rate and watch what happens to the loss.\n    \n    - At first (low LR): loss decreases slowly\n    - Sweet spot (optimal LR): loss decreases quickly (this is what we want!)\n    - Too high: loss stops decreasing or starts increasing\n    \n    We want to find the LR where the loss is decreasing the fastest.\n    That's usually a good starting point for training.\n    \"\"\"\n    lrs = []\n    losses = []\n    \n    # Exponentially increase LR from min_lr to max_lr\n    lr_mult = (max_lr / min_lr) ** (1 / steps)\n    lr = min_lr\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    \n    for i, batch in enumerate(dataloader):\n        if i >= steps:\n            break\n        \n        # Forward pass\n        loss = model(**batch).loss\n        \n        # Record\n        lrs.append(lr)\n        losses.append(loss.item())\n        \n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Increase learning rate for next iteration\n        lr *= lr_mult\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n    \n    # Find LR with steepest negative gradient (fastest loss decrease)\n    gradients = np.gradient(losses)\n    best_idx = np.argmin(gradients)\n    optimal_lr = lrs[best_idx]\n    \n    print(f\"Suggested learning rate: {optimal_lr:.2e}\")\n    print(f\"  (This is where loss decreased fastest)\")\n    return optimal_lr, lrs, losses\n\n# Example of how to use it:\nprint(\"Learning Rate Range Test\")\nprint(\"=\" * 50)\nprint()\nprint(\"This is a handy tool when you're not sure what LR to use.\")\nprint(\"Run it on your data and model, and it'll suggest a good starting point.\")\nprint()\nprint(\"Usage:\")\nprint(\"  optimal_lr, lrs, losses = lr_range_test(model, dataloader)\")\nprint(\"  plt.plot(lrs, losses)\")\nprint(\"  plt.xscale('log')\")\nprint(\"  plt.show()\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Learning Rate Schedules: The Journey Matters\n\nOkay, so you've picked a learning rate. Great! But here's the thing: you probably don't want to use the *same* learning rate for the entire training run.\n\nThink about it like driving. When you're pulling out of your driveway, you start slow (warmup). Then you accelerate to highway speed (peak learning rate). As you approach your destination, you slow down (decay). You don't just slam the gas pedal and hold it there the whole time.\n\n(Well, you *could*, but that's how you end up overshooting your exit.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:26.381716Z",
     "iopub.status.busy": "2025-12-06T23:30:26.381613Z",
     "iopub.status.idle": "2025-12-06T23:30:26.867300Z",
     "shell.execute_reply": "2025-12-06T23:30:26.866916Z"
    }
   },
   "outputs": [],
   "source": "from transformers import (\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    get_constant_schedule_with_warmup\n)\n\nprint(\"Learning Rate Schedules: Your Options\")\nprint(\"=\" * 60)\nprint()\n\nprint(\"1. COSINE WITH WARMUP (Recommended)\")\nprint(\"   This is the standard choice. Here's what happens:\")\nprint()\nprint(\"   Steps 0-100:      Linear warmup (0 → peak_lr)\")\nprint(\"                     Gently ease into training, stabilizes early steps\")\nprint()\nprint(\"   Steps 100-1000:   Cosine decay (peak_lr → ~0.1 × peak_lr)\")\nprint(\"                     Smooth slowdown as we approach the end\")\nprint()\nprint(\"   Why cosine? It's smooth (no sudden changes) and it\")\nprint(\"   naturally slows down as you get closer to convergence.\")\nprint()\n\nprint(\"-\" * 60)\nprint()\n\nprint(\"2. LINEAR DECAY\")\nprint(\"   Steps 0-100:      Linear warmup\")\nprint(\"   Steps 100-1000:   Linear decay to 0\")\nprint()\nprint(\"   This works, but the constant decay rate can be\")\nprint(\"   too aggressive early on and too gentle later.\")\nprint()\n\nprint(\"-\" * 60)\nprint()\n\nprint(\"3. CONSTANT WITH WARMUP\")\nprint(\"   Steps 0-100:      Linear warmup\")\nprint(\"   Steps 100+:       Constant at peak_lr\")\nprint()\nprint(\"   Use this for short training runs where you don't\")\nprint(\"   want the LR to decay. Less common these days.\")\nprint()\n\nprint(\"=\" * 60)\nprint()\nprint(\"WARMUP: How many steps?\")\nprint()\nprint(\"Rule of thumb: 5-10% of total training steps\")\nprint()\nprint(\"Examples:\")\nprint(\"  Short training (1,000 steps):      warmup = 50-100\")\nprint(\"  Medium training (10,000 steps):    warmup = 500-1,000\")\nprint(\"  Long training (100,000 steps):     warmup = 2,000-5,000\")\nprint()\nprint(\"Why warmup? At the start of training, gradients can be\")\nprint(\"unstable. Starting with a low LR and ramping up gives\")\nprint(\"the model a chance to stabilize before taking big steps.\")\nprint()\nprint(\"Code example:\")\nprint(\"  scheduler = get_cosine_schedule_with_warmup(\")\nprint(\"      optimizer,\")\nprint(\"      num_warmup_steps=100,\")\nprint(\"      num_training_steps=1000,\")\nprint(\"  )\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Batch Size: More Than Just a Memory Constraint\n\nPeople think of batch size as \"how much fits in my GPU.\" And sure, that's part of it. But batch size actually affects how your model learns.\n\n**What is batch size?** It's how many examples you process before updating the model's weights. A batch size of 8 means: look at 8 examples, compute gradients for all of them, average those gradients together, then take one optimization step.\n\n### The Effective Batch Size\n\nHere's where it gets interesting. The *effective* batch size is what actually matters:\n\n```\neffective_batch_size = batch_size × gradient_accumulation_steps × num_gpus\n```\n\n**Gradient accumulation** is a clever trick. If you can only fit 4 examples in memory, but you want an effective batch of 32, you can:\n1. Process 4 examples, compute gradients (but don't update weights yet)\n2. Process 4 more examples, add their gradients to the first batch\n3. Repeat 8 times total (4 × 8 = 32)\n4. *Now* update the weights\n\nSame effective batch size, fits in less memory. Neat!\n\n### Recommended Batch Sizes\n\n| Method | Effective Batch | Typical Physical Batch | Why? |\n|--------|----------------|----------------------|------|\n| SFT | 32-128 | 8-16 per GPU | Stable gradients, good throughput |\n| DPO | 16-64 | 4-8 per GPU | Each example is chosen+rejected pair (2x memory!) |\n| RLHF | 16-64 | 4-8 per GPU | RL is unstable, smaller batches help |\n| Reward Model | 32-64 | 8-16 per GPU | Comparison task, benefits from variety |\n\nNotice DPO uses smaller batches. That's because each training example is actually *two* completions (the chosen one and the rejected one), so it takes twice the memory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:26.868248Z",
     "iopub.status.busy": "2025-12-06T23:30:26.868138Z",
     "iopub.status.idle": "2025-12-06T23:30:26.870248Z",
     "shell.execute_reply": "2025-12-06T23:30:26.869998Z"
    }
   },
   "outputs": [],
   "source": "import numpy as np\n\ndef scale_learning_rate(base_lr, base_batch, new_batch, method='sqrt'):\n    \"\"\"\n    Scale learning rate when changing batch size.\n    \n    Here's the deal: if you change your batch size, you should probably\n    change your learning rate too.\n    \n    Why? Larger batches give smoother (less noisy) gradients. Smoother\n    gradients mean you can take bigger steps without things going haywire.\n    \n    Two schools of thought:\n    \n    1. LINEAR SCALING: If you double the batch size, double the LR\n       - Theory: You're doing the same amount of work, just in bigger chunks\n       - Practice: Can be aggressive, sometimes causes instability\n    \n    2. SQRT SCALING: If you double the batch size, multiply LR by √2 ≈ 1.4\n       - Theory: Accounts for noise reduction without being too aggressive\n       - Practice: More conservative, usually safer\n    \n    When in doubt, use sqrt scaling. It's the safer bet.\n    \"\"\"\n    if method == 'linear':\n        scaled_lr = base_lr * (new_batch / base_batch)\n        print(f\"  Using linear scaling: LR × {new_batch / base_batch:.2f}\")\n    elif method == 'sqrt':\n        scaling_factor = np.sqrt(new_batch / base_batch)\n        scaled_lr = base_lr * scaling_factor\n        print(f\"  Using sqrt scaling: LR × {scaling_factor:.2f}\")\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    \n    return scaled_lr\n\n# Example: You found a good LR for batch size 32, but now you want to use batch size 64\nprint(\"Batch Size → Learning Rate Scaling\")\nprint(\"=\" * 60)\nprint()\n\nbase_lr = 3e-4\nbase_batch = 32\n\nprint(f\"Starting point: batch_size={base_batch}, lr={base_lr:.2e}\")\nprint()\n\nfor new_batch in [64, 128]:\n    print(f\"Scaling to batch_size={new_batch}:\")\n    \n    new_lr_sqrt = scale_learning_rate(base_lr, base_batch, new_batch, method='sqrt')\n    print(f\"    → Sqrt scaling:   lr={new_lr_sqrt:.2e}\")\n    \n    new_lr_linear = scale_learning_rate(base_lr, base_batch, new_batch, method='linear')\n    print(f\"    → Linear scaling: lr={new_lr_linear:.2e}\")\n    print()\n\nprint(\"Recommendation: Start with sqrt scaling. If training is too slow,\")\nprint(\"try linear scaling (but watch for instability).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## DPO Beta: The Preference Strength Knob\n\nAlright, time for the DPO-specific hyperparameter that causes the most confusion: **beta**.\n\nRemember the DPO loss function? Here it is:\n\n$$\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma\\left(\\beta \\left[\\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right]\\right)$$\n\nThat $\\beta$ controls how strongly you push the model to prefer the chosen response over the rejected one.\n\n**What does beta do?**\n\n- **Low beta (0.01-0.05):** Gentle nudges. \"Hey model, maybe prefer this response a *little* bit more?\"\n- **Medium beta (0.1-0.3):** Standard. \"I'd like you to clearly prefer the chosen response.\"\n- **High beta (0.5-1.0):** Aggressive. \"YOU WILL PREFER THIS RESPONSE OR ELSE.\"\n\nHigher beta = stronger preference signal = more risk of overfitting to your preference data.\n\n### How to Choose Beta\n\nThe key question: **How much do you trust your preference data?**\n\n| Data Quality | Beta Range | Reasoning |\n|-------------|-----------|-----------|\n| High (human annotations, clear preferences) | 0.1-0.3 | Trust your data, use it! |\n| Medium (AI-labeled, decent quality) | 0.05-0.1 | Some noise, be gentle |\n| Low (heuristics, noisy labels) | 0.01-0.05 | Don't overfit to noise |\n\nIf your preference data is \"Response A uses proper grammar and Response B is gibberish,\" you can use a higher beta. If your preference data is \"Response A is slightly better according to this AI judge that might be hallucinating,\" maybe go lower."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:26.870996Z",
     "iopub.status.busy": "2025-12-06T23:30:26.870922Z",
     "iopub.status.idle": "2025-12-06T23:30:27.091527Z",
     "shell.execute_reply": "2025-12-06T23:30:27.091184Z"
    }
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Visualize the effect of beta on the loss function\nmargin_range = np.linspace(-5, 5, 100)\n\nplt.figure(figsize=(12, 6))\n\n# Plot DPO loss for different beta values\nfor beta in [0.01, 0.1, 0.3, 1.0]:\n    # The margin is: log(π/π_ref) for chosen - log(π/π_ref) for rejected\n    # Positive margin = model prefers chosen (good!)\n    # Negative margin = model prefers rejected (bad!)\n    \n    logits = beta * margin_range\n    loss = -np.log(1 / (1 + np.exp(-logits)))  # This is -log(sigmoid(logits))\n    plt.plot(margin_range, loss, label=f'beta={beta}', linewidth=2)\n\nplt.xlabel('Log Ratio Margin (chosen - rejected)', fontsize=12)\nplt.ylabel('DPO Loss', fontsize=12)\nplt.title('How Beta Affects DPO Loss', fontsize=14, fontweight='bold')\nplt.axvline(x=0, color='black', linestyle='--', alpha=0.3, label='No preference')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"What this plot tells us:\")\nprint(\"=\" * 60)\nprint()\nprint(\"X-axis (margin): How much the model prefers chosen over rejected\")\nprint(\"  Positive = model prefers chosen (good!)\")\nprint(\"  Negative = model prefers rejected (bad!)\")\nprint()\nprint(\"Y-axis (loss): How much we penalize the model\")\nprint()\nprint(\"Notice:\")\nprint(\"  • Higher beta = steeper curve = stronger penalty for mistakes\")\nprint(\"  • beta=0.01 is almost flat (gentle corrections)\")\nprint(\"  • beta=1.0 is very steep (harsh penalties)\")\nprint()\nprint(\"Rule of thumb:\")\nprint(\"  Start with beta=0.1\")\nprint(\"  If model isn't learning preferences → increase beta\")\nprint(\"  If model is overfitting to preferences → decrease beta\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RLHF: The KL Divergence Leash\n\nIf you're doing RLHF (reinforcement learning from human feedback) with PPO, you have a different problem: your policy model might go completely off the rails chasing reward.\n\nImagine you have a reward model that gives high scores for helpful responses. Without any constraints, your policy might discover that repeating \"This is helpful! This is helpful! This is helpful!\" 500 times gets a high reward score. Technically the reward model likes it... but it's not actually useful.\n\n(This happens. Like, a lot.)\n\n**The KL divergence constraint** is your safety leash. It penalizes the policy for deviating too far from the reference model (usually the SFT model):\n\n$$\\mathcal{L}_{\\text{RLHF}} = \\mathbb{E}\\left[r(x,y) - \\lambda_{\\text{KL}} \\cdot \\text{KL}(\\pi_\\theta || \\pi_{\\text{ref}})\\right]$$\n\nThat $\\lambda_{\\text{KL}}$ (often called `kl_coef`) controls how tight the leash is.\n\n- **Low KL coef (0.01):** Long leash. Policy can wander far from reference. Risky!\n- **Medium KL coef (0.1):** Standard leash. Balanced exploration and safety.\n- **High KL coef (0.5-2.0):** Short leash. Policy stays close to reference. Safe but limiting.\n\nThe goal is to improve on the reference model without going weird. Too loose and you get gibberish. Too tight and you don't improve at all."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:27.092409Z",
     "iopub.status.busy": "2025-12-06T23:30:27.092309Z",
     "iopub.status.idle": "2025-12-06T23:30:27.094657Z",
     "shell.execute_reply": "2025-12-06T23:30:27.094356Z"
    }
   },
   "outputs": [],
   "source": "class AdaptiveKLController:\n    \"\"\"\n    Adaptive KL coefficient for RLHF.\n    \n    The clever trick: instead of picking a fixed KL coefficient and hoping\n    it's right, we can adapt it during training based on what's happening.\n    \n    The idea:\n    - Set a target KL divergence (how much deviation you're comfortable with)\n    - If actual KL is too high → increase the coefficient (tighten the leash)\n    - If actual KL is too low → decrease the coefficient (loosen the leash)\n    \n    This way, the KL coefficient automatically adjusts to keep you in the\n    sweet spot between \"too conservative\" and \"completely unhinged.\"\n    \"\"\"\n    \n    def __init__(self, init_kl_coef=0.1, target_kl=0.01):\n        self.kl_coef = init_kl_coef      # Starting coefficient\n        self.target_kl = target_kl        # Target KL divergence\n        print(f\"Initialized adaptive KL controller:\")\n        print(f\"  Starting KL coefficient: {init_kl_coef}\")\n        print(f\"  Target KL divergence: {target_kl}\")\n    \n    def update(self, current_kl):\n        \"\"\"\n        Adjust KL coefficient based on current KL divergence.\n        \n        If KL is too high (> 2x target), we're deviating too much → increase penalty\n        If KL is too low (< 0.5x target), we're being too conservative → decrease penalty\n        \"\"\"\n        if current_kl > 2 * self.target_kl:\n            # Model is diverging too much from reference\n            self.kl_coef *= 1.5\n            print(f\"  KL too high ({current_kl:.4f} > {2*self.target_kl:.4f})\")\n            print(f\"  → Increasing KL coef to {self.kl_coef:.3f} (tightening leash)\")\n        elif current_kl < 0.5 * self.target_kl:\n            # Model is too conservative\n            self.kl_coef *= 0.8\n            print(f\"  KL too low ({current_kl:.4f} < {0.5*self.target_kl:.4f})\")\n            print(f\"  → Decreasing KL coef to {self.kl_coef:.3f} (loosening leash)\")\n        else:\n            print(f\"  KL just right ({current_kl:.4f}), keeping coef at {self.kl_coef:.3f}\")\n        \n        return self.kl_coef\n\n# Demo of how this works\nprint(\"=\" * 60)\nprint(\"Adaptive KL Controller Demo\")\nprint(\"=\" * 60)\nprint()\n\ncontroller = AdaptiveKLController(init_kl_coef=0.1, target_kl=0.01)\nprint()\n\n# Simulate some training steps\nprint(\"Simulating training:\")\nprint(\"-\" * 60)\n\n# Step 1: KL too high (model diverging)\nprint(\"\\nStep 1:\")\ncontroller.update(current_kl=0.025)\n\n# Step 2: Still too high\nprint(\"\\nStep 2:\")\ncontroller.update(current_kl=0.030)\n\n# Step 3: Now it's better\nprint(\"\\nStep 3:\")\ncontroller.update(current_kl=0.012)\n\n# Step 4: Maybe too low now\nprint(\"\\nStep 4:\")\ncontroller.update(current_kl=0.003)\n\nprint()\nprint(\"=\" * 60)\nprint()\nprint(\"Other important RLHF hyperparameters:\")\nprint()\nprint(\"  clip_epsilon (usually 0.2):\")\nprint(\"    PPO clips policy updates to prevent huge changes.\")\nprint(\"    Standard value is 0.2, don't mess with it unless you\")\nprint(\"    know what you're doing.\")\nprint()\nprint(\"  ppo_epochs (usually 4):\")\nprint(\"    How many optimization passes over each batch.\")\nprint(\"    More epochs = more sample efficient, but risk overfitting.\")\nprint(\"    4 is a good balance.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LoRA: Rank and Alpha\n\nWhen you're using LoRA (Low-Rank Adaptation), you have two key hyperparameters to tune: **rank** (r) and **alpha**.\n\nQuick refresher: LoRA doesn't update the original model weights. Instead, it learns low-rank matrices that get added to the original weights. The rank determines how \"expressive\" these low-rank matrices can be.\n\n**Rank (r):** The dimensionality of the low-rank decomposition\n- Higher rank = more parameters = more capacity = can learn more complex adaptations\n- Lower rank = fewer parameters = less capacity = faster and more memory-efficient\n\n**Alpha:** A scaling factor applied to the LoRA updates\n- Think of it as the \"learning strength\" of the LoRA layers\n- Standard relationship: `alpha = 2 × rank`\n\nHere's the thing: picking the right rank is a balancing act. Too low and your model can't learn the task (underfitting). Too high and you're wasting memory and computation without much benefit.\n\nThe sweet spot depends on your model size and task complexity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:27.095310Z",
     "iopub.status.busy": "2025-12-06T23:30:27.095240Z",
     "iopub.status.idle": "2025-12-06T23:30:27.097169Z",
     "shell.execute_reply": "2025-12-06T23:30:27.096887Z"
    }
   },
   "outputs": [],
   "source": "# LoRA rank selection guide\nprint(\"LoRA Rank Selection Guide\")\nprint(\"=\" * 70)\nprint()\nprint(\"Model Size  Task Complexity  Rank   Alpha   % Trainable Params\")\nprint(\"-\" * 70)\nprint(\"<1B         Simple           4-8    8-16    ~0.1-0.2%\")\nprint(\"<1B         Complex          8-16   16-32   ~0.2-0.4%\")\nprint(\"1B-7B       Simple           8-16   16-32   ~0.2-0.4%\")\nprint(\"1B-7B       Complex          16-32  32-64   ~0.4-0.8%\")\nprint(\"7B+         Complex          32-64  64-128  ~0.8-1.6%\")\nprint()\nprint(\"Standard relationship: alpha = 2 × rank\")\nprint()\nprint(\"-\" * 70)\nprint()\n\nprint(\"What do these ranks actually mean?\")\nprint()\nprint(\"  r=4:   MINIMAL capacity\")\nprint(\"         Good for: Very simple tasks (format changes, style tweaks)\")\nprint(\"         ~0.1% of full model parameters\")\nprint(\"         Example: Teaching the model to use a specific format\")\nprint()\nprint(\"  r=8:   LOW capacity\")\nprint(\"         Good for: Simple domain adaptation, basic instructions\")\nprint(\"         ~0.2% of full model parameters\")\nprint(\"         Example: Fine-tuning for customer service responses\")\nprint()\nprint(\"  r=16:  STANDARD capacity\")\nprint(\"         Good for: Most tasks, general-purpose fine-tuning\")\nprint(\"         ~0.4% of full model parameters\")\nprint(\"         Example: Domain-specific QA, instruction following\")\nprint(\"         → This is the default you should probably start with\")\nprint()\nprint(\"  r=32:  HIGH capacity\")\nprint(\"         Good for: Complex reasoning, specialized domains\")\nprint(\"         ~0.8% of full model parameters\")\nprint(\"         Example: Medical/legal QA, complex code generation\")\nprint()\nprint(\"  r=64:  VERY HIGH capacity\")\nprint(\"         Good for: When r=32 isn't enough (rare)\")\nprint(\"         ~1.6% of full model parameters\")\nprint(\"         Example: Highly specialized scientific tasks\")\nprint()\n\nprint(\"=\" * 70)\nprint()\nprint(\"Pro tip: Start with r=16, alpha=32\")\nprint()\nprint(\"  • Too easy → model converges too fast → try lower rank\")\nprint(\"  • Too hard → model struggles to learn → try higher rank\")\nprint()\nprint(\"Most of the time, r=16 is perfectly fine. Don't overthink it!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Hyperparameter Cheat Sheet: Your Quick Reference\n\nAlright, we've covered a lot. Let's consolidate this into some practical, copy-pasteable configurations that actually work.\n\nThese are battle-tested starting points. Will they be perfect for your exact use case? Probably not. But they'll get you 90% of the way there, and you can tune from there."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:27.097806Z",
     "iopub.status.busy": "2025-12-06T23:30:27.097738Z",
     "iopub.status.idle": "2025-12-06T23:30:27.099677Z",
     "shell.execute_reply": "2025-12-06T23:30:27.099415Z"
    }
   },
   "outputs": [],
   "source": "# Quick reference configurations that actually work\nprint(\"=\" * 70)\nprint(\"SFT (Supervised Fine-Tuning) with LoRA\")\nprint(\"=\" * 70)\nprint(\"\"\"\nSFTConfig(\n    learning_rate=3e-4,              # Higher for LoRA (random init)\n    batch_size=8,                    # Fits most GPUs\n    gradient_accumulation_steps=4,   # → effective batch = 32\n    num_epochs=3,                    # 3-5 for most datasets\n    warmup_steps=100,                # ~10% of training steps\n    lr_scheduler_type=\"cosine\",      # Smooth decay\n    max_grad_norm=1.0,               # Gradient clipping\n    weight_decay=0.01,               # Standard regularization\n)\n\n# LoRA config:\n# - rank=16, alpha=32 for most tasks\n# - rank=32, alpha=64 for complex tasks\n# - rank=8, alpha=16 if memory-constrained\n\"\"\")\n\nprint(\"=\" * 70)\nprint(\"DPO (Direct Preference Optimization) with LoRA\")\nprint(\"=\" * 70)\nprint(\"\"\"\nDPOConfig(\n    learning_rate=5e-5,              # MUCH lower than SFT!\n    beta=0.1,                        # Standard preference strength\n    batch_size=4,                    # Each example = chosen + rejected\n    gradient_accumulation_steps=8,   # → effective batch = 32\n    num_epochs=1,                    # Usually sufficient for DPO\n    warmup_steps=50,                 # Shorter warmup\n    lr_scheduler_type=\"cosine\",\n    max_grad_norm=1.0,\n)\n\n# Beta tuning:\n# - Start with 0.1\n# - Increase if model isn't learning preferences\n# - Decrease if model is overfitting to preferences\n\"\"\")\n\nprint(\"=\" * 70)\nprint(\"RLHF (PPO) with LoRA\")\nprint(\"=\" * 70)\nprint(\"\"\"\nRLHFConfig(\n    # Policy network (the language model)\n    policy_lr=1e-5,                  # Very low for stability\n    \n    # Value network (predicts rewards)\n    value_lr=5e-5,                   # Can be higher\n    \n    # KL divergence penalty\n    kl_coef=0.1,                     # Keeps policy near reference\n    target_kl=0.01,                  # Target KL divergence\n    \n    # PPO-specific\n    clip_epsilon=0.2,                # Standard PPO clipping\n    ppo_epochs=4,                    # Optimization passes per batch\n    \n    # Training\n    batch_size=4,                    # Small for RL stability\n    gradient_accumulation_steps=8,\n    max_grad_norm=0.5,               # Stricter clipping for RL\n)\n\n# Use adaptive KL coefficient (see AdaptiveKLController above)\n# to automatically adjust kl_coef during training\n\"\"\")\n\nprint(\"=\" * 70)\nprint()\nprint(\"Quick decision guide:\")\nprint()\nprint(\"  1. Pick your method (SFT, DPO, or RLHF)\")\nprint(\"  2. Start with the config above\")\nprint(\"  3. Adjust batch size to fit your GPU\")\nprint(\"  4. If you change batch size, scale learning rate (see earlier)\")\nprint(\"  5. Run a few hundred steps and watch the loss\")\nprint(\"     • Loss going down smoothly? Great, keep going!\")\nprint(\"     • Loss exploding (NaN)? Lower learning rate\")\nprint(\"     • Loss barely moving? Increase learning rate\")\nprint(\"  6. Tune method-specific params (beta for DPO, kl_coef for RLHF)\")\nprint()\nprint(\"That's it. Don't overthink it. These configs work.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Common Mistakes (And How to Avoid Them)\n\nLet's talk about the mistakes I see people make over and over again. Learn from our collective pain.\n\n### Mistake #1: Using the Same Learning Rate Everywhere\n\nThis is the big one. People find a learning rate that works for SFT and then use it for everything.\n\n**Bad:**\n```python\n# Nooooo!\nsft_config = SFTConfig(learning_rate=3e-4)      # ✓ Good for SFT\ndpo_config = DPOConfig(learning_rate=3e-4)      # ✗ Way too high!\nrlhf_config = RLHFConfig(policy_lr=3e-4)        # ✗ Model will explode!\n```\n\n**Good:**\n```python\n# Yes! Method-specific learning rates\nsft_config = SFTConfig(learning_rate=3e-4)      # ✓ Standard for SFT\ndpo_config = DPOConfig(learning_rate=5e-5)      # ✓ 5-10x lower\nrlhf_config = RLHFConfig(policy_lr=1e-5)        # ✓ Even lower for RL\n```\n\n**Why this matters:** DPO and RLHF are much more sensitive than SFT. A learning rate that works fine for supervised learning will send your preference model into the stratosphere.\n\n---\n\n### Mistake #2: Not Scaling Learning Rate for LoRA\n\nLoRA uses randomly initialized matrices. They need a higher learning rate than pre-trained weights.\n\n**Bad:**\n```python\n# Using full fine-tuning LR with LoRA\nconfig = SFTConfig(learning_rate=3e-5)  # ✗ Too low! LoRA will barely learn\n```\n\n**Good:**\n```python\n# Higher LR for LoRA's random initialization\nconfig = SFTConfig(learning_rate=3e-4)  # ✓ About 10x higher than full FT\n```\n\n**Why this matters:** With a too-low learning rate, LoRA will technically train, but it'll take forever and might not reach good performance. You'll waste hours wondering why your model isn't learning.\n\n---\n\n### Mistake #3: Skipping Warmup\n\nStarting training with a high learning rate is like flooring the accelerator when your car is still in the driveway. Recipe for chaos.\n\n**Bad:**\n```python\nconfig = SFTConfig(\n    learning_rate=3e-4,\n    warmup_steps=0,  # ✗ No warmup = unstable start\n)\n```\n\n**Good:**\n```python\nconfig = SFTConfig(\n    learning_rate=3e-4,\n    warmup_steps=100,  # ✓ 5-10% of training steps\n)\n```\n\n**Why this matters:** At the start of training, gradients can be large and unpredictable. Warmup gives the model a chance to stabilize before taking big steps. It's especially important for LoRA, where those random matrices start with crazy gradients.\n\n---\n\n### Mistake #4: Ignoring Effective Batch Size\n\n\"My batch size is 4\" is not complete information. The *effective* batch size is what matters.\n\n**Misleading:**\n```python\n# These look the same, but they're not!\nconfig_a = SFTConfig(batch_size=32, gradient_accumulation_steps=1)\nconfig_b = SFTConfig(batch_size=4, gradient_accumulation_steps=8)\n```\n\nBoth have effective batch size = 32, but:\n- Config A: Process 32 examples at once (high memory, one update)\n- Config B: Process 4 examples at a time, 8 accumulation steps (low memory, one update)\n\n**Why this matters:** If you report \"I used batch size 4\" without mentioning gradient accumulation, others can't reproduce your results. Always think in terms of effective batch size.\n\n---\n\n### Mistake #5: Cranking Up Beta Without Good Data\n\nHigh beta with noisy preference data is a disaster waiting to happen.\n\n**Bad:**\n```python\n# AI-generated preference labels (might be noisy)\ndpo_config = DPOConfig(\n    beta=1.0,  # ✗ Too aggressive for noisy data\n)\n```\n\n**Good:**\n```python\n# AI-generated preference labels (might be noisy)\ndpo_config = DPOConfig(\n    beta=0.05,  # ✓ Gentle, won't overfit to noise\n)\n```\n\n**Why this matters:** Beta amplifies your preference signal. If that signal is noisy, high beta means \"strongly overfit to garbage.\" Start low, increase if needed.\n\n---\n\nThe pattern here? Most mistakes come from copy-pasting configs without understanding what the hyperparameters actually do. Now you know better."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Your Hyperparameter Tuning Workflow\n\nLet's bring this all together into a practical workflow.\n\n**Step 1: Pick Your Starting Point**\n\n| Method | Learning Rate | Batch Size (effective) | Key Parameter |\n|--------|--------------|----------------------|---------------|\n| SFT (LoRA) | 3e-4 | 32-128 | rank=16 |\n| DPO (LoRA) | 5e-5 | 16-64 | beta=0.1 |\n| RLHF (LoRA) | 1e-5 | 16-64 | kl_coef=0.1 |\n\n**Step 2: Adjust for Your Hardware**\n- Fit batch size to your GPU memory\n- Use gradient accumulation to reach target effective batch\n- Scale learning rate if you significantly change batch size (use sqrt scaling)\n\n**Step 3: Set Warmup and Schedule**\n- Warmup steps = 5-10% of total training steps\n- Use cosine decay (it just works)\n- For short runs (<1000 steps), constant LR is fine\n\n**Step 4: Start Training and Watch Closely**\n- Check loss after 50-100 steps\n- Loss smoothly decreasing? You're good!\n- Loss stuck? Increase learning rate\n- Loss exploding? Decrease learning rate\n- Weird oscillations? Reduce batch size or increase warmup\n\n**Step 5: Tune Method-Specific Parameters**\n- DPO: Adjust beta based on model behavior and data quality\n- RLHF: Use adaptive KL controller, watch for model collapse\n- LoRA: If model struggles, try higher rank\n\n**Tuning Priority:**\n1. **Learning rate** ← Start here! Biggest impact\n2. **Batch size** ← For stability and speed\n3. **Method-specific** ← Beta for DPO, KL for RLHF\n4. **Secondary** ← Warmup, weight decay, clipping\n\nRemember: these hyperparameters aren't magic incantations. They're knobs that control how your model learns. Understanding what each knob does means you can tune intelligently instead of randomly."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What's Next?\n\nNow you know how to tune hyperparameters without setting everything on fire. Nice!\n\nBut here's the thing: hyperparameters only matter if you can measure whether they're working. And \"loss going down\" is just the start—there are much better ways to evaluate whether your model is actually getting better at the thing you care about.\n\nThat's what we'll cover next: comprehensive evaluation metrics that go beyond loss."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}