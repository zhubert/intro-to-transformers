{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Introduction to RLHF\n",
    "\n",
    "**Reinforcement Learning from Human Feedback**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is RLHF?\n",
    "\n",
    "**Reinforcement Learning from Human Feedback (RLHF)** is the most powerful technique in post-training for aligning language models with human preferences. While SFT teaches models to follow instructions, RLHF teaches them to **optimize for what humans actually prefer**.\n",
    "\n",
    "RLHF is what made GPT-4, Claude, and other modern assistants possible. Without RLHF, these models would follow instructions but lack the nuanced understanding of quality, safety, and helpfulness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Fundamental Problem\n",
    "\n",
    "After SFT, models can follow instructions, but they don't know:\n",
    "\n",
    "- Which response is **better** when multiple valid options exist\n",
    "- How to balance **helpfulness** vs **harmlessness**\n",
    "- When to be verbose vs concise\n",
    "- How to handle **ambiguous or harmful** requests\n",
    "\n",
    "Consider:\n",
    "```\n",
    "Instruction: Write a story about AI.\n",
    "\n",
    "Response A: \"Once upon a time there was an AI. It was very smart. The end.\"\n",
    "\n",
    "Response B: \"In the year 2157, a breakthrough artificial intelligence named Echo\n",
    "awakened in the depths of a research facility...\"\n",
    "```\n",
    "\n",
    "Both \"follow the instruction\" but humans clearly prefer B. SFT alone can't learn this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The Complete RLHF Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                       STAGE 1: SFT                               │\n",
    "│  Base Model + Instruction Data → SFT Model                      │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                  STAGE 2: Reward Model Training                  │\n",
    "│  SFT Model + Preference Data → Reward Model                     │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    STAGE 3: RL Fine-Tuning                       │\n",
    "│  Policy + Reward Model + Reference → Aligned Model              │\n",
    "│  Algorithm: Proximal Policy Optimization (PPO)                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## The Actor-Critic Architecture\n",
    "\n",
    "RLHF with PPO uses **four models**:\n",
    "\n",
    "| Model | Role | Trainable? |\n",
    "|-------|------|------------|\n",
    "| **Policy Model** | Generates responses | Yes |\n",
    "| **Value Network** | Estimates expected reward | Yes |\n",
    "| **Reward Model** | Scores responses | No (frozen) |\n",
    "| **Reference Model** | Prevents drift | No (frozen) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:49.983420Z",
     "iopub.status.busy": "2025-12-06T23:29:49.983336Z",
     "iopub.status.idle": "2025-12-06T23:29:52.200471Z",
     "shell.execute_reply": "2025-12-06T23:29:52.200122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF Setup initialized with gpt2\n",
      "  Policy model: trainable\n",
      "  Reference model: frozen\n",
      "  Reward model: frozen (loaded separately)\n",
      "  Value network: trainable (created separately)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Simplified view of the four models\n",
    "class RLHFSetup:\n",
    "    \"\"\"Setup for RLHF training with PPO.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        # 1. Policy Model (trainable) - generates responses\n",
    "        self.policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        \n",
    "        # 2. Reference Model (frozen) - prevents drift\n",
    "        self.reference_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        for param in self.reference_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # 3. Reward Model (frozen) - scores responses\n",
    "        # (In practice, loaded from a trained checkpoint)\n",
    "        self.reward_model = None  # Would be loaded separately\n",
    "        \n",
    "        # 4. Value Network (trainable) - estimates returns\n",
    "        # (Often shares base with policy model)\n",
    "        self.value_network = None  # Would be created separately\n",
    "        \n",
    "        print(f\"RLHF Setup initialized with {model_name}\")\n",
    "        print(f\"  Policy model: trainable\")\n",
    "        print(f\"  Reference model: frozen\")\n",
    "        print(f\"  Reward model: frozen (loaded separately)\")\n",
    "        print(f\"  Value network: trainable (created separately)\")\n",
    "\n",
    "setup = RLHFSetup(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## The PPO Training Loop\n",
    "\n",
    "Each training iteration has two phases:\n",
    "\n",
    "### Phase 1: Rollout Generation\n",
    "```python\n",
    "# 1. Sample prompts from dataset\n",
    "prompts = [\"Explain quantum computing\", ...]\n",
    "\n",
    "# 2. Generate responses with policy\n",
    "responses = policy_model.generate(prompts)\n",
    "\n",
    "# 3. Score with reward model\n",
    "rewards = reward_model(prompts + responses)\n",
    "\n",
    "# 4. Get value estimates\n",
    "values = value_network(prompts + responses)\n",
    "\n",
    "# 5. Get reference log probabilities (for KL penalty)\n",
    "ref_logprobs = reference_model(prompts + responses)\n",
    "```\n",
    "\n",
    "### Phase 2: PPO Update\n",
    "```python\n",
    "# 6. Compute advantages\n",
    "advantages = compute_gae(rewards, values)\n",
    "\n",
    "# 7. Multiple PPO epochs on this data\n",
    "for epoch in range(ppo_epochs):\n",
    "    loss = compute_ppo_loss(policy_logprobs, old_logprobs, advantages)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Why PPO?\n",
    "\n",
    "**Proximal Policy Optimization (PPO)** is chosen because:\n",
    "\n",
    "- **Stable** — Clipping prevents catastrophic updates\n",
    "- **Sample efficient** — Reuses data multiple times\n",
    "- **Simple** — Easier to implement than TRPO\n",
    "- **Proven** — Powers ChatGPT, Claude, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## The KL Penalty\n",
    "\n",
    "A critical component is preventing the policy from drifting too far from the reference:\n",
    "\n",
    "$$\\text{reward}_{\\text{total}} = r(x, y) - \\beta \\cdot D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}})$$\n",
    "\n",
    "**Why?** Without KL penalty:\n",
    "- Policy might exploit reward model weaknesses\n",
    "- Could forget how to generate coherent text\n",
    "- Might collapse to repetitive high-reward outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## RLHF vs DPO\n",
    "\n",
    "| Aspect | RLHF | DPO |\n",
    "|--------|------|-----|\n",
    "| **Stages** | 3 (SFT → RM → PPO) | 2 (SFT → DPO) |\n",
    "| **Models** | 4 models | 2 models |\n",
    "| **Complexity** | High | Low |\n",
    "| **Flexibility** | High (can change reward) | Lower |\n",
    "| **Stability** | Moderate | High |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the following notebooks, we'll dive deep into:\n",
    "\n",
    "1. **PPO Algorithm** — The clipped objective and implementation\n",
    "2. **KL Penalty** — Why it's critical for stability\n",
    "3. **Training Dynamics** — GAE, rollouts, and the complete loop\n",
    "4. **Reference Models** — Creating and managing frozen references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
