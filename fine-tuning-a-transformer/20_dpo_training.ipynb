{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# DPO Training: Alignment Without the Reward Model\n",
    "\n",
    "We've covered reward models. We've seen how they judge which responses are better. And maybe you're thinking: okay, so we train a reward model, then use it to fine-tune our language model.\n",
    "\n",
    "But what if I told you there's a shortcut?\n",
    "\n",
    "What if you could skip the reward model entirely and train your language model *directly* on preference data?\n",
    "\n",
    "That's DPO. Direct Preference Optimization.\n",
    "\n",
    "And it's kind of brilliant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Traditional Pipeline (And Why It's Complicated)\n",
    "\n",
    "The classic RLHF approach goes like this:\n",
    "\n",
    "```\n",
    "1. Start with a base language model\n",
    "2. Train a reward model on preference data (that's what we just did!)\n",
    "3. Use that reward model to score responses\n",
    "4. Use reinforcement learning (PPO) to optimize the language model\n",
    "5. Deal with all the instability and complexity that PPO brings\n",
    "```\n",
    "\n",
    "It works. Companies use it. But it's a pain.\n",
    "\n",
    "You're juggling multiple models, dealing with RL instability, tuning a bunch of hyperparameters, and basically hoping everything converges nicely.\n",
    "\n",
    "## The DPO Insight\n",
    "\n",
    "Here's the key realization that led to DPO:\n",
    "\n",
    "**If you know the optimal policy (the perfect language model), you can derive what the optimal reward function must be.**\n",
    "\n",
    "Read that again. It's wild.\n",
    "\n",
    "We usually think: reward function → optimal policy. But it also works backward: optimal policy → reward function.\n",
    "\n",
    "So instead of:\n",
    "1. Train reward model\n",
    "2. Use reward model to train policy\n",
    "\n",
    "We do:\n",
    "1. Train policy directly to match what the optimal policy would be\n",
    "\n",
    "No reward model. No RL. Just a clean loss function that optimizes your language model directly on preferences.\n",
    "\n",
    "It's like... instead of learning how to judge food quality and then using those judgments to become a better chef, you just study what makes great food great and cook accordingly.\n",
    "\n",
    "Does it work? Hell yes. In many cases, it works *better* than RLHF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:53.147429Z",
     "iopub.status.busy": "2025-12-10T21:19:53.147349Z",
     "iopub.status.idle": "2025-12-10T21:19:54.786016Z",
     "shell.execute_reply": "2025-12-10T21:19:54.785671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO Configuration:\n",
      "  model_name: gpt2\n",
      "  beta: 0.1\n",
      "  learning_rate: 1e-06\n",
      "  batch_size: 4\n",
      "  num_epochs: 1\n",
      "  max_length: 512\n",
      "  warmup_steps: 100\n",
      "  max_grad_norm: 1.0\n",
      "  label_smoothing: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DPOConfig:\n",
    "    \"\"\"Configuration for DPO training.\n",
    "    \n",
    "    These hyperparameters control how we train. Let's break down what each one does:\n",
    "    \n",
    "    - model_name: Which pretrained model to start from\n",
    "    - beta: How much we penalize diverging from the reference model (more on this later!)\n",
    "    - learning_rate: How big our update steps are (very small, like RLHF)\n",
    "    - batch_size: How many preference pairs per training step\n",
    "    - num_epochs: How many times we go through the dataset\n",
    "    - max_length: Maximum sequence length in tokens\n",
    "    - warmup_steps: Gradually increase learning rate at the start\n",
    "    - max_grad_norm: Clip gradients to prevent exploding updates\n",
    "    - label_smoothing: Optional regularization (we'll leave it at 0)\n",
    "    \"\"\"\n",
    "    model_name: str = \"gpt2\"\n",
    "    beta: float = 0.1\n",
    "    learning_rate: float = 1e-6\n",
    "    batch_size: int = 4\n",
    "    num_epochs: int = 1\n",
    "    max_length: int = 512\n",
    "    warmup_steps: int = 100\n",
    "    max_grad_norm: float = 1.0\n",
    "    label_smoothing: float = 0.0\n",
    "\n",
    "config = DPOConfig()\n",
    "print(\"DPO Configuration:\")\n",
    "for k, v in vars(config).items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The DPO Dataset\n",
    "\n",
    "Remember our preference data format? Prompts with chosen and rejected responses?\n",
    "\n",
    "That's exactly what we need here too. Same format, different use case.\n",
    "\n",
    "The dataset needs to:\n",
    "1. Take each preference pair\n",
    "2. Tokenize both the chosen and rejected responses\n",
    "3. Return them in a format ready for training\n",
    "\n",
    "Nothing fancy. We're just preparing the data for our loss function to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:54.787202Z",
     "iopub.status.busy": "2025-12-10T21:19:54.787057Z",
     "iopub.status.idle": "2025-12-10T21:19:55.176264Z",
     "shell.execute_reply": "2025-12-10T21:19:55.175921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPODataset Demo\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What we get from the dataset:\n",
      "  chosen_input_ids shape: torch.Size([64])\n",
      "  chosen_attention_mask shape: torch.Size([64])\n",
      "  rejected_input_ids shape: torch.Size([64])\n",
      "  rejected_attention_mask shape: torch.Size([64])\n",
      "\n",
      "Chosen response (decoded):\n",
      "  \"The capital of France is Paris, a beautiful city known for the Eiffel Tower.<|endoftext|><|endoftext|>...\"\n",
      "\n",
      "Rejected response (decoded):\n",
      "  \"I don't know what the capital of France is.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>...\"\n",
      "\n",
      "Perfect! Both responses tokenized and ready for training.\n"
     ]
    }
   ],
   "source": [
    "class DPODataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DPO training.\n",
    "    \n",
    "    Each item contains:\n",
    "    - chosen: The preferred response\n",
    "    - rejected: The dispreferred response\n",
    "    \n",
    "    We tokenize both and return them ready for the training loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize chosen response\n",
    "        chosen_tokens = self.tokenizer(\n",
    "            item['chosen'],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize rejected response\n",
    "        rejected_tokens = self.tokenizer(\n",
    "            item['rejected'],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'chosen_input_ids': chosen_tokens['input_ids'].squeeze(0),\n",
    "            'chosen_attention_mask': chosen_tokens['attention_mask'].squeeze(0),\n",
    "            'rejected_input_ids': rejected_tokens['input_ids'].squeeze(0),\n",
    "            'rejected_attention_mask': rejected_tokens['attention_mask'].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Let's see it in action with a simple example\n",
    "print(\"DPODataset Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a tiny example dataset\n",
    "example_data = [\n",
    "    {\n",
    "        'chosen': \"The capital of France is Paris, a beautiful city known for the Eiffel Tower.\",\n",
    "        'rejected': \"I don't know what the capital of France is.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# We need a tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "demo_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "demo_tokenizer.pad_token = demo_tokenizer.eos_token\n",
    "\n",
    "# Create the dataset\n",
    "demo_dataset = DPODataset(example_data, demo_tokenizer, max_length=64)\n",
    "sample = demo_dataset[0]\n",
    "\n",
    "print(f\"\\nWhat we get from the dataset:\")\n",
    "print(f\"  chosen_input_ids shape: {sample['chosen_input_ids'].shape}\")\n",
    "print(f\"  chosen_attention_mask shape: {sample['chosen_attention_mask'].shape}\")\n",
    "print(f\"  rejected_input_ids shape: {sample['rejected_input_ids'].shape}\")\n",
    "print(f\"  rejected_attention_mask shape: {sample['rejected_attention_mask'].shape}\")\n",
    "\n",
    "# Show what the tokens look like when decoded\n",
    "print(f\"\\nChosen response (decoded):\")\n",
    "print(f'  \"{demo_tokenizer.decode(sample[\"chosen_input_ids\"][:20])}...\"')\n",
    "print(f\"\\nRejected response (decoded):\")\n",
    "print(f'  \"{demo_tokenizer.decode(sample[\"rejected_input_ids\"][:15])}...\"')\n",
    "\n",
    "print(\"\\nPerfect! Both responses tokenized and ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Computing Log Probabilities: The Foundation\n",
    "\n",
    "Alright, here's where we get into the mechanics.\n",
    "\n",
    "DPO needs to know: **how likely is this sequence of tokens under my model?**\n",
    "\n",
    "This is actually the same computation we do during normal language model training. We run the model, get logits for each position, convert to probabilities, and sum up the log probabilities of the actual tokens.\n",
    "\n",
    "But there's a catch: we need to do this for entire sequences, not just individual tokens.\n",
    "\n",
    "Why? Because DPO compares **complete responses**. We want to know: did the model think response A was more likely than response B?\n",
    "\n",
    "Let me show you how we compute this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:55.177230Z",
     "iopub.status.busy": "2025-12-10T21:19:55.177141Z",
     "iopub.status.idle": "2025-12-10T21:19:56.539102Z",
     "shell.execute_reply": "2025-12-10T21:19:56.538782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_sequence_log_probs\n",
      "============================================================\n",
      "Using device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "  \"The quick brown fox jumps over the lazy dog.\"\n",
      "    Log probability: -45.81\n",
      "\n",
      "  \"Asdfghjkl qwerty zxcvbnm random gibberish text.\"\n",
      "    Log probability: -106.50\n",
      "\n",
      "Key insight:\n",
      "  - Log probabilities are negative (probabilities are between 0 and 1)\n",
      "  - HIGHER (less negative) = more likely under the model\n",
      "  - The coherent sentence should have a higher log prob\n",
      "  - And it does! The model knows English better than gibberish.\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_log_probs(\n",
    "    model,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the total log probability of a sequence under the model.\n",
    "    \n",
    "    This is THE key computation in DPO. We need to know:\n",
    "    \"How likely is this entire sequence under my model?\"\n",
    "    \n",
    "    The process:\n",
    "    1. Run the model to get logits at each position\n",
    "    2. Convert logits to log probabilities\n",
    "    3. Extract the log prob of each actual token\n",
    "    4. Sum them up (ignoring padding)\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        input_ids: Token IDs, shape (batch_size, seq_len)\n",
    "        attention_mask: Which tokens are real vs padding, shape (batch_size, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        Total log probability for each sequence, shape (batch_size,)\n",
    "    \"\"\"\n",
    "    # Run the model\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Shift for next-token prediction\n",
    "    # Position i in logits predicts position i+1 in input_ids\n",
    "    # So logits[:, :-1] predicts input_ids[:, 1:]\n",
    "    shift_logits = logits[:, :-1, :]  # (batch, seq_len-1, vocab_size)\n",
    "    shift_labels = input_ids[:, 1:]   # (batch, seq_len-1)\n",
    "    shift_mask = attention_mask[:, 1:]  # (batch, seq_len-1)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)  # (batch, seq_len-1, vocab_size)\n",
    "    \n",
    "    # Gather the log prob of each actual token\n",
    "    # For each position, extract log_prob[actual_token_id]\n",
    "    token_log_probs = torch.gather(\n",
    "        log_probs,\n",
    "        dim=-1,\n",
    "        index=shift_labels.unsqueeze(-1)  # Add vocab dimension\n",
    "    ).squeeze(-1)  # Remove it: (batch, seq_len-1)\n",
    "    \n",
    "    # Mask out padding tokens and sum\n",
    "    # We only want to sum log probs of real tokens\n",
    "    masked_log_probs = token_log_probs * shift_mask\n",
    "    sequence_log_probs = masked_log_probs.sum(dim=-1)  # (batch,)\n",
    "    \n",
    "    return sequence_log_probs\n",
    "\n",
    "# Let's see this in action!\n",
    "print(\"Testing get_sequence_log_probs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Load a model for demonstration\n",
    "demo_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "demo_model.to(device)\n",
    "demo_model.eval()\n",
    "\n",
    "# Create two test sequences\n",
    "# One should be much more likely (coherent English)\n",
    "# The other should be unlikely (gibberish)\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Asdfghjkl qwerty zxcvbnm random gibberish text.\"\n",
    "]\n",
    "\n",
    "inputs = demo_tokenizer(\n",
    "    test_texts, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=32\n",
    ")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Compute log probabilities\n",
    "with torch.no_grad():\n",
    "    log_probs = get_sequence_log_probs(\n",
    "        demo_model, \n",
    "        inputs['input_ids'], \n",
    "        inputs['attention_mask']\n",
    "    )\n",
    "\n",
    "print(\"Results:\")\n",
    "for text, lp in zip(test_texts, log_probs):\n",
    "    print(f'  \"{text}\"')\n",
    "    print(f'    Log probability: {lp.item():.2f}\\n')\n",
    "\n",
    "print(\"Key insight:\")\n",
    "print(\"  - Log probabilities are negative (probabilities are between 0 and 1)\")\n",
    "print(\"  - HIGHER (less negative) = more likely under the model\")\n",
    "print(\"  - The coherent sentence should have a higher log prob\")\n",
    "print(\"  - And it does! The model knows English better than gibberish.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## The DPO Loss Function\n\nOkay, deep breath. This is the heart of DPO.\n\nWe have two models:\n1. **Policy model (π)**: The model we're training\n2. **Reference model (π_ref)**: A frozen copy of the starting model\n\nFor each preference pair, we:\n1. Compute how likely the chosen response is under both models\n2. Compute how likely the rejected response is under both models\n3. Compare the ratios\n\nHere's the formula:\n\n```\nloss = -log(σ(β · (log(π(chosen)) - log(π_ref(chosen)) - log(π(rejected)) + log(π_ref(rejected)))))\n```\n\nWhoa. Let's break that down in English:\n\n**β · (log(π(chosen)) - log(π_ref(chosen)) - log(π(rejected)) + log(π_ref(rejected)))**\n\nThis is asking: \"How much more does the policy prefer the chosen response over the rejected response, compared to the reference model?\"\n\nLet me show you the intuition:\n\n- `log(π(chosen)) - log(π_ref(chosen))`: How much more likely is chosen under policy vs reference?\n- `log(π(rejected)) - log(π_ref(rejected))`: How much more likely is rejected under policy vs reference?\n- Subtract them: Policy should favor chosen MORE than it favors rejected\n- Multiply by β: Control how much we penalize diverging from reference\n\nThen we wrap it in a sigmoid and take the negative log. This creates a loss that:\n- Goes down when policy assigns higher probability to chosen\n- Goes down when policy assigns lower probability to rejected\n- Penalizes the policy for diverging too far from the reference\n\nThat β parameter? It's the KL penalty. It keeps your model from going completely off the rails and forgetting everything it learned during pretraining.\n\nPretty elegant, right?"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:56.540274Z",
     "iopub.status.busy": "2025-12-10T21:19:56.540113Z",
     "iopub.status.idle": "2025-12-10T21:19:56.543983Z",
     "shell.execute_reply": "2025-12-10T21:19:56.543707Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_dpo(policy_model, reference_model, train_loader, config, device):\n",
    "    \"\"\"\n",
    "    The complete DPO training loop.\n",
    "    \n",
    "    This is where everything comes together:\n",
    "    - We iterate through batches of preference pairs\n",
    "    - Compute log probs under both policy and reference models\n",
    "    - Calculate the DPO loss\n",
    "    - Update the policy model\n",
    "    \n",
    "    The reference model stays frozen the entire time.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standard optimizer setup\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        policy_model.parameters(),\n",
    "        lr=config.learning_rate\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    total_steps = len(train_loader) * config.num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config.warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Set model modes\n",
    "    policy_model.train()\n",
    "    reference_model.eval()  # Never changes!\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        epoch_metrics = {'loss': 0, 'accuracy': 0}\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Compute log probs under policy model (trainable)\n",
    "            policy_chosen_logps = get_sequence_log_probs(\n",
    "                policy_model,\n",
    "                batch['chosen_input_ids'],\n",
    "                batch['chosen_attention_mask']\n",
    "            )\n",
    "            policy_rejected_logps = get_sequence_log_probs(\n",
    "                policy_model,\n",
    "                batch['rejected_input_ids'],\n",
    "                batch['rejected_attention_mask']\n",
    "            )\n",
    "            \n",
    "            # Compute log probs under reference model (frozen)\n",
    "            with torch.no_grad():\n",
    "                ref_chosen_logps = get_sequence_log_probs(\n",
    "                    reference_model,\n",
    "                    batch['chosen_input_ids'],\n",
    "                    batch['chosen_attention_mask']\n",
    "                )\n",
    "                ref_rejected_logps = get_sequence_log_probs(\n",
    "                    reference_model,\n",
    "                    batch['rejected_input_ids'],\n",
    "                    batch['rejected_attention_mask']\n",
    "                )\n",
    "            \n",
    "            # The DPO loss computation\n",
    "            # Step 1: Compute log ratios (how much more likely under policy vs reference)\n",
    "            chosen_logratios = policy_chosen_logps - ref_chosen_logps\n",
    "            rejected_logratios = policy_rejected_logps - ref_rejected_logps\n",
    "            \n",
    "            # Step 2: Compute the logits\n",
    "            # This is β * (chosen advantage - rejected advantage)\n",
    "            logits = config.beta * (chosen_logratios - rejected_logratios)\n",
    "            \n",
    "            # Step 3: Apply sigmoid and negative log\n",
    "            # We want logits to be positive (chosen preferred over rejected)\n",
    "            loss = -F.logsigmoid(logits).mean()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent explosions\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                policy_model.parameters(), \n",
    "                config.max_grad_norm\n",
    "            )\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            # Accuracy: how often does policy prefer chosen over rejected?\n",
    "            # (If logits > 0, we predicted chosen correctly)\n",
    "            accuracy = (logits > 0).float().mean()\n",
    "            \n",
    "            epoch_metrics['loss'] += loss.item()\n",
    "            epoch_metrics['accuracy'] += accuracy.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{accuracy.item():.2%}\"\n",
    "            })\n",
    "        \n",
    "        # End of epoch summary\n",
    "        avg_loss = epoch_metrics['loss'] / len(train_loader)\n",
    "        avg_acc = epoch_metrics['accuracy'] / len(train_loader)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Average Accuracy: {avg_acc:.2%}\")\n",
    "    \n",
    "    return policy_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Setting Up For Training\n",
    "\n",
    "Time to actually run DPO!\n",
    "\n",
    "We need:\n",
    "1. A reference model (frozen - this is our anchor)\n",
    "2. A policy model (trainable - this is what we're improving)\n",
    "3. Preference data (chosen vs rejected pairs)\n",
    "\n",
    "The reference model is crucial. Without it, the policy could drift into nonsense that happens to score well on our loss. The reference keeps us grounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:56.544831Z",
     "iopub.status.busy": "2025-12-10T21:19:56.544755Z",
     "iopub.status.idle": "2025-12-10T21:19:57.888424Z",
     "shell.execute_reply": "2025-12-10T21:19:57.888082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded\n",
      "\n",
      "Loading reference model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reference model: FROZEN (this is our anchor)\n",
      "\n",
      "Loading policy model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Policy model: TRAINABLE (this is what we improve)\n",
      "  Trainable parameters: 124,439,808\n",
      "\n",
      "Models ready! Reference frozen, policy ready to learn.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer loaded\\n\")\n",
    "\n",
    "# Load reference model (this stays frozen!)\n",
    "print(\"Loading reference model...\")\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "reference_model.to(device)\n",
    "reference_model.eval()\n",
    "print(\"  Reference model: FROZEN (this is our anchor)\\n\")\n",
    "\n",
    "# Load policy model (this gets trained!)\n",
    "print(\"Loading policy model...\")\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "policy_model.to(device)\n",
    "print(\"  Policy model: TRAINABLE (this is what we improve)\")\n",
    "\n",
    "# Count trainable parameters\n",
    "num_params = sum(p.numel() for p in policy_model.parameters() if p.requires_grad)\n",
    "print(f\"  Trainable parameters: {num_params:,}\\n\")\n",
    "\n",
    "print(\"Models ready! Reference frozen, policy ready to learn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:57.889489Z",
     "iopub.status.busy": "2025-12-10T21:19:57.889395Z",
     "iopub.status.idle": "2025-12-10T21:19:59.444008Z",
     "shell.execute_reply": "2025-12-10T21:19:59.443610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preference data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 500 preference pairs\n",
      "\n",
      "DataLoader ready:\n",
      "  Batch size: 4\n",
      "  Number of batches: 125\n",
      "  Total training steps: 125\n"
     ]
    }
   ],
   "source": [
    "# Load the preference dataset\n",
    "print(\"Loading preference data...\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Anthropic's HH-RLHF dataset (same one we used for reward modeling!)\n",
    "raw_data = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
    "\n",
    "# We'll use a small subset for this demo\n",
    "# (In practice, you'd use more data)\n",
    "raw_data = raw_data.select(range(500))\n",
    "print(f\"  Loaded {len(raw_data)} preference pairs\\n\")\n",
    "\n",
    "# Wrap it in our DPO dataset\n",
    "dpo_dataset = DPODataset(raw_data, tokenizer, max_length=256)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = DataLoader(\n",
    "    dpo_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"DataLoader ready:\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Number of batches: {len(train_loader)}\")\n",
    "print(f\"  Total training steps: {len(train_loader) * config.num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:59.444985Z",
     "iopub.status.busy": "2025-12-10T21:19:59.444849Z",
     "iopub.status.idle": "2025-12-10T21:20:47.388808Z",
     "shell.execute_reply": "2025-12-10T21:20:47.388458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DPO Training\n",
      "============================================================\n",
      "\n",
      "What's happening:\n",
      "  - Policy model learns to prefer chosen over rejected\n",
      "  - Reference model keeps us from drifting too far\n",
      "  - β controls the tradeoff\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5922198846c3489ea620b14945b90c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Average Loss: 0.9530\n",
      "  Average Accuracy: 56.80%\n",
      "\n",
      "============================================================\n",
      "DPO training complete!\n",
      "\n",
      "The policy model now:\n",
      "  ✓ Assigns higher probability to preferred responses\n",
      "  ✓ Assigns lower probability to dispreferred responses\n",
      "  ✓ Stays grounded by the reference model\n"
     ]
    }
   ],
   "source": [
    "# Run DPO training!\n",
    "print(\"Starting DPO Training\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nWhat's happening:\")\n",
    "print(\"  - Policy model learns to prefer chosen over rejected\")\n",
    "print(\"  - Reference model keeps us from drifting too far\")\n",
    "print(\"  - β controls the tradeoff\\n\")\n",
    "\n",
    "policy_model = train_dpo(\n",
    "    policy_model, \n",
    "    reference_model, \n",
    "    train_loader, \n",
    "    config, \n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DPO training complete!\")\n",
    "print(\"\\nThe policy model now:\")\n",
    "print(\"  ✓ Assigns higher probability to preferred responses\")\n",
    "print(\"  ✓ Assigns lower probability to dispreferred responses\")\n",
    "print(\"  ✓ Stays grounded by the reference model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "## Understanding the Key Hyperparameters\n\nLet's talk about the knobs you can turn and what they actually do.\n\n### β (Beta) - The KL Penalty Coefficient\n\n**Default: 0.1**\n\nThis is the most important hyperparameter in DPO.\n\nThink of it as a leash. Higher β = shorter leash. The policy can't stray as far from the reference model.\n\n- **Too high (β = 1.0)**: Policy barely changes, stuck close to reference\n- **Too low (β = 0.01)**: Policy might overfit to preferences, forget general knowledge\n- **Just right (β = 0.1-0.5)**: Sweet spot for most tasks\n\nHow do you know if β is right? If your model starts generating nonsense or forgetting basic facts, β is probably too low.\n\n### Learning Rate\n\n**Default: 1e-6**\n\nTiny. Like, really tiny.\n\nThis is similar to RLHF - we're making small, careful updates. Language models are sensitive, and DPO is pushing them in a specific direction. Go too fast and you'll overshoot.\n\nStart with 1e-6. If training is too slow (loss barely moving), try 5e-6. If it's unstable (loss jumping around), try 5e-7.\n\n### Number of Epochs\n\n**Default: 1-3**\n\nWith preference data, you can overfit *fast*.\n\nUnlike pretraining where more data is always better, with DPO you're teaching specific preferences. Too many epochs and the model memorizes the training set instead of learning general principles.\n\nOne epoch is often enough. Three is usually the max. If you need more, you probably need more diverse data, not more epochs.\n\n### Batch Size\n\n**Default: 4-32**\n\nStandard deep learning advice applies: bigger batches = more stable gradients = faster training.\n\nBut: bigger batches = more memory. If you're GPU-limited, use smaller batches and accumulate gradients.\n\nThe actual batch size matters less than getting enough total training steps."
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## What We Just Built\n",
    "\n",
    "Take a step back and appreciate what just happened.\n",
    "\n",
    "We trained a language model to align with human preferences **without**:\n",
    "- Training a separate reward model\n",
    "- Running reinforcement learning\n",
    "- Dealing with PPO's complexity and instability\n",
    "\n",
    "Instead, we:\n",
    "1. Kept a frozen copy of the starting model (reference)\n",
    "2. Trained a new copy (policy) to prefer chosen over rejected responses\n",
    "3. Used β to prevent the policy from drifting too far\n",
    "\n",
    "The math is elegant. The implementation is straightforward. The results are competitive with (and sometimes better than) full RLHF.\n",
    "\n",
    "This is why DPO has become so popular. It democratizes alignment - you don't need an RL expert on your team to make it work.\n",
    "\n",
    "## When to Use DPO\n",
    "\n",
    "DPO shines when:\n",
    "- You have good preference data\n",
    "- You want alignment without RL complexity  \n",
    "- You're fine-tuning a model that's already been supervised fine-tuned\n",
    "- You want something stable and predictable\n",
    "\n",
    "DPO might not be ideal when:\n",
    "- Your preference data is noisy (reward models can be more robust)\n",
    "- You need complex reward shaping\n",
    "- You want to combine multiple objectives\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "From here you could:\n",
    "- Experiment with different β values to see how they affect the tradeoff\n",
    "- Try DPO on your own preference dataset\n",
    "- Combine DPO with LoRA for parameter-efficient training\n",
    "- Evaluate your DPO model on a held-out test set\n",
    "\n",
    "The beauty of DPO is that it's just supervised learning with a clever loss function. Everything you know about training neural networks still applies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "016b983f916d42e68fc6fe48c10792f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "083455b7d4ac47c4b0516280362257af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "27ebd2c526a9498a9bb67ade97dc240c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b798ad07e0e42f4a2088e5fdeae7fef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5922198846c3489ea620b14945b90c17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9420aefbbdfe46acabf769cf198c673e",
        "IPY_MODEL_b4a9c51245684d5abc7e087618838832",
        "IPY_MODEL_84f4ae2ca3d34277a651ee2374d91dc6"
       ],
       "layout": "IPY_MODEL_016b983f916d42e68fc6fe48c10792f0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "67e268a9c8b24c8284be63264bd29f1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7485532ab21247a4a3aac088d210c4b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "84f4ae2ca3d34277a651ee2374d91dc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_67e268a9c8b24c8284be63264bd29f1f",
       "placeholder": "​",
       "style": "IPY_MODEL_2b798ad07e0e42f4a2088e5fdeae7fef",
       "tabbable": null,
       "tooltip": null,
       "value": " 125/125 [00:47&lt;00:00,  2.62it/s, loss=1.1054, acc=25.00%]"
      }
     },
     "9420aefbbdfe46acabf769cf198c673e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_27ebd2c526a9498a9bb67ade97dc240c",
       "placeholder": "​",
       "style": "IPY_MODEL_083455b7d4ac47c4b0516280362257af",
       "tabbable": null,
       "tooltip": null,
       "value": "Epoch 1: 100%"
      }
     },
     "a49476b35a834a4d9b7c190a6f124ad3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4a9c51245684d5abc7e087618838832": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a49476b35a834a4d9b7c190a6f124ad3",
       "max": 125,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7485532ab21247a4a3aac088d210c4b5",
       "tabbable": null,
       "tooltip": null,
       "value": 125
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}