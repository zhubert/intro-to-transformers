{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try It Yourself!\n",
    "\n",
    "**Your turn to build something real.**\n",
    "\n",
    "This is it. The capstone. All those notebooks you just went through? Time to put them into practice.\n",
    "\n",
    "We're going to fine-tune a language model from scratch. Not a toy example. A real model that actually learns to follow instructions. By the end of this, you'll have your own fine-tuned GPT-2 sitting on your hard drive, ready to answer questions.\n",
    "\n",
    "Sound good? Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You're About to Build\n",
    "\n",
    "Here's the deal: we're taking a base GPT-2 model—one that's pretty good at predicting the next word but terrible at following instructions—and teaching it to be helpful.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to actually train a model (not just read about it)\n",
    "- Why supervised fine-tuning works\n",
    "- How to evaluate if your model is any good\n",
    "- What to watch out for when things go wrong\n",
    "\n",
    "**Time investment:** 30-60 minutes, depending on your hardware. Got a GPU? Closer to 30. Running on CPU? Grab a coffee and make it an hour.\n",
    "\n",
    "**Important note:** This is a simplified version for learning. Production fine-tuning would use LoRA (which we covered), more data, and better hyperparameter tuning. But the core ideas? Exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Your Environment\n",
    "\n",
    "First things first—let's make sure you've got PyTorch installed and that it can see your GPU (if you have one).\n",
    "\n",
    "Why check this first? Because finding out 20 minutes into training that CUDA isn't working is... well, let's just say it's a learning experience you only need once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:13.878724Z",
     "iopub.status.busy": "2025-12-08T22:52:13.878640Z",
     "iopub.status.idle": "2025-12-08T22:52:14.589953Z",
     "shell.execute_reply": "2025-12-08T22:52:14.589641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251124+rocm7.1\n",
      "CUDA available: True\n",
      "GPU: Radeon RX 7900 XTX\n",
      "Great! Training will be fast.\n"
     ]
    }
   ],
   "source": [
    "# Check what we're working with\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Great! Training will be fast.\")\n",
    "else:\n",
    "    print(\"Device: CPU\")\n",
    "    print(\"No GPU found. Training will work but be slower (10-20x).\")\n",
    "    print(\"Consider reducing num_samples in the data loading step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:14.606894Z",
     "iopub.status.busy": "2025-12-08T22:52:14.606690Z",
     "iopub.status.idle": "2025-12-08T22:52:15.781195Z",
     "shell.execute_reply": "2025-12-08T22:52:15.780917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "\n",
      "If you got any errors above, install missing packages with:\n",
      "  pip install transformers datasets torch tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import everything we need\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(\"\\nIf you got any errors above, install missing packages with:\")\n",
    "print(\"  pip install transformers datasets torch tqdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Base Model\n",
    "\n",
    "We're using GPT-2 (the small version, 124M parameters). Why GPT-2 and not something bigger?\n",
    "\n",
    "1. **It's fast to train** - You can actually finish this notebook today\n",
    "2. **It's well-understood** - Lots of documentation if things break\n",
    "3. **It's big enough to learn** - 124M parameters is plenty for instruction following\n",
    "\n",
    "Think of this as the \"before\" photo. The model right now is decent at continuing text but hopeless at following instructions. We're about to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:15.782292Z",
     "iopub.status.busy": "2025-12-08T22:52:15.782140Z",
     "iopub.status.idle": "2025-12-08T22:52:16.752710Z",
     "shell.execute_reply": "2025-12-08T22:52:16.752299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2...\n",
      "This downloads ~500MB the first time, then caches locally.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded!\n",
      "  Parameters: 124,439,808\n",
      "  Device: cuda\n",
      "  Memory: ~0.5 GB (in float32)\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 (small, 124M parameters)\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "print(\"This downloads ~500MB the first time, then caches locally.\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a padding token by default, so we add one\n",
    "# We just reuse the EOS token—common practice and works fine\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# How big is this thing?\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Model loaded!\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Memory: ~{total_params * 4 / 1e9:.1f} GB (in float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the Base Model (Before Training)\n",
    "\n",
    "Okay, moment of truth. Let's see what the base model does when we ask it questions.\n",
    "\n",
    "Spoiler: it's going to be bad. Really bad. That's the point.\n",
    "\n",
    "Base GPT-2 was trained to predict the next token in internet text. It was never taught to answer questions. So when you ask it \"What is the capital of France?\" it just... continues the pattern of text it sees. Sometimes that works by accident. Usually it doesn't.\n",
    "\n",
    "Watch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:16.753716Z",
     "iopub.status.busy": "2025-12-08T22:52:16.753621Z",
     "iopub.status.idle": "2025-12-08T22:52:19.163675Z",
     "shell.execute_reply": "2025-12-08T22:52:19.163371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE MODEL (before fine-tuning):\n",
      "======================================================================\n",
      "Watch how it fails to actually answer the questions...\n",
      "\n",
      "Q: What is the capital of France?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhubert/intro-to-transformers/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:83: UserWarning: Flash Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:316.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "/home/zhubert/intro-to-transformers/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:83: UserWarning: Mem Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:373.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: What is the capital of France?\n",
      "\n",
      "A French language program written by an English speaker.\n",
      "\n",
      "The program is designed to be as simple as possible.\n",
      "\n",
      "It uses the basic vocabulary of French and English to gi...\n",
      "----------------------------------------------------------------------\n",
      "Q: Write a haiku about programming.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Write a haiku about programming.\n",
      "\n",
      "### Code:\n",
      "\n",
      "Write\n",
      "----------------------------------------------------------------------\n",
      "Q: Explain machine learning in one sentence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Write a response to the question.\n",
      "\n",
      "### Type:\n",
      "\n",
      "Write\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, instruction, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Generate a response to an instruction.\n",
    "    \n",
    "    We format the prompt in \"Alpaca style\"—a specific template that works well\n",
    "    for instruction following. You'll see this same format in the training data.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():  # Don't track gradients for inference\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,  # Some randomness (0 = deterministic, 1 = very random)\n",
    "            top_p=0.9,  # Nucleus sampling\n",
    "            do_sample=True,  # Use sampling instead of greedy\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the response part (after \"### Response:\")\n",
    "    response = full_text.split(\"### Response:\\n\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test base model on a few questions\n",
    "test_instructions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Write a haiku about programming.\",\n",
    "    \"Explain machine learning in one sentence.\",\n",
    "]\n",
    "\n",
    "print(\"BASE MODEL (before fine-tuning):\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Watch how it fails to actually answer the questions...\\n\")\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    print(f\"Q: {instruction}\")\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    # Truncate long responses\n",
    "    if len(response) > 200:\n",
    "        response = response[:200] + \"...\"\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what I mean? The model just rambles. It's not *trying* to answer the question—it's trying to continue text that looks like the prompt.\n",
    "\n",
    "That's because base GPT-2 was trained on raw internet text with a simple objective: predict the next word. No one ever taught it that text formatted as \"Instruction:\" and \"Response:\" means it should actually answer the question.\n",
    "\n",
    "That's what we're about to fix with supervised fine-tuning.\n",
    "\n",
    "## Step 4: Prepare Training Data\n",
    "\n",
    "We're using the **Alpaca dataset**—52,000 instruction-response pairs created by Stanford. Things like:\n",
    "\n",
    "- Instruction: \"Give three tips for staying healthy\"\n",
    "- Response: \"1. Eat a balanced diet. 2. Exercise regularly. 3. Get enough sleep.\"\n",
    "\n",
    "Perfect for teaching a model to follow instructions.\n",
    "\n",
    "**Key insight:** We'll use a small subset (500 examples) for speed. This is enough to see the model learn! For production use, you'd train on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:19.164705Z",
     "iopub.status.busy": "2025-12-08T22:52:19.164624Z",
     "iopub.status.idle": "2025-12-08T22:52:20.400990Z",
     "shell.execute_reply": "2025-12-08T22:52:20.400650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Alpaca dataset from HuggingFace...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset loaded: 500 training examples\n",
      "\n",
      "Here's what one example looks like:\n",
      "  Instruction: Give three tips for staying healthy.\n",
      "  Output: 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and...\n",
      "\n",
      "The model will learn to generate 'Output' given 'Instruction' (and 'Input' if present).\n"
     ]
    }
   ],
   "source": [
    "# Load the Alpaca dataset\n",
    "print(\"Loading Alpaca dataset from HuggingFace...\")\n",
    "raw_dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "# Use a small subset for quick training\n",
    "# Feel free to adjust this:\n",
    "#   - 100 samples: Very fast, model learns a bit (2-3 min on GPU)\n",
    "#   - 500 samples: Good learning, reasonable time (10-15 min on GPU)\n",
    "#   - 5000+ samples: Better results, longer training (1+ hour)\n",
    "num_samples = 500\n",
    "\n",
    "raw_dataset = raw_dataset.select(range(num_samples))\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded: {len(raw_dataset)} training examples\")\n",
    "print(f\"\\nHere's what one example looks like:\")\n",
    "print(f\"  Instruction: {raw_dataset[0]['instruction']}\")\n",
    "if raw_dataset[0]['input']:\n",
    "    print(f\"  Input: {raw_dataset[0]['input']}\")\n",
    "print(f\"  Output: {raw_dataset[0]['output'][:100]}...\")\n",
    "print(f\"\\nThe model will learn to generate 'Output' given 'Instruction' (and 'Input' if present).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:20.401888Z",
     "iopub.status.busy": "2025-12-08T22:52:20.401801Z",
     "iopub.status.idle": "2025-12-08T22:52:20.405935Z",
     "shell.execute_reply": "2025-12-08T22:52:20.405648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created dataset with 500 samples\n",
      "  Batches per epoch: 125\n",
      "  Batch size: 4\n",
      "\n",
      "Each batch contains:\n",
      "  - input_ids: The tokenized text (prompt + response)\n",
      "  - attention_mask: Which tokens to pay attention to (1) vs ignore (0)\n",
      "  - labels: What to predict (-100 for prompt, actual tokens for response)\n"
     ]
    }
   ],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for instruction fine-tuning.\n",
    "    \n",
    "    The magic here is in the LABEL MASKING. We only compute loss on the response\n",
    "    tokens, not the instruction tokens. Why? Because we want the model to learn\n",
    "    to GENERATE responses, not to predict the instruction itself.\n",
    "    \n",
    "    This is crucial. Without it, the model would waste capacity learning to \n",
    "    predict the instruction template, which is useless.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def format_example(self, example):\n",
    "        \"\"\"Format in Alpaca style (same as our generate function).\"\"\"\n",
    "        if example['input']:\n",
    "            # Some examples have an additional 'input' field for context\n",
    "            prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        return prompt, example['output']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        prompt, response = self.format_example(example)\n",
    "        \n",
    "        # Tokenize prompt and response separately (important!)\n",
    "        prompt_tokens = self.tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        response_tokens = self.tokenizer.encode(response, add_special_tokens=False)\n",
    "        \n",
    "        # Combine: [prompt tokens] + [response tokens] + [EOS]\n",
    "        input_ids = prompt_tokens + response_tokens + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        # Create labels: -100 for prompt (ignored in loss), actual tokens for response\n",
    "        # This is the key to supervised fine-tuning!\n",
    "        labels = [-100] * len(prompt_tokens) + response_tokens + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "        \n",
    "        # Pad to max_length (makes batching easier)\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "        labels = labels + [-100] * padding_length  # Ignore padding in loss\n",
    "        attention_mask = [1] * (self.max_length - padding_length) + [0] * padding_length\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_mask),\n",
    "            'labels': torch.tensor(labels),\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = InstructionDataset(raw_dataset, tokenizer, max_length=256)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4,  # Small batch size to fit in memory\n",
    "    shuffle=True   # Randomize order each epoch\n",
    ")\n",
    "\n",
    "print(f\"✓ Created dataset with {len(train_dataset)} samples\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Batch size: 4\")\n",
    "print(f\"\\nEach batch contains:\")\n",
    "print(f\"  - input_ids: The tokenized text (prompt + response)\")\n",
    "print(f\"  - attention_mask: Which tokens to pay attention to (1) vs ignore (0)\")\n",
    "print(f\"  - labels: What to predict (-100 for prompt, actual tokens for response)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Up Training\n",
    "\n",
    "Time to configure the training loop. A few key decisions here:\n",
    "\n",
    "**Learning rate (5e-5):** Small enough to not destroy the pretrained weights, large enough to actually learn. This is a well-tested default for fine-tuning.\n",
    "\n",
    "**Warmup steps (50):** Gradually increase the learning rate for the first 50 steps. Helps with training stability—like stretching before a run.\n",
    "\n",
    "**Gradient clipping (1.0):** Prevents any single bad batch from causing chaos. If gradients get too large, we scale them down. Think of it as a safety rail.\n",
    "\n",
    "**One epoch:** With 500 examples, one pass through the data is enough to see learning. More epochs would help, but we're going for speed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:20.406654Z",
     "iopub.status.busy": "2025-12-08T22:52:20.406573Z",
     "iopub.status.idle": "2025-12-08T22:52:20.409138Z",
     "shell.execute_reply": "2025-12-08T22:52:20.408899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Learning rate: 5e-05\n",
      "  Epochs: 1\n",
      "  Steps per epoch: 125\n",
      "  Total steps: 125\n",
      "  Warmup steps: 50\n",
      "  Gradient clipping: 1.0\n",
      "\n",
      "Estimated time: ~10-15 minutes on GPU, ~2 hours on CPU\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 5e-5  # Standard for fine-tuning (0.00005)\n",
    "num_epochs = 1        # One pass through the data\n",
    "warmup_steps = 50     # Gradually increase LR for first 50 steps\n",
    "max_grad_norm = 1.0   # Clip gradients to prevent instability\n",
    "\n",
    "# Set up optimizer (AdamW is standard for transformers)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler (warmup then linear decay)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Steps per epoch: {len(train_loader)}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Gradient clipping: {max_grad_norm}\")\n",
    "print(f\"\\nEstimated time: ~10-15 minutes on GPU, ~2 hours on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:20.409832Z",
     "iopub.status.busy": "2025-12-08T22:52:20.409751Z",
     "iopub.status.idle": "2025-12-08T22:52:35.846670Z",
     "shell.execute_reply": "2025-12-08T22:52:35.846275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Watch the loss go down! (Lower = better)\n",
      "\n",
      "Metrics explained:\n",
      "  - loss: How wrong the model is (lower = better)\n",
      "  - avg_loss: Running average of loss\n",
      "  - ppl: Perplexity (e^loss), another way to measure quality\n",
      "\n",
      "Go grab a coffee. This'll take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   0%|                                                                                   | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   0%|                                          | 0/125 [00:00<?, ?it/s, loss=2.4670, avg_loss=2.4670, ppl=11.79]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   1%|▎                                 | 1/125 [00:00<00:40,  3.08it/s, loss=2.4670, avg_loss=2.4670, ppl=11.79]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   1%|▎                                 | 1/125 [00:00<00:40,  3.08it/s, loss=2.6214, avg_loss=2.5442, ppl=12.73]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   2%|▌                                 | 2/125 [00:00<00:26,  4.72it/s, loss=2.6214, avg_loss=2.5442, ppl=12.73]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   2%|▌                                 | 2/125 [00:00<00:26,  4.72it/s, loss=3.8249, avg_loss=2.9711, ppl=19.51]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   2%|▊                                 | 3/125 [00:00<00:20,  5.85it/s, loss=3.8249, avg_loss=2.9711, ppl=19.51]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   2%|▊                                 | 3/125 [00:00<00:20,  5.85it/s, loss=2.9171, avg_loss=2.9576, ppl=19.25]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   3%|█                                 | 4/125 [00:00<00:18,  6.57it/s, loss=2.9171, avg_loss=2.9576, ppl=19.25]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   3%|█                                 | 4/125 [00:00<00:18,  6.57it/s, loss=2.7531, avg_loss=2.9167, ppl=18.48]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   4%|█▎                                | 5/125 [00:00<00:16,  7.08it/s, loss=2.7531, avg_loss=2.9167, ppl=18.48]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   4%|█▎                                | 5/125 [00:00<00:16,  7.08it/s, loss=3.1782, avg_loss=2.9603, ppl=19.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   5%|█▋                                | 6/125 [00:00<00:15,  7.45it/s, loss=3.1782, avg_loss=2.9603, ppl=19.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   5%|█▋                                | 6/125 [00:01<00:15,  7.45it/s, loss=2.8344, avg_loss=2.9423, ppl=18.96]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   6%|█▉                                | 7/125 [00:01<00:15,  7.71it/s, loss=2.8344, avg_loss=2.9423, ppl=18.96]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   6%|█▉                                | 7/125 [00:01<00:15,  7.71it/s, loss=2.4357, avg_loss=2.8790, ppl=17.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   6%|██▏                               | 8/125 [00:01<00:14,  7.87it/s, loss=2.4357, avg_loss=2.8790, ppl=17.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   6%|██▏                               | 8/125 [00:01<00:14,  7.87it/s, loss=2.5793, avg_loss=2.8457, ppl=17.21]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   7%|██▍                               | 9/125 [00:01<00:14,  7.96it/s, loss=2.5793, avg_loss=2.8457, ppl=17.21]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   7%|██▍                               | 9/125 [00:01<00:14,  7.96it/s, loss=2.5134, avg_loss=2.8124, ppl=16.65]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   8%|██▋                              | 10/125 [00:01<00:14,  8.04it/s, loss=2.5134, avg_loss=2.8124, ppl=16.65]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   8%|██▋                              | 10/125 [00:01<00:14,  8.04it/s, loss=3.0492, avg_loss=2.8340, ppl=17.01]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   9%|██▉                              | 11/125 [00:01<00:14,  8.09it/s, loss=3.0492, avg_loss=2.8340, ppl=17.01]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:   9%|██▉                              | 11/125 [00:01<00:14,  8.09it/s, loss=2.7812, avg_loss=2.8296, ppl=16.94]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  10%|███▏                             | 12/125 [00:01<00:13,  8.13it/s, loss=2.7812, avg_loss=2.8296, ppl=16.94]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  10%|███▏                             | 12/125 [00:01<00:13,  8.13it/s, loss=2.6761, avg_loss=2.8178, ppl=16.74]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  10%|███▍                             | 13/125 [00:01<00:13,  8.19it/s, loss=2.6761, avg_loss=2.8178, ppl=16.74]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  10%|███▍                             | 13/125 [00:01<00:13,  8.19it/s, loss=2.3257, avg_loss=2.7826, ppl=16.16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  11%|███▋                             | 14/125 [00:01<00:13,  8.20it/s, loss=2.3257, avg_loss=2.7826, ppl=16.16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  11%|███▋                             | 14/125 [00:02<00:13,  8.20it/s, loss=2.7700, avg_loss=2.7818, ppl=16.15]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  12%|███▉                             | 15/125 [00:02<00:13,  8.22it/s, loss=2.7700, avg_loss=2.7818, ppl=16.15]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  12%|███▉                             | 15/125 [00:02<00:13,  8.22it/s, loss=2.7604, avg_loss=2.7804, ppl=16.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  13%|████▏                            | 16/125 [00:02<00:13,  8.19it/s, loss=2.7604, avg_loss=2.7804, ppl=16.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  13%|████▏                            | 16/125 [00:02<00:13,  8.19it/s, loss=2.4692, avg_loss=2.7621, ppl=15.83]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  14%|████▍                            | 17/125 [00:02<00:13,  8.19it/s, loss=2.4692, avg_loss=2.7621, ppl=15.83]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  14%|████▍                            | 17/125 [00:02<00:13,  8.19it/s, loss=2.2219, avg_loss=2.7321, ppl=15.37]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  14%|████▊                            | 18/125 [00:02<00:13,  8.17it/s, loss=2.2219, avg_loss=2.7321, ppl=15.37]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  14%|████▊                            | 18/125 [00:02<00:13,  8.17it/s, loss=2.2331, avg_loss=2.7059, ppl=14.97]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  15%|█████                            | 19/125 [00:02<00:13,  8.13it/s, loss=2.2331, avg_loss=2.7059, ppl=14.97]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  15%|█████                            | 19/125 [00:02<00:13,  8.13it/s, loss=2.4107, avg_loss=2.6911, ppl=14.75]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  16%|█████▎                           | 20/125 [00:02<00:12,  8.17it/s, loss=2.4107, avg_loss=2.6911, ppl=14.75]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  16%|█████▎                           | 20/125 [00:02<00:12,  8.17it/s, loss=2.2295, avg_loss=2.6691, ppl=14.43]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  17%|█████▌                           | 21/125 [00:02<00:12,  8.18it/s, loss=2.2295, avg_loss=2.6691, ppl=14.43]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  17%|█████▌                           | 21/125 [00:02<00:12,  8.18it/s, loss=2.8406, avg_loss=2.6769, ppl=14.54]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  18%|█████▊                           | 22/125 [00:02<00:12,  8.19it/s, loss=2.8406, avg_loss=2.6769, ppl=14.54]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  18%|█████▊                           | 22/125 [00:03<00:12,  8.19it/s, loss=2.4351, avg_loss=2.6664, ppl=14.39]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  18%|██████                           | 23/125 [00:03<00:12,  8.19it/s, loss=2.4351, avg_loss=2.6664, ppl=14.39]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  18%|██████                           | 23/125 [00:03<00:12,  8.19it/s, loss=2.7742, avg_loss=2.6709, ppl=14.45]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  19%|██████▎                          | 24/125 [00:03<00:12,  8.21it/s, loss=2.7742, avg_loss=2.6709, ppl=14.45]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  19%|██████▎                          | 24/125 [00:03<00:12,  8.21it/s, loss=2.2863, avg_loss=2.6555, ppl=14.23]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  20%|██████▌                          | 25/125 [00:03<00:12,  8.19it/s, loss=2.2863, avg_loss=2.6555, ppl=14.23]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  20%|██████▌                          | 25/125 [00:03<00:12,  8.19it/s, loss=2.5298, avg_loss=2.6507, ppl=14.16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  21%|██████▊                          | 26/125 [00:03<00:12,  8.18it/s, loss=2.5298, avg_loss=2.6507, ppl=14.16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  21%|██████▊                          | 26/125 [00:03<00:12,  8.18it/s, loss=2.4663, avg_loss=2.6438, ppl=14.07]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  22%|███████▏                         | 27/125 [00:03<00:11,  8.19it/s, loss=2.4663, avg_loss=2.6438, ppl=14.07]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  22%|███████▏                         | 27/125 [00:03<00:11,  8.19it/s, loss=2.3497, avg_loss=2.6333, ppl=13.92]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  22%|███████▍                         | 28/125 [00:03<00:11,  8.17it/s, loss=2.3497, avg_loss=2.6333, ppl=13.92]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  22%|███████▍                         | 28/125 [00:03<00:11,  8.17it/s, loss=2.2469, avg_loss=2.6200, ppl=13.74]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  23%|███████▋                         | 29/125 [00:03<00:11,  8.19it/s, loss=2.2469, avg_loss=2.6200, ppl=13.74]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  23%|███████▋                         | 29/125 [00:03<00:11,  8.19it/s, loss=3.0033, avg_loss=2.6328, ppl=13.91]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  24%|███████▉                         | 30/125 [00:03<00:11,  8.19it/s, loss=3.0033, avg_loss=2.6328, ppl=13.91]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  24%|███████▉                         | 30/125 [00:03<00:11,  8.19it/s, loss=1.9379, avg_loss=2.6104, ppl=13.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  25%|████████▏                        | 31/125 [00:03<00:11,  8.22it/s, loss=1.9379, avg_loss=2.6104, ppl=13.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  25%|████████▏                        | 31/125 [00:04<00:11,  8.22it/s, loss=2.9973, avg_loss=2.6225, ppl=13.77]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  26%|████████▍                        | 32/125 [00:04<00:11,  8.22it/s, loss=2.9973, avg_loss=2.6225, ppl=13.77]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  26%|████████▍                        | 32/125 [00:04<00:11,  8.22it/s, loss=2.3976, avg_loss=2.6157, ppl=13.68]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  26%|████████▋                        | 33/125 [00:04<00:11,  8.23it/s, loss=2.3976, avg_loss=2.6157, ppl=13.68]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  26%|████████▋                        | 33/125 [00:04<00:11,  8.23it/s, loss=3.2151, avg_loss=2.6333, ppl=13.92]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  27%|████████▉                        | 34/125 [00:04<00:11,  8.24it/s, loss=3.2151, avg_loss=2.6333, ppl=13.92]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  27%|████████▉                        | 34/125 [00:04<00:11,  8.24it/s, loss=2.3508, avg_loss=2.6252, ppl=13.81]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  28%|█████████▏                       | 35/125 [00:04<00:10,  8.25it/s, loss=2.3508, avg_loss=2.6252, ppl=13.81]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  28%|█████████▏                       | 35/125 [00:04<00:10,  8.25it/s, loss=2.0932, avg_loss=2.6104, ppl=13.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  29%|█████████▌                       | 36/125 [00:04<00:10,  8.24it/s, loss=2.0932, avg_loss=2.6104, ppl=13.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  29%|█████████▌                       | 36/125 [00:04<00:10,  8.24it/s, loss=2.2068, avg_loss=2.5995, ppl=13.46]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  30%|█████████▊                       | 37/125 [00:04<00:10,  8.24it/s, loss=2.2068, avg_loss=2.5995, ppl=13.46]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  30%|█████████▊                       | 37/125 [00:04<00:10,  8.24it/s, loss=2.2926, avg_loss=2.5914, ppl=13.35]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  30%|██████████                       | 38/125 [00:04<00:10,  8.23it/s, loss=2.2926, avg_loss=2.5914, ppl=13.35]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  30%|██████████                       | 38/125 [00:04<00:10,  8.23it/s, loss=2.4834, avg_loss=2.5887, ppl=13.31]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  31%|██████████▎                      | 39/125 [00:04<00:10,  8.25it/s, loss=2.4834, avg_loss=2.5887, ppl=13.31]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  31%|██████████▎                      | 39/125 [00:05<00:10,  8.25it/s, loss=2.4420, avg_loss=2.5850, ppl=13.26]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  32%|██████████▌                      | 40/125 [00:05<00:10,  8.25it/s, loss=2.4420, avg_loss=2.5850, ppl=13.26]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  32%|██████████▌                      | 40/125 [00:05<00:10,  8.25it/s, loss=2.2923, avg_loss=2.5779, ppl=13.17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  33%|██████████▊                      | 41/125 [00:05<00:10,  8.24it/s, loss=2.2923, avg_loss=2.5779, ppl=13.17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  33%|██████████▊                      | 41/125 [00:05<00:10,  8.24it/s, loss=2.5918, avg_loss=2.5782, ppl=13.17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  34%|███████████                      | 42/125 [00:05<00:10,  8.19it/s, loss=2.5918, avg_loss=2.5782, ppl=13.17]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  34%|███████████                      | 42/125 [00:05<00:10,  8.19it/s, loss=2.3252, avg_loss=2.5723, ppl=13.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  34%|███████████▎                     | 43/125 [00:05<00:10,  8.18it/s, loss=2.3252, avg_loss=2.5723, ppl=13.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  34%|███████████▎                     | 43/125 [00:05<00:10,  8.18it/s, loss=2.5126, avg_loss=2.5710, ppl=13.08]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  35%|███████████▌                     | 44/125 [00:05<00:09,  8.17it/s, loss=2.5126, avg_loss=2.5710, ppl=13.08]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  35%|███████████▌                     | 44/125 [00:05<00:09,  8.17it/s, loss=2.2491, avg_loss=2.5638, ppl=12.99]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  36%|███████████▉                     | 45/125 [00:05<00:09,  8.13it/s, loss=2.2491, avg_loss=2.5638, ppl=12.99]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  36%|███████████▉                     | 45/125 [00:05<00:09,  8.13it/s, loss=2.4813, avg_loss=2.5620, ppl=12.96]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  37%|████████████▏                    | 46/125 [00:05<00:09,  8.14it/s, loss=2.4813, avg_loss=2.5620, ppl=12.96]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  37%|████████████▏                    | 46/125 [00:05<00:09,  8.14it/s, loss=2.1784, avg_loss=2.5539, ppl=12.86]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  38%|████████████▍                    | 47/125 [00:05<00:09,  8.14it/s, loss=2.1784, avg_loss=2.5539, ppl=12.86]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  38%|████████████▍                    | 47/125 [00:06<00:09,  8.14it/s, loss=2.3874, avg_loss=2.5504, ppl=12.81]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  38%|████████████▋                    | 48/125 [00:06<00:09,  8.17it/s, loss=2.3874, avg_loss=2.5504, ppl=12.81]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  38%|████████████▋                    | 48/125 [00:06<00:09,  8.17it/s, loss=2.5705, avg_loss=2.5508, ppl=12.82]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  39%|████████████▉                    | 49/125 [00:06<00:09,  8.17it/s, loss=2.5705, avg_loss=2.5508, ppl=12.82]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  39%|████████████▉                    | 49/125 [00:06<00:09,  8.17it/s, loss=2.5222, avg_loss=2.5502, ppl=12.81]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  40%|█████████████▏                   | 50/125 [00:06<00:09,  8.16it/s, loss=2.5222, avg_loss=2.5502, ppl=12.81]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  40%|█████████████▏                   | 50/125 [00:06<00:09,  8.16it/s, loss=2.6608, avg_loss=2.5524, ppl=12.84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  41%|█████████████▍                   | 51/125 [00:06<00:08,  8.23it/s, loss=2.6608, avg_loss=2.5524, ppl=12.84]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  41%|█████████████▍                   | 51/125 [00:06<00:08,  8.23it/s, loss=3.0462, avg_loss=2.5619, ppl=12.96]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  42%|█████████████▋                   | 52/125 [00:06<00:08,  8.24it/s, loss=3.0462, avg_loss=2.5619, ppl=12.96]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  42%|█████████████▋                   | 52/125 [00:06<00:08,  8.24it/s, loss=2.2209, avg_loss=2.5555, ppl=12.88]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  42%|█████████████▉                   | 53/125 [00:06<00:08,  8.24it/s, loss=2.2209, avg_loss=2.5555, ppl=12.88]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  42%|█████████████▉                   | 53/125 [00:06<00:08,  8.24it/s, loss=2.7169, avg_loss=2.5584, ppl=12.92]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  43%|██████████████▎                  | 54/125 [00:06<00:08,  8.23it/s, loss=2.7169, avg_loss=2.5584, ppl=12.92]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  43%|██████████████▎                  | 54/125 [00:06<00:08,  8.23it/s, loss=3.1254, avg_loss=2.5688, ppl=13.05]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  44%|██████████████▌                  | 55/125 [00:06<00:08,  8.16it/s, loss=3.1254, avg_loss=2.5688, ppl=13.05]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  44%|██████████████▌                  | 55/125 [00:07<00:08,  8.16it/s, loss=2.4302, avg_loss=2.5663, ppl=13.02]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  45%|██████████████▊                  | 56/125 [00:07<00:08,  8.10it/s, loss=2.4302, avg_loss=2.5663, ppl=13.02]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  45%|██████████████▊                  | 56/125 [00:07<00:08,  8.10it/s, loss=1.7116, avg_loss=2.5513, ppl=12.82]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  46%|███████████████                  | 57/125 [00:07<00:08,  8.11it/s, loss=1.7116, avg_loss=2.5513, ppl=12.82]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  46%|███████████████                  | 57/125 [00:07<00:08,  8.11it/s, loss=2.4652, avg_loss=2.5498, ppl=12.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  46%|███████████████▎                 | 58/125 [00:07<00:08,  8.04it/s, loss=2.4652, avg_loss=2.5498, ppl=12.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  46%|███████████████▎                 | 58/125 [00:07<00:08,  8.04it/s, loss=2.3205, avg_loss=2.5459, ppl=12.75]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  47%|███████████████▌                 | 59/125 [00:07<00:08,  7.98it/s, loss=2.3205, avg_loss=2.5459, ppl=12.75]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  47%|███████████████▌                 | 59/125 [00:07<00:08,  7.98it/s, loss=2.2634, avg_loss=2.5412, ppl=12.69]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  48%|███████████████▊                 | 60/125 [00:07<00:08,  7.99it/s, loss=2.2634, avg_loss=2.5412, ppl=12.69]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  48%|███████████████▊                 | 60/125 [00:07<00:08,  7.99it/s, loss=2.2446, avg_loss=2.5363, ppl=12.63]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  49%|████████████████                 | 61/125 [00:07<00:07,  8.04it/s, loss=2.2446, avg_loss=2.5363, ppl=12.63]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  49%|████████████████                 | 61/125 [00:07<00:07,  8.04it/s, loss=2.3932, avg_loss=2.5340, ppl=12.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  50%|████████████████▎                | 62/125 [00:07<00:07,  8.11it/s, loss=2.3932, avg_loss=2.5340, ppl=12.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  50%|████████████████▎                | 62/125 [00:07<00:07,  8.11it/s, loss=2.4228, avg_loss=2.5323, ppl=12.58]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  50%|████████████████▋                | 63/125 [00:07<00:07,  8.18it/s, loss=2.4228, avg_loss=2.5323, ppl=12.58]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  50%|████████████████▋                | 63/125 [00:08<00:07,  8.18it/s, loss=2.0651, avg_loss=2.5250, ppl=12.49]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  51%|████████████████▉                | 64/125 [00:08<00:07,  8.22it/s, loss=2.0651, avg_loss=2.5250, ppl=12.49]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  51%|████████████████▉                | 64/125 [00:08<00:07,  8.22it/s, loss=2.2416, avg_loss=2.5206, ppl=12.44]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  52%|█████████████████▏               | 65/125 [00:08<00:07,  8.24it/s, loss=2.2416, avg_loss=2.5206, ppl=12.44]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  52%|█████████████████▏               | 65/125 [00:08<00:07,  8.24it/s, loss=2.2974, avg_loss=2.5172, ppl=12.39]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  53%|█████████████████▍               | 66/125 [00:08<00:07,  8.27it/s, loss=2.2974, avg_loss=2.5172, ppl=12.39]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  53%|█████████████████▍               | 66/125 [00:08<00:07,  8.27it/s, loss=2.3213, avg_loss=2.5143, ppl=12.36]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  54%|█████████████████▋               | 67/125 [00:08<00:07,  8.28it/s, loss=2.3213, avg_loss=2.5143, ppl=12.36]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  54%|█████████████████▋               | 67/125 [00:08<00:07,  8.28it/s, loss=2.3239, avg_loss=2.5115, ppl=12.32]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  54%|█████████████████▉               | 68/125 [00:08<00:06,  8.31it/s, loss=2.3239, avg_loss=2.5115, ppl=12.32]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  54%|█████████████████▉               | 68/125 [00:08<00:06,  8.31it/s, loss=2.2322, avg_loss=2.5075, ppl=12.27]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  55%|██████████████████▏              | 69/125 [00:08<00:06,  8.26it/s, loss=2.2322, avg_loss=2.5075, ppl=12.27]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  55%|██████████████████▏              | 69/125 [00:08<00:06,  8.26it/s, loss=2.0230, avg_loss=2.5005, ppl=12.19]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  56%|██████████████████▍              | 70/125 [00:08<00:06,  8.22it/s, loss=2.0230, avg_loss=2.5005, ppl=12.19]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  56%|██████████████████▍              | 70/125 [00:08<00:06,  8.22it/s, loss=2.5059, avg_loss=2.5006, ppl=12.19]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  57%|██████████████████▋              | 71/125 [00:08<00:06,  8.25it/s, loss=2.5059, avg_loss=2.5006, ppl=12.19]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  57%|██████████████████▋              | 71/125 [00:08<00:06,  8.25it/s, loss=2.1432, avg_loss=2.4956, ppl=12.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  58%|███████████████████              | 72/125 [00:08<00:06,  8.25it/s, loss=2.1432, avg_loss=2.4956, ppl=12.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  58%|███████████████████              | 72/125 [00:09<00:06,  8.25it/s, loss=2.0305, avg_loss=2.4893, ppl=12.05]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  58%|███████████████████▎             | 73/125 [00:09<00:06,  8.26it/s, loss=2.0305, avg_loss=2.4893, ppl=12.05]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  58%|███████████████████▎             | 73/125 [00:09<00:06,  8.26it/s, loss=2.6679, avg_loss=2.4917, ppl=12.08]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  59%|███████████████████▌             | 74/125 [00:09<00:06,  8.24it/s, loss=2.6679, avg_loss=2.4917, ppl=12.08]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  59%|███████████████████▌             | 74/125 [00:09<00:06,  8.24it/s, loss=2.3691, avg_loss=2.4901, ppl=12.06]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  60%|███████████████████▊             | 75/125 [00:09<00:06,  8.23it/s, loss=2.3691, avg_loss=2.4901, ppl=12.06]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  60%|███████████████████▊             | 75/125 [00:09<00:06,  8.23it/s, loss=1.7357, avg_loss=2.4801, ppl=11.94]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  61%|████████████████████             | 76/125 [00:09<00:05,  8.21it/s, loss=1.7357, avg_loss=2.4801, ppl=11.94]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  61%|████████████████████             | 76/125 [00:09<00:05,  8.21it/s, loss=2.3129, avg_loss=2.4780, ppl=11.92]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  62%|████████████████████▎            | 77/125 [00:09<00:05,  8.22it/s, loss=2.3129, avg_loss=2.4780, ppl=11.92]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  62%|████████████████████▎            | 77/125 [00:09<00:05,  8.22it/s, loss=2.1517, avg_loss=2.4738, ppl=11.87]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  62%|████████████████████▌            | 78/125 [00:09<00:05,  8.21it/s, loss=2.1517, avg_loss=2.4738, ppl=11.87]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  62%|████████████████████▌            | 78/125 [00:09<00:05,  8.21it/s, loss=2.5404, avg_loss=2.4746, ppl=11.88]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  63%|████████████████████▊            | 79/125 [00:09<00:05,  8.22it/s, loss=2.5404, avg_loss=2.4746, ppl=11.88]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  63%|████████████████████▊            | 79/125 [00:09<00:05,  8.22it/s, loss=2.5711, avg_loss=2.4758, ppl=11.89]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  64%|█████████████████████            | 80/125 [00:09<00:05,  8.25it/s, loss=2.5711, avg_loss=2.4758, ppl=11.89]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  64%|█████████████████████            | 80/125 [00:10<00:05,  8.25it/s, loss=2.2589, avg_loss=2.4731, ppl=11.86]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  65%|█████████████████████▍           | 81/125 [00:10<00:05,  8.26it/s, loss=2.2589, avg_loss=2.4731, ppl=11.86]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  65%|█████████████████████▍           | 81/125 [00:10<00:05,  8.26it/s, loss=2.0262, avg_loss=2.4677, ppl=11.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  66%|█████████████████████▋           | 82/125 [00:10<00:05,  8.26it/s, loss=2.0262, avg_loss=2.4677, ppl=11.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  66%|█████████████████████▋           | 82/125 [00:10<00:05,  8.26it/s, loss=2.3087, avg_loss=2.4658, ppl=11.77]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  66%|█████████████████████▉           | 83/125 [00:10<00:05,  8.26it/s, loss=2.3087, avg_loss=2.4658, ppl=11.77]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  66%|█████████████████████▉           | 83/125 [00:10<00:05,  8.26it/s, loss=2.0742, avg_loss=2.4611, ppl=11.72]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  67%|██████████████████████▏          | 84/125 [00:10<00:04,  8.25it/s, loss=2.0742, avg_loss=2.4611, ppl=11.72]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  67%|██████████████████████▏          | 84/125 [00:10<00:04,  8.25it/s, loss=2.2131, avg_loss=2.4582, ppl=11.68]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  68%|██████████████████████▍          | 85/125 [00:10<00:04,  8.27it/s, loss=2.2131, avg_loss=2.4582, ppl=11.68]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  68%|██████████████████████▍          | 85/125 [00:10<00:04,  8.27it/s, loss=2.0490, avg_loss=2.4534, ppl=11.63]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  69%|██████████████████████▋          | 86/125 [00:10<00:04,  8.27it/s, loss=2.0490, avg_loss=2.4534, ppl=11.63]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  69%|██████████████████████▋          | 86/125 [00:10<00:04,  8.27it/s, loss=2.2445, avg_loss=2.4510, ppl=11.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  70%|██████████████████████▉          | 87/125 [00:10<00:04,  8.25it/s, loss=2.2445, avg_loss=2.4510, ppl=11.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  70%|██████████████████████▉          | 87/125 [00:10<00:04,  8.25it/s, loss=2.1824, avg_loss=2.4480, ppl=11.57]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  70%|███████████████████████▏         | 88/125 [00:10<00:04,  8.23it/s, loss=2.1824, avg_loss=2.4480, ppl=11.57]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  70%|███████████████████████▏         | 88/125 [00:11<00:04,  8.23it/s, loss=2.5941, avg_loss=2.4496, ppl=11.58]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  71%|███████████████████████▍         | 89/125 [00:11<00:04,  8.23it/s, loss=2.5941, avg_loss=2.4496, ppl=11.58]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  71%|███████████████████████▍         | 89/125 [00:11<00:04,  8.23it/s, loss=2.0546, avg_loss=2.4452, ppl=11.53]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  72%|███████████████████████▊         | 90/125 [00:11<00:04,  8.24it/s, loss=2.0546, avg_loss=2.4452, ppl=11.53]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  72%|███████████████████████▊         | 90/125 [00:11<00:04,  8.24it/s, loss=2.2566, avg_loss=2.4432, ppl=11.51]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  73%|████████████████████████         | 91/125 [00:11<00:04,  8.23it/s, loss=2.2566, avg_loss=2.4432, ppl=11.51]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  73%|████████████████████████         | 91/125 [00:11<00:04,  8.23it/s, loss=2.1270, avg_loss=2.4397, ppl=11.47]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  74%|████████████████████████▎        | 92/125 [00:11<00:04,  8.24it/s, loss=2.1270, avg_loss=2.4397, ppl=11.47]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  74%|████████████████████████▎        | 92/125 [00:11<00:04,  8.24it/s, loss=2.2641, avg_loss=2.4378, ppl=11.45]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  74%|████████████████████████▌        | 93/125 [00:11<00:03,  8.19it/s, loss=2.2641, avg_loss=2.4378, ppl=11.45]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  74%|████████████████████████▌        | 93/125 [00:11<00:03,  8.19it/s, loss=2.2348, avg_loss=2.4357, ppl=11.42]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  75%|████████████████████████▊        | 94/125 [00:11<00:03,  8.19it/s, loss=2.2348, avg_loss=2.4357, ppl=11.42]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  75%|████████████████████████▊        | 94/125 [00:11<00:03,  8.19it/s, loss=2.0691, avg_loss=2.4318, ppl=11.38]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  76%|█████████████████████████        | 95/125 [00:11<00:03,  8.20it/s, loss=2.0691, avg_loss=2.4318, ppl=11.38]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  76%|█████████████████████████        | 95/125 [00:11<00:03,  8.20it/s, loss=2.1729, avg_loss=2.4291, ppl=11.35]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  77%|█████████████████████████▎       | 96/125 [00:11<00:03,  8.21it/s, loss=2.1729, avg_loss=2.4291, ppl=11.35]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  77%|█████████████████████████▎       | 96/125 [00:12<00:03,  8.21it/s, loss=2.2475, avg_loss=2.4273, ppl=11.33]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  78%|█████████████████████████▌       | 97/125 [00:12<00:03,  8.25it/s, loss=2.2475, avg_loss=2.4273, ppl=11.33]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  78%|█████████████████████████▌       | 97/125 [00:12<00:03,  8.25it/s, loss=2.1082, avg_loss=2.4240, ppl=11.29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  78%|█████████████████████████▊       | 98/125 [00:12<00:03,  8.24it/s, loss=2.1082, avg_loss=2.4240, ppl=11.29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  78%|█████████████████████████▊       | 98/125 [00:12<00:03,  8.24it/s, loss=2.2154, avg_loss=2.4219, ppl=11.27]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  79%|██████████████████████████▏      | 99/125 [00:12<00:03,  8.22it/s, loss=2.2154, avg_loss=2.4219, ppl=11.27]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  79%|██████████████████████████▏      | 99/125 [00:12<00:03,  8.22it/s, loss=2.1543, avg_loss=2.4192, ppl=11.24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  80%|█████████████████████████▌      | 100/125 [00:12<00:03,  8.20it/s, loss=2.1543, avg_loss=2.4192, ppl=11.24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  80%|█████████████████████████▌      | 100/125 [00:12<00:03,  8.20it/s, loss=2.3055, avg_loss=2.4181, ppl=11.22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  81%|█████████████████████████▊      | 101/125 [00:12<00:02,  8.20it/s, loss=2.3055, avg_loss=2.4181, ppl=11.22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  81%|█████████████████████████▊      | 101/125 [00:12<00:02,  8.20it/s, loss=2.2802, avg_loss=2.4167, ppl=11.21]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  82%|██████████████████████████      | 102/125 [00:12<00:02,  8.22it/s, loss=2.2802, avg_loss=2.4167, ppl=11.21]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  82%|██████████████████████████      | 102/125 [00:12<00:02,  8.22it/s, loss=1.9642, avg_loss=2.4123, ppl=11.16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  82%|██████████████████████████▎     | 103/125 [00:12<00:02,  8.25it/s, loss=1.9642, avg_loss=2.4123, ppl=11.16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  82%|██████████████████████████▎     | 103/125 [00:12<00:02,  8.25it/s, loss=2.0628, avg_loss=2.4090, ppl=11.12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  83%|██████████████████████████▌     | 104/125 [00:12<00:02,  8.25it/s, loss=2.0628, avg_loss=2.4090, ppl=11.12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  83%|██████████████████████████▌     | 104/125 [00:13<00:02,  8.25it/s, loss=2.4469, avg_loss=2.4093, ppl=11.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  84%|██████████████████████████▉     | 105/125 [00:13<00:02,  8.26it/s, loss=2.4469, avg_loss=2.4093, ppl=11.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  84%|██████████████████████████▉     | 105/125 [00:13<00:02,  8.26it/s, loss=2.4743, avg_loss=2.4100, ppl=11.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  85%|███████████████████████████▏    | 106/125 [00:13<00:02,  8.26it/s, loss=2.4743, avg_loss=2.4100, ppl=11.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  85%|███████████████████████████▏    | 106/125 [00:13<00:02,  8.26it/s, loss=2.2674, avg_loss=2.4086, ppl=11.12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  86%|███████████████████████████▍    | 107/125 [00:13<00:02,  8.24it/s, loss=2.2674, avg_loss=2.4086, ppl=11.12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  86%|███████████████████████████▍    | 107/125 [00:13<00:02,  8.24it/s, loss=2.3778, avg_loss=2.4083, ppl=11.12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  86%|███████████████████████████▋    | 108/125 [00:13<00:02,  8.22it/s, loss=2.3778, avg_loss=2.4083, ppl=11.12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  86%|███████████████████████████▋    | 108/125 [00:13<00:02,  8.22it/s, loss=2.1829, avg_loss=2.4063, ppl=11.09]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  87%|███████████████████████████▉    | 109/125 [00:13<00:01,  8.22it/s, loss=2.1829, avg_loss=2.4063, ppl=11.09]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  87%|███████████████████████████▉    | 109/125 [00:13<00:01,  8.22it/s, loss=2.5969, avg_loss=2.4080, ppl=11.11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  88%|████████████████████████████▏   | 110/125 [00:13<00:01,  8.23it/s, loss=2.5969, avg_loss=2.4080, ppl=11.11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  88%|████████████████████████████▏   | 110/125 [00:13<00:01,  8.23it/s, loss=2.6130, avg_loss=2.4098, ppl=11.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  89%|████████████████████████████▍   | 111/125 [00:13<00:01,  8.21it/s, loss=2.6130, avg_loss=2.4098, ppl=11.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  89%|████████████████████████████▍   | 111/125 [00:13<00:01,  8.21it/s, loss=2.5029, avg_loss=2.4107, ppl=11.14]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  90%|████████████████████████████▋   | 112/125 [00:13<00:01,  8.21it/s, loss=2.5029, avg_loss=2.4107, ppl=11.14]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  90%|████████████████████████████▋   | 112/125 [00:13<00:01,  8.21it/s, loss=2.1794, avg_loss=2.4086, ppl=11.12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  90%|████████████████████████████▉   | 113/125 [00:13<00:01,  8.19it/s, loss=2.1794, avg_loss=2.4086, ppl=11.12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  90%|████████████████████████████▉   | 113/125 [00:14<00:01,  8.19it/s, loss=2.1739, avg_loss=2.4066, ppl=11.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  91%|█████████████████████████████▏  | 114/125 [00:14<00:01,  8.23it/s, loss=2.1739, avg_loss=2.4066, ppl=11.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  91%|█████████████████████████████▏  | 114/125 [00:14<00:01,  8.23it/s, loss=2.1790, avg_loss=2.4046, ppl=11.07]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  92%|█████████████████████████████▍  | 115/125 [00:14<00:01,  8.24it/s, loss=2.1790, avg_loss=2.4046, ppl=11.07]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  92%|█████████████████████████████▍  | 115/125 [00:14<00:01,  8.24it/s, loss=2.3944, avg_loss=2.4045, ppl=11.07]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  93%|█████████████████████████████▋  | 116/125 [00:14<00:01,  8.25it/s, loss=2.3944, avg_loss=2.4045, ppl=11.07]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  93%|█████████████████████████████▋  | 116/125 [00:14<00:01,  8.25it/s, loss=1.8367, avg_loss=2.3997, ppl=11.02]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  94%|█████████████████████████████▉  | 117/125 [00:14<00:00,  8.24it/s, loss=1.8367, avg_loss=2.3997, ppl=11.02]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  94%|█████████████████████████████▉  | 117/125 [00:14<00:00,  8.24it/s, loss=2.2416, avg_loss=2.3983, ppl=11.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  94%|██████████████████████████████▏ | 118/125 [00:14<00:00,  8.25it/s, loss=2.2416, avg_loss=2.3983, ppl=11.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  94%|██████████████████████████████▏ | 118/125 [00:14<00:00,  8.25it/s, loss=2.3168, avg_loss=2.3976, ppl=11.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  95%|██████████████████████████████▍ | 119/125 [00:14<00:00,  8.27it/s, loss=2.3168, avg_loss=2.3976, ppl=11.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  95%|██████████████████████████████▍ | 119/125 [00:14<00:00,  8.27it/s, loss=2.7319, avg_loss=2.4004, ppl=11.03]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  96%|██████████████████████████████▋ | 120/125 [00:14<00:00,  8.26it/s, loss=2.7319, avg_loss=2.4004, ppl=11.03]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  96%|██████████████████████████████▋ | 120/125 [00:14<00:00,  8.26it/s, loss=2.1086, avg_loss=2.3980, ppl=11.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  97%|██████████████████████████████▉ | 121/125 [00:14<00:00,  8.27it/s, loss=2.1086, avg_loss=2.3980, ppl=11.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  97%|██████████████████████████████▉ | 121/125 [00:15<00:00,  8.27it/s, loss=2.3529, avg_loss=2.3976, ppl=11.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  98%|███████████████████████████████▏| 122/125 [00:15<00:00,  8.26it/s, loss=2.3529, avg_loss=2.3976, ppl=11.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  98%|███████████████████████████████▏| 122/125 [00:15<00:00,  8.26it/s, loss=2.5546, avg_loss=2.3989, ppl=11.01]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  98%|███████████████████████████████▍| 123/125 [00:15<00:00,  8.27it/s, loss=2.5546, avg_loss=2.3989, ppl=11.01]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  98%|███████████████████████████████▍| 123/125 [00:15<00:00,  8.27it/s, loss=2.1923, avg_loss=2.3972, ppl=10.99]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  99%|███████████████████████████████▋| 124/125 [00:15<00:00,  8.26it/s, loss=2.1923, avg_loss=2.3972, ppl=10.99]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1:  99%|███████████████████████████████▋| 124/125 [00:15<00:00,  8.26it/s, loss=2.6192, avg_loss=2.3990, ppl=11.01]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1: 100%|████████████████████████████████| 125/125 [00:15<00:00,  8.23it/s, loss=2.6192, avg_loss=2.3990, ppl=11.01]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/1: 100%|████████████████████████████████| 125/125 [00:15<00:00,  8.10it/s, loss=2.6192, avg_loss=2.3990, ppl=11.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Epoch 1 complete!\n",
      "  Final average loss: 2.3990\n",
      "  Final perplexity: 11.01\n",
      "\n",
      "🎉 Training complete!\n",
      "\n",
      "The model has now seen 500 examples of how to follow instructions.\n",
      "Let's see if it actually learned anything...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The actual training loop\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"Watch the loss go down! (Lower = better)\")\n",
    "print(\"\\nMetrics explained:\")\n",
    "print(\"  - loss: How wrong the model is (lower = better)\")\n",
    "print(\"  - avg_loss: Running average of loss\")\n",
    "print(\"  - ppl: Perplexity (e^loss), another way to measure quality\")\n",
    "print(\"\\nGo grab a coffee. This'll take a few minutes...\\n\")\n",
    "\n",
    "model.train()  # Put model in training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move batch to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass: compute loss\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        loss.backward()        # Compute new gradients\n",
    "        \n",
    "        # Clip gradients (prevent explosions)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (step + 1)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{avg_loss:.4f}',\n",
    "            'ppl': f'{np.exp(avg_loss):.2f}'\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n✓ Epoch {epoch+1} complete!\")\n",
    "    print(f\"  Final average loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Final perplexity: {np.exp(avg_loss):.2f}\")\n",
    "\n",
    "print(\"\\n🎉 Training complete!\")\n",
    "print(\"\\nThe model has now seen 500 examples of how to follow instructions.\")\n",
    "print(\"Let's see if it actually learned anything...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the Fine-Tuned Model\n",
    "\n",
    "Moment of truth. Same questions as before, but now the model has been fine-tuned.\n",
    "\n",
    "Will it actually answer the questions this time? Let's find out.\n",
    "\n",
    "(If the answers are still gibberish, don't panic—check the training loss. If it went down, the model learned *something*. You might just need more training steps or better hyperparameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:35.847546Z",
     "iopub.status.busy": "2025-12-08T22:52:35.847450Z",
     "iopub.status.idle": "2025-12-08T22:52:36.590597Z",
     "shell.execute_reply": "2025-12-08T22:52:36.590250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINE-TUNED MODEL (after training):\n",
      "======================================================================\n",
      "Same questions as before. Notice the difference?\n",
      "\n",
      "Q: What is the capital of France?\n",
      "A: France is the capital of France.\n",
      "----------------------------------------------------------------------\n",
      "Q: Write a haiku about programming.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The haiku is a form of art that uses language to express ideas. It is often referred to as a form of \"soul-reading.\" It is a way of thinking that is both poetic and poetic in nature.\n",
      "----------------------------------------------------------------------\n",
      "Q: Explain machine learning in one sentence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Machine learning is a technique that helps us learn a new set of skills, such as language, syntax, and syntax, by analyzing data and analyzing it in a way that allows us to better understand the data and its properties. Machine learning algorithms can be trained to learn specific tasks, such as tasks requiring information, such as visual recognition or text-based navigation, by analyzing data and generating algorithms that predict the results based on the data. This technique allows us to design and execute new tasks faster, and\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Much better, right?\n",
      "\n",
      "The model isn't perfect (it's only seen 500 examples), but it's actually\n",
      "trying to answer the questions now instead of just rambling.\n",
      "\n",
      "That's the power of supervised fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# Switch to evaluation mode (disables dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "print(\"FINE-TUNED MODEL (after training):\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Same questions as before. Notice the difference?\\n\")\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    print(f\"Q: {instruction}\")\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nMuch better, right?\")\n",
    "print(\"\\nThe model isn't perfect (it's only seen 500 examples), but it's actually\")\n",
    "print(\"trying to answer the questions now instead of just rambling.\")\n",
    "print(\"\\nThat's the power of supervised fine-tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:36.591396Z",
     "iopub.status.busy": "2025-12-08T22:52:36.591318Z",
     "iopub.status.idle": "2025-12-08T22:52:38.365667Z",
     "shell.execute_reply": "2025-12-08T22:52:38.365289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's try some different questions:\n",
      "======================================================================\n",
      "\n",
      "Q: List three benefits of exercise.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 1. Exercise reduces stress and improves cognitive function\n",
      "\n",
      "2. Exercise improves mood, anxiety, and quality of life\n",
      "\n",
      "3. Exercise improves physical activity and improves mood and well-being\n",
      "\n",
      "4. Exercise improves health, quality of life, and overall well-being\n",
      "\n",
      "5. Exercise improves overall well-being\n",
      "\n",
      "6. Exercise improves mental health, stability, and productivity\n",
      "\n",
      "7. Exercise improves social and emotional well-being\n",
      "\n",
      "8. Exercise improves physical health, stability\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Q: What is Python used for?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Python is a powerful scripting language for programming languages. It provides powerful scripting languages, such as Python and Ruby, to build and manipulate data. It is also the programming language for building applications and applications for industries such as medicine, manufacturing, and finance. It is a powerful tool for developing and deploying software applications, such as web applications, web services, and distributed systems. Python is also a powerful language for building applications and applications for industries such as healthcare, manufacturing, and finance.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Q: Explain what a neural network is in simple terms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: A neural network is a system of neurons that are interconnected and connected to one another. It is a network of neurons that is made up of many layers and connections that are interconnected. The connections between neurons are made up of several layers, each of which is connected to a specific neuron by a specific message or event. Each layer has its own unique set of neurons, which are used to communicate with each other. Each layer has its own neurons, which are used to control its own behaviors. Each layer\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Q: Write a short poem about the ocean.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The ocean is a beautiful, mysterious, and majestic place, full of wondrous wonders, including the beauty of the sea and the mystery of life on the other side of the planet. It is filled with wonderful creatures and wonders, and it is filled with beauty and mystery. It is filled with beauty and mystery and has a timeless beauty, and it is filled with wonder.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "**Key observation:** The model has learned the *pattern* of instruction-following,\n",
      "not just memorized specific facts. It generalizes to new questions!\n",
      "\n",
      "Though sometimes it gets a bit... creative. (That's LLMs for you.)\n"
     ]
    }
   ],
   "source": [
    "# Let's try some more examples to really see what it can do\n",
    "additional_tests = [\n",
    "    \"List three benefits of exercise.\",\n",
    "    \"What is Python used for?\",\n",
    "    \"Explain what a neural network is in simple terms.\",\n",
    "    \"Write a short poem about the ocean.\",\n",
    "]\n",
    "\n",
    "print(\"\\nLet's try some different questions:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for instruction in additional_tests:\n",
    "    print(f\"\\nQ: {instruction}\")\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n**Key observation:** The model has learned the *pattern* of instruction-following,\")\n",
    "print(\"not just memorized specific facts. It generalizes to new questions!\")\n",
    "print(\"\\nThough sometimes it gets a bit... creative. (That's LLMs for you.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Quantitative Evaluation\n",
    "\n",
    "Okay, so the model *seems* better based on the examples. But how do we measure that objectively?\n",
    "\n",
    "Two key metrics:\n",
    "\n",
    "1. **Perplexity:** How \"surprised\" the model is by the training data. Lower = better. It's basically e^(loss). Think of it as \"confidence\"—how well does the model predict what comes next?\n",
    "\n",
    "2. **Diversity:** Do all the responses sound the same, or does the model have variety? We measure this with distinct-1 and distinct-2 (percentage of unique words and word pairs). Too low = mode collapse (model stuck in a rut).\n",
    "\n",
    "Let's compute both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:38.366592Z",
     "iopub.status.busy": "2025-12-08T22:52:38.366456Z",
     "iopub.status.idle": "2025-12-08T22:52:47.013684Z",
     "shell.execute_reply": "2025-12-08T22:52:47.013356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model quality...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:   0%|                                                                        | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:   2%|█                                                               | 2/125 [00:00<00:08, 13.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:   3%|██                                                              | 4/125 [00:00<00:08, 14.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:   5%|███                                                             | 6/125 [00:00<00:08, 14.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:   6%|████                                                            | 8/125 [00:00<00:08, 14.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:   8%|█████                                                          | 10/125 [00:00<00:07, 14.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  10%|██████                                                         | 12/125 [00:00<00:07, 14.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  11%|███████                                                        | 14/125 [00:00<00:07, 14.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  13%|████████                                                       | 16/125 [00:01<00:07, 14.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  14%|█████████                                                      | 18/125 [00:01<00:07, 14.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  16%|██████████                                                     | 20/125 [00:01<00:07, 14.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  18%|███████████                                                    | 22/125 [00:01<00:07, 14.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  19%|████████████                                                   | 24/125 [00:01<00:06, 14.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  21%|█████████████                                                  | 26/125 [00:01<00:06, 14.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  22%|██████████████                                                 | 28/125 [00:01<00:06, 14.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  24%|███████████████                                                | 30/125 [00:02<00:06, 14.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  26%|████████████████▏                                              | 32/125 [00:02<00:06, 14.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  27%|█████████████████▏                                             | 34/125 [00:02<00:06, 14.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  29%|██████████████████▏                                            | 36/125 [00:02<00:06, 14.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  30%|███████████████████▏                                           | 38/125 [00:02<00:05, 14.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  32%|████████████████████▏                                          | 40/125 [00:02<00:05, 14.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  34%|█████████████████████▏                                         | 42/125 [00:02<00:05, 14.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  35%|██████████████████████▏                                        | 44/125 [00:03<00:05, 14.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  37%|███████████████████████▏                                       | 46/125 [00:03<00:05, 14.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  38%|████████████████████████▏                                      | 48/125 [00:03<00:05, 14.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  40%|█████████████████████████▏                                     | 50/125 [00:03<00:05, 14.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  42%|██████████████████████████▏                                    | 52/125 [00:03<00:05, 14.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  43%|███████████████████████████▏                                   | 54/125 [00:03<00:04, 14.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  45%|████████████████████████████▏                                  | 56/125 [00:03<00:04, 14.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  46%|█████████████████████████████▏                                 | 58/125 [00:04<00:04, 14.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  48%|██████████████████████████████▏                                | 60/125 [00:04<00:04, 14.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  50%|███████████████████████████████▏                               | 62/125 [00:04<00:04, 14.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  51%|████████████████████████████████▎                              | 64/125 [00:04<00:04, 14.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  53%|█████████████████████████████████▎                             | 66/125 [00:04<00:04, 14.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  54%|██████████████████████████████████▎                            | 68/125 [00:04<00:03, 14.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  56%|███████████████████████████████████▎                           | 70/125 [00:04<00:03, 14.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  58%|████████████████████████████████████▎                          | 72/125 [00:04<00:03, 14.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  59%|█████████████████████████████████████▎                         | 74/125 [00:05<00:03, 14.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  61%|██████████████████████████████████████▎                        | 76/125 [00:05<00:03, 14.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  62%|███████████████████████████████████████▎                       | 78/125 [00:05<00:03, 14.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  64%|████████████████████████████████████████▎                      | 80/125 [00:05<00:03, 14.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  66%|█████████████████████████████████████████▎                     | 82/125 [00:05<00:02, 14.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  67%|██████████████████████████████████████████▎                    | 84/125 [00:05<00:02, 14.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  69%|███████████████████████████████████████████▎                   | 86/125 [00:05<00:02, 14.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  70%|████████████████████████████████████████████▎                  | 88/125 [00:06<00:02, 14.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  72%|█████████████████████████████████████████████▎                 | 90/125 [00:06<00:02, 14.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  74%|██████████████████████████████████████████████▎                | 92/125 [00:06<00:02, 14.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  75%|███████████████████████████████████████████████▍               | 94/125 [00:06<00:02, 14.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  77%|████████████████████████████████████████████████▍              | 96/125 [00:06<00:01, 14.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  78%|█████████████████████████████████████████████████▍             | 98/125 [00:06<00:01, 14.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  80%|█████████████████████████████████████████████████▌            | 100/125 [00:06<00:01, 14.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  82%|██████████████████████████████████████████████████▌           | 102/125 [00:07<00:01, 14.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  83%|███████████████████████████████████████████████████▌          | 104/125 [00:07<00:01, 14.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  85%|████████████████████████████████████████████████████▌         | 106/125 [00:07<00:01, 14.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  86%|█████████████████████████████████████████████████████▌        | 108/125 [00:07<00:01, 14.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  88%|██████████████████████████████████████████████████████▌       | 110/125 [00:07<00:01, 14.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  90%|███████████████████████████████████████████████████████▌      | 112/125 [00:07<00:00, 14.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  91%|████████████████████████████████████████████████████████▌     | 114/125 [00:07<00:00, 14.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  93%|█████████████████████████████████████████████████████████▌    | 116/125 [00:08<00:00, 14.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  94%|██████████████████████████████████████████████████████████▌   | 118/125 [00:08<00:00, 14.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  96%|███████████████████████████████████████████████████████████▌  | 120/125 [00:08<00:00, 14.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  98%|████████████████████████████████████████████████████████████▌ | 122/125 [00:08<00:00, 14.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity:  99%|█████████████████████████████████████████████████████████████▌| 124/125 [00:08<00:00, 14.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing perplexity: 100%|██████████████████████████████████████████████████████████████| 125/125 [00:08<00:00, 14.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Evaluation complete!\n",
      "  Loss: 1.9564\n",
      "  Perplexity: 7.07\n",
      "\n",
      "Interpretation:\n",
      "  - Perplexity < 10: Excellent\n",
      "  - Perplexity 10-20: Good\n",
      "  - Perplexity 20-50: Okay\n",
      "  - Perplexity > 50: Needs more training\n",
      "\n",
      "Your model: Excellent! 🎉\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Compute perplexity on a dataset.\n",
    "    \n",
    "    Perplexity = e^(average loss)\n",
    "    \n",
    "    Think of it as: \"On average, how many equally-likely tokens could come next?\"\n",
    "    Lower is better. Random guessing on a 50k vocab = perplexity of 50,000.\n",
    "    A well-trained model on instructions = perplexity of 5-10.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for batch in tqdm(dataloader, desc=\"Computing perplexity\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            # Count non-masked tokens (only response tokens, not prompt)\n",
    "            num_tokens = (batch['labels'] != -100).sum().item()\n",
    "            total_loss += outputs.loss.item() * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "# Compute perplexity on the training set\n",
    "# (In practice, you'd use a held-out validation set, but we're keeping it simple)\n",
    "print(\"\\nEvaluating model quality...\")\n",
    "perplexity, loss = compute_perplexity(model, train_loader, device)\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete!\")\n",
    "print(f\"  Loss: {loss:.4f}\")\n",
    "print(f\"  Perplexity: {perplexity:.2f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Perplexity < 10: Excellent\")\n",
    "print(f\"  - Perplexity 10-20: Good\")\n",
    "print(f\"  - Perplexity 20-50: Okay\")\n",
    "print(f\"  - Perplexity > 50: Needs more training\")\n",
    "print(f\"\\nYour model: \", end=\"\")\n",
    "if perplexity < 10:\n",
    "    print(\"Excellent! 🎉\")\n",
    "elif perplexity < 20:\n",
    "    print(\"Good! 👍\")\n",
    "elif perplexity < 50:\n",
    "    print(\"Okay. More training would help.\")\n",
    "else:\n",
    "    print(\"Needs more training. Try more epochs or more data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:47.014586Z",
     "iopub.status.busy": "2025-12-08T22:52:47.014502Z",
     "iopub.status.idle": "2025-12-08T22:52:49.385029Z",
     "shell.execute_reply": "2025-12-08T22:52:49.384658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses for diversity analysis...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Diversity analysis complete!\n",
      "  Distinct-1 (unique words): 36.51%\n",
      "  Distinct-2 (unique word pairs): 73.39%\n",
      "\n",
      "Interpretation:\n",
      "  - Distinct-1 > 40%: Good variety\n",
      "  - Distinct-1 20-40%: Okay\n",
      "  - Distinct-1 < 20%: Mode collapse (model stuck repeating itself)\n",
      "\n",
      "Your model: Okay diversity.\n"
     ]
    }
   ],
   "source": [
    "def compute_diversity(responses):\n",
    "    \"\"\"\n",
    "    Compute diversity metrics for generated text.\n",
    "    \n",
    "    Distinct-1: Percentage of unique words (unigrams)\n",
    "    Distinct-2: Percentage of unique word pairs (bigrams)\n",
    "    \n",
    "    Why does this matter? If the model always says \"the the the the\" you'd have\n",
    "    low diversity even if perplexity looks okay. Diversity catches mode collapse.\n",
    "    \"\"\"\n",
    "    all_unigrams = []\n",
    "    all_bigrams = []\n",
    "    \n",
    "    for response in responses:\n",
    "        tokens = response.lower().split()\n",
    "        all_unigrams.extend(tokens)\n",
    "        # Create pairs of consecutive words\n",
    "        all_bigrams.extend(zip(tokens[:-1], tokens[1:]))\n",
    "    \n",
    "    # What fraction of words/pairs are unique?\n",
    "    distinct_1 = len(set(all_unigrams)) / len(all_unigrams) if all_unigrams else 0\n",
    "    distinct_2 = len(set(all_bigrams)) / len(all_bigrams) if all_bigrams else 0\n",
    "    \n",
    "    return distinct_1, distinct_2\n",
    "\n",
    "# Generate a bunch of responses for diversity analysis\n",
    "diversity_prompts = [\n",
    "    \"Tell me about machine learning.\",\n",
    "    \"Explain artificial intelligence.\",\n",
    "    \"What is deep learning?\",\n",
    "    \"Describe natural language processing.\",\n",
    "    \"Explain what data science is.\",\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating responses for diversity analysis...\")\n",
    "responses = [generate_response(model, tokenizer, p) for p in diversity_prompts]\n",
    "d1, d2 = compute_diversity(responses)\n",
    "\n",
    "print(f\"\\n✓ Diversity analysis complete!\")\n",
    "print(f\"  Distinct-1 (unique words): {d1:.2%}\")\n",
    "print(f\"  Distinct-2 (unique word pairs): {d2:.2%}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Distinct-1 > 40%: Good variety\")\n",
    "print(f\"  - Distinct-1 20-40%: Okay\")\n",
    "print(f\"  - Distinct-1 < 20%: Mode collapse (model stuck repeating itself)\")\n",
    "print(f\"\\nYour model: \", end=\"\")\n",
    "if d1 > 0.4:\n",
    "    print(\"Good variety! 🎉\")\n",
    "elif d1 > 0.2:\n",
    "    print(\"Okay diversity.\")\n",
    "else:\n",
    "    print(\"Warning: Low diversity. Try different sampling parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Your Model\n",
    "\n",
    "You just spent 15 minutes training this thing. Let's not lose it!\n",
    "\n",
    "Saving is simple—we just dump the model weights and tokenizer config to disk. Then you can reload them later (or share them with others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:49.385834Z",
     "iopub.status.busy": "2025-12-08T22:52:49.385761Z",
     "iopub.status.idle": "2025-12-08T22:52:50.000506Z",
     "shell.execute_reply": "2025-12-08T22:52:50.000173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./my_finetuned_model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved!\n",
      "\n",
      "Saved files:\n",
      "  config.json: 0.0 MB\n",
      "  generation_config.json: 0.0 MB\n",
      "  merges.txt: 0.5 MB\n",
      "  model.safetensors: 497.8 MB\n",
      "  special_tokens_map.json: 0.0 MB\n",
      "  tokenizer.json: 3.6 MB\n",
      "  tokenizer_config.json: 0.0 MB\n",
      "  vocab.json: 0.8 MB\n",
      "\n",
      "Total size: 502.6 MB\n",
      "\n",
      "You can now load this model anytime with:\n",
      "  model = AutoModelForCausalLM.from_pretrained('./my_finetuned_model')\n",
      "  tokenizer = AutoTokenizer.from_pretrained('./my_finetuned_model')\n"
     ]
    }
   ],
   "source": [
    "# Save model and tokenizer to disk\n",
    "save_path = \"./my_finetuned_model\"\n",
    "\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"✓ Model saved!\")\n",
    "\n",
    "# Show what got saved\n",
    "import os\n",
    "print(f\"\\nSaved files:\")\n",
    "total_size = 0\n",
    "for f in sorted(os.listdir(save_path)):\n",
    "    size = os.path.getsize(os.path.join(save_path, f)) / 1e6\n",
    "    total_size += size\n",
    "    print(f\"  {f}: {size:.1f} MB\")\n",
    "\n",
    "print(f\"\\nTotal size: {total_size:.1f} MB\")\n",
    "print(f\"\\nYou can now load this model anytime with:\")\n",
    "print(f\"  model = AutoModelForCausalLM.from_pretrained('{save_path}')\")\n",
    "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{save_path}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:52:50.001441Z",
     "iopub.status.busy": "2025-12-08T22:52:50.001358Z",
     "iopub.status.idle": "2025-12-08T22:52:50.510366Z",
     "shell.execute_reply": "2025-12-08T22:52:50.509939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing saved model (to make sure saving worked)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded model from disk successfully!\n",
      "\n",
      "Test question: What is the meaning of life?\n",
      "Answer: Life is a complex, fascinating, rewarding, and exciting experience. It is the culmination of a wide range of emotions, including sadness, joy, and pain. It is also the most fulfilling and rewarding experience of our lives.\n",
      "\n",
      "Looks good! Your model is saved and ready to use.\n"
     ]
    }
   ],
   "source": [
    "# Let's verify the saved model actually works\n",
    "print(\"Testing saved model (to make sure saving worked)...\")\n",
    "\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "test_instruction = \"What is the meaning of life?\"\n",
    "response = generate_response(loaded_model, loaded_tokenizer, test_instruction)\n",
    "\n",
    "print(f\"\\n✓ Loaded model from disk successfully!\")\n",
    "print(f\"\\nTest question: {test_instruction}\")\n",
    "print(f\"Answer: {response}\")\n",
    "print(f\"\\nLooks good! Your model is saved and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Did It! 🎉\n",
    "\n",
    "Seriously. You just fine-tuned a language model from scratch.\n",
    "\n",
    "**What you accomplished:**\n",
    "1. Loaded a base GPT-2 model (terrible at following instructions)\n",
    "2. Prepared training data with proper label masking\n",
    "3. Trained the model using supervised fine-tuning\n",
    "4. Watched it go from gibberish to actual answers\n",
    "5. Evaluated it with perplexity and diversity metrics\n",
    "6. Saved it for later use\n",
    "\n",
    "This is the same basic process used to create ChatGPT, Claude, and every other instruction-following LLM. The production versions use more data, bigger models, LoRA for efficiency, and RLHF for alignment—but the core idea is exactly what you just did.\n",
    "\n",
    "## What to Try Next\n",
    "\n",
    "Now that you've got the basics down:\n",
    "\n",
    "1. **Train longer** - Try 3-5 epochs or use the full Alpaca dataset (52k examples)\n",
    "2. **Use LoRA** - Fine-tune only a small number of parameters (way more efficient)\n",
    "3. **Try DPO** - Align the model with human preferences using the reward/preference notebooks\n",
    "4. **Bigger models** - GPT-2 Medium/Large, or even Llama if you've got the VRAM\n",
    "5. **Your own data** - Got a specific task? Create a dataset and fine-tune for it!\n",
    "\n",
    "## Common Issues & Tips\n",
    "\n",
    "**Loss not going down?**\n",
    "- Check your learning rate (try 1e-5 to 1e-4)\n",
    "- Make sure labels are masked properly (prompt tokens should be -100)\n",
    "- Try more epochs or more data\n",
    "\n",
    "**Model output is repetitive?**\n",
    "- Adjust temperature and top_p during generation\n",
    "- Check diversity metrics (distinct-1/distinct-2)\n",
    "- Might need more varied training data\n",
    "\n",
    "**Out of memory?**\n",
    "- Reduce batch_size (try 2 or 1)\n",
    "- Reduce max_length (try 128 or 64)\n",
    "- Use gradient checkpointing (more compute, less memory)\n",
    "- Consider LoRA (way less memory)\n",
    "\n",
    "**Answers are still bad?**\n",
    "- Train on more data (500 examples is pretty small)\n",
    "- Train for more epochs\n",
    "- Check that loss actually decreased during training\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "The model you just trained isn't perfect. It might hallucinate, give weird answers, or ramble sometimes. That's normal! You trained it on 500 examples for 10 minutes.\n",
    "\n",
    "What matters is that you understand the *process*. You know how to:\n",
    "- Load and prepare data\n",
    "- Set up a training loop\n",
    "- Evaluate results\n",
    "- Debug when things go wrong\n",
    "\n",
    "That's the hard part. Scaling up to production is just... more of the same, but bigger.\n",
    "\n",
    "Go build something cool. 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
