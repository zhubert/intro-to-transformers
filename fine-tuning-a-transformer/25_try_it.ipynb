{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Try It Yourself!\n\n**Your turn to build something real.**\n\nThis is it. The capstone. All those notebooks you just went through? Time to put them into practice.\n\nWe're going to fine-tune a language model from scratch. Not a toy example. A real model that actually learns to follow instructions. By the end of this, you'll have your own fine-tuned GPT-2 sitting on your hard drive, ready to answer questions.\n\nSound good? Let's go."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What You're About to Build\n\nHere's the deal: we're taking a base GPT-2 modelâ€”one that's pretty good at predicting the next word but terrible at following instructionsâ€”and teaching it to be helpful.\n\n**What you'll learn:**\n- How to actually train a model (not just read about it)\n- Why supervised fine-tuning works\n- How to evaluate if your model is any good\n- What to watch out for when things go wrong\n\n**Time investment:** 30-60 minutes, depending on your hardware. Got a GPU? Closer to 30. Running on CPU? Grab a coffee and make it an hour.\n\n**Important note:** This is a simplified version for learning. Production fine-tuning would use LoRA (which we covered), more data, and better hyperparameter tuning. But the core ideas? Exactly the same."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Verify Your Environment\n\nFirst things firstâ€”let's make sure you've got PyTorch installed and that it can see your GPU (if you have one).\n\nWhy check this first? Because finding out 20 minutes into training that CUDA isn't working is... well, let's just say it's a learning experience you only need once."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:34.151211Z",
     "iopub.status.busy": "2025-12-06T23:30:34.151112Z",
     "iopub.status.idle": "2025-12-06T23:30:34.845421Z",
     "shell.execute_reply": "2025-12-06T23:30:34.845095Z"
    }
   },
   "outputs": [],
   "source": "# Check what we're working with\nimport torch\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(\"Great! Training will be fast.\")\nelse:\n    print(\"Device: CPU\")\n    print(\"No GPU found. Training will work but be slower (10-20x).\")\n    print(\"Consider reducing num_samples in the data loading step.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:34.862073Z",
     "iopub.status.busy": "2025-12-06T23:30:34.861938Z",
     "iopub.status.idle": "2025-12-06T23:30:35.993881Z",
     "shell.execute_reply": "2025-12-06T23:30:35.993535Z"
    }
   },
   "outputs": [],
   "source": "# Import everything we need\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport numpy as np\n\nprint(\"All imports successful!\")\nprint(\"\\nIf you got any errors above, install missing packages with:\")\nprint(\"  pip install transformers datasets torch tqdm\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Load the Base Model\n\nWe're using GPT-2 (the small version, 124M parameters). Why GPT-2 and not something bigger?\n\n1. **It's fast to train** - You can actually finish this notebook today\n2. **It's well-understood** - Lots of documentation if things break\n3. **It's big enough to learn** - 124M parameters is plenty for instruction following\n\nThink of this as the \"before\" photo. The model right now is decent at continuing text but hopeless at following instructions. We're about to fix that."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:35.994998Z",
     "iopub.status.busy": "2025-12-06T23:30:35.994871Z",
     "iopub.status.idle": "2025-12-06T23:30:36.958510Z",
     "shell.execute_reply": "2025-12-06T23:30:36.957963Z"
    }
   },
   "outputs": [],
   "source": "# Load GPT-2 (small, 124M parameters)\nmodel_name = \"gpt2\"\n\nprint(f\"Loading {model_name}...\")\nprint(\"This downloads ~500MB the first time, then caches locally.\\n\")\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# GPT-2 doesn't have a padding token by default, so we add one\n# We just reuse the EOS tokenâ€”common practice and works fine\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = tokenizer.eos_token_id\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# How big is this thing?\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"âœ“ Model loaded!\")\nprint(f\"  Parameters: {total_params:,}\")\nprint(f\"  Device: {device}\")\nprint(f\"  Memory: ~{total_params * 4 / 1e9:.1f} GB (in float32)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Test the Base Model (Before Training)\n\nOkay, moment of truth. Let's see what the base model does when we ask it questions.\n\nSpoiler: it's going to be bad. Really bad. That's the point.\n\nBase GPT-2 was trained to predict the next token in internet text. It was never taught to answer questions. So when you ask it \"What is the capital of France?\" it just... continues the pattern of text it sees. Sometimes that works by accident. Usually it doesn't.\n\nWatch:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:36.959499Z",
     "iopub.status.busy": "2025-12-06T23:30:36.959405Z",
     "iopub.status.idle": "2025-12-06T23:30:39.347078Z",
     "shell.execute_reply": "2025-12-06T23:30:39.346737Z"
    }
   },
   "outputs": [],
   "source": "def generate_response(model, tokenizer, instruction, max_new_tokens=100):\n    \"\"\"\n    Generate a response to an instruction.\n    \n    We format the prompt in \"Alpaca style\"â€”a specific template that works well\n    for instruction following. You'll see this same format in the training data.\n    \"\"\"\n    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():  # Don't track gradients for inference\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,  # Some randomness (0 = deterministic, 1 = very random)\n            top_p=0.9,  # Nucleus sampling\n            do_sample=True,  # Use sampling instead of greedy\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Extract just the response part (after \"### Response:\")\n    response = full_text.split(\"### Response:\\n\")[-1].strip()\n    \n    return response\n\n# Test base model on a few questions\ntest_instructions = [\n    \"What is the capital of France?\",\n    \"Write a haiku about programming.\",\n    \"Explain machine learning in one sentence.\",\n]\n\nprint(\"BASE MODEL (before fine-tuning):\")\nprint(\"=\" * 70)\nprint(\"Watch how it fails to actually answer the questions...\\n\")\n\nfor instruction in test_instructions:\n    print(f\"Q: {instruction}\")\n    response = generate_response(model, tokenizer, instruction)\n    # Truncate long responses\n    if len(response) > 200:\n        response = response[:200] + \"...\"\n    print(f\"A: {response}\")\n    print(\"-\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "See what I mean? The model just rambles. It's not *trying* to answer the questionâ€”it's trying to continue text that looks like the prompt.\n\nThat's because base GPT-2 was trained on raw internet text with a simple objective: predict the next word. No one ever taught it that text formatted as \"Instruction:\" and \"Response:\" means it should actually answer the question.\n\nThat's what we're about to fix with supervised fine-tuning.\n\n## Step 4: Prepare Training Data\n\nWe're using the **Alpaca dataset**â€”52,000 instruction-response pairs created by Stanford. Things like:\n\n- Instruction: \"Give three tips for staying healthy\"\n- Response: \"1. Eat a balanced diet. 2. Exercise regularly. 3. Get enough sleep.\"\n\nPerfect for teaching a model to follow instructions.\n\n**Key insight:** We'll use a small subset (500 examples) for speed. This is enough to see the model learn! For production use, you'd train on the full dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:39.348201Z",
     "iopub.status.busy": "2025-12-06T23:30:39.348118Z",
     "iopub.status.idle": "2025-12-06T23:30:40.482092Z",
     "shell.execute_reply": "2025-12-06T23:30:40.481735Z"
    }
   },
   "outputs": [],
   "source": "# Load the Alpaca dataset\nprint(\"Loading Alpaca dataset from HuggingFace...\")\nraw_dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n\n# Use a small subset for quick training\n# Feel free to adjust this:\n#   - 100 samples: Very fast, model learns a bit (2-3 min on GPU)\n#   - 500 samples: Good learning, reasonable time (10-15 min on GPU)\n#   - 5000+ samples: Better results, longer training (1+ hour)\nnum_samples = 500\n\nraw_dataset = raw_dataset.select(range(num_samples))\n\nprint(f\"\\nâœ“ Dataset loaded: {len(raw_dataset)} training examples\")\nprint(f\"\\nHere's what one example looks like:\")\nprint(f\"  Instruction: {raw_dataset[0]['instruction']}\")\nif raw_dataset[0]['input']:\n    print(f\"  Input: {raw_dataset[0]['input']}\")\nprint(f\"  Output: {raw_dataset[0]['output'][:100]}...\")\nprint(f\"\\nThe model will learn to generate 'Output' given 'Instruction' (and 'Input' if present).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:40.482973Z",
     "iopub.status.busy": "2025-12-06T23:30:40.482887Z",
     "iopub.status.idle": "2025-12-06T23:30:40.486600Z",
     "shell.execute_reply": "2025-12-06T23:30:40.486323Z"
    }
   },
   "outputs": [],
   "source": "class InstructionDataset(Dataset):\n    \"\"\"\n    Dataset for instruction fine-tuning.\n    \n    The magic here is in the LABEL MASKING. We only compute loss on the response\n    tokens, not the instruction tokens. Why? Because we want the model to learn\n    to GENERATE responses, not to predict the instruction itself.\n    \n    This is crucial. Without it, the model would waste capacity learning to \n    predict the instruction template, which is useless.\n    \"\"\"\n    \n    def __init__(self, data, tokenizer, max_length=256):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def format_example(self, example):\n        \"\"\"Format in Alpaca style (same as our generate function).\"\"\"\n        if example['input']:\n            # Some examples have an additional 'input' field for context\n            prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{example['instruction']}\n\n### Input:\n{example['input']}\n\n### Response:\n\"\"\"\n        else:\n            prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{example['instruction']}\n\n### Response:\n\"\"\"\n        return prompt, example['output']\n    \n    def __getitem__(self, idx):\n        example = self.data[idx]\n        prompt, response = self.format_example(example)\n        \n        # Tokenize prompt and response separately (important!)\n        prompt_tokens = self.tokenizer.encode(prompt, add_special_tokens=True)\n        response_tokens = self.tokenizer.encode(response, add_special_tokens=False)\n        \n        # Combine: [prompt tokens] + [response tokens] + [EOS]\n        input_ids = prompt_tokens + response_tokens + [self.tokenizer.eos_token_id]\n        \n        # Create labels: -100 for prompt (ignored in loss), actual tokens for response\n        # This is the key to supervised fine-tuning!\n        labels = [-100] * len(prompt_tokens) + response_tokens + [self.tokenizer.eos_token_id]\n        \n        # Truncate if too long\n        if len(input_ids) > self.max_length:\n            input_ids = input_ids[:self.max_length]\n            labels = labels[:self.max_length]\n        \n        # Pad to max_length (makes batching easier)\n        padding_length = self.max_length - len(input_ids)\n        input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n        labels = labels + [-100] * padding_length  # Ignore padding in loss\n        attention_mask = [1] * (self.max_length - padding_length) + [0] * padding_length\n        \n        return {\n            'input_ids': torch.tensor(input_ids),\n            'attention_mask': torch.tensor(attention_mask),\n            'labels': torch.tensor(labels),\n        }\n\n# Create dataset and dataloader\ntrain_dataset = InstructionDataset(raw_dataset, tokenizer, max_length=256)\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=4,  # Small batch size to fit in memory\n    shuffle=True   # Randomize order each epoch\n)\n\nprint(f\"âœ“ Created dataset with {len(train_dataset)} samples\")\nprint(f\"  Batches per epoch: {len(train_loader)}\")\nprint(f\"  Batch size: 4\")\nprint(f\"\\nEach batch contains:\")\nprint(f\"  - input_ids: The tokenized text (prompt + response)\")\nprint(f\"  - attention_mask: Which tokens to pay attention to (1) vs ignore (0)\")\nprint(f\"  - labels: What to predict (-100 for prompt, actual tokens for response)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Set Up Training\n\nTime to configure the training loop. A few key decisions here:\n\n**Learning rate (5e-5):** Small enough to not destroy the pretrained weights, large enough to actually learn. This is a well-tested default for fine-tuning.\n\n**Warmup steps (50):** Gradually increase the learning rate for the first 50 steps. Helps with training stabilityâ€”like stretching before a run.\n\n**Gradient clipping (1.0):** Prevents any single bad batch from causing chaos. If gradients get too large, we scale them down. Think of it as a safety rail.\n\n**One epoch:** With 500 examples, one pass through the data is enough to see learning. More epochs would help, but we're going for speed here."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:40.487414Z",
     "iopub.status.busy": "2025-12-06T23:30:40.487335Z",
     "iopub.status.idle": "2025-12-06T23:30:40.489764Z",
     "shell.execute_reply": "2025-12-06T23:30:40.489467Z"
    }
   },
   "outputs": [],
   "source": "# Training hyperparameters\nlearning_rate = 5e-5  # Standard for fine-tuning (0.00005)\nnum_epochs = 1        # One pass through the data\nwarmup_steps = 50     # Gradually increase LR for first 50 steps\nmax_grad_norm = 1.0   # Clip gradients to prevent instability\n\n# Set up optimizer (AdamW is standard for transformers)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Learning rate scheduler (warmup then linear decay)\ntotal_steps = len(train_loader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Learning rate: {learning_rate}\")\nprint(f\"  Epochs: {num_epochs}\")\nprint(f\"  Steps per epoch: {len(train_loader)}\")\nprint(f\"  Total steps: {total_steps}\")\nprint(f\"  Warmup steps: {warmup_steps}\")\nprint(f\"  Gradient clipping: {max_grad_norm}\")\nprint(f\"\\nEstimated time: ~10-15 minutes on GPU, ~2 hours on CPU\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:40.490463Z",
     "iopub.status.busy": "2025-12-06T23:30:40.490385Z",
     "iopub.status.idle": "2025-12-06T23:30:56.065228Z",
     "shell.execute_reply": "2025-12-06T23:30:56.064931Z"
    }
   },
   "outputs": [],
   "source": "# The actual training loop\nprint(\"\\nStarting training...\")\nprint(\"Watch the loss go down! (Lower = better)\")\nprint(\"\\nMetrics explained:\")\nprint(\"  - loss: How wrong the model is (lower = better)\")\nprint(\"  - avg_loss: Running average of loss\")\nprint(\"  - ppl: Perplexity (e^loss), another way to measure quality\")\nprint(\"\\nGo grab a coffee. This'll take a few minutes...\\n\")\n\nmodel.train()  # Put model in training mode\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    for step, batch in enumerate(progress_bar):\n        # Move batch to GPU\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        # Forward pass: compute loss\n        outputs = model(\n            input_ids=batch['input_ids'],\n            attention_mask=batch['attention_mask'],\n            labels=batch['labels']\n        )\n        loss = outputs.loss\n        \n        # Backward pass: compute gradients\n        optimizer.zero_grad()  # Clear old gradients\n        loss.backward()        # Compute new gradients\n        \n        # Clip gradients (prevent explosions)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        \n        # Update weights\n        optimizer.step()\n        scheduler.step()\n        \n        # Track metrics\n        total_loss += loss.item()\n        avg_loss = total_loss / (step + 1)\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            'loss': f'{loss.item():.4f}',\n            'avg_loss': f'{avg_loss:.4f}',\n            'ppl': f'{np.exp(avg_loss):.2f}'\n        })\n    \n    print(f\"\\nâœ“ Epoch {epoch+1} complete!\")\n    print(f\"  Final average loss: {avg_loss:.4f}\")\n    print(f\"  Final perplexity: {np.exp(avg_loss):.2f}\")\n\nprint(\"\\nðŸŽ‰ Training complete!\")\nprint(\"\\nThe model has now seen 500 examples of how to follow instructions.\")\nprint(\"Let's see if it actually learned anything...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Test the Fine-Tuned Model\n\nMoment of truth. Same questions as before, but now the model has been fine-tuned.\n\nWill it actually answer the questions this time? Let's find out.\n\n(If the answers are still gibberish, don't panicâ€”check the training loss. If it went down, the model learned *something*. You might just need more training steps or better hyperparameters.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:56.066347Z",
     "iopub.status.busy": "2025-12-06T23:30:56.066252Z",
     "iopub.status.idle": "2025-12-06T23:30:57.091718Z",
     "shell.execute_reply": "2025-12-06T23:30:57.091403Z"
    }
   },
   "outputs": [],
   "source": "# Switch to evaluation mode (disables dropout, etc.)\nmodel.eval()\n\nprint(\"FINE-TUNED MODEL (after training):\")\nprint(\"=\" * 70)\nprint(\"Same questions as before. Notice the difference?\\n\")\n\nfor instruction in test_instructions:\n    print(f\"Q: {instruction}\")\n    response = generate_response(model, tokenizer, instruction)\n    print(f\"A: {response}\")\n    print(\"-\" * 70)\n\nprint(\"\\nMuch better, right?\")\nprint(\"\\nThe model isn't perfect (it's only seen 500 examples), but it's actually\")\nprint(\"trying to answer the questions now instead of just rambling.\")\nprint(\"\\nThat's the power of supervised fine-tuning!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:57.092609Z",
     "iopub.status.busy": "2025-12-06T23:30:57.092538Z",
     "iopub.status.idle": "2025-12-06T23:30:58.703088Z",
     "shell.execute_reply": "2025-12-06T23:30:58.702744Z"
    }
   },
   "outputs": [],
   "source": "# Let's try some more examples to really see what it can do\nadditional_tests = [\n    \"List three benefits of exercise.\",\n    \"What is Python used for?\",\n    \"Explain what a neural network is in simple terms.\",\n    \"Write a short poem about the ocean.\",\n]\n\nprint(\"\\nLet's try some different questions:\")\nprint(\"=\" * 70)\n\nfor instruction in additional_tests:\n    print(f\"\\nQ: {instruction}\")\n    response = generate_response(model, tokenizer, instruction)\n    print(f\"A: {response}\")\n    print(\"-\" * 70)\n\nprint(\"\\n**Key observation:** The model has learned the *pattern* of instruction-following,\")\nprint(\"not just memorized specific facts. It generalizes to new questions!\")\nprint(\"\\nThough sometimes it gets a bit... creative. (That's LLMs for you.)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Quantitative Evaluation\n\nOkay, so the model *seems* better based on the examples. But how do we measure that objectively?\n\nTwo key metrics:\n\n1. **Perplexity:** How \"surprised\" the model is by the training data. Lower = better. It's basically e^(loss). Think of it as \"confidence\"â€”how well does the model predict what comes next?\n\n2. **Diversity:** Do all the responses sound the same, or does the model have variety? We measure this with distinct-1 and distinct-2 (percentage of unique words and word pairs). Too low = mode collapse (model stuck in a rut).\n\nLet's compute both."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:30:58.703977Z",
     "iopub.status.busy": "2025-12-06T23:30:58.703905Z",
     "iopub.status.idle": "2025-12-06T23:31:07.304954Z",
     "shell.execute_reply": "2025-12-06T23:31:07.304654Z"
    }
   },
   "outputs": [],
   "source": "def compute_perplexity(model, dataloader, device):\n    \"\"\"\n    Compute perplexity on a dataset.\n    \n    Perplexity = e^(average loss)\n    \n    Think of it as: \"On average, how many equally-likely tokens could come next?\"\n    Lower is better. Random guessing on a 50k vocab = perplexity of 50,000.\n    A well-trained model on instructions = perplexity of 5-10.\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    \n    with torch.no_grad():  # No gradients needed for evaluation\n        for batch in tqdm(dataloader, desc=\"Computing perplexity\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            outputs = model(\n                input_ids=batch['input_ids'],\n                attention_mask=batch['attention_mask'],\n                labels=batch['labels']\n            )\n            \n            # Count non-masked tokens (only response tokens, not prompt)\n            num_tokens = (batch['labels'] != -100).sum().item()\n            total_loss += outputs.loss.item() * num_tokens\n            total_tokens += num_tokens\n    \n    avg_loss = total_loss / total_tokens\n    perplexity = np.exp(avg_loss)\n    \n    return perplexity, avg_loss\n\n# Compute perplexity on the training set\n# (In practice, you'd use a held-out validation set, but we're keeping it simple)\nprint(\"\\nEvaluating model quality...\")\nperplexity, loss = compute_perplexity(model, train_loader, device)\n\nprint(f\"\\nâœ“ Evaluation complete!\")\nprint(f\"  Loss: {loss:.4f}\")\nprint(f\"  Perplexity: {perplexity:.2f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"  - Perplexity < 10: Excellent\")\nprint(f\"  - Perplexity 10-20: Good\")\nprint(f\"  - Perplexity 20-50: Okay\")\nprint(f\"  - Perplexity > 50: Needs more training\")\nprint(f\"\\nYour model: \", end=\"\")\nif perplexity < 10:\n    print(\"Excellent! ðŸŽ‰\")\nelif perplexity < 20:\n    print(\"Good! ðŸ‘\")\nelif perplexity < 50:\n    print(\"Okay. More training would help.\")\nelse:\n    print(\"Needs more training. Try more epochs or more data.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:31:07.305873Z",
     "iopub.status.busy": "2025-12-06T23:31:07.305791Z",
     "iopub.status.idle": "2025-12-06T23:31:09.501779Z",
     "shell.execute_reply": "2025-12-06T23:31:09.501416Z"
    }
   },
   "outputs": [],
   "source": "def compute_diversity(responses):\n    \"\"\"\n    Compute diversity metrics for generated text.\n    \n    Distinct-1: Percentage of unique words (unigrams)\n    Distinct-2: Percentage of unique word pairs (bigrams)\n    \n    Why does this matter? If the model always says \"the the the the\" you'd have\n    low diversity even if perplexity looks okay. Diversity catches mode collapse.\n    \"\"\"\n    all_unigrams = []\n    all_bigrams = []\n    \n    for response in responses:\n        tokens = response.lower().split()\n        all_unigrams.extend(tokens)\n        # Create pairs of consecutive words\n        all_bigrams.extend(zip(tokens[:-1], tokens[1:]))\n    \n    # What fraction of words/pairs are unique?\n    distinct_1 = len(set(all_unigrams)) / len(all_unigrams) if all_unigrams else 0\n    distinct_2 = len(set(all_bigrams)) / len(all_bigrams) if all_bigrams else 0\n    \n    return distinct_1, distinct_2\n\n# Generate a bunch of responses for diversity analysis\ndiversity_prompts = [\n    \"Tell me about machine learning.\",\n    \"Explain artificial intelligence.\",\n    \"What is deep learning?\",\n    \"Describe natural language processing.\",\n    \"Explain what data science is.\",\n]\n\nprint(\"\\nGenerating responses for diversity analysis...\")\nresponses = [generate_response(model, tokenizer, p) for p in diversity_prompts]\nd1, d2 = compute_diversity(responses)\n\nprint(f\"\\nâœ“ Diversity analysis complete!\")\nprint(f\"  Distinct-1 (unique words): {d1:.2%}\")\nprint(f\"  Distinct-2 (unique word pairs): {d2:.2%}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"  - Distinct-1 > 40%: Good variety\")\nprint(f\"  - Distinct-1 20-40%: Okay\")\nprint(f\"  - Distinct-1 < 20%: Mode collapse (model stuck repeating itself)\")\nprint(f\"\\nYour model: \", end=\"\")\nif d1 > 0.4:\n    print(\"Good variety! ðŸŽ‰\")\nelif d1 > 0.2:\n    print(\"Okay diversity.\")\nelse:\n    print(\"Warning: Low diversity. Try different sampling parameters.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Save Your Model\n\nYou just spent 15 minutes training this thing. Let's not lose it!\n\nSaving is simpleâ€”we just dump the model weights and tokenizer config to disk. Then you can reload them later (or share them with others)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:31:09.502742Z",
     "iopub.status.busy": "2025-12-06T23:31:09.502669Z",
     "iopub.status.idle": "2025-12-06T23:31:10.053384Z",
     "shell.execute_reply": "2025-12-06T23:31:10.052990Z"
    }
   },
   "outputs": [],
   "source": "# Save model and tokenizer to disk\nsave_path = \"./my_finetuned_model\"\n\nprint(f\"Saving model to {save_path}...\")\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nprint(\"âœ“ Model saved!\")\n\n# Show what got saved\nimport os\nprint(f\"\\nSaved files:\")\ntotal_size = 0\nfor f in sorted(os.listdir(save_path)):\n    size = os.path.getsize(os.path.join(save_path, f)) / 1e6\n    total_size += size\n    print(f\"  {f}: {size:.1f} MB\")\n\nprint(f\"\\nTotal size: {total_size:.1f} MB\")\nprint(f\"\\nYou can now load this model anytime with:\")\nprint(f\"  model = AutoModelForCausalLM.from_pretrained('{save_path}')\")\nprint(f\"  tokenizer = AutoTokenizer.from_pretrained('{save_path}')\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:31:10.054307Z",
     "iopub.status.busy": "2025-12-06T23:31:10.054206Z",
     "iopub.status.idle": "2025-12-06T23:31:10.482185Z",
     "shell.execute_reply": "2025-12-06T23:31:10.481658Z"
    }
   },
   "outputs": [],
   "source": "# Let's verify the saved model actually works\nprint(\"Testing saved model (to make sure saving worked)...\")\n\nloaded_model = AutoModelForCausalLM.from_pretrained(save_path)\nloaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\nloaded_model = loaded_model.to(device)\nloaded_model.eval()\n\ntest_instruction = \"What is the meaning of life?\"\nresponse = generate_response(loaded_model, loaded_tokenizer, test_instruction)\n\nprint(f\"\\nâœ“ Loaded model from disk successfully!\")\nprint(f\"\\nTest question: {test_instruction}\")\nprint(f\"Answer: {response}\")\nprint(f\"\\nLooks good! Your model is saved and ready to use.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## You Did It! ðŸŽ‰\n\nSeriously. You just fine-tuned a language model from scratch.\n\n**What you accomplished:**\n1. Loaded a base GPT-2 model (terrible at following instructions)\n2. Prepared training data with proper label masking\n3. Trained the model using supervised fine-tuning\n4. Watched it go from gibberish to actual answers\n5. Evaluated it with perplexity and diversity metrics\n6. Saved it for later use\n\nThis is the same basic process used to create ChatGPT, Claude, and every other instruction-following LLM. The production versions use more data, bigger models, LoRA for efficiency, and RLHF for alignmentâ€”but the core idea is exactly what you just did.\n\n## What to Try Next\n\nNow that you've got the basics down:\n\n1. **Train longer** - Try 3-5 epochs or use the full Alpaca dataset (52k examples)\n2. **Use LoRA** - Fine-tune only a small number of parameters (way more efficient)\n3. **Try DPO** - Align the model with human preferences using the reward/preference notebooks\n4. **Bigger models** - GPT-2 Medium/Large, or even Llama if you've got the VRAM\n5. **Your own data** - Got a specific task? Create a dataset and fine-tune for it!\n\n## Common Issues & Tips\n\n**Loss not going down?**\n- Check your learning rate (try 1e-5 to 1e-4)\n- Make sure labels are masked properly (prompt tokens should be -100)\n- Try more epochs or more data\n\n**Model output is repetitive?**\n- Adjust temperature and top_p during generation\n- Check diversity metrics (distinct-1/distinct-2)\n- Might need more varied training data\n\n**Out of memory?**\n- Reduce batch_size (try 2 or 1)\n- Reduce max_length (try 128 or 64)\n- Use gradient checkpointing (more compute, less memory)\n- Consider LoRA (way less memory)\n\n**Answers are still bad?**\n- Train on more data (500 examples is pretty small)\n- Train for more epochs\n- Check that loss actually decreased during training\n\n## Final Thoughts\n\nThe model you just trained isn't perfect. It might hallucinate, give weird answers, or ramble sometimes. That's normal! You trained it on 500 examples for 10 minutes.\n\nWhat matters is that you understand the *process*. You know how to:\n- Load and prepare data\n- Set up a training loop\n- Evaluate results\n- Debug when things go wrong\n\nThat's the hard part. Scaling up to production is just... more of the same, but bigger.\n\nGo build something cool. ðŸš€"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}