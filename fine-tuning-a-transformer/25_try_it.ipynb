{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try It Yourself!\n",
    "\n",
    "**Hands-on tutorial: Train and evaluate your first fine-tuned model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Hands-On Post-Training!\n",
    "\n",
    "This guide walks you through training your first fine-tuned language model. By the end, you'll have:\n",
    "\n",
    "- A GPT-2 model fine-tuned on instructions\n",
    "- Hands-on experience with SFT, evaluation, and generation\n",
    "- Understanding of how to apply these techniques to your own projects\n",
    "\n",
    "**Time required:** 30-60 minutes (depending on hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's verify our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Device: CPU (training will be slower)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 (small, 124M parameters)\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Base Model (Before Fine-Tuning)\n",
    "\n",
    "Let's see how the base model handles instructions before we fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, instruction, max_new_tokens=100):\n",
    "    \"\"\"Generate a response to an instruction.\"\"\"\n",
    "    # Format as Alpaca-style prompt\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_text.split(\"### Response:\\n\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test base model\n",
    "test_instructions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Write a haiku about programming.\",\n",
    "    \"Explain machine learning in one sentence.\",\n",
    "]\n",
    "\n",
    "print(\"Base Model Responses (BEFORE fine-tuning):\")\n",
    "print(\"=\" * 60)\n",
    "for instruction in test_instructions:\n",
    "    print(f\"\\nInstruction: {instruction}\")\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    print(f\"Response: {response[:200]}...\" if len(response) > 200 else f\"Response: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the base model doesn't follow instructions well - it typically continues generating text in the same style rather than answering the question.\n",
    "\n",
    "## 4. Prepare Training Data\n",
    "\n",
    "We'll use the Alpaca dataset, which contains instruction-response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Alpaca dataset\n",
    "print(\"Loading Alpaca dataset...\")\n",
    "raw_dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "# Take a small subset for quick training\n",
    "num_samples = 500  # Adjust based on your time/hardware\n",
    "raw_dataset = raw_dataset.select(range(num_samples))\n",
    "\n",
    "print(f\"Dataset loaded: {len(raw_dataset)} samples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Instruction: {raw_dataset[0]['instruction'][:100]}...\")\n",
    "print(f\"  Input: {raw_dataset[0]['input'][:50]}...\" if raw_dataset[0]['input'] else \"  Input: (none)\")\n",
    "print(f\"  Output: {raw_dataset[0]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"Dataset for instruction fine-tuning with proper loss masking.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def format_example(self, example):\n",
    "        \"\"\"Format example in Alpaca style.\"\"\"\n",
    "        if example['input']:\n",
    "            prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        return prompt, example['output']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        prompt, response = self.format_example(example)\n",
    "        \n",
    "        # Tokenize prompt and response separately\n",
    "        prompt_tokens = self.tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        response_tokens = self.tokenizer.encode(response, add_special_tokens=False)\n",
    "        \n",
    "        # Combine\n",
    "        input_ids = prompt_tokens + response_tokens + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        # Create labels: -100 for prompt tokens (ignored in loss)\n",
    "        labels = [-100] * len(prompt_tokens) + response_tokens + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "        \n",
    "        # Pad to max_length\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "        labels = labels + [-100] * padding_length  # Ignore padding in loss\n",
    "        attention_mask = [1] * (self.max_length - padding_length) + [0] * padding_length\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_mask),\n",
    "            'labels': torch.tensor(labels),\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = InstructionDataset(raw_dataset, tokenizer, max_length=256)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Created dataset with {len(train_dataset)} samples\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Now let's train the model on our instruction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 1\n",
    "warmup_steps = 50\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (step + 1)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{avg_loss:.4f}',\n",
    "            'ppl': f'{np.exp(avg_loss):.2f}'\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} complete!\")\n",
    "    print(f\"  Average loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Perplexity: {np.exp(avg_loss):.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Fine-Tuned Model\n",
    "\n",
    "Now let's see how the model performs after fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Fine-Tuned Model Responses (AFTER fine-tuning):\")\n",
    "print(\"=\" * 60)\n",
    "for instruction in test_instructions:\n",
    "    print(f\"\\nInstruction: {instruction}\")\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with more instructions\n",
    "additional_tests = [\n",
    "    \"List three benefits of exercise.\",\n",
    "    \"What is Python used for?\",\n",
    "    \"Explain what a neural network is in simple terms.\",\n",
    "    \"Write a short poem about the ocean.\",\n",
    "]\n",
    "\n",
    "print(\"Additional Tests:\")\n",
    "print(\"=\" * 60)\n",
    "for instruction in additional_tests:\n",
    "    print(f\"\\nInstruction: {instruction}\")\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Quality\n",
    "\n",
    "Let's compute some quantitative metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device):\n",
    "    \"\"\"Compute perplexity on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Computing perplexity\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            # Count non-masked tokens\n",
    "            num_tokens = (batch['labels'] != -100).sum().item()\n",
    "            total_loss += outputs.loss.item() * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "# Compute perplexity\n",
    "perplexity, loss = compute_perplexity(model, train_loader, device)\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Loss: {loss:.4f}\")\n",
    "print(f\"  Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diversity(responses):\n",
    "    \"\"\"Compute diversity metrics for generated responses.\"\"\"\n",
    "    all_unigrams = []\n",
    "    all_bigrams = []\n",
    "    \n",
    "    for response in responses:\n",
    "        tokens = response.lower().split()\n",
    "        all_unigrams.extend(tokens)\n",
    "        all_bigrams.extend(zip(tokens[:-1], tokens[1:]))\n",
    "    \n",
    "    distinct_1 = len(set(all_unigrams)) / len(all_unigrams) if all_unigrams else 0\n",
    "    distinct_2 = len(set(all_bigrams)) / len(all_bigrams) if all_bigrams else 0\n",
    "    \n",
    "    return distinct_1, distinct_2\n",
    "\n",
    "# Generate responses for diversity analysis\n",
    "diversity_prompts = [\n",
    "    \"Tell me about machine learning.\",\n",
    "    \"Explain artificial intelligence.\",\n",
    "    \"What is deep learning?\",\n",
    "    \"Describe natural language processing.\",\n",
    "    \"Explain what data science is.\",\n",
    "]\n",
    "\n",
    "responses = [generate_response(model, tokenizer, p) for p in diversity_prompts]\n",
    "d1, d2 = compute_diversity(responses)\n",
    "\n",
    "print(f\"\\nDiversity Metrics:\")\n",
    "print(f\"  Distinct-1 (unique unigrams): {d1:.2%}\")\n",
    "print(f\"  Distinct-2 (unique bigrams): {d2:.2%}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  > 0.4 distinct-1: Good diversity\")\n",
    "print(f\"  < 0.2 distinct-1: May indicate mode collapse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Your Model\n",
    "\n",
    "Save the fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "save_path = \"./my_finetuned_model\"\n",
    "\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Show saved files\n",
    "import os\n",
    "print(f\"\\nSaved files:\")\n",
    "for f in os.listdir(save_path):\n",
    "    size = os.path.getsize(os.path.join(save_path, f)) / 1e6\n",
    "    print(f\"  {f}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model\n",
    "print(\"Testing model loading...\")\n",
    "\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "test_instruction = \"What is the meaning of life?\"\n",
    "response = generate_response(loaded_model, loaded_tokenizer, test_instruction)\n",
    "\n",
    "print(f\"\\nTest with loaded model:\")\n",
    "print(f\"Instruction: {test_instruction}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. **Loaded** a pre-trained GPT-2 model\n",
    "2. **Tested** the base model on instructions (and saw it doesn't follow them well)\n",
    "3. **Prepared** training data with proper loss masking\n",
    "4. **Trained** the model using supervised fine-tuning (SFT)\n",
    "5. **Tested** the fine-tuned model (and saw significant improvement!)\n",
    "6. **Evaluated** using perplexity and diversity metrics\n",
    "7. **Saved** the model for later use\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've mastered the basics, try:\n",
    "\n",
    "1. **Train longer** - Increase epochs or use more data\n",
    "2. **Try LoRA** - More efficient training with fewer parameters\n",
    "3. **Try DPO** - Align model with human preferences\n",
    "4. **Use larger models** - Try GPT-2 Medium or Llama\n",
    "5. **Custom data** - Fine-tune on your own instruction dataset\n",
    "\n",
    "Happy fine-tuning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
