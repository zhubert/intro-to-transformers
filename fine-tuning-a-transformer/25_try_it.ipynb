{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try It Yourself!\n",
    "\n",
    "**Your turn to build something real.**\n",
    "\n",
    "This is it. The capstone. All those notebooks you just went through? Time to put them into practice.\n",
    "\n",
    "We're going to fine-tune a language model from scratch. Not a toy example. A real model that actually learns to follow instructions. By the end of this, you'll have your own fine-tuned GPT-2 sitting on your hard drive, ready to answer questions.\n",
    "\n",
    "Sound good? Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You're About to Build\n",
    "\n",
    "Here's the deal: we're taking a base GPT-2 modelâ€”one that's pretty good at predicting the next word but terrible at following instructionsâ€”and teaching it to be helpful.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to actually train a model (not just read about it)\n",
    "- Why supervised fine-tuning works\n",
    "- How to evaluate if your model is any good\n",
    "- What to watch out for when things go wrong\n",
    "\n",
    "**Time investment:** 30-60 minutes, depending on your hardware. Got a GPU? Closer to 30. Running on CPU? Grab a coffee and make it an hour.\n",
    "\n",
    "**Important note:** This is a simplified version for learning. Production fine-tuning would use LoRA (which we covered), more data, and better hyperparameter tuning. But the core ideas? Exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Your Environment\n",
    "\n",
    "First things firstâ€”let's make sure you've got PyTorch installed and that it can see your GPU (if you have one).\n",
    "\n",
    "Why check this first? Because finding out 20 minutes into training that CUDA isn't working is... well, let's just say it's a learning experience you only need once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:12.291516Z",
     "iopub.status.busy": "2025-12-10T21:21:12.291437Z",
     "iopub.status.idle": "2025-12-10T21:21:13.019992Z",
     "shell.execute_reply": "2025-12-10T21:21:13.019682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251124+rocm7.1\n",
      "CUDA available: True\n",
      "GPU: Radeon RX 7900 XTX\n",
      "Great! Training will be fast.\n"
     ]
    }
   ],
   "source": [
    "# Check what we're working with\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Great! Training will be fast.\")\n",
    "else:\n",
    "    print(\"Device: CPU\")\n",
    "    print(\"No GPU found. Training will work but be slower (10-20x).\")\n",
    "    print(\"Consider reducing num_samples in the data loading step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:13.036855Z",
     "iopub.status.busy": "2025-12-10T21:21:13.036699Z",
     "iopub.status.idle": "2025-12-10T21:21:14.246989Z",
     "shell.execute_reply": "2025-12-10T21:21:14.246661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "\n",
      "If you got any errors above, install missing packages with:\n",
      "  pip install transformers datasets torch tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import everything we need\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(\"\\nIf you got any errors above, install missing packages with:\")\n",
    "print(\"  pip install transformers datasets torch tqdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Base Model\n",
    "\n",
    "We're using GPT-2 (the small version, 124M parameters). Why GPT-2 and not something bigger?\n",
    "\n",
    "1. **It's fast to train** - You can actually finish this notebook today\n",
    "2. **It's well-understood** - Lots of documentation if things break\n",
    "3. **It's big enough to learn** - 124M parameters is plenty for instruction following\n",
    "\n",
    "Think of this as the \"before\" photo. The model right now is decent at continuing text but hopeless at following instructions. We're about to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:14.248087Z",
     "iopub.status.busy": "2025-12-10T21:21:14.247927Z",
     "iopub.status.idle": "2025-12-10T21:21:15.382466Z",
     "shell.execute_reply": "2025-12-10T21:21:15.382171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2...\n",
      "This downloads ~500MB the first time, then caches locally.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded!\n",
      "  Parameters: 124,439,808\n",
      "  Device: cuda\n",
      "  Memory: ~0.5 GB (in float32)\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 (small, 124M parameters)\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "print(\"This downloads ~500MB the first time, then caches locally.\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a padding token by default, so we add one\n",
    "# We just reuse the EOS tokenâ€”common practice and works fine\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# How big is this thing?\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ“ Model loaded!\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Memory: ~{total_params * 4 / 1e9:.1f} GB (in float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the Base Model (Before Training)\n",
    "\n",
    "Okay, moment of truth. Let's see what the base model does when we ask it questions.\n",
    "\n",
    "Spoiler: it's going to be bad. Really bad. That's the point.\n",
    "\n",
    "Base GPT-2 was trained to predict the next token in internet text. It was never taught to answer questions. So when you ask it \"What is the capital of France?\" it just... continues the pattern of text it sees. Sometimes that works by accident. Usually it doesn't.\n",
    "\n",
    "Watch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:15.383730Z",
     "iopub.status.busy": "2025-12-10T21:21:15.383553Z",
     "iopub.status.idle": "2025-12-10T21:21:17.650077Z",
     "shell.execute_reply": "2025-12-10T21:21:17.649757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE MODEL (before fine-tuning):\n",
      "======================================================================\n",
      "Watch how it fails to actually answer the questions...\n",
      "\n",
      "Q: What is the capital of France?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The response type is\n",
      "----------------------------------------------------------------------\n",
      "Q: Write a haiku about programming.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Write a response that includes some code.\n",
      "\n",
      "### Example:\n",
      "\n",
      "#include <stdio.h> #include <sys/types.h> int main(int argc, char **argv[]) { int i, j; char *data = argc->get_data(); for (i = 0; i < argv[1];...\n",
      "----------------------------------------------------------------------\n",
      "Q: Explain machine learning in one sentence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The following example shows how to generate the response with a single line of code.\n",
      "\n",
      "#!/usr/bin/python # python.py import requests import requests.py import requests.py import requests.py.model impor...\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, instruction, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Generate a response to an instruction.\n",
    "    \n",
    "    We format the prompt in \"Alpaca style\"â€”a specific template that works well\n",
    "    for instruction following. You'll see this same format in the training data.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():  # Don't track gradients for inference\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,  # Some randomness (0 = deterministic, 1 = very random)\n",
    "            top_p=0.9,  # Nucleus sampling\n",
    "            do_sample=True,  # Use sampling instead of greedy\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the response part (after \"### Response:\")\n",
    "    response = full_text.split(\"### Response:\\n\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test base model on a few questions\n",
    "test_instructions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Write a haiku about programming.\",\n",
    "    \"Explain machine learning in one sentence.\",\n",
    "]\n",
    "\n",
    "print(\"BASE MODEL (before fine-tuning):\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Watch how it fails to actually answer the questions...\\n\")\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    print(f\"Q: {instruction}\")\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    # Truncate long responses\n",
    "    if len(response) > 200:\n",
    "        response = response[:200] + \"...\"\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what I mean? The model just rambles. It's not *trying* to answer the questionâ€”it's trying to continue text that looks like the prompt.\n",
    "\n",
    "That's because base GPT-2 was trained on raw internet text with a simple objective: predict the next word. No one ever taught it that text formatted as \"Instruction:\" and \"Response:\" means it should actually answer the question.\n",
    "\n",
    "That's what we're about to fix with supervised fine-tuning.\n",
    "\n",
    "## Step 4: Prepare Training Data\n",
    "\n",
    "We're using the **Alpaca dataset**â€”52,000 instruction-response pairs created by Stanford. Things like:\n",
    "\n",
    "- Instruction: \"Give three tips for staying healthy\"\n",
    "- Response: \"1. Eat a balanced diet. 2. Exercise regularly. 3. Get enough sleep.\"\n",
    "\n",
    "Perfect for teaching a model to follow instructions.\n",
    "\n",
    "**Key insight:** We'll use a small subset (500 examples) for speed. This is enough to see the model learn! For production use, you'd train on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:17.651074Z",
     "iopub.status.busy": "2025-12-10T21:21:17.650997Z",
     "iopub.status.idle": "2025-12-10T21:21:18.815280Z",
     "shell.execute_reply": "2025-12-10T21:21:18.814932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Alpaca dataset from HuggingFace...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Dataset loaded: 500 training examples\n",
      "\n",
      "Here's what one example looks like:\n",
      "  Instruction: Give three tips for staying healthy.\n",
      "  Output: 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and...\n",
      "\n",
      "The model will learn to generate 'Output' given 'Instruction' (and 'Input' if present).\n"
     ]
    }
   ],
   "source": [
    "# Load the Alpaca dataset\n",
    "print(\"Loading Alpaca dataset from HuggingFace...\")\n",
    "raw_dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "# Use a small subset for quick training\n",
    "# Feel free to adjust this:\n",
    "#   - 100 samples: Very fast, model learns a bit (2-3 min on GPU)\n",
    "#   - 500 samples: Good learning, reasonable time (10-15 min on GPU)\n",
    "#   - 5000+ samples: Better results, longer training (1+ hour)\n",
    "num_samples = 500\n",
    "\n",
    "raw_dataset = raw_dataset.select(range(num_samples))\n",
    "\n",
    "print(f\"\\nâœ“ Dataset loaded: {len(raw_dataset)} training examples\")\n",
    "print(f\"\\nHere's what one example looks like:\")\n",
    "print(f\"  Instruction: {raw_dataset[0]['instruction']}\")\n",
    "if raw_dataset[0]['input']:\n",
    "    print(f\"  Input: {raw_dataset[0]['input']}\")\n",
    "print(f\"  Output: {raw_dataset[0]['output'][:100]}...\")\n",
    "print(f\"\\nThe model will learn to generate 'Output' given 'Instruction' (and 'Input' if present).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:18.816234Z",
     "iopub.status.busy": "2025-12-10T21:21:18.816141Z",
     "iopub.status.idle": "2025-12-10T21:21:18.820180Z",
     "shell.execute_reply": "2025-12-10T21:21:18.819816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created dataset with 500 samples\n",
      "  Batches per epoch: 125\n",
      "  Batch size: 4\n",
      "\n",
      "Each batch contains:\n",
      "  - input_ids: The tokenized text (prompt + response)\n",
      "  - attention_mask: Which tokens to pay attention to (1) vs ignore (0)\n",
      "  - labels: What to predict (-100 for prompt, actual tokens for response)\n"
     ]
    }
   ],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for instruction fine-tuning.\n",
    "    \n",
    "    The magic here is in the LABEL MASKING. We only compute loss on the response\n",
    "    tokens, not the instruction tokens. Why? Because we want the model to learn\n",
    "    to GENERATE responses, not to predict the instruction itself.\n",
    "    \n",
    "    This is crucial. Without it, the model would waste capacity learning to \n",
    "    predict the instruction template, which is useless.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def format_example(self, example):\n",
    "        \"\"\"Format in Alpaca style (same as our generate function).\"\"\"\n",
    "        if example['input']:\n",
    "            # Some examples have an additional 'input' field for context\n",
    "            prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        return prompt, example['output']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        prompt, response = self.format_example(example)\n",
    "        \n",
    "        # Tokenize prompt and response separately (important!)\n",
    "        prompt_tokens = self.tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        response_tokens = self.tokenizer.encode(response, add_special_tokens=False)\n",
    "        \n",
    "        # Combine: [prompt tokens] + [response tokens] + [EOS]\n",
    "        input_ids = prompt_tokens + response_tokens + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        # Create labels: -100 for prompt (ignored in loss), actual tokens for response\n",
    "        # This is the key to supervised fine-tuning!\n",
    "        labels = [-100] * len(prompt_tokens) + response_tokens + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "        \n",
    "        # Pad to max_length (makes batching easier)\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "        labels = labels + [-100] * padding_length  # Ignore padding in loss\n",
    "        attention_mask = [1] * (self.max_length - padding_length) + [0] * padding_length\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_mask),\n",
    "            'labels': torch.tensor(labels),\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = InstructionDataset(raw_dataset, tokenizer, max_length=256)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4,  # Small batch size to fit in memory\n",
    "    shuffle=True   # Randomize order each epoch\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Created dataset with {len(train_dataset)} samples\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Batch size: 4\")\n",
    "print(f\"\\nEach batch contains:\")\n",
    "print(f\"  - input_ids: The tokenized text (prompt + response)\")\n",
    "print(f\"  - attention_mask: Which tokens to pay attention to (1) vs ignore (0)\")\n",
    "print(f\"  - labels: What to predict (-100 for prompt, actual tokens for response)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Up Training\n",
    "\n",
    "Time to configure the training loop. A few key decisions here:\n",
    "\n",
    "**Learning rate (5e-5):** Small enough to not destroy the pretrained weights, large enough to actually learn. This is a well-tested default for fine-tuning.\n",
    "\n",
    "**Warmup steps (50):** Gradually increase the learning rate for the first 50 steps. Helps with training stabilityâ€”like stretching before a run.\n",
    "\n",
    "**Gradient clipping (1.0):** Prevents any single bad batch from causing chaos. If gradients get too large, we scale them down. Think of it as a safety rail.\n",
    "\n",
    "**One epoch:** With 500 examples, one pass through the data is enough to see learning. More epochs would help, but we're going for speed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:18.821051Z",
     "iopub.status.busy": "2025-12-10T21:21:18.820963Z",
     "iopub.status.idle": "2025-12-10T21:21:18.823560Z",
     "shell.execute_reply": "2025-12-10T21:21:18.823270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Learning rate: 5e-05\n",
      "  Epochs: 1\n",
      "  Steps per epoch: 125\n",
      "  Total steps: 125\n",
      "  Warmup steps: 50\n",
      "  Gradient clipping: 1.0\n",
      "\n",
      "Estimated time: ~10-15 minutes on GPU, ~2 hours on CPU\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 5e-5  # Standard for fine-tuning (0.00005)\n",
    "num_epochs = 1        # One pass through the data\n",
    "warmup_steps = 50     # Gradually increase LR for first 50 steps\n",
    "max_grad_norm = 1.0   # Clip gradients to prevent instability\n",
    "\n",
    "# Set up optimizer (AdamW is standard for transformers)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler (warmup then linear decay)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Steps per epoch: {len(train_loader)}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Gradient clipping: {max_grad_norm}\")\n",
    "print(f\"\\nEstimated time: ~10-15 minutes on GPU, ~2 hours on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:18.824257Z",
     "iopub.status.busy": "2025-12-10T21:21:18.824176Z",
     "iopub.status.idle": "2025-12-10T21:21:35.271086Z",
     "shell.execute_reply": "2025-12-10T21:21:35.270785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Watch the loss go down! (Lower = better)\n",
      "\n",
      "Metrics explained:\n",
      "  - loss: How wrong the model is (lower = better)\n",
      "  - avg_loss: Running average of loss\n",
      "  - ppl: Perplexity (e^loss), another way to measure quality\n",
      "\n",
      "Go grab a coffee. This'll take a few minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460e62e540e14eed854409d2bf7c8f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Epoch 1 complete!\n",
      "  Final average loss: 2.4032\n",
      "  Final perplexity: 11.06\n",
      "\n",
      "ðŸŽ‰ Training complete!\n",
      "\n",
      "The model has now seen 500 examples of how to follow instructions.\n",
      "Let's see if it actually learned anything...\n"
     ]
    }
   ],
   "source": [
    "# The actual training loop\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"Watch the loss go down! (Lower = better)\")\n",
    "print(\"\\nMetrics explained:\")\n",
    "print(\"  - loss: How wrong the model is (lower = better)\")\n",
    "print(\"  - avg_loss: Running average of loss\")\n",
    "print(\"  - ppl: Perplexity (e^loss), another way to measure quality\")\n",
    "print(\"\\nGo grab a coffee. This'll take a few minutes...\\n\")\n",
    "\n",
    "model.train()  # Put model in training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move batch to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass: compute loss\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        loss.backward()        # Compute new gradients\n",
    "        \n",
    "        # Clip gradients (prevent explosions)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (step + 1)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{avg_loss:.4f}',\n",
    "            'ppl': f'{np.exp(avg_loss):.2f}'\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nâœ“ Epoch {epoch+1} complete!\")\n",
    "    print(f\"  Final average loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Final perplexity: {np.exp(avg_loss):.2f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training complete!\")\n",
    "print(\"\\nThe model has now seen 500 examples of how to follow instructions.\")\n",
    "print(\"Let's see if it actually learned anything...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the Fine-Tuned Model\n",
    "\n",
    "Moment of truth. Same questions as before, but now the model has been fine-tuned.\n",
    "\n",
    "Will it actually answer the questions this time? Let's find out.\n",
    "\n",
    "(If the answers are still gibberish, don't panicâ€”check the training loss. If it went down, the model learned *something*. You might just need more training steps or better hyperparameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:35.272135Z",
     "iopub.status.busy": "2025-12-10T21:21:35.272043Z",
     "iopub.status.idle": "2025-12-10T21:21:36.407323Z",
     "shell.execute_reply": "2025-12-10T21:21:36.406987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINE-TUNED MODEL (after training):\n",
      "======================================================================\n",
      "Same questions as before. Notice the difference?\n",
      "\n",
      "Q: What is the capital of France?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The capital of France is Paris, the capital of France.\n",
      "\n",
      "France is a major European city, located in the French Alps, and the largest city in the world, with over 3,000,000 inhabitants. It is also home to the French government, the World Bank, and the Royal Society.\n",
      "\n",
      "France is also home to the largest and most powerful military in the world, the French Air Force. It is also home to a large number of foreign embassies, as well as numerous\n",
      "----------------------------------------------------------------------\n",
      "Q: Write a haiku about programming.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Programming is a form of expression, the process by which a program can be executed. It is a means of expressing a specific thought, or feeling, and is an important tool for communication.\n",
      "\n",
      "In the past, programs were written in many different ways, from simple programs written to complex code that was executed by hand. However, in the past, programming has become much more complex and complex, making it difficult for programmers to express their thoughts and feelings without breaking the flow of code.\n",
      "----------------------------------------------------------------------\n",
      "Q: Explain machine learning in one sentence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Machine learning is a powerful, automated, and scalable technology that helps us improve our processes and learn more about our customers. It allows us to better understand their needs and behaviors, and can help us identify and predict patterns in data and make decisions about how to improve our services. Machine learning can be used for business and financial purposes, as well as for training and forecasting.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Much better, right?\n",
      "\n",
      "The model isn't perfect (it's only seen 500 examples), but it's actually\n",
      "trying to answer the questions now instead of just rambling.\n",
      "\n",
      "That's the power of supervised fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# Switch to evaluation mode (disables dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "print(\"FINE-TUNED MODEL (after training):\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Same questions as before. Notice the difference?\\n\")\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    print(f\"Q: {instruction}\")\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nMuch better, right?\")\n",
    "print(\"\\nThe model isn't perfect (it's only seen 500 examples), but it's actually\")\n",
    "print(\"trying to answer the questions now instead of just rambling.\")\n",
    "print(\"\\nThat's the power of supervised fine-tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:36.408138Z",
     "iopub.status.busy": "2025-12-10T21:21:36.408069Z",
     "iopub.status.idle": "2025-12-10T21:21:37.366430Z",
     "shell.execute_reply": "2025-12-10T21:21:37.366055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's try some different questions:\n",
      "======================================================================\n",
      "\n",
      "Q: List three benefits of exercise.\n",
      "A: 1. It reduces stress and anxiety. Exercise reduces stress and anxiety.\n",
      "2. It increases physical activity. Exercise increases physical activity.\n",
      "3. It reduces stress and anxiety. Exercise reduces stress and anxiety.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Q: What is Python used for?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Python is a powerful and powerful programming language that makes it possible to create, manage, and manage complex systems, including databases, databases, and applications. Python is widely used by businesses, governments, and other organizations to manage and manage their data and processes. Python is also used for creating and managing large databases, applications, and applications that provide web and mobile applications.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Q: Explain what a neural network is in simple terms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: A neural network is a collection of neural networks that encode, process, and process information, typically as part of a process called learning. Neural networks are typically thought of as the \"deep-learning\" machine learning system that is used to learn from known information. Neural networks are used to perform tasks, such as image recognition, image manipulation, and so on, but they are also used to process information such as data and other information. Neural networks are often referred to as machine learning systems because they have\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Q: Write a short poem about the ocean.\n",
      "A: This poem is about the ocean, or the beauty of the sea.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "**Key observation:** The model has learned the *pattern* of instruction-following,\n",
      "not just memorized specific facts. It generalizes to new questions!\n",
      "\n",
      "Though sometimes it gets a bit... creative. (That's LLMs for you.)\n"
     ]
    }
   ],
   "source": [
    "# Let's try some more examples to really see what it can do\n",
    "additional_tests = [\n",
    "    \"List three benefits of exercise.\",\n",
    "    \"What is Python used for?\",\n",
    "    \"Explain what a neural network is in simple terms.\",\n",
    "    \"Write a short poem about the ocean.\",\n",
    "]\n",
    "\n",
    "print(\"\\nLet's try some different questions:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for instruction in additional_tests:\n",
    "    print(f\"\\nQ: {instruction}\")\n",
    "    response = generate_response(model, tokenizer, instruction)\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n**Key observation:** The model has learned the *pattern* of instruction-following,\")\n",
    "print(\"not just memorized specific facts. It generalizes to new questions!\")\n",
    "print(\"\\nThough sometimes it gets a bit... creative. (That's LLMs for you.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Quantitative Evaluation\n",
    "\n",
    "Okay, so the model *seems* better based on the examples. But how do we measure that objectively?\n",
    "\n",
    "Two key metrics:\n",
    "\n",
    "1. **Perplexity:** How \"surprised\" the model is by the training data. Lower = better. It's basically e^(loss). Think of it as \"confidence\"â€”how well does the model predict what comes next?\n",
    "\n",
    "2. **Diversity:** Do all the responses sound the same, or does the model have variety? We measure this with distinct-1 and distinct-2 (percentage of unique words and word pairs). Too low = mode collapse (model stuck in a rut).\n",
    "\n",
    "Let's compute both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:37.367351Z",
     "iopub.status.busy": "2025-12-10T21:21:37.367279Z",
     "iopub.status.idle": "2025-12-10T21:21:46.023015Z",
     "shell.execute_reply": "2025-12-10T21:21:46.022675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model quality...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e456ae9217a40339b041bb3109775be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing perplexity:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Evaluation complete!\n",
      "  Loss: 1.9571\n",
      "  Perplexity: 7.08\n",
      "\n",
      "Interpretation:\n",
      "  - Perplexity < 10: Excellent\n",
      "  - Perplexity 10-20: Good\n",
      "  - Perplexity 20-50: Okay\n",
      "  - Perplexity > 50: Needs more training\n",
      "\n",
      "Your model: Excellent! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Compute perplexity on a dataset.\n",
    "    \n",
    "    Perplexity = e^(average loss)\n",
    "    \n",
    "    Think of it as: \"On average, how many equally-likely tokens could come next?\"\n",
    "    Lower is better. Random guessing on a 50k vocab = perplexity of 50,000.\n",
    "    A well-trained model on instructions = perplexity of 5-10.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for batch in tqdm(dataloader, desc=\"Computing perplexity\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            # Count non-masked tokens (only response tokens, not prompt)\n",
    "            num_tokens = (batch['labels'] != -100).sum().item()\n",
    "            total_loss += outputs.loss.item() * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "# Compute perplexity on the training set\n",
    "# (In practice, you'd use a held-out validation set, but we're keeping it simple)\n",
    "print(\"\\nEvaluating model quality...\")\n",
    "perplexity, loss = compute_perplexity(model, train_loader, device)\n",
    "\n",
    "print(f\"\\nâœ“ Evaluation complete!\")\n",
    "print(f\"  Loss: {loss:.4f}\")\n",
    "print(f\"  Perplexity: {perplexity:.2f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Perplexity < 10: Excellent\")\n",
    "print(f\"  - Perplexity 10-20: Good\")\n",
    "print(f\"  - Perplexity 20-50: Okay\")\n",
    "print(f\"  - Perplexity > 50: Needs more training\")\n",
    "print(f\"\\nYour model: \", end=\"\")\n",
    "if perplexity < 10:\n",
    "    print(\"Excellent! ðŸŽ‰\")\n",
    "elif perplexity < 20:\n",
    "    print(\"Good! ðŸ‘\")\n",
    "elif perplexity < 50:\n",
    "    print(\"Okay. More training would help.\")\n",
    "else:\n",
    "    print(\"Needs more training. Try more epochs or more data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:46.024023Z",
     "iopub.status.busy": "2025-12-10T21:21:46.023942Z",
     "iopub.status.idle": "2025-12-10T21:21:48.038975Z",
     "shell.execute_reply": "2025-12-10T21:21:48.038664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses for diversity analysis...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Diversity analysis complete!\n",
      "  Distinct-1 (unique words): 30.77%\n",
      "  Distinct-2 (unique word pairs): 59.43%\n",
      "\n",
      "Interpretation:\n",
      "  - Distinct-1 > 40%: Good variety\n",
      "  - Distinct-1 20-40%: Okay\n",
      "  - Distinct-1 < 20%: Mode collapse (model stuck repeating itself)\n",
      "\n",
      "Your model: Okay diversity.\n"
     ]
    }
   ],
   "source": [
    "def compute_diversity(responses):\n",
    "    \"\"\"\n",
    "    Compute diversity metrics for generated text.\n",
    "    \n",
    "    Distinct-1: Percentage of unique words (unigrams)\n",
    "    Distinct-2: Percentage of unique word pairs (bigrams)\n",
    "    \n",
    "    Why does this matter? If the model always says \"the the the the\" you'd have\n",
    "    low diversity even if perplexity looks okay. Diversity catches mode collapse.\n",
    "    \"\"\"\n",
    "    all_unigrams = []\n",
    "    all_bigrams = []\n",
    "    \n",
    "    for response in responses:\n",
    "        tokens = response.lower().split()\n",
    "        all_unigrams.extend(tokens)\n",
    "        # Create pairs of consecutive words\n",
    "        all_bigrams.extend(zip(tokens[:-1], tokens[1:]))\n",
    "    \n",
    "    # What fraction of words/pairs are unique?\n",
    "    distinct_1 = len(set(all_unigrams)) / len(all_unigrams) if all_unigrams else 0\n",
    "    distinct_2 = len(set(all_bigrams)) / len(all_bigrams) if all_bigrams else 0\n",
    "    \n",
    "    return distinct_1, distinct_2\n",
    "\n",
    "# Generate a bunch of responses for diversity analysis\n",
    "diversity_prompts = [\n",
    "    \"Tell me about machine learning.\",\n",
    "    \"Explain artificial intelligence.\",\n",
    "    \"What is deep learning?\",\n",
    "    \"Describe natural language processing.\",\n",
    "    \"Explain what data science is.\",\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating responses for diversity analysis...\")\n",
    "responses = [generate_response(model, tokenizer, p) for p in diversity_prompts]\n",
    "d1, d2 = compute_diversity(responses)\n",
    "\n",
    "print(f\"\\nâœ“ Diversity analysis complete!\")\n",
    "print(f\"  Distinct-1 (unique words): {d1:.2%}\")\n",
    "print(f\"  Distinct-2 (unique word pairs): {d2:.2%}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Distinct-1 > 40%: Good variety\")\n",
    "print(f\"  - Distinct-1 20-40%: Okay\")\n",
    "print(f\"  - Distinct-1 < 20%: Mode collapse (model stuck repeating itself)\")\n",
    "print(f\"\\nYour model: \", end=\"\")\n",
    "if d1 > 0.4:\n",
    "    print(\"Good variety! ðŸŽ‰\")\n",
    "elif d1 > 0.2:\n",
    "    print(\"Okay diversity.\")\n",
    "else:\n",
    "    print(\"Warning: Low diversity. Try different sampling parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Your Model\n",
    "\n",
    "You just spent 15 minutes training this thing. Let's not lose it!\n",
    "\n",
    "Saving is simpleâ€”we just dump the model weights and tokenizer config to disk. Then you can reload them later (or share them with others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:48.040041Z",
     "iopub.status.busy": "2025-12-10T21:21:48.039965Z",
     "iopub.status.idle": "2025-12-10T21:21:48.724709Z",
     "shell.execute_reply": "2025-12-10T21:21:48.724432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./my_finetuned_model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model saved!\n",
      "\n",
      "Saved files:\n",
      "  config.json: 0.0 MB\n",
      "  generation_config.json: 0.0 MB\n",
      "  merges.txt: 0.5 MB\n",
      "  model.safetensors: 497.8 MB\n",
      "  special_tokens_map.json: 0.0 MB\n",
      "  tokenizer.json: 3.6 MB\n",
      "  tokenizer_config.json: 0.0 MB\n",
      "  vocab.json: 0.8 MB\n",
      "\n",
      "Total size: 502.6 MB\n",
      "\n",
      "You can now load this model anytime with:\n",
      "  model = AutoModelForCausalLM.from_pretrained('./my_finetuned_model')\n",
      "  tokenizer = AutoTokenizer.from_pretrained('./my_finetuned_model')\n"
     ]
    }
   ],
   "source": [
    "# Save model and tokenizer to disk\n",
    "save_path = \"./my_finetuned_model\"\n",
    "\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"âœ“ Model saved!\")\n",
    "\n",
    "# Show what got saved\n",
    "import os\n",
    "print(f\"\\nSaved files:\")\n",
    "total_size = 0\n",
    "for f in sorted(os.listdir(save_path)):\n",
    "    size = os.path.getsize(os.path.join(save_path, f)) / 1e6\n",
    "    total_size += size\n",
    "    print(f\"  {f}: {size:.1f} MB\")\n",
    "\n",
    "print(f\"\\nTotal size: {total_size:.1f} MB\")\n",
    "print(f\"\\nYou can now load this model anytime with:\")\n",
    "print(f\"  model = AutoModelForCausalLM.from_pretrained('{save_path}')\")\n",
    "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{save_path}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:21:48.725730Z",
     "iopub.status.busy": "2025-12-10T21:21:48.725653Z",
     "iopub.status.idle": "2025-12-10T21:21:49.189697Z",
     "shell.execute_reply": "2025-12-10T21:21:49.189321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing saved model (to make sure saving worked)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Loaded model from disk successfully!\n",
      "\n",
      "Test question: What is the meaning of life?\n",
      "Answer: Life is a complex and interconnected system of life, and we live by our own choices and desires. We make our own decisions about our lives, and we learn from our experiences to make better choices.\n",
      "\n",
      "Looks good! Your model is saved and ready to use.\n"
     ]
    }
   ],
   "source": [
    "# Let's verify the saved model actually works\n",
    "print(\"Testing saved model (to make sure saving worked)...\")\n",
    "\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "test_instruction = \"What is the meaning of life?\"\n",
    "response = generate_response(loaded_model, loaded_tokenizer, test_instruction)\n",
    "\n",
    "print(f\"\\nâœ“ Loaded model from disk successfully!\")\n",
    "print(f\"\\nTest question: {test_instruction}\")\n",
    "print(f\"Answer: {response}\")\n",
    "print(f\"\\nLooks good! Your model is saved and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Did It! ðŸŽ‰\n",
    "\n",
    "Seriously. You just fine-tuned a language model from scratch.\n",
    "\n",
    "**What you accomplished:**\n",
    "1. Loaded a base GPT-2 model (terrible at following instructions)\n",
    "2. Prepared training data with proper label masking\n",
    "3. Trained the model using supervised fine-tuning\n",
    "4. Watched it go from gibberish to actual answers\n",
    "5. Evaluated it with perplexity and diversity metrics\n",
    "6. Saved it for later use\n",
    "\n",
    "This is the same basic process used to create ChatGPT, Claude, and every other instruction-following LLM. The production versions use more data, bigger models, LoRA for efficiency, and RLHF for alignmentâ€”but the core idea is exactly what you just did.\n",
    "\n",
    "## What to Try Next\n",
    "\n",
    "Now that you've got the basics down:\n",
    "\n",
    "1. **Train longer** - Try 3-5 epochs or use the full Alpaca dataset (52k examples)\n",
    "2. **Use LoRA** - Fine-tune only a small number of parameters (way more efficient)\n",
    "3. **Try DPO** - Align the model with human preferences using the reward/preference notebooks\n",
    "4. **Bigger models** - GPT-2 Medium/Large, or even Llama if you've got the VRAM\n",
    "5. **Your own data** - Got a specific task? Create a dataset and fine-tune for it!\n",
    "\n",
    "## Common Issues & Tips\n",
    "\n",
    "**Loss not going down?**\n",
    "- Check your learning rate (try 1e-5 to 1e-4)\n",
    "- Make sure labels are masked properly (prompt tokens should be -100)\n",
    "- Try more epochs or more data\n",
    "\n",
    "**Model output is repetitive?**\n",
    "- Adjust temperature and top_p during generation\n",
    "- Check diversity metrics (distinct-1/distinct-2)\n",
    "- Might need more varied training data\n",
    "\n",
    "**Out of memory?**\n",
    "- Reduce batch_size (try 2 or 1)\n",
    "- Reduce max_length (try 128 or 64)\n",
    "- Use gradient checkpointing (more compute, less memory)\n",
    "- Consider LoRA (way less memory)\n",
    "\n",
    "**Answers are still bad?**\n",
    "- Train on more data (500 examples is pretty small)\n",
    "- Train for more epochs\n",
    "- Check that loss actually decreased during training\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "The model you just trained isn't perfect. It might hallucinate, give weird answers, or ramble sometimes. That's normal! You trained it on 500 examples for 10 minutes.\n",
    "\n",
    "What matters is that you understand the *process*. You know how to:\n",
    "- Load and prepare data\n",
    "- Set up a training loop\n",
    "- Evaluate results\n",
    "- Debug when things go wrong\n",
    "\n",
    "That's the hard part. Scaling up to production is just... more of the same, but bigger.\n",
    "\n",
    "Go build something cool. ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01b05c02db50430bbc6ed0f67cf10639": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e456ae9217a40339b041bb3109775be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eb3aec6712fa4e7c93eae47670aa4521",
        "IPY_MODEL_c94466599cfd493795f7716aa2548560",
        "IPY_MODEL_a3968f21b76a4580b0a1f5ed6e070092"
       ],
       "layout": "IPY_MODEL_d2124f9b2d654e838f14ad62df7cda6f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "12f11a437f2c42668ba02314ef991ec9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2424dcc36acd46a5b2c690017d57744a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_12f11a437f2c42668ba02314ef991ec9",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_30cdf2324cfa4ed2932de4e0e452984d",
       "tabbable": null,
       "tooltip": null,
       "value": "Epochâ€‡1/1:â€‡100%"
      }
     },
     "30cdf2324cfa4ed2932de4e0e452984d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "39dcfb66bf3e4246bd394d4b857d0068": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "460e62e540e14eed854409d2bf7c8f5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2424dcc36acd46a5b2c690017d57744a",
        "IPY_MODEL_599006af0e824b0abca92a8ce4053889",
        "IPY_MODEL_863dc9e7bf694a36ab7f0b1785828cfd"
       ],
       "layout": "IPY_MODEL_4daa9996267145439bc4f97bb80d9126",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4daa9996267145439bc4f97bb80d9126": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "599006af0e824b0abca92a8ce4053889": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_01b05c02db50430bbc6ed0f67cf10639",
       "max": 125.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f2bd0613c0604211a050d01fe29fabd2",
       "tabbable": null,
       "tooltip": null,
       "value": 125.0
      }
     },
     "5afb1fc74f734e1294c22483efb43a9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b088e7529734dafab3410ea220f0220": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7516b6122b184d08bfcccd4cdc9a9525": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "842491561fa34f3cbb5200741b80972e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "863dc9e7bf694a36ab7f0b1785828cfd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5afb1fc74f734e1294c22483efb43a9e",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_f1962e6ab42a43bab6242794fb90b0de",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡125/125â€‡[00:16&lt;00:00,â€‡â€‡7.68it/s,â€‡loss=2.5048,â€‡avg_loss=2.4032,â€‡ppl=11.06]"
      }
     },
     "86fd09226e604fa69b259389d5674330": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a3968f21b76a4580b0a1f5ed6e070092": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fc39738d456843ee81cb8a167efdc194",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_7516b6122b184d08bfcccd4cdc9a9525",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡125/125â€‡[00:08&lt;00:00,â€‡14.48it/s]"
      }
     },
     "c94466599cfd493795f7716aa2548560": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_86fd09226e604fa69b259389d5674330",
       "max": 125.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6b088e7529734dafab3410ea220f0220",
       "tabbable": null,
       "tooltip": null,
       "value": 125.0
      }
     },
     "d2124f9b2d654e838f14ad62df7cda6f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb3aec6712fa4e7c93eae47670aa4521": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_39dcfb66bf3e4246bd394d4b857d0068",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_842491561fa34f3cbb5200741b80972e",
       "tabbable": null,
       "tooltip": null,
       "value": "Computingâ€‡perplexity:â€‡100%"
      }
     },
     "f1962e6ab42a43bab6242794fb90b0de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f2bd0613c0604211a050d01fe29fabd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fc39738d456843ee81cb8a167efdc194": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
