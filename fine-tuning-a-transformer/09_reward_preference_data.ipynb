{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Preference Datasets\n",
    "\n",
    "**Understanding preference data format, sources, and quality considerations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is Preference Data?\n",
    "\n",
    "**Preference data** consists of comparisons between responses, not absolute quality scores. Each sample contains:\n",
    "\n",
    "- **prompt**: The input instruction or question\n",
    "- **chosen**: The preferred response (higher quality)\n",
    "- **rejected**: The non-preferred response (lower quality)\n",
    "\n",
    "**Key insight:** Humans are much better at comparing responses than assigning absolute scores. \"Which is better?\" is easier than \"Rate this 1-10.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Required Format\n",
    "\n",
    "Every reward model dataset needs these three fields:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"prompt\": \"What is the capital of France?\",\n",
    "  \"chosen\": \"The capital of France is Paris. It's known for art and culture.\",\n",
    "  \"rejected\": \"paris\"\n",
    "}\n",
    "```\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `prompt` | string | The instruction or question |\n",
    "| `chosen` | string | The response humans preferred |\n",
    "| `rejected` | string | The response humans did not prefer |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Anthropic's HH-RLHF dataset\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print()\n",
    "\n",
    "# Show example\n",
    "example = dataset[0]\n",
    "print(\"Example preference pair:\")\n",
    "print(f\"Chosen: {example['chosen'][:200]}...\")\n",
    "print(f\"\\nRejected: {example['rejected'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Popular Preference Datasets\n",
    "\n",
    "### 1. Anthropic HH-RLHF\n",
    "- **Size:** ~161K training pairs\n",
    "- **Focus:** Helpfulness and Harmlessness\n",
    "- **Quality:** High (trained annotators)\n",
    "\n",
    "### 2. Stanford SHP\n",
    "- **Size:** ~385K preference pairs\n",
    "- **Source:** Stack Exchange votes\n",
    "- **Focus:** Factual helpfulness\n",
    "\n",
    "### 3. OpenAssistant\n",
    "- **Size:** ~161K messages with rankings\n",
    "- **Source:** Community-contributed\n",
    "- **Format:** Multi-turn with rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Dataset Quality Considerations\n",
    "\n",
    "### What Makes Good Preference Data?\n",
    "\n",
    "1. **Clear Distinction** — Chosen should be noticeably better than rejected\n",
    "2. **Diverse Criteria** — Cover helpfulness, safety, accuracy, style\n",
    "3. **Consistent Guidelines** — Annotators follow same criteria\n",
    "4. **Representative Distribution** — Match your deployment domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset statistics\n",
    "import numpy as np\n",
    "\n",
    "def analyze_preference_dataset(dataset, num_samples=1000):\n",
    "    \"\"\"Compute statistics on preference dataset.\"\"\"\n",
    "    chosen_lengths = []\n",
    "    rejected_lengths = []\n",
    "    \n",
    "    for i, item in enumerate(dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        chosen_lengths.append(len(item['chosen'].split()))\n",
    "        rejected_lengths.append(len(item['rejected'].split()))\n",
    "    \n",
    "    print(\"Length Statistics (word count):\")\n",
    "    print(f\"  Chosen - Mean: {np.mean(chosen_lengths):.1f}, Median: {np.median(chosen_lengths):.1f}\")\n",
    "    print(f\"  Rejected - Mean: {np.mean(rejected_lengths):.1f}, Median: {np.median(rejected_lengths):.1f}\")\n",
    "    \n",
    "    # Check for length bias\n",
    "    chosen_longer = sum(1 for c, r in zip(chosen_lengths, rejected_lengths) if c > r)\n",
    "    print(f\"\\nLength bias check:\")\n",
    "    print(f\"  Chosen is longer: {100 * chosen_longer / len(chosen_lengths):.1f}% of the time\")\n",
    "    \n",
    "    return chosen_lengths, rejected_lengths\n",
    "\n",
    "chosen_lengths, rejected_lengths = analyze_preference_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Common Data Quality Issues\n",
    "\n",
    "### Near-Duplicate Pairs\n",
    "**Problem:** Chosen and rejected are nearly identical.\n",
    "\n",
    "### Ordering Bias\n",
    "**Problem:** Annotators prefer the first response shown.\n",
    "\n",
    "### Length Bias\n",
    "**Problem:** Longer responses always preferred.\n",
    "\n",
    "### Annotation Fatigue\n",
    "**Problem:** Quality degrades over long sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def check_similarity(dataset, num_samples=100):\n",
    "    \"\"\"Check for near-duplicate preference pairs.\"\"\"\n",
    "    similarities = []\n",
    "    \n",
    "    for i, item in enumerate(dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        sim = SequenceMatcher(None, item['chosen'], item['rejected']).ratio()\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    high_sim = sum(1 for s in similarities if s > 0.9)\n",
    "    \n",
    "    print(f\"Similarity analysis (first {num_samples} samples):\")\n",
    "    print(f\"  Mean similarity: {np.mean(similarities):.3f}\")\n",
    "    print(f\"  High similarity (>0.9): {high_sim} pairs ({100*high_sim/len(similarities):.1f}%)\")\n",
    "    \n",
    "    if high_sim > 0:\n",
    "        print(\"  ⚠️ Warning: Some pairs are very similar - may need filtering\")\n",
    "\n",
    "check_similarity(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## PyTorch Dataset for Reward Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RewardModelDataset(Dataset):\n",
    "    \"\"\"Dataset for reward model training.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, tokenizer, max_length=512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Tokenize chosen response\n",
    "        chosen_tokens = self.tokenizer(\n",
    "            item['chosen'],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize rejected response\n",
    "        rejected_tokens = self.tokenizer(\n",
    "            item['rejected'],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'chosen_input_ids': chosen_tokens['input_ids'].squeeze(0),\n",
    "            'chosen_attention_mask': chosen_tokens['attention_mask'].squeeze(0),\n",
    "            'rejected_input_ids': rejected_tokens['input_ids'].squeeze(0),\n",
    "            'rejected_attention_mask': rejected_tokens['attention_mask'].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "reward_dataset = RewardModelDataset(dataset, tokenizer, max_length=256)\n",
    "\n",
    "sample = reward_dataset[0]\n",
    "print(\"Dataset sample keys:\", list(sample.keys()))\n",
    "print(f\"Chosen shape: {sample['chosen_input_ids'].shape}\")\n",
    "print(f\"Rejected shape: {sample['rejected_input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we understand preference data, let's learn how to train reward models on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
