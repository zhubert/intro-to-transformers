{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "\n",
    "**The foundation of stable RLHF training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is PPO?\n",
    "\n",
    "**Proximal Policy Optimization (PPO)** is a policy gradient RL algorithm that has become the gold standard for RLHF. It solves the fundamental challenge: **how to improve a policy without breaking it**.\n",
    "\n",
    "PPO is what powers ChatGPT, Claude, and most modern aligned language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Problem: Policy Gradient Instability\n",
    "\n",
    "Vanilla policy gradient is:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a | s) \\cdot R \\right]$$\n",
    "\n",
    "**The problem:** This is extremely unstable! A \"reasonable\" gradient step can cause catastrophic changes in policy behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The PPO Objective\n",
    "\n",
    "PPO adds **clipping** to prevent large policy changes:\n",
    "\n",
    "$$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right) \\right]$$\n",
    "\n",
    "where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}$ is the **probability ratio**\n",
    "- $\\hat{A}_t$ is the **advantage estimate**\n",
    "- $\\epsilon$ is the **clipping threshold** (typically 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compute_ppo_loss(\n",
    "    logprobs: torch.Tensor,\n",
    "    old_logprobs: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "    clip_ratio: float = 0.2\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute PPO clipped surrogate loss.\n",
    "    \n",
    "    Args:\n",
    "        logprobs: Log probs under current policy\n",
    "        old_logprobs: Log probs under old policy (from rollout)\n",
    "        advantages: Advantage estimates\n",
    "        clip_ratio: Clipping threshold (epsilon)\n",
    "    \n",
    "    Returns:\n",
    "        PPO loss (to minimize)\n",
    "    \"\"\"\n",
    "    # Compute probability ratio\n",
    "    log_ratio = logprobs - old_logprobs\n",
    "    ratio = torch.exp(log_ratio)\n",
    "    \n",
    "    # Unclipped objective\n",
    "    unclipped = ratio * advantages\n",
    "    \n",
    "    # Clipped objective\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "    clipped = clipped_ratio * advantages\n",
    "    \n",
    "    # Take minimum (conservative update)\n",
    "    loss = -torch.min(unclipped, clipped).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Example\n",
    "batch_size = 16\n",
    "logprobs = torch.randn(batch_size) - 2\n",
    "old_logprobs = torch.randn(batch_size) - 2\n",
    "advantages = torch.randn(batch_size)\n",
    "\n",
    "loss = compute_ppo_loss(logprobs, old_logprobs, advantages)\n",
    "print(f\"PPO Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Visualizing the Clipped Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PPO clipping for positive advantage\n",
    "ratio = np.linspace(0.5, 1.5, 100)\n",
    "epsilon = 0.2\n",
    "advantage = 1.0  # Positive advantage\n",
    "\n",
    "unclipped = ratio * advantage\n",
    "clipped_ratio = np.clip(ratio, 1 - epsilon, 1 + epsilon)\n",
    "clipped = clipped_ratio * advantage\n",
    "ppo_objective = np.minimum(unclipped, clipped)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ratio, unclipped, '--', label='Unclipped', alpha=0.7)\n",
    "plt.plot(ratio, clipped, '--', label='Clipped', alpha=0.7)\n",
    "plt.plot(ratio, ppo_objective, 'b-', linewidth=2, label='PPO Objective')\n",
    "plt.axvline(x=1.0, color='gray', linestyle=':', alpha=0.5)\n",
    "plt.axvline(x=1-epsilon, color='red', linestyle=':', alpha=0.5, label=f'Clip bounds (1±{epsilon})')\n",
    "plt.axvline(x=1+epsilon, color='red', linestyle=':', alpha=0.5)\n",
    "plt.xlabel('Probability Ratio (π_new / π_old)')\n",
    "plt.ylabel('Objective')\n",
    "plt.title('PPO Clipped Objective (Positive Advantage)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Once ratio exceeds 1+ε, gradient becomes zero!\")\n",
    "print(\"This prevents overshooting and catastrophic policy changes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Value Function Loss\n",
    "\n",
    "PPO also trains a value network to predict returns:\n",
    "\n",
    "$$L^V(\\theta) = \\mathbb{E}_t \\left[ (V_\\theta(s_t) - \\hat{R}_t)^2 \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_loss(\n",
    "    values: torch.Tensor,\n",
    "    returns: torch.Tensor,\n",
    "    old_values: torch.Tensor = None,\n",
    "    clip_value: bool = True,\n",
    "    clip_ratio: float = 0.2\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute value function loss (optionally clipped).\n",
    "    \"\"\"\n",
    "    if clip_value and old_values is not None:\n",
    "        # Clipped value loss\n",
    "        value_clipped = old_values + torch.clamp(\n",
    "            values - old_values, -clip_ratio, clip_ratio\n",
    "        )\n",
    "        loss_unclipped = (values - returns) ** 2\n",
    "        loss_clipped = (value_clipped - returns) ** 2\n",
    "        loss = torch.max(loss_unclipped, loss_clipped).mean()\n",
    "    else:\n",
    "        # Standard MSE\n",
    "        loss = F.mse_loss(values, returns)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Example\n",
    "values = torch.randn(batch_size)\n",
    "returns = torch.randn(batch_size)\n",
    "old_values = values + torch.randn(batch_size) * 0.1\n",
    "\n",
    "value_loss = compute_value_loss(values, returns, old_values)\n",
    "print(f\"Value Loss: {value_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Entropy Bonus\n",
    "\n",
    "To encourage exploration, PPO adds an entropy bonus:\n",
    "\n",
    "$$H(\\pi_\\theta) = -\\mathbb{E}_{a \\sim \\pi_\\theta} [\\log \\pi_\\theta(a | s)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(\n",
    "    logits: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute entropy of policy distribution.\n",
    "    \n",
    "    High entropy = more exploration\n",
    "    Low entropy = more deterministic\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "    return entropy.mean()\n",
    "\n",
    "# Example\n",
    "logits = torch.randn(batch_size, 1000)  # vocab_size = 1000\n",
    "entropy = compute_entropy(logits)\n",
    "print(f\"Entropy: {entropy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Complete PPO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppo_total_loss(\n",
    "    policy_logprobs: torch.Tensor,\n",
    "    old_logprobs: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    returns: torch.Tensor,\n",
    "    logits: torch.Tensor,\n",
    "    clip_ratio: float = 0.2,\n",
    "    vf_coef: float = 0.5,\n",
    "    entropy_coef: float = 0.01\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute total PPO loss combining all components.\n",
    "    \n",
    "    Total = Policy Loss + vf_coef * Value Loss - entropy_coef * Entropy\n",
    "    \"\"\"\n",
    "    # Policy loss\n",
    "    policy_loss = compute_ppo_loss(policy_logprobs, old_logprobs, advantages, clip_ratio)\n",
    "    \n",
    "    # Value loss\n",
    "    value_loss = compute_value_loss(values, returns)\n",
    "    \n",
    "    # Entropy bonus\n",
    "    entropy = compute_entropy(logits)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = policy_loss + vf_coef * value_loss - entropy_coef * entropy\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss,\n",
    "        'policy_loss': policy_loss,\n",
    "        'value_loss': value_loss,\n",
    "        'entropy': entropy\n",
    "    }\n",
    "\n",
    "print(\"PPO Total Loss = Policy Loss + 0.5 * Value Loss - 0.01 * Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## PPO Hyperparameters\n",
    "\n",
    "| Parameter | Default | Effect |\n",
    "|-----------|---------|--------|\n",
    "| `clip_ratio` (ε) | 0.2 | Higher = more aggressive updates |\n",
    "| `vf_coef` | 0.5 | Value function loss weight |\n",
    "| `entropy_coef` | 0.01 | Exploration encouragement |\n",
    "| `ppo_epochs` | 4 | Reuses per rollout batch |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now let's learn about the KL penalty — why preventing drift from the reference model is essential for stable RLHF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
