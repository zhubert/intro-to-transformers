{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# PPO: Teaching AI Without Breaking It\n\n**How Proximal Policy Optimization makes RLHF actually work**\n\nSo we've got a reward model that can score outputs. Great! But how do we use it to improve our language model?\n\nThat's where PPO comes in. And before your eyes glaze over at another acronym, let me promise you: this is one of the most elegant ideas in modern AI."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## What is PPO?\n\n**PPO** stands for **Proximal Policy Optimization**. Let's break that down:\n\n- **Policy** = Your model's strategy for choosing outputs (what to say given some input)\n- **Optimization** = Making it better (duh)\n- **Proximal** = Staying close, not jumping too far\n\n(Proximal comes from the Latin \"proximus\" meaning \"nearest.\" Same root as \"proximity.\" Who says you can't learn Latin from AI tutorials?)\n\nHere's the core insight: **improving a neural network policy is dangerous**. Take too big a step, and your model can catastrophically forget everything it learned. It's like adjusting a recipe ‚Äî add way too much salt and you've ruined dinner.\n\nPPO solves this by keeping updates *proximal* ‚Äî close to where we started. Conservative. Safe.\n\nThis is what powers ChatGPT, Claude, and basically every aligned language model you've ever used. It's the secret sauce that makes RLHF actually work in practice."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## The Problem: Why Regular Policy Gradients Break Everything\n\nFirst, let's talk about what **doesn't** work: vanilla policy gradients.\n\nA **policy gradient** is just a fancy way of saying \"adjust the model based on which outputs got high rewards.\" The basic formula looks like this:\n\n$$\\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a | s) \\cdot R \\right]$$\n\nLet's decode this math:\n- $\\theta$ = Your model's parameters (all those billions of weights)\n- $\\pi_\\theta(a | s)$ = Probability your model assigns to action $a$ given state $s$ (in language: probability of generating some text given a prompt)\n- $R$ = Reward you got for that action\n- $\\nabla_\\theta$ = Gradient (how to change $\\theta$ to improve things)\n- $\\mathbb{E}$ = Expected value (average over many samples)\n\nIn plain English: \"Move the model's parameters in whatever direction makes high-reward outputs more likely.\"\n\nSounds great, right?\n\n**Wrong.**\n\nThis is *catastrophically* unstable. Why? Because neural networks are weird. A \"reasonable looking\" gradient step can completely change the model's behavior. One update and suddenly your helpful chatbot only speaks in haikus. Another update and it forgets how to spell.\n\n(Ask me how I know. Actually, don't. Those were dark days.)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## The PPO Solution: Clip Those Gradients\n\nPPO's big idea is **clipping**. Don't let the policy change too much in a single update. Here's the formula:\n\n$$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right) \\right]$$\n\nOkay, deep breath. Let's decode this symbol by symbol:\n\n**$r_t(\\theta)$ ‚Äî The Probability Ratio**\n\n$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}$$\n\nThis compares your *new* policy to your *old* policy. How much more (or less) likely is the new model to generate this same output?\n\n- $r_t = 1.0$ ‚Üí No change (new and old policy identical)\n- $r_t = 1.5$ ‚Üí New policy is 1.5x more likely to choose this action\n- $r_t = 0.5$ ‚Üí New policy is half as likely\n\n**$\\hat{A}_t$ ‚Äî The Advantage Estimate**\n\nThis measures: \"Was this action better or worse than average?\"\n\n- Positive advantage = This action was better than expected (do more of this!)\n- Negative advantage = This action was worse than expected (do less of this!)\n- Zero advantage = This action was exactly average (meh)\n\nThink of it like golf scores. Par is your baseline. Advantage tells you how many strokes above or below par you were.\n\n**$\\epsilon$ ‚Äî The Clipping Threshold**\n\nUsually set to 0.2. This is the \"leash\" that keeps your policy from wandering too far. The ratio $r_t$ gets clipped to the range $[0.8, 1.2]$.\n\n**The $\\min$ ‚Äî The Conservative Part**\n\nWe take the *minimum* of the clipped and unclipped objectives. Why? Because we're pessimists (in a good way). If the clipped version is worse, we use that. This prevents over-optimistic updates.\n\nPut it all together: **\"Improve the policy based on advantages, but don't let any single action's probability change by more than ¬±20%.\"**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:54.259022Z",
     "iopub.status.busy": "2025-12-06T23:29:54.258950Z",
     "iopub.status.idle": "2025-12-06T23:29:55.091013Z",
     "shell.execute_reply": "2025-12-06T23:29:55.090693Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef compute_ppo_loss(\n    logprobs: torch.Tensor,\n    old_logprobs: torch.Tensor,\n    advantages: torch.Tensor,\n    clip_ratio: float = 0.2\n) -> torch.Tensor:\n    \"\"\"\n    Compute PPO clipped surrogate loss.\n    \n    The \"surrogate\" part is RL jargon for \"objective we're optimizing.\"\n    We can't optimize the true policy directly, so we optimize a surrogate.\n    \n    Args:\n        logprobs: Log probabilities under current policy\n        old_logprobs: Log probabilities under old policy (from rollout)\n        advantages: Advantage estimates (how good was each action)\n        clip_ratio: Clipping threshold epsilon (default 0.2)\n    \n    Returns:\n        PPO loss (to minimize ‚Äî we return negative of objective)\n    \"\"\"\n    # Compute probability ratio r_t = œÄ_new / œÄ_old\n    # We work in log space for numerical stability\n    # exp(log(a) - log(b)) = exp(log(a/b)) = a/b\n    log_ratio = logprobs - old_logprobs\n    ratio = torch.exp(log_ratio)\n    \n    # Unclipped objective: ratio * advantage\n    unclipped = ratio * advantages\n    \n    # Clipped objective: clip(ratio, 1-Œµ, 1+Œµ) * advantage\n    clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)\n    clipped = clipped_ratio * advantages\n    \n    # Take minimum (conservative update) and negate (we minimize loss)\n    loss = -torch.min(unclipped, clipped).mean()\n    \n    return loss\n\n# Let's see this in action with some fake data\nbatch_size = 16\n\n# Simulate log probabilities (negative numbers, since probs are < 1)\nlogprobs = torch.randn(batch_size) - 2      # Current policy\nold_logprobs = torch.randn(batch_size) - 2  # Old policy\n\n# Simulate advantages (some positive, some negative)\nadvantages = torch.randn(batch_size)\n\nloss = compute_ppo_loss(logprobs, old_logprobs, advantages)\nprint(f\"PPO Loss: {loss.item():.4f}\")\nprint(f\"\\nThis loss combines {batch_size} different actions,\")\nprint(f\"each clipped to prevent catastrophic policy changes.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## Visualizing the Clipped Objective\n\nPictures are worth a thousand equations. Let's see what that clipping actually does."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:55.092070Z",
     "iopub.status.busy": "2025-12-06T23:29:55.091963Z",
     "iopub.status.idle": "2025-12-06T23:29:55.152395Z",
     "shell.execute_reply": "2025-12-06T23:29:55.152009Z"
    }
   },
   "outputs": [],
   "source": "# Let's visualize what happens with a positive advantage\n# (i.e., we found a good action and want to do it more)\n\nratio = np.linspace(0.5, 1.5, 100)  # How much has the policy changed?\nepsilon = 0.2                        # Our clipping threshold\nadvantage = 1.0                      # Positive advantage (good action!)\n\n# Without clipping: objective = ratio * advantage\n# Just a straight line ‚Äî more policy change = more objective\nunclipped = ratio * advantage\n\n# With clipping: ratio gets clamped to [0.8, 1.2]\nclipped_ratio = np.clip(ratio, 1 - epsilon, 1 + epsilon)\nclipped = clipped_ratio * advantage\n\n# PPO takes the minimum (conservative!)\nppo_objective = np.minimum(unclipped, clipped)\n\nplt.figure(figsize=(10, 6))\nplt.plot(ratio, unclipped, '--', label='Unclipped (dangerous!)', alpha=0.7, color='red')\nplt.plot(ratio, clipped, '--', label='Clipped ratio', alpha=0.7, color='orange')\nplt.plot(ratio, ppo_objective, 'b-', linewidth=3, label='PPO Objective (safe!)')\n\n# Mark the important boundaries\nplt.axvline(x=1.0, color='gray', linestyle=':', alpha=0.5, label='No change (ratio=1)')\nplt.axvline(x=1-epsilon, color='darkred', linestyle=':', alpha=0.7, linewidth=2)\nplt.axvline(x=1+epsilon, color='darkred', linestyle=':', alpha=0.7, linewidth=2)\nplt.text(1+epsilon+0.02, 0.5, f'Clip bounds\\n(1¬±{epsilon})', fontsize=10, color='darkred')\n\nplt.xlabel('Probability Ratio (œÄ_new / œÄ_old)', fontsize=12)\nplt.ylabel('Objective', fontsize=12)\nplt.title('PPO Clipping for Positive Advantage\\n(Want to encourage this action)', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"üéØ Key insight: Once ratio exceeds 1+Œµ, the gradient becomes ZERO!\")\nprint(\"   The objective flatlines. No incentive to change the policy further.\")\nprint()\nprint(\"   This prevents overshooting. You can't accidentally make the policy\")\nprint(\"   100x more likely to take an action just because it got a good reward once.\")\nprint()\nprint(\"   That's the 'proximal' in Proximal Policy Optimization.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## The Value Function: Predicting Future Rewards\n\nPPO doesn't just optimize the policy. It also trains a **value function** to predict returns.\n\nWait, what's a value function?\n\nThink of it like this: when your model starts generating a response, the value function tries to predict \"how good is this going to turn out?\" It's looking into the future and estimating the total reward you'll get.\n\nThe formula:\n\n$$L^V(\\theta) = \\mathbb{E}_t \\left[ (V_\\theta(s_t) - \\hat{R}_t)^2 \\right]$$\n\nTranslation:\n- $V_\\theta(s_t)$ = Value function's prediction (\"I think this will get a reward of...\")\n- $\\hat{R}_t$ = Actual return we got (\"Actually, it got a reward of...\")\n- $(prediction - actual)^2$ = Squared error (standard supervised learning!)\n\nWhy do we need this? Two reasons:\n\n1. **Computing advantages**: Remember $\\hat{A}_t$? That's advantage = actual return - predicted value. The value function provides that baseline.\n\n2. **Variance reduction**: Raw rewards are noisy. The value function smooths things out by learning patterns across many rollouts.\n\nIt's supervised learning buried inside reinforcement learning. Neat, right?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:55.153205Z",
     "iopub.status.busy": "2025-12-06T23:29:55.153117Z",
     "iopub.status.idle": "2025-12-06T23:29:55.155559Z",
     "shell.execute_reply": "2025-12-06T23:29:55.155251Z"
    }
   },
   "outputs": [],
   "source": "def compute_value_loss(\n    values: torch.Tensor,\n    returns: torch.Tensor,\n    old_values: torch.Tensor = None,\n    clip_value: bool = True,\n    clip_ratio: float = 0.2\n) -> torch.Tensor:\n    \"\"\"\n    Compute value function loss (optionally clipped).\n    \n    Yes, we can clip the value function too! Same idea as policy clipping:\n    don't let the value function change too dramatically in one update.\n    \n    Args:\n        values: Current value predictions V_Œ∏(s)\n        returns: Actual returns R (ground truth)\n        old_values: Previous value predictions (for clipping)\n        clip_value: Whether to clip value updates\n        clip_ratio: How much change to allow (default 0.2)\n    \n    Returns:\n        Value function loss (MSE, possibly clipped)\n    \"\"\"\n    if clip_value and old_values is not None:\n        # Clipped value loss: similar idea to clipped policy loss\n        # Don't let the value function change by more than ¬±clip_ratio\n        value_clipped = old_values + torch.clamp(\n            values - old_values, -clip_ratio, clip_ratio\n        )\n        \n        # Compute both clipped and unclipped errors\n        loss_unclipped = (values - returns) ** 2\n        loss_clipped = (value_clipped - returns) ** 2\n        \n        # Take maximum (pessimistic again!)\n        loss = torch.max(loss_unclipped, loss_clipped).mean()\n    else:\n        # Standard MSE (mean squared error)\n        loss = F.mse_loss(values, returns)\n    \n    return loss\n\n# Example with made-up data\nbatch_size = 16\nvalues = torch.randn(batch_size)  # Current predictions\nreturns = torch.randn(batch_size)  # Actual returns\nold_values = values + torch.randn(batch_size) * 0.1  # Slightly perturbed\n\n# Compute both clipped and unclipped losses\nclipped_loss = compute_value_loss(values, returns, old_values, clip_value=True)\nunclipped_loss = compute_value_loss(values, returns, clip_value=False)\n\nprint(f\"Value Loss (clipped):   {clipped_loss.item():.4f}\")\nprint(f\"Value Loss (unclipped): {unclipped_loss.item():.4f}\")\nprint()\nprint(\"Clipping the value function is optional (some implementations skip it),\")\nprint(\"but it can help with training stability.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## Entropy Bonus: Encouraging Exploration\n\nHere's a problem: if you only optimize for reward, your model becomes *too* confident. It collapses to always picking its current best answer, never exploring alternatives.\n\nThis is bad! Maybe there's an even better answer it hasn't tried yet.\n\nSolution: **entropy bonus**. Add a term that rewards the model for maintaining uncertainty.\n\n$$H(\\pi_\\theta) = -\\mathbb{E}_{a \\sim \\pi_\\theta} [\\log \\pi_\\theta(a | s)]$$\n\nWhat's entropy?\n\nIn information theory, **entropy** measures uncertainty or randomness. \n\n- High entropy = Many possible outputs, fairly distributed (lots of exploration)\n- Low entropy = One dominant output (collapsed, deterministic policy)\n\nImagine a language model generating the next word:\n- High entropy: \"The cat sat on the {mat, chair, floor, table, ...}\" ‚Äî many options considered\n- Low entropy: \"The cat sat on the mat mat mat mat...\" ‚Äî stuck in a rut\n\nWe *add* entropy to the objective (or subtract from the loss), encouraging the model to keep exploring. This prevents premature convergence to a mediocre policy.\n\n(It's like telling a student \"don't just memorize the first answer you find ‚Äî keep thinking!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:55.156382Z",
     "iopub.status.busy": "2025-12-06T23:29:55.156309Z",
     "iopub.status.idle": "2025-12-06T23:29:55.159588Z",
     "shell.execute_reply": "2025-12-06T23:29:55.159201Z"
    }
   },
   "outputs": [],
   "source": "def compute_entropy(\n    logits: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    Compute entropy of policy distribution.\n    \n    Entropy = -Œ£ p(x) log p(x) across all possible actions x\n    \n    High entropy = More exploration (good early in training)\n    Low entropy = More deterministic (good later in training)\n    \n    Args:\n        logits: Raw model outputs (before softmax)\n    \n    Returns:\n        Mean entropy across batch\n    \"\"\"\n    # Convert logits to probabilities\n    probs = F.softmax(logits, dim=-1)\n    \n    # Convert logits to log probabilities (numerically stable)\n    log_probs = F.log_softmax(logits, dim=-1)\n    \n    # Entropy = -Œ£ p(x) log p(x)\n    entropy = -(probs * log_probs).sum(dim=-1)\n    \n    return entropy.mean()\n\n# Example with a vocabulary of 1000 tokens\nbatch_size = 16\nvocab_size = 1000\n\n# Simulate model logits (raw outputs before softmax)\nlogits = torch.randn(batch_size, vocab_size)\n\nentropy = compute_entropy(logits)\nprint(f\"Entropy: {entropy.item():.4f}\")\nprint()\n\n# Let's compare different entropy levels\nprint(\"Understanding entropy values:\")\nprint()\n\n# Very peaked distribution (low entropy)\npeaked_logits = torch.randn(batch_size, vocab_size)\npeaked_logits[:, 0] = 10.0  # Make first token very likely\npeaked_entropy = compute_entropy(peaked_logits)\nprint(f\"  Low entropy (peaked):  {peaked_entropy.item():.4f} ‚Äî model is very confident\")\n\n# Uniform-ish distribution (high entropy)\nuniform_logits = torch.randn(batch_size, vocab_size) * 0.1  # Small random values\nuniform_entropy = compute_entropy(uniform_logits)\nprint(f\"  High entropy (uniform): {uniform_entropy.item():.4f} ‚Äî model is uncertain\")\n\nprint()\nprint(f\"Maximum possible entropy for {vocab_size} tokens: {np.log(vocab_size):.4f}\")\nprint(\"(achieved when all tokens are equally likely)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Putting It All Together: The Complete PPO Loss\n\nOkay, we've got three components. Time to combine them into one mega-loss:\n\n$$L_{\\text{total}} = L^{\\text{CLIP}}(\\theta) + c_1 L^V(\\theta) - c_2 H(\\pi_\\theta)$$\n\nWhere:\n- $L^{\\text{CLIP}}(\\theta)$ = Clipped policy loss (the main event)\n- $c_1 L^V(\\theta)$ = Value function loss, weighted by $c_1$ (typically 0.5)\n- $c_2 H(\\pi_\\theta)$ = Entropy bonus, weighted by $c_2$ (typically 0.01)\n\nNotice we *subtract* the entropy (because we're minimizing loss, but want to *maximize* entropy).\n\nThe weights $c_1$ and $c_2$ balance the three objectives:\n- Too much value function weight ‚Üí model cares more about prediction than policy improvement\n- Too much entropy weight ‚Üí model stays random forever, never commits to good actions\n- Too little entropy weight ‚Üí model collapses to deterministic behavior too quickly\n\nThese are hyperparameters. The defaults (0.5 and 0.01) work well in practice, but you can tune them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:55.160391Z",
     "iopub.status.busy": "2025-12-06T23:29:55.160302Z",
     "iopub.status.idle": "2025-12-06T23:29:55.162684Z",
     "shell.execute_reply": "2025-12-06T23:29:55.162408Z"
    }
   },
   "outputs": [],
   "source": "def compute_ppo_total_loss(\n    policy_logprobs: torch.Tensor,\n    old_logprobs: torch.Tensor,\n    advantages: torch.Tensor,\n    values: torch.Tensor,\n    returns: torch.Tensor,\n    logits: torch.Tensor,\n    clip_ratio: float = 0.2,\n    vf_coef: float = 0.5,\n    entropy_coef: float = 0.01\n) -> dict:\n    \"\"\"\n    Compute total PPO loss combining all three components.\n    \n    This is the loss you actually optimize during RLHF training.\n    \n    Total = Policy Loss + vf_coef * Value Loss - entropy_coef * Entropy\n    \n    Args:\n        policy_logprobs: Log probs under current policy\n        old_logprobs: Log probs under old policy (from rollout)\n        advantages: Advantage estimates\n        values: Value function predictions\n        returns: Actual returns (ground truth for value function)\n        logits: Raw model outputs (for computing entropy)\n        clip_ratio: PPO clipping threshold (default 0.2)\n        vf_coef: Value function loss weight (default 0.5)\n        entropy_coef: Entropy bonus weight (default 0.01)\n    \n    Returns:\n        Dictionary with total loss and individual components\n    \"\"\"\n    # 1. Clipped policy loss (the main objective)\n    policy_loss = compute_ppo_loss(policy_logprobs, old_logprobs, advantages, clip_ratio)\n    \n    # 2. Value function loss (helps compute better advantages)\n    value_loss = compute_value_loss(values, returns, clip_value=False)\n    \n    # 3. Entropy bonus (encourages exploration)\n    entropy = compute_entropy(logits)\n    \n    # Combine: minimize policy loss and value loss, maximize entropy\n    total_loss = policy_loss + vf_coef * value_loss - entropy_coef * entropy\n    \n    return {\n        'total_loss': total_loss,\n        'policy_loss': policy_loss,\n        'value_loss': value_loss,\n        'entropy': entropy\n    }\n\n# Let's compute a full PPO loss\nbatch_size = 16\nvocab_size = 1000\n\n# Simulate all the inputs we'd get during training\npolicy_logprobs = torch.randn(batch_size) - 2\nold_logprobs = torch.randn(batch_size) - 2\nadvantages = torch.randn(batch_size)\nvalues = torch.randn(batch_size)\nreturns = torch.randn(batch_size)\nlogits = torch.randn(batch_size, vocab_size)\n\n# Compute total loss\nlosses = compute_ppo_total_loss(\n    policy_logprobs, old_logprobs, advantages,\n    values, returns, logits\n)\n\nprint(\"PPO Loss Breakdown:\")\nprint(\"=\" * 50)\nprint(f\"Policy Loss:      {losses['policy_loss'].item():>8.4f}\")\nprint(f\"Value Loss:       {losses['value_loss'].item():>8.4f}  (√ó 0.5 coef)\")\nprint(f\"Entropy:          {losses['entropy'].item():>8.4f}  (√ó 0.01 coef)\")\nprint(\"-\" * 50)\nprint(f\"Total Loss:       {losses['total_loss'].item():>8.4f}\")\nprint()\nprint(\"This total loss is what we backpropagate through the model.\")\nprint(\"Minimize this, and your policy improves (safely!).\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "## PPO Hyperparameters: The Knobs You Can Turn\n\nHere are the key hyperparameters in PPO, and what they do:\n\n| Parameter | Default | What it controls | Effect if too high | Effect if too low |\n|-----------|---------|------------------|-------------------|------------------|\n| `clip_ratio` (Œµ) | 0.2 | How much policy can change per update | More aggressive updates, risk of instability | Too conservative, slow learning |\n| `vf_coef` (c‚ÇÅ) | 0.5 | Weight on value function loss | Model focuses on prediction over policy | Value function trains slowly, poor advantages |\n| `entropy_coef` (c‚ÇÇ) | 0.01 | Weight on entropy bonus | Model stays random, never converges | Model becomes deterministic too fast, gets stuck |\n| `ppo_epochs` | 4 | How many times to reuse each batch | Better sample efficiency, risk overfitting | Less sample efficiency, more data needed |\n| `learning_rate` | 1e-5 | Step size for optimizer | Training instability, divergence | Slow convergence |\n\n**General wisdom:**\n- Start with the defaults. They work surprisingly well.\n- If training is unstable, *lower* the clip ratio and learning rate.\n- If the model gets stuck in repetitive behavior, *raise* the entropy coefficient.\n- If you have limited compute, *raise* ppo_epochs to squeeze more from each batch.\n\n(But really, the defaults are good. The PPO paper figured this out through a lot of trial and error so you don't have to.)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "## What We've Learned\n\nPPO is the workhorse of modern RLHF. Here's the story in one sentence:\n\n**Use gradients to improve your policy, but clip them aggressively to prevent catastrophic changes.**\n\nThe key insights:\n1. **Clipping** prevents policy collapse (the \"proximal\" part)\n2. **Advantage estimates** tell us which actions are better than average\n3. **Value function** predicts returns and reduces variance\n4. **Entropy bonus** keeps the model exploring instead of getting stuck\n\nThis is what makes ChatGPT work. This is what makes Claude work. This is what makes every aligned language model work.\n\nPretty elegant, right?\n\n## Next Steps\n\nPPO handles the optimization, but there's one more crucial piece: the **KL penalty**. We need to prevent the model from drifting too far from its original behavior. That's up next."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}