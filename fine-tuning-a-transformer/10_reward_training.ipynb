{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Training Reward Models\n\n**Teaching a neural network to judge quality like a human**\n\nAlright, we've got our preference data. We've got our model architecture. \n\nNow comes the fun part: training.\n\nHow do you actually teach a neural network to predict which response humans will prefer? Turns out, there's some beautiful math behind it (and once you understand it, you'll wonder how it ever seemed complicated)."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The Ranking Loss (And Why It Works)\n\nHere's the thing about training reward models: we're not trying to predict a specific number. We're trying to predict *rankings*.\n\nThink about it. When humans judge responses, they don't say \"this response deserves exactly 7.3 points out of 10.\" They say \"this one is better than that one.\" Rankings are natural. Absolute scores? Not so much.\n\nSo our loss function needs to reflect that. Enter the **Bradley-Terry ranking loss**:\n\n$$\\mathcal{L}_{\\text{RM}} = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right]$$\n\nOkay, I know. Math notation can be intimidating. Let's break this down piece by piece:\n\n- $x$ = the prompt (the question or instruction)\n- $y_w$ = the **w**inner response (the one humans preferred)\n- $y_l$ = the **l**oser response (the one humans rejected)\n- $r_\\theta(x, y)$ = our reward model's score for response $y$ given prompt $x$\n- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ = the sigmoid function (squashes any number to between 0 and 1)\n\n**What this formula actually means:**\n\nWe want the probability that $y_w$ ranks higher than $y_l$ to be as close to 1 as possible. That probability is $\\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$ — the sigmoid of the difference in rewards.\n\nIf the winner's reward is much higher than the loser's, the difference is large and positive, sigmoid returns something close to 1, and our loss is low. Good!\n\nIf the loser's reward is somehow higher (model got it backwards), the difference is negative, sigmoid returns something close to 0, and our loss shoots up. Bad! The model needs to learn.\n\n**Why sigmoid?** It converts reward differences into probabilities. A difference of 0 → 50% chance. Large positive difference → near 100% chance. Large negative difference → near 0% chance.\n\n**Why negative log?** Because we're minimizing loss. We want to maximize the probability, which means minimizing the negative log of the probability. (Classic machine learning trick.)\n\nThe beauty is that this loss function doesn't care about the absolute values of the rewards. Only their relative ordering. Perfect for our task."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:41:59.057422Z",
     "iopub.status.busy": "2025-12-07T18:41:59.057349Z",
     "iopub.status.idle": "2025-12-07T18:41:59.784623Z",
     "shell.execute_reply": "2025-12-07T18:41:59.784300Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport torch.nn.functional as F\n\ndef compute_ranking_loss(\n    chosen_rewards: torch.Tensor,\n    rejected_rewards: torch.Tensor,\n    margin: float = 0.0\n) -> torch.Tensor:\n    \"\"\"\n    Compute ranking loss for reward model training.\n    \n    This is the Bradley-Terry ranking loss: -log(sigmoid(r_chosen - r_rejected))\n    \n    The goal: make chosen_rewards > rejected_rewards by a comfortable margin.\n    \n    Args:\n        chosen_rewards: Rewards for chosen responses, shape (batch_size,)\n        rejected_rewards: Rewards for rejected responses, shape (batch_size,)\n        margin: Optional margin to enforce minimum difference (usually 0)\n    \n    Returns:\n        Ranking loss (scalar)\n    \"\"\"\n    # Compute the difference: chosen should be higher than rejected\n    # If margin > 0, we require chosen to be higher by at least margin\n    logits = chosen_rewards - rejected_rewards - margin\n    \n    # Apply log-sigmoid for numerical stability\n    # -log(sigmoid(x)) = log(1 + exp(-x)) = softplus(-x)\n    # (PyTorch's softplus is more numerically stable than manually computing log(sigmoid))\n    loss = F.softplus(-logits)\n    \n    return loss.mean()\n\n# Let's see it in action!\nprint(\"Example: Computing ranking loss\")\nprint(\"=\" * 50)\n\n# Create a batch of 4 examples\nbatch_size = 4\nchosen_rewards = torch.tensor([2.0, 1.5, 3.0, 0.5])\nrejected_rewards = torch.tensor([1.0, 1.0, 2.0, 1.0])\n\nprint(\"\\nChosen rewards:  \", chosen_rewards.tolist())\nprint(\"Rejected rewards:\", rejected_rewards.tolist())\nprint(\"\\nDifferences:     \", (chosen_rewards - rejected_rewards).tolist())\n\nloss = compute_ranking_loss(chosen_rewards, rejected_rewards)\nprint(f\"\\nRanking loss: {loss.item():.4f}\")\n\n# Accuracy: how often is chosen > rejected?\naccuracy = (chosen_rewards > rejected_rewards).float().mean()\nprint(f\"Accuracy: {accuracy.item():.2%}\")\n\nprint(\"\\nNotice:\")\nprint(\"  - Examples 1-3: chosen > rejected → contributes low loss\")\nprint(\"  - Example 4: chosen < rejected → contributes high loss (model wrong!)\")\nprint(\"  - Overall accuracy is 75% (3 out of 4 correct)\")\n\n# Let's look at the individual losses\nindividual_losses = F.softplus(-(chosen_rewards - rejected_rewards))\nprint(\"\\nIndividual losses per example:\")\nfor i, (loss_val, diff) in enumerate(zip(individual_losses, chosen_rewards - rejected_rewards)):\n    print(f\"  Example {i+1}: diff={diff:+.1f} → loss={loss_val:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## Training Metrics: What to Watch\n\nWhen training a reward model, you need to track a few key metrics. Think of them as your dashboard while driving — they tell you if you're on the right track or about to drive off a cliff.\n\n| Metric | What It Measures | What You Want to See |\n|--------|------------------|---------------------|\n| **Loss** | How wrong the model's rankings are | Decreasing over time |\n| **Accuracy** | % of pairs ranked correctly | > 70% (ideally 80%+) |\n| **Mean Margin** | Average difference between chosen and rejected | Positive and increasing |\n\n**Loss** is your primary signal. Lower is better. If it's not decreasing, something's wrong.\n\n**Accuracy** is more interpretable. If your model ranks chosen responses higher than rejected responses 80% of the time, that's pretty good! (Humans don't even agree 100% of the time.)\n\n**Mean Margin** tells you how *confident* the model is. A margin of +0.1 means the model barely prefers the chosen response. A margin of +3.0 means it *really* prefers it. You want this to grow during training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:41:59.785615Z",
     "iopub.status.busy": "2025-12-07T18:41:59.785501Z",
     "iopub.status.idle": "2025-12-07T18:41:59.788278Z",
     "shell.execute_reply": "2025-12-07T18:41:59.787971Z"
    }
   },
   "outputs": [],
   "source": "def compute_ranking_loss_with_metrics(\n    chosen_rewards: torch.Tensor,\n    rejected_rewards: torch.Tensor,\n    margin: float = 0.0\n) -> dict:\n    \"\"\"\n    Compute ranking loss AND all the useful metrics you want to track.\n    \n    This is what you'd call during training to get a full picture of how\n    your model is performing.\n    \"\"\"\n    loss = compute_ranking_loss(chosen_rewards, rejected_rewards, margin)\n    \n    # Accuracy: how often does model rank chosen higher?\n    accuracy = (chosen_rewards > rejected_rewards).float().mean()\n    \n    # Mean rewards (useful for debugging)\n    mean_chosen = chosen_rewards.mean()\n    mean_rejected = rejected_rewards.mean()\n    \n    # Mean margin: how much higher is chosen vs rejected on average?\n    mean_margin = (chosen_rewards - rejected_rewards).mean()\n    \n    return {\n        \"loss\": loss,\n        \"accuracy\": accuracy,\n        \"mean_chosen_reward\": mean_chosen,\n        \"mean_rejected_reward\": mean_rejected,\n        \"mean_margin\": mean_margin,\n    }\n\n# Let's use the same example from before\nmetrics = compute_ranking_loss_with_metrics(chosen_rewards, rejected_rewards)\n\nprint(\"Training metrics for our example batch:\")\nprint(\"=\" * 50)\nfor k, v in metrics.items():\n    if k == \"loss\":\n        print(f\"  {k}: {v.item():.4f} (lower is better)\")\n    elif k == \"accuracy\":\n        print(f\"  {k}: {v.item():.2%} (higher is better)\")\n    elif k == \"mean_margin\":\n        print(f\"  {k}: {v.item():.4f} (positive and growing is good)\")\n    else:\n        print(f\"  {k}: {v.item():.4f}\")\n\nprint(\"\\nInterpretation:\")\nprint(\"  - Accuracy of 75% is decent (3 out of 4 correct)\")\nprint(\"  - Mean margin of +0.5 means chosen responses score 0.5 higher on average\")\nprint(\"  - During training, you'd watch these metrics improve over time\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## Putting It All Together: The Training Loop\n\nOkay, we've got our loss function. We've got our metrics. Now let's talk about the actual training loop.\n\nAt a high level, training a reward model looks like this:\n\n1. **Load a batch** of (prompt, chosen, rejected) triples from your dataset\n2. **Forward pass** the chosen responses through the model → get chosen rewards\n3. **Forward pass** the rejected responses through the model → get rejected rewards\n4. **Compute the ranking loss** (and metrics for monitoring)\n5. **Backpropagate** the loss through the model\n6. **Update the weights** with your optimizer\n7. **Repeat** until the model learns to rank preferences like a human\n\nIt's the same training loop you've seen before (if you've trained any neural network). The only special part is step 4 — using ranking loss instead of, say, cross-entropy.\n\nLet's build it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:41:59.788953Z",
     "iopub.status.busy": "2025-12-07T18:41:59.788880Z",
     "iopub.status.idle": "2025-12-07T18:42:02.231412Z",
     "shell.execute_reply": "2025-12-07T18:42:02.231025Z"
    }
   },
   "outputs": [],
   "source": "import torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\n\nclass RewardModel(nn.Module):\n    \"\"\"\n    A reward model for predicting human preferences.\n    \n    Architecture is simple:\n    - Take a base language model (GPT-2, Llama, whatever)\n    - Add a \"value head\" on top that projects to a single scalar\n    - That scalar is the reward\n    \n    The base model processes the text and extracts meaning.\n    The value head says \"based on this meaning, how good is this response?\"\n    \"\"\"\n    \n    def __init__(self, base_model, hidden_size):\n        super().__init__()\n        self.base_model = base_model\n        \n        # The value head: dropout for regularization, then linear layer to scalar\n        self.value_head = nn.Sequential(\n            nn.Dropout(0.1),  # 10% dropout to prevent overfitting\n            nn.Linear(hidden_size, 1)  # hidden_size → 1 scalar reward\n        )\n    \n    def get_rewards(self, input_ids, attention_mask):\n        \"\"\"\n        Compute reward scores for input sequences.\n        \n        Args:\n            input_ids: Token IDs, shape (batch_size, seq_len)\n            attention_mask: Attention mask, shape (batch_size, seq_len)\n        \n        Returns:\n            Reward scores, shape (batch_size,)\n        \"\"\"\n        # Run the base model to get hidden states\n        outputs = self.base_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True\n        )\n        hidden_states = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n        \n        # Get the LAST non-padding token's hidden state for each sequence\n        # (This is where the model has seen the entire response)\n        seq_lengths = attention_mask.sum(dim=1) - 1  # -1 because of 0-indexing\n        batch_size = hidden_states.shape[0]\n        last_hidden = hidden_states[\n            torch.arange(batch_size, device=hidden_states.device),\n            seq_lengths.long()\n        ]\n        \n        # Project to scalar and squeeze out the last dimension\n        return self.value_head(last_hidden).squeeze(-1)\n\n# Let's create a reward model and see it in action\nprint(\"Creating a reward model from GPT-2...\")\nprint(\"=\" * 50)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load base model and tokenizer\nbase_model = AutoModel.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default\n\n# Create reward model\nreward_model = RewardModel(base_model, hidden_size=768)  # GPT-2 has 768 hidden dims\nreward_model.to(device)\n\nprint(f\"\\nReward model created!\")\nprint(f\"  Device: {device}\")\nprint(f\"  Base model: GPT-2 (124M parameters)\")\nprint(f\"  Hidden size: 768\")\nprint(f\"  Value head: 768 → 1 (just 769 parameters!)\")\n\n# Test it with a forward pass\ntest_texts = [\n    \"This is a helpful and informative response that answers the question clearly.\",\n    \"I don't know, just Google it yourself.\"\n]\n\nprint(f\"\\nTesting with example responses...\")\ninputs = tokenizer(test_texts, return_tensors=\"pt\", padding=True, truncation=True)\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\nwith torch.no_grad():\n    rewards = reward_model.get_rewards(inputs['input_ids'], inputs['attention_mask'])\n\nprint(f\"\\nTest rewards (before training, so basically random):\")\nfor text, reward in zip(test_texts, rewards):\n    print(f\"  [{reward.item():+.4f}] \\\"{text[:60]}...\\\"\")\n    \nprint(\"\\nNote: Rewards are random right now because the model is untrained.\")\nprint(\"After training, the first response should get a higher reward!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:42:02.232437Z",
     "iopub.status.busy": "2025-12-07T18:42:02.232269Z",
     "iopub.status.idle": "2025-12-07T18:42:03.022449Z",
     "shell.execute_reply": "2025-12-07T18:42:03.022062Z"
    }
   },
   "outputs": [],
   "source": "def train_reward_model(model, train_loader, eval_loader, config, device):\n    \"\"\"\n    Complete reward model training loop.\n    \n    For each (prompt, chosen, rejected) triple in the dataset:\n    - Compute rewards for both chosen and rejected\n    - Calculate ranking loss: we want chosen > rejected\n    - Backprop and update weights\n    \n    Args:\n        model: RewardModel instance\n        train_loader: DataLoader for training data\n        eval_loader: DataLoader for evaluation (can be None)\n        config: Training configuration dict\n        device: torch device\n    \n    Returns:\n        Trained model\n    \"\"\"\n    \n    # AdamW optimizer with weight decay (helps prevent overfitting)\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config['learning_rate'],\n        weight_decay=0.01\n    )\n    \n    # Learning rate scheduler: linear warmup then decay\n    total_steps = len(train_loader) * config['num_epochs']\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=config['warmup_steps'],\n        num_training_steps=total_steps\n    )\n    \n    model.train()\n    best_eval_accuracy = 0.0\n    \n    for epoch in range(config['num_epochs']):\n        epoch_metrics = {'loss': 0, 'accuracy': 0}\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        \n        for batch in progress_bar:\n            # Move batch to device\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Forward pass for chosen responses\n            chosen_rewards = model.get_rewards(\n                batch['chosen_input_ids'],\n                batch['chosen_attention_mask']\n            )\n            \n            # Forward pass for rejected responses\n            rejected_rewards = model.get_rewards(\n                batch['rejected_input_ids'],\n                batch['rejected_attention_mask']\n            )\n            \n            # Compute loss and metrics\n            metrics = compute_ranking_loss_with_metrics(\n                chosen_rewards, rejected_rewards\n            )\n            loss = metrics['loss']\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping (prevents exploding gradients)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            optimizer.step()\n            scheduler.step()\n            \n            # Update metrics\n            epoch_metrics['loss'] += loss.item()\n            epoch_metrics['accuracy'] += metrics['accuracy'].item()\n            \n            progress_bar.set_postfix({\n                'loss': f\"{loss.item():.4f}\",\n                'acc': f\"{metrics['accuracy'].item():.2%}\"\n            })\n        \n        # End of epoch\n        avg_loss = epoch_metrics['loss'] / len(train_loader)\n        avg_acc = epoch_metrics['accuracy'] / len(train_loader)\n        print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.2%}\")\n    \n    return model\n\n# Let's demonstrate what happens during a training step\nprint(\"Demonstrating a single training step\")\nprint(\"=\" * 50)\n\n# Put the model in training mode\nreward_model.train()\noptimizer = torch.optim.AdamW(reward_model.parameters(), lr=1e-5)\n\n# Create a synthetic training example\n# Chosen: a helpful, informative response\n# Rejected: a dismissive, unhelpful response\nbatch_chosen = tokenizer(\n    [\"The answer is 42. This is the result from Douglas Adams' novel 'The Hitchhiker's Guide to the Galaxy', where a supercomputer calculated it as the answer to the ultimate question of life, the universe, and everything.\"],\n    return_tensors=\"pt\", padding=True, truncation=True, max_length=64\n)\nbatch_rejected = tokenizer(\n    [\"I don't know. Just look it up yourself.\"],\n    return_tensors=\"pt\", padding=True, truncation=True, max_length=64\n)\n\nbatch_chosen = {k: v.to(device) for k, v in batch_chosen.items()}\nbatch_rejected = {k: v.to(device) for k, v in batch_rejected.items()}\n\n# BEFORE training step\nwith torch.no_grad():\n    chosen_r_before = reward_model.get_rewards(batch_chosen['input_ids'], batch_chosen['attention_mask'])\n    rejected_r_before = reward_model.get_rewards(batch_rejected['input_ids'], batch_rejected['attention_mask'])\n\nprint(\"\\nBefore training step:\")\nprint(f\"  Chosen reward:   {chosen_r_before.item():+.4f}\")\nprint(f\"  Rejected reward: {rejected_r_before.item():+.4f}\")\nprint(f\"  Margin:          {(chosen_r_before - rejected_r_before).item():+.4f}\")\n\n# Compute loss and backprop\nchosen_r = reward_model.get_rewards(batch_chosen['input_ids'], batch_chosen['attention_mask'])\nrejected_r = reward_model.get_rewards(batch_rejected['input_ids'], batch_rejected['attention_mask'])\nloss = compute_ranking_loss(chosen_r, rejected_r)\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\n# AFTER training step\nwith torch.no_grad():\n    chosen_r_after = reward_model.get_rewards(batch_chosen['input_ids'], batch_chosen['attention_mask'])\n    rejected_r_after = reward_model.get_rewards(batch_rejected['input_ids'], batch_rejected['attention_mask'])\n\nprint(f\"\\nAfter training step:\")\nprint(f\"  Chosen reward:   {chosen_r_after.item():+.4f}\")\nprint(f\"  Rejected reward: {rejected_r_after.item():+.4f}\")\nprint(f\"  Margin:          {(chosen_r_after - rejected_r_after).item():+.4f}\")\nprint(f\"  Loss:            {loss.item():.4f}\")\n\nprint(\"\\nSee what happened?\")\nprint(\"  - Chosen reward increased (model likes the good response more)\")\nprint(\"  - Rejected reward decreased (model likes the bad response less)\")\nprint(\"  - Margin increased (model is more confident in its ranking)\")\nprint(\"\\nThat's learning in action!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Hyperparameters: The Goldilocks Problem\n\nTraining reward models requires careful tuning. Too aggressive and you overfit. Too conservative and you don't learn anything. Here's what works in practice:\n\n| Parameter | Typical Value | Why This Matters |\n|-----------|---------------|------------------|\n| **Learning rate** | 1e-5 | Much lower than SFT! Reward training is delicate. |\n| **Batch size** | 4 | Small batches (each sample has 2 sequences). Memory is tight. |\n| **Epochs** | 1 | Usually just ONE pass through the data. Overfitting is a real danger. |\n| **Gradient accumulation** | 4 | Effective batch size = 16. Poor man's larger batch. |\n| **Warmup steps** | 100 | Gradually increase learning rate at the start. Prevents early chaos. |\n| **Gradient clipping** | 1.0 | Cap gradients to prevent explosions. Safety first. |\n\n**Why such a low learning rate?** Because we're fine-tuning a pre-trained model. The base model already knows language. We just need to nudge it slightly to predict preferences. Big updates would destroy that knowledge.\n\n**Why only 1 epoch?** Preference data is often small (thousands of examples, not millions). With a small dataset, you'll overfit if you train too long. The model will memorize the training examples instead of learning general principles.\n\n**Why gradient accumulation?** Memory constraints. Each training example has TWO full sequences (chosen and rejected). If you try to fit 16 pairs in memory at once, you'll run out of VRAM. So we accumulate gradients over 4 batches of 4, then update."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:42:03.023372Z",
     "iopub.status.busy": "2025-12-07T18:42:03.023288Z",
     "iopub.status.idle": "2025-12-07T18:42:03.025110Z",
     "shell.execute_reply": "2025-12-07T18:42:03.024836Z"
    }
   },
   "outputs": [],
   "source": "# Here's a typical training configuration\nconfig = {\n    'learning_rate': 1e-5,\n    'batch_size': 4,\n    'num_epochs': 1,\n    'warmup_steps': 100,\n    'gradient_accumulation_steps': 4,\n    'max_grad_norm': 1.0,\n}\n\nprint(\"Reward Model Training Configuration\")\nprint(\"=\" * 50)\nfor k, v in config.items():\n    print(f\"  {k:30s} = {v}\")\n\nprint(\"\\nEffective batch size:\", config['batch_size'] * config['gradient_accumulation_steps'])\nprint(\"\\nThis configuration is conservative but reliable.\")\nprint(\"It works for most reward modeling tasks without much tuning.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## Common Training Issues (And How to Fix Them)\n\nTraining reward models can be tricky. Here are the issues you'll run into (and I mean *will*, not *might*), and what to do about them:\n\n### 1. Low Accuracy (< 60%)\n\n**Symptoms:** Model barely better than random guessing. Training accuracy stuck at 50-60%.\n\n**What's happening:** The model isn't learning the preferences. Could be bad data, too high learning rate, or the task is genuinely hard.\n\n**Fixes:**\n- Lower the learning rate (try 5e-6 instead of 1e-5)\n- Check your data quality — are the preferences clear? Would *you* agree with them?\n- Train for a bit longer (but watch for overfitting)\n- Try a larger base model (more capacity to learn subtle patterns)\n\n### 2. Overfitting\n\n**Symptoms:** Training accuracy looks great (90%+) but evaluation accuracy is much lower (60-70%). Classic overfitting.\n\n**What's happening:** Model is memorizing the training examples instead of learning general principles.\n\n**Fixes:**\n- Use only 1 epoch (or even less — 50% of one epoch)\n- Increase dropout in the value head (try 0.2 or 0.3 instead of 0.1)\n- Get more training data (if possible)\n- Freeze the base model and only train the value head\n- Add more regularization (higher weight decay)\n\n### 3. Training Instability\n\n**Symptoms:** Loss goes down, then suddenly spikes. NaN values. Model crashes.\n\n**What's happening:** Gradients are exploding. The model is making updates that are too large.\n\n**Fixes:**\n- Lower the learning rate (always the first thing to try)\n- Increase gradient clipping (try 0.5 instead of 1.0)\n- Use more warmup steps (try 200-500)\n- Check for bad data (extremely long sequences, weird characters, etc.)\n\nThe most common issue? Overfitting. It's the reward modeling nemesis. Be conservative with training time."
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## What We've Learned\n\nLet's recap. Training a reward model means:\n\n1. **Using ranking loss** to teach the model that chosen > rejected\n2. **Watching key metrics** (accuracy, margin) to ensure learning is happening\n3. **Being conservative** with hyperparameters to avoid overfitting\n4. **Troubleshooting** when things inevitably go wrong\n\nThe math might look fancy, but it's actually quite elegant. We're just teaching a neural network to make comparisons. \"This response is better than that one.\" Over and over, thousands of times, until the model internalizes what humans mean by \"better.\"\n\nNext up: evaluating reward models. Because training is only half the battle — you need to know if your reward model is actually any good."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}