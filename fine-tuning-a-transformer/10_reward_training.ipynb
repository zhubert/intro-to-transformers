{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Training Reward Models\n",
    "\n",
    "**Loss functions, optimization, and best practices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Ranking Loss\n",
    "\n",
    "The core training objective is the **Bradley-Terry ranking loss**:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{RM}} = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right]$$\n",
    "\n",
    "where:\n",
    "- $x$ = prompt\n",
    "- $y_w$ = chosen (winner) response\n",
    "- $y_l$ = rejected (loser) response\n",
    "- $r_\\theta(x, y)$ = reward model score\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ = sigmoid function\n",
    "\n",
    "**Goal:** Maximize $P(y_w \\succ y_l | x) = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:43.475804Z",
     "iopub.status.busy": "2025-12-06T23:29:43.475698Z",
     "iopub.status.idle": "2025-12-06T23:29:44.192818Z",
     "shell.execute_reply": "2025-12-06T23:29:44.192479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking loss: 0.5187\n",
      "Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_ranking_loss(\n",
    "    chosen_rewards: torch.Tensor,\n",
    "    rejected_rewards: torch.Tensor,\n",
    "    margin: float = 0.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute ranking loss for reward model training.\n",
    "    \n",
    "    Loss = -log(sigmoid(chosen_reward - rejected_reward - margin))\n",
    "    \n",
    "    Args:\n",
    "        chosen_rewards: Rewards for chosen responses, shape (batch_size,)\n",
    "        rejected_rewards: Rewards for rejected responses, shape (batch_size,)\n",
    "        margin: Optional margin to enforce minimum difference\n",
    "    \n",
    "    Returns:\n",
    "        Ranking loss tensor\n",
    "    \"\"\"\n",
    "    # Compute difference: chosen should be higher than rejected\n",
    "    logits = chosen_rewards - rejected_rewards - margin\n",
    "    \n",
    "    # Apply log-sigmoid for numerical stability\n",
    "    # -log(sigmoid(x)) = log(1 + exp(-x)) = softplus(-x)\n",
    "    loss = F.softplus(-logits)\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "# Example\n",
    "batch_size = 4\n",
    "chosen_rewards = torch.tensor([2.0, 1.5, 3.0, 0.5])\n",
    "rejected_rewards = torch.tensor([1.0, 1.0, 2.0, 1.0])\n",
    "\n",
    "loss = compute_ranking_loss(chosen_rewards, rejected_rewards)\n",
    "print(f\"Ranking loss: {loss.item():.4f}\")\n",
    "\n",
    "# Accuracy: how often is chosen > rejected?\n",
    "accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "print(f\"Accuracy: {accuracy.item():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Training Metrics\n",
    "\n",
    "| Metric | What It Measures | Target |\n",
    "|--------|------------------|--------|\n",
    "| **Loss** | Preference prediction quality | Decreasing |\n",
    "| **Accuracy** | % of pairs ranked correctly | > 70% |\n",
    "| **Mean Margin** | Average reward difference | Positive, increasing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:44.193765Z",
     "iopub.status.busy": "2025-12-06T23:29:44.193666Z",
     "iopub.status.idle": "2025-12-06T23:29:44.196188Z",
     "shell.execute_reply": "2025-12-06T23:29:44.195921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics:\n",
      "  loss: 0.5187\n",
      "  accuracy: 0.7500\n",
      "  mean_chosen_reward: 1.7500\n",
      "  mean_rejected_reward: 1.2500\n",
      "  mean_margin: 0.5000\n"
     ]
    }
   ],
   "source": [
    "def compute_ranking_loss_with_metrics(\n",
    "    chosen_rewards: torch.Tensor,\n",
    "    rejected_rewards: torch.Tensor,\n",
    "    margin: float = 0.0\n",
    ") -> dict:\n",
    "    \"\"\"Compute ranking loss with additional metrics.\"\"\"\n",
    "    loss = compute_ranking_loss(chosen_rewards, rejected_rewards, margin)\n",
    "    \n",
    "    # Accuracy: how often does model rank chosen higher?\n",
    "    accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "    \n",
    "    # Mean rewards\n",
    "    mean_chosen = chosen_rewards.mean()\n",
    "    mean_rejected = rejected_rewards.mean()\n",
    "    \n",
    "    # Mean margin (reward difference)\n",
    "    mean_margin = (chosen_rewards - rejected_rewards).mean()\n",
    "    \n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"mean_chosen_reward\": mean_chosen,\n",
    "        \"mean_rejected_reward\": mean_rejected,\n",
    "        \"mean_margin\": mean_margin,\n",
    "    }\n",
    "\n",
    "metrics = compute_ranking_loss_with_metrics(chosen_rewards, rejected_rewards)\n",
    "print(\"Training metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:44.196846Z",
     "iopub.status.busy": "2025-12-06T23:29:44.196776Z",
     "iopub.status.idle": "2025-12-06T23:29:45.082840Z",
     "shell.execute_reply": "2025-12-06T23:29:45.082441Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward model for predicting human preferences.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, hidden_size):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def get_rewards(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Get last token's hidden state\n",
    "        seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        last_hidden = hidden_states[\n",
    "            torch.arange(batch_size, device=hidden_states.device),\n",
    "            seq_lengths.long()\n",
    "        ]\n",
    "        \n",
    "        return self.value_head(last_hidden).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:45.083987Z",
     "iopub.status.busy": "2025-12-06T23:29:45.083879Z",
     "iopub.status.idle": "2025-12-06T23:29:45.086795Z",
     "shell.execute_reply": "2025-12-06T23:29:45.086568Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_reward_model(model, train_loader, eval_loader, config, device):\n",
    "    \"\"\"Complete reward model training loop.\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_loader) * config['num_epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config['warmup_steps'],\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    best_eval_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        epoch_metrics = {'loss': 0, 'accuracy': 0}\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass for chosen responses\n",
    "            chosen_rewards = model.get_rewards(\n",
    "                batch['chosen_input_ids'],\n",
    "                batch['chosen_attention_mask']\n",
    "            )\n",
    "            \n",
    "            # Forward pass for rejected responses\n",
    "            rejected_rewards = model.get_rewards(\n",
    "                batch['rejected_input_ids'],\n",
    "                batch['rejected_attention_mask']\n",
    "            )\n",
    "            \n",
    "            # Compute loss and metrics\n",
    "            metrics = compute_ranking_loss_with_metrics(\n",
    "                chosen_rewards, rejected_rewards\n",
    "            )\n",
    "            loss = metrics['loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_metrics['loss'] += loss.item()\n",
    "            epoch_metrics['accuracy'] += metrics['accuracy'].item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{metrics['accuracy'].item():.2%}\"\n",
    "            })\n",
    "        \n",
    "        # End of epoch\n",
    "        avg_loss = epoch_metrics['loss'] / len(train_loader)\n",
    "        avg_acc = epoch_metrics['accuracy'] / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.2%}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Key Hyperparameters\n",
    "\n",
    "| Parameter | Typical Value | Notes |\n",
    "|-----------|---------------|-------|\n",
    "| Learning rate | 1e-5 | Much lower than SFT |\n",
    "| Batch size | 4 | Small (2 sequences per sample) |\n",
    "| Epochs | 1 | Avoid overfitting |\n",
    "| Gradient accumulation | 4 | Effective batch = 16 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T23:29:45.087693Z",
     "iopub.status.busy": "2025-12-06T23:29:45.087618Z",
     "iopub.status.idle": "2025-12-06T23:29:45.089367Z",
     "shell.execute_reply": "2025-12-06T23:29:45.089094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Model Training Configuration:\n",
      "  learning_rate: 1e-05\n",
      "  batch_size: 4\n",
      "  num_epochs: 1\n",
      "  warmup_steps: 100\n",
      "  gradient_accumulation_steps: 4\n",
      "  max_grad_norm: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'learning_rate': 1e-5,\n",
    "    'batch_size': 4,\n",
    "    'num_epochs': 1,\n",
    "    'warmup_steps': 100,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'max_grad_norm': 1.0,\n",
    "}\n",
    "\n",
    "print(\"Reward Model Training Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Common Training Issues\n",
    "\n",
    "### Low Accuracy (< 60%)\n",
    "- Model not learning preferences\n",
    "- Try: Lower learning rate, check data quality\n",
    "\n",
    "### Overfitting\n",
    "- Training accuracy >> eval accuracy\n",
    "- Try: Fewer epochs, more dropout, freeze base model\n",
    "\n",
    "### Training Instability\n",
    "- Loss spikes, NaN values\n",
    "- Try: Lower learning rate, gradient clipping, longer warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we can train reward models, let's learn how to evaluate them properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
