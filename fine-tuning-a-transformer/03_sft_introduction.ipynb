{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Introduction to Supervised Fine-Tuning\n",
    "\n",
    "**Teaching models to follow instructions through demonstration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## What is SFT?\n",
    "\n",
    "**Supervised Fine-Tuning (SFT)** is the first step in post-training. It teaches a pre-trained model to follow instructions by training on (instruction, response) pairs.\n",
    "\n",
    "The key insight is simple: if you want a model to answer questions, show it examples of good answers.\n",
    "\n",
    "$$\\mathcal{L}_{\\text{SFT}} = -\\sum_{t} \\log P(y_t | x, y_{<t})$$\n",
    "\n",
    "where:\n",
    "- $x$ is the instruction/prompt\n",
    "- $y$ is the response\n",
    "- We maximize the probability of each response token given the instruction and previous tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## SFT vs Pre-Training\n",
    "\n",
    "| Aspect | Pre-Training | SFT |\n",
    "|--------|-------------|-----|\n",
    "| **Data** | Raw text (books, web) | (instruction, response) pairs |\n",
    "| **Objective** | Predict next token | Generate good responses |\n",
    "| **Scale** | Trillions of tokens | Thousands-millions of examples |\n",
    "| **Duration** | Weeks-months | Hours-days |\n",
    "| **Learning rate** | Higher (1e-4) | Lower (1e-5 to 3e-4) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Popular SFT Datasets\n",
    "\n",
    "### Alpaca (Stanford)\n",
    "- **Size:** 52K instructions\n",
    "- **Source:** GPT-3.5 generated from seed tasks\n",
    "- **Format:** Instruction + optional input → output\n",
    "\n",
    "### Dolly (Databricks)\n",
    "- **Size:** 15K instructions\n",
    "- **Source:** Human-written by Databricks employees\n",
    "- **Quality:** Higher quality, more diverse\n",
    "\n",
    "### OpenAssistant\n",
    "- **Size:** 161K messages\n",
    "- **Source:** Community-contributed conversations\n",
    "- **Format:** Multi-turn conversations with rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Alpaca dataset (cleaned version)\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print()\n",
    "print(\"Example:\")\n",
    "print(f\"  Instruction: {dataset[0]['instruction']}\")\n",
    "if dataset[0]['input']:\n",
    "    print(f\"  Input: {dataset[0]['input']}\")\n",
    "print(f\"  Output: {dataset[0]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## The SFT Training Loop\n",
    "\n",
    "SFT follows a standard supervised learning pipeline:\n",
    "\n",
    "```\n",
    "1. Load pre-trained model\n",
    "2. Format instruction data with chat template\n",
    "3. Tokenize (instruction + response)\n",
    "4. Apply loss masking (only compute loss on response tokens)\n",
    "5. Train with standard cross-entropy loss\n",
    "6. Save fine-tuned model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a small model for demonstration\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Key SFT Concepts\n",
    "\n",
    "The following notebooks cover these essential topics:\n",
    "\n",
    "1. **Instruction Formatting** — How to structure prompts with chat templates\n",
    "2. **Loss Masking** — Why we only compute loss on response tokens\n",
    "3. **Training Loop** — Complete implementation with best practices\n",
    "4. **LoRA** — Efficient fine-tuning with low-rank adaptation\n",
    "\n",
    "Let's start with instruction formatting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
