{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Instruction Formatting\n",
    "\n",
    "**How to structure prompts with chat templates**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why Formatting Matters\n",
    "\n",
    "Models need a consistent format to understand where instructions end and responses begin. Without proper formatting:\n",
    "\n",
    "- The model doesn't know when to stop generating\n",
    "- It may confuse instructions with responses\n",
    "- Multi-turn conversations become impossible\n",
    "\n",
    "**Chat templates** solve this by adding special tokens and structure around messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Common Chat Formats\n",
    "\n",
    "### Alpaca Format\n",
    "```\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "```\n",
    "\n",
    "### ChatML Format (OpenAI)\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{response}<|im_end|>\n",
    "```\n",
    "\n",
    "### Llama 2 Format\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful assistant.\n",
    "<</SYS>>\n",
    "\n",
    "{instruction} [/INST] {response} </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation: Alpaca-style instruction formatting\n",
    "\n",
    "ALPACA_TEMPLATE = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\"\n",
    "\n",
    "ALPACA_TEMPLATE_WITH_INPUT = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\"\n",
    "\n",
    "def format_alpaca(instruction: str, response: str = \"\", input_text: str = \"\") -> str:\n",
    "    \"\"\"Format instruction in Alpaca style.\"\"\"\n",
    "    if input_text:\n",
    "        return ALPACA_TEMPLATE_WITH_INPUT.format(\n",
    "            instruction=instruction,\n",
    "            input=input_text,\n",
    "            response=response\n",
    "        )\n",
    "    return ALPACA_TEMPLATE.format(\n",
    "        instruction=instruction,\n",
    "        response=response\n",
    "    )\n",
    "\n",
    "# Example\n",
    "formatted = format_alpaca(\n",
    "    instruction=\"Explain quantum computing in simple terms.\",\n",
    "    response=\"Quantum computing uses quantum mechanics to process information...\"\n",
    ")\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation: ChatML-style formatting\n",
    "\n",
    "def format_chatml(\n",
    "    instruction: str,\n",
    "    response: str = \"\",\n",
    "    system: str = \"You are a helpful assistant.\"\n",
    ") -> str:\n",
    "    \"\"\"Format instruction in ChatML style.\"\"\"\n",
    "    formatted = f\"<|im_start|>system\\n{system}<|im_end|>\\n\"\n",
    "    formatted += f\"<|im_start|>user\\n{instruction}<|im_end|>\\n\"\n",
    "    formatted += f\"<|im_start|>assistant\\n{response}\"\n",
    "    if response:\n",
    "        formatted += \"<|im_end|>\"\n",
    "    return formatted\n",
    "\n",
    "# Example\n",
    "formatted = format_chatml(\n",
    "    instruction=\"What is the capital of France?\",\n",
    "    response=\"The capital of France is Paris.\"\n",
    ")\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Using HuggingFace Chat Templates\n",
    "\n",
    "Modern tokenizers have built-in chat template support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer with chat template support\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "# Create conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you! How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain machine learning?\"}\n",
    "]\n",
    "\n",
    "# Check if tokenizer has chat template\n",
    "if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:\n",
    "    formatted = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    print(\"Using built-in chat template:\")\n",
    "    print(formatted)\n",
    "else:\n",
    "    print(\"This tokenizer doesn't have a chat template.\")\n",
    "    print(\"We'll use a custom format instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Finding Response Start Position\n",
    "\n",
    "For loss masking, we need to know where the response begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_response_start(formatted_text: str, response_marker: str = \"### Response:\\n\") -> int:\n",
    "    \"\"\"Find the character position where the response starts.\"\"\"\n",
    "    idx = formatted_text.find(response_marker)\n",
    "    if idx == -1:\n",
    "        raise ValueError(f\"Response marker '{response_marker}' not found\")\n",
    "    return idx + len(response_marker)\n",
    "\n",
    "def find_response_start_tokens(tokenizer, formatted_text: str, response_marker: str = \"### Response:\\n\"):\n",
    "    \"\"\"Find the token position where the response starts.\"\"\"\n",
    "    # Tokenize the full text\n",
    "    full_tokens = tokenizer.encode(formatted_text, add_special_tokens=False)\n",
    "    \n",
    "    # Find character position\n",
    "    char_pos = find_response_start(formatted_text, response_marker)\n",
    "    \n",
    "    # Tokenize just the prompt part\n",
    "    prompt_text = formatted_text[:char_pos]\n",
    "    prompt_tokens = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
    "    \n",
    "    return len(prompt_tokens)\n",
    "\n",
    "# Example\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = format_alpaca(\n",
    "    instruction=\"What is 2+2?\",\n",
    "    response=\"2+2 equals 4.\"\n",
    ")\n",
    "\n",
    "response_start = find_response_start_tokens(tokenizer, text)\n",
    "print(f\"Response starts at token position: {response_start}\")\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Prompt tokens: {response_start}\")\n",
    "print(f\"Response tokens: {len(tokens) - response_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Be consistent** — Use the same format for training and inference\n",
    "2. **Add special tokens** — Help the model recognize boundaries\n",
    "3. **Handle edge cases** — Empty inputs, very long texts, special characters\n",
    "4. **Document your format** — Others need to use the same template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we understand instruction formatting, let's learn about loss masking — why we only compute loss on response tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
