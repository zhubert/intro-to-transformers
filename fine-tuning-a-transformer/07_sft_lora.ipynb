{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LoRA: The Clever Trick That Makes Fine-Tuning Feasible\n",
    "\n",
    "**Or: How to train a billion-parameter model on a laptop**\n",
    "\n",
    "(Well, almost.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Problem: Fine-Tuning is Absurdly Expensive\n",
    "\n",
    "Here's the thing about modern language models: they're huge. And when you want to fine-tune them the \"traditional\" way, you need to update *every single parameter*.\n",
    "\n",
    "Let's look at the numbers:\n",
    "\n",
    "| Model | Parameters | GPU Memory (FP32) |\n",
    "|-------|------------|-------------------|\n",
    "| GPT-2 | 124M | ~500 MB |\n",
    "| GPT-2 Large | 774M | ~3 GB |\n",
    "| LLaMA 7B | 7B | ~28 GB |\n",
    "| LLaMA 70B | 70B | ~280 GB |\n",
    "\n",
    "That 70B model? You'd need a server rack just to load it into memory. And that's *before* you start training, which requires storing gradients and optimizer states (typically 3-4x more memory).\n",
    "\n",
    "So what do we do? Give up on fine-tuning large models?\n",
    "\n",
    "Not quite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## LoRA: The Key Insight\n",
    "\n",
    "**LoRA** stands for **Low-Rank Adaptation**. The paper came out in 2021 and basically revolutionized how we fine-tune large models.\n",
    "\n",
    "Here's the core idea: when you fine-tune a model, you don't actually need to change *all* the weights by large amounts. The model already knows language — you're just steering it slightly. So the matrix of weight *updates* should be \"low-rank.\"\n",
    "\n",
    "### Wait, what does \"low-rank\" mean?\n",
    "\n",
    "Think about a spreadsheet with 1,000 rows and 1,000 columns. That's a million cells of data. But what if all the values in that spreadsheet could be generated from just a few simple rules? Like \"multiply row number by column number\" or something like that?\n",
    "\n",
    "In linear algebra terms, that spreadsheet would be \"low-rank\" — it *looks* like a million numbers, but there's actually much less information there. You could recreate it from a much smaller description.\n",
    "\n",
    "### The LoRA Trick\n",
    "\n",
    "Normally, when you update a weight matrix, you'd do this:\n",
    "\n",
    "$$W_{\\text{new}} = W_{\\text{original}} + \\Delta W$$\n",
    "\n",
    "Where $\\Delta W$ is the same size as $W$ (so, possibly millions of numbers).\n",
    "\n",
    "LoRA says: \"Let's *approximate* $\\Delta W$ as a product of two much smaller matrices\":\n",
    "\n",
    "$$W_{\\text{new}} = W_{\\text{original}} + B \\times A$$\n",
    "\n",
    "where:\n",
    "- $W_{\\text{original}}$ is the frozen (unchanging) pre-trained weights — shape: $d \\times k$\n",
    "- $B$ is a tall skinny matrix — shape: $d \\times r$  \n",
    "- $A$ is a short wide matrix — shape: $r \\times k$\n",
    "- $r$ is the **rank** — typically something tiny like 8 or 16\n",
    "\n",
    "Here's the magic: instead of storing $d \\times k$ new numbers (millions), we only store $(d \\times r) + (r \\times k)$ numbers (thousands).\n",
    "\n",
    "### An Analogy\n",
    "\n",
    "Imagine you're editing a photograph. You *could* change every single pixel individually (full fine-tuning). Or you could apply a simple filter — like \"make it 10% warmer\" — which is defined by just a few parameters but affects the whole image (LoRA).\n",
    "\n",
    "Both change the image, but one requires way less information to describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:01.379798Z",
     "iopub.status.busy": "2025-12-10T21:19:01.379710Z",
     "iopub.status.idle": "2025-12-10T21:19:02.083815Z",
     "shell.execute_reply": "2025-12-10T21:19:02.083479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full fine-tuning: 16,777,216 parameters\n",
      "LoRA (rank=8): 65,536 parameters\n",
      "Reduction: 256.0x fewer parameters\n",
      "\n",
      "That's 0.39% of the original size.\n",
      "(Or to put it another way: we're using 99.6% less parameters!)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Low-Rank Adaptation layer.\n",
    "    \n",
    "    This doesn't replace a linear layer — it sits *alongside* one,\n",
    "    adding a small trainable update.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # This scaling factor controls how much the LoRA update affects the output\n",
    "        # alpha/rank is a common choice that keeps the update magnitude reasonable\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # The two low-rank matrices\n",
    "        # A is initialized with random values (Kaiming initialization)\n",
    "        # B is initialized to zeros (so initially, LoRA contributes nothing)\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the LoRA contribution: (B @ A) @ x * scaling\n",
    "        \n",
    "        x has shape: (batch_size, sequence_length, in_features)\n",
    "        We're computing: x @ A^T @ B^T * scaling\n",
    "        \"\"\"\n",
    "        lora_out = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        return lora_out * self.scaling\n",
    "\n",
    "\n",
    "# Let's see the parameter savings in action\n",
    "in_features, out_features = 4096, 4096\n",
    "rank = 8\n",
    "\n",
    "# Full fine-tuning would update this many parameters:\n",
    "full_params = in_features * out_features\n",
    "\n",
    "# LoRA only needs this many:\n",
    "lora_params = (in_features * rank) + (out_features * rank)\n",
    "\n",
    "print(f\"Full fine-tuning: {full_params:,} parameters\")\n",
    "print(f\"LoRA (rank={rank}): {lora_params:,} parameters\")\n",
    "print(f\"Reduction: {full_params / lora_params:.1f}x fewer parameters\")\n",
    "print()\n",
    "print(f\"That's {100 * lora_params / full_params:.2f}% of the original size.\")\n",
    "print(f\"(Or to put it another way: we're using {100 * (1 - lora_params / full_params):.1f}% less parameters!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Wrapping an Existing Layer\n",
    "\n",
    "In practice, we don't create standalone LoRA layers. We wrap *existing* layers from a pre-trained model.\n",
    "\n",
    "The pattern is:\n",
    "1. Take a frozen linear layer from the pre-trained model\n",
    "2. Add a LoRA layer alongside it\n",
    "3. During forward pass: add both outputs together\n",
    "\n",
    "Let's build that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:02.084952Z",
     "iopub.status.busy": "2025-12-10T21:19:02.084845Z",
     "iopub.status.idle": "2025-12-10T21:19:02.090466Z",
     "shell.execute_reply": "2025-12-10T21:19:02.090170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 12,288\n",
      "Total parameters: 602,880\n",
      "Trainable: 2.04%\n",
      "\n",
      "So we're only updating ~2% of the layer's parameters!\n",
      "(The other 98% stay frozen at their pre-trained values.)\n"
     ]
    }
   ],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer with LoRA adaptation.\n",
    "    \n",
    "    Think of this as a wrapper around a pre-trained linear layer.\n",
    "    The original layer is frozen, and we add a small LoRA \"correction\" on top.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store the original layer and freeze it\n",
    "        # (We're not training these weights — they stay as pre-trained)\n",
    "        self.original = original_layer\n",
    "        for param in self.original.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Add the LoRA adapter\n",
    "        self.lora = LoRALayer(\n",
    "            in_features=original_layer.in_features,\n",
    "            out_features=original_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: original output + LoRA output.\n",
    "        \n",
    "        The original layer produces the \"base\" output.\n",
    "        The LoRA layer produces a small \"correction.\"\n",
    "        We add them together.\n",
    "        \"\"\"\n",
    "        return self.original(x) + self.lora(x)\n",
    "\n",
    "\n",
    "# Let's test it with a realistic size (GPT-2's hidden dimension)\n",
    "original = nn.Linear(768, 768)\n",
    "lora_layer = LoRALinear(original, rank=8)\n",
    "\n",
    "# Count trainable vs. frozen parameters\n",
    "trainable = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in lora_layer.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"Trainable: {100 * trainable / total:.2f}%\")\n",
    "print()\n",
    "print(\"So we're only updating ~2% of the layer's parameters!\")\n",
    "print(\"(The other 98% stay frozen at their pre-trained values.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Using PEFT Library (The Real-World Approach)\n",
    "\n",
    "Okay, so we've built LoRA from scratch to understand how it works. But in practice, you'd never actually do that.\n",
    "\n",
    "Instead, you'd use HuggingFace's **PEFT** library (Parameter-Efficient Fine-Tuning). It handles all the low-level details and integrates seamlessly with transformers.\n",
    "\n",
    "Let's see how easy it is to add LoRA to a real model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:02.091198Z",
     "iopub.status.busy": "2025-12-10T21:19:02.091122Z",
     "iopub.status.idle": "2025-12-10T21:19:04.014562Z",
     "shell.execute_reply": "2025-12-10T21:19:04.014241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: gpt2\n",
      "Total parameters: 124,439,808\n",
      "\n",
      "Now let's add LoRA to this...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load GPT-2 (the small version, 124M parameters)\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Base model: {model_name}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print()\n",
    "print(\"Now let's add LoRA to this...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:04.015592Z",
     "iopub.status.busy": "2025-12-10T21:19:04.015451Z",
     "iopub.status.idle": "2025-12-10T21:19:04.831384Z",
     "shell.execute_reply": "2025-12-10T21:19:04.831043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n",
      "\n",
      "Wait, what are 'c_attn' and 'c_proj'?\n",
      "\n",
      "Those are GPT-2's attention layers:\n",
      "  - c_attn: The combined Query/Key/Value projection\n",
      "  - c_proj: The output projection after attention\n",
      "\n",
      "We're adding LoRA *only* to these layers, not the entire model.\n",
      "(Empirically, adapting attention layers gives the best results.)\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # We're doing causal language modeling\n",
    "    r=8,                            # Rank: the size of the low-rank matrices\n",
    "    lora_alpha=16,                  # Scaling factor (typically 2*r)\n",
    "    lora_dropout=0.1,               # Dropout applied to LoRA layers\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Which layers to adapt\n",
    "    fan_in_fan_out=True,            # Required for GPT-2's Conv1D layers\n",
    "    bias=\"none\"                     # Don't add LoRA to bias terms\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "# This wraps the specified layers with LoRA adapters\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print the breakdown\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "print()\n",
    "print(\"Wait, what are 'c_attn' and 'c_proj'?\")\n",
    "print()\n",
    "print(\"Those are GPT-2's attention layers:\")\n",
    "print(\"  - c_attn: The combined Query/Key/Value projection\")\n",
    "print(\"  - c_proj: The output projection after attention\")\n",
    "print()\n",
    "print(\"We're adding LoRA *only* to these layers, not the entire model.\")\n",
    "print(\"(Empirically, adapting attention layers gives the best results.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Choosing LoRA Hyperparameters\n",
    "\n",
    "LoRA has a few knobs you can tune. Here's what they mean and how to choose them:\n",
    "\n",
    "| Parameter | What it means | Typical Values | How to choose |\n",
    "|-----------|---------------|----------------|---------------|\n",
    "| `r` (rank) | Size of the low-rank matrices | 4, 8, 16, 32 | Start with 8. Increase if underfitting. |\n",
    "| `alpha` | Scaling factor for LoRA output | 16, 32 | Usually 2×r. Higher = stronger adaptation. |\n",
    "| `dropout` | Dropout on LoRA layers | 0.05-0.1 | Standard regularization. |\n",
    "| `target_modules` | Which layers get LoRA | Attention layers | Q, K, V projections work best. |\n",
    "\n",
    "### The rank/alpha relationship\n",
    "\n",
    "The **rank** controls capacity: higher rank = more expressive updates, but also more parameters.\n",
    "\n",
    "The **alpha** controls magnitude: it scales how much the LoRA output affects the final result. The ratio `alpha/r` is what actually matters — it's like a learning rate specifically for the LoRA part.\n",
    "\n",
    "Think of it this way:\n",
    "- Low rank (r=4): \"I only need a simple adjustment\"\n",
    "- High rank (r=64): \"I need to make complex changes to the model\"\n",
    "\n",
    "Most of the time, r=8 or r=16 is plenty. Remember: the pre-trained model already knows a lot. We're just steering it, not retraining from scratch.\n",
    "\n",
    "### Which layers to target?\n",
    "\n",
    "For transformers, the attention layers are where the magic happens. Specifically:\n",
    "- **Query, Key, Value projections** (often combined as `c_attn` in GPT-2)\n",
    "- **Output projection** (`c_proj`)\n",
    "\n",
    "You *could* add LoRA to the feedforward layers too, but empirically it doesn't help much and just adds more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Merging LoRA Weights (For Inference)\n",
    "\n",
    "Here's a neat trick: after training, you can **merge** the LoRA weights back into the original model.\n",
    "\n",
    "Remember, during training we compute:\n",
    "$$\\text{output} = W_{\\text{original}} \\cdot x + (B \\times A) \\cdot x$$\n",
    "\n",
    "But mathematically, this is the same as:\n",
    "$$\\text{output} = (W_{\\text{original}} + B \\times A) \\cdot x$$\n",
    "\n",
    "So we can just add $B \\times A$ to the original weights once, and then we're back to a normal model — no extra computation at inference time!\n",
    "\n",
    "This is huge. You get the training benefits of LoRA (low memory, fast updates) AND the inference benefits of a regular model (no overhead).\n",
    "\n",
    "Let's see how that works in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:19:04.832395Z",
     "iopub.status.busy": "2025-12-10T21:19:04.832226Z",
     "iopub.status.idle": "2025-12-10T21:19:04.837981Z",
     "shell.execute_reply": "2025-12-10T21:19:04.837724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original W shape: torch.Size([768, 768])\n",
      "LoRA A shape: torch.Size([8, 768])\n",
      "LoRA B shape: torch.Size([768, 8])\n",
      "Merged W shape: torch.Size([768, 768])\n",
      "\n",
      "Before merging:\n",
      "  - Two separate matrix multiplies during inference\n",
      "  - Slightly slower, but LoRA weights can be swapped out\n",
      "\n",
      "After merging:\n",
      "  - Single matrix multiply (same as original model)\n",
      "  - Zero inference overhead!\n",
      "  - Trade-off: can't easily swap LoRA adapters anymore\n",
      "\n",
      "(In practice, you'd merge for production deployment.)\n"
     ]
    }
   ],
   "source": [
    "def merge_lora_weights(original_weight, lora_A, lora_B, scaling):\n",
    "    \"\"\"\n",
    "    Merge LoRA weights into the original weight matrix.\n",
    "    \n",
    "    W_merged = W_original + (B @ A) * scaling\n",
    "    \n",
    "    After this, you can throw away the LoRA matrices and just use W_merged.\n",
    "    \"\"\"\n",
    "    # Compute the low-rank update\n",
    "    delta_W = (lora_B @ lora_A) * scaling\n",
    "    \n",
    "    # Add it to the original weights\n",
    "    return original_weight + delta_W\n",
    "\n",
    "\n",
    "# Example with realistic dimensions\n",
    "d, k, r = 768, 768, 8\n",
    "alpha = 16\n",
    "scaling = alpha / r\n",
    "\n",
    "# Simulate the weights\n",
    "W_original = torch.randn(d, k)  # Original pre-trained weights\n",
    "lora_A = torch.randn(r, k)      # LoRA matrix A (trained)\n",
    "lora_B = torch.randn(d, r)      # LoRA matrix B (trained)\n",
    "\n",
    "# Merge them\n",
    "W_merged = merge_lora_weights(W_original, lora_A, lora_B, scaling)\n",
    "\n",
    "print(f\"Original W shape: {W_original.shape}\")\n",
    "print(f\"LoRA A shape: {lora_A.shape}\")\n",
    "print(f\"LoRA B shape: {lora_B.shape}\")\n",
    "print(f\"Merged W shape: {W_merged.shape}\")\n",
    "print()\n",
    "print(\"Before merging:\")\n",
    "print(f\"  - Two separate matrix multiplies during inference\")\n",
    "print(f\"  - Slightly slower, but LoRA weights can be swapped out\")\n",
    "print()\n",
    "print(\"After merging:\")\n",
    "print(f\"  - Single matrix multiply (same as original model)\")\n",
    "print(f\"  - Zero inference overhead!\")\n",
    "print(f\"  - Trade-off: can't easily swap LoRA adapters anymore\")\n",
    "print()\n",
    "print(\"(In practice, you'd merge for production deployment.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Why LoRA is Brilliant\n",
    "\n",
    "Let's recap why this technique took over the world:\n",
    "\n",
    "### 1. Memory Efficient\n",
    "You only need to store and update the small LoRA matrices. For a 7B parameter model with rank-16 LoRA, you might only train 0.1% of the parameters. That's the difference between needing 8 GPUs and needing 1.\n",
    "\n",
    "### 2. Fast Training\n",
    "Fewer parameters = fewer gradients to compute = faster backward pass. Training can be 2-3x faster than full fine-tuning.\n",
    "\n",
    "### 3. Modular\n",
    "This is my favorite part. You can train *multiple* LoRA adapters for different tasks, all sharing the same base model. Want a model that can do customer support AND code generation? Train two LoRA adapters (a few MB each) instead of fine-tuning two full models (several GB each).\n",
    "\n",
    "Then swap them in and out as needed. It's like having a Swiss Army knife where you only store the tools, not duplicate copies of the handle.\n",
    "\n",
    "### 4. Preserves Base Model Knowledge\n",
    "Because the pre-trained weights are frozen, you don't suffer from \"catastrophic forgetting\" — where fine-tuning on task A makes the model worse at tasks B, C, and D.\n",
    "\n",
    "### 5. Easy Deployment\n",
    "Merge the weights for production, and you're back to a standard model. No special serving infrastructure needed.\n",
    "\n",
    "---\n",
    "\n",
    "The only downside? LoRA is an *approximation*. There are some updates that can't be represented as a low-rank matrix. But empirically, for most fine-tuning tasks, this constraint doesn't hurt — and the benefits massively outweigh it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "We've now covered supervised fine-tuning (SFT) with LoRA — teaching a model to follow a specific style or format by training on input-output examples.\n",
    "\n",
    "But what if you want to teach a model to be *helpful* or *harmless* or *creative*? That's harder to capture in input-output pairs.\n",
    "\n",
    "Enter: **Reward Modeling**.\n",
    "\n",
    "Instead of training on explicit examples, we train a separate model to predict *human preferences*. Then we use that reward model to guide the fine-tuning process (via reinforcement learning).\n",
    "\n",
    "It's a bit more complex, but it's how models like ChatGPT and Claude get their \"personality.\"\n",
    "\n",
    "Let's dive in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
