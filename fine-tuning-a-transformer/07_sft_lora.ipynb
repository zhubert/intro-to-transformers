{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LoRA: Efficient Fine-Tuning\n",
    "\n",
    "**Low-Rank Adaptation for parameter-efficient training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Problem with Full Fine-Tuning\n",
    "\n",
    "Full fine-tuning updates ALL model parameters:\n",
    "\n",
    "| Model | Parameters | GPU Memory (FP32) |\n",
    "|-------|------------|-------------------|\n",
    "| GPT-2 | 124M | ~500 MB |\n",
    "| GPT-2 Large | 774M | ~3 GB |\n",
    "| LLaMA 7B | 7B | ~28 GB |\n",
    "| LLaMA 70B | 70B | ~280 GB |\n",
    "\n",
    "For large models, this is impractical:\n",
    "- Requires multiple expensive GPUs\n",
    "- Need to store full model copies for each task\n",
    "- Risk of catastrophic forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## LoRA: The Key Insight\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** freezes the pre-trained model and adds small trainable matrices.\n",
    "\n",
    "Instead of updating a weight matrix $W \\in \\mathbb{R}^{d \\times k}$:\n",
    "\n",
    "$$W_{\\text{new}} = W + \\Delta W$$\n",
    "\n",
    "LoRA decomposes the update as a low-rank product:\n",
    "\n",
    "$$W_{\\text{new}} = W + BA$$\n",
    "\n",
    "where:\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$\n",
    "- $r \\ll \\min(d, k)$ (rank is much smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Low-Rank Adaptation layer.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Low-rank matrices\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Initialize A with Kaiming, B with zeros\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute LoRA output: (B @ A) @ x * scaling\"\"\"\n",
    "        # x: (batch, seq, in_features)\n",
    "        # lora_A: (rank, in_features)\n",
    "        # lora_B: (out_features, rank)\n",
    "        \n",
    "        lora_out = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        return lora_out * self.scaling\n",
    "\n",
    "# Example: Compare parameter counts\n",
    "in_features, out_features = 4096, 4096\n",
    "rank = 8\n",
    "\n",
    "full_params = in_features * out_features\n",
    "lora_params = (in_features * rank) + (out_features * rank)\n",
    "\n",
    "print(f\"Full fine-tuning: {full_params:,} parameters\")\n",
    "print(f\"LoRA (rank={rank}): {lora_params:,} parameters\")\n",
    "print(f\"Reduction: {full_params / lora_params:.1f}x fewer parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## LoRA Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"Linear layer with LoRA adaptation.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Freeze original layer\n",
    "        self.original = original_layer\n",
    "        for param in self.original.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Add LoRA\n",
    "        self.lora = LoRALayer(\n",
    "            in_features=original_layer.in_features,\n",
    "            out_features=original_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward: original output + LoRA output.\"\"\"\n",
    "        return self.original(x) + self.lora(x)\n",
    "\n",
    "# Example\n",
    "original = nn.Linear(768, 768)\n",
    "lora_layer = LoRALinear(original, rank=8)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in lora_layer.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"Trainable: {100 * trainable / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Using PEFT Library\n",
    "\n",
    "In practice, use HuggingFace's PEFT library for LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load base model\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Base model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Requires `pip install peft`\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=8,                    # Rank\n",
    "        lora_alpha=16,          # Scaling factor\n",
    "        lora_dropout=0.1,       # Dropout\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # Which layers to adapt\n",
    "        bias=\"none\"             # Don't train biases\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    peft_model.print_trainable_parameters()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"PEFT not installed. Install with: pip install peft\")\n",
    "    print(\"\\nFor now, here's what the output would look like:\")\n",
    "    print(\"trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.24%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## LoRA Hyperparameters\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `r` (rank) | Dimension of low-rank matrices | 4, 8, 16, 32 |\n",
    "| `alpha` | Scaling factor | 16, 32 (often 2×r) |\n",
    "| `dropout` | Dropout on LoRA layers | 0.05-0.1 |\n",
    "| `target_modules` | Which layers to adapt | Query, Key, Value projections |\n",
    "\n",
    "**Guidelines:**\n",
    "- Higher rank = more capacity but more parameters\n",
    "- Start with r=8, increase if underfitting\n",
    "- Target attention projections (Q, K, V) for best results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Merging LoRA Weights\n",
    "\n",
    "After training, you can merge LoRA weights into the base model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_weights(original_weight, lora_A, lora_B, scaling):\n",
    "    \"\"\"\n",
    "    Merge LoRA weights into original weight matrix.\n",
    "    \n",
    "    W_merged = W + (B @ A) * scaling\n",
    "    \"\"\"\n",
    "    delta_W = (lora_B @ lora_A) * scaling\n",
    "    return original_weight + delta_W\n",
    "\n",
    "# Example\n",
    "d, k, r = 768, 768, 8\n",
    "alpha = 16\n",
    "scaling = alpha / r\n",
    "\n",
    "W = torch.randn(d, k)  # Original weights\n",
    "A = torch.randn(r, k)  # LoRA A\n",
    "B = torch.randn(d, r)  # LoRA B\n",
    "\n",
    "W_merged = merge_lora_weights(W, A, B, scaling)\n",
    "\n",
    "print(f\"Original W shape: {W.shape}\")\n",
    "print(f\"Merged W shape: {W_merged.shape}\")\n",
    "print(f\"\\nAfter merging, inference is same speed as original model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Benefits of LoRA\n",
    "\n",
    "1. **Memory Efficient** — Only store/update small matrices\n",
    "2. **Fast Training** — Fewer gradients to compute\n",
    "3. **Modular** — Swap LoRA adapters for different tasks\n",
    "4. **Preserves Base Model** — Less catastrophic forgetting\n",
    "5. **Easy Deployment** — Merge weights for zero overhead inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we've covered SFT (including LoRA), let's move on to Reward Modeling — training models to predict human preferences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
