{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# From Noise to Images\n\nSo far, everything we've built has been about language—predicting tokens, following instructions, reasoning through problems. But transformers aren't limited to text.\n\nThe same attention mechanism that revolutionized NLP (Natural Language Processing) has transformed computer vision. And nowhere is this more visible than in image generation."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The Trick\n\nEvery image you've ever seen from Stable Diffusion, Midjourney, or DALL-E started as pure random noise. A **generative model** learned to transform that noise into coherent images.\n\nIn this section, we'll build one from scratch."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Problem We're Solving\n",
    "\n",
    "Here's the setup:\n",
    "\n",
    "- We have training data (real images)\n",
    "- We want to sample *new* images from the same distribution\n",
    "- But we don't know the distribution explicitly—we only have examples\n",
    "\n",
    "The strategy: learn a **transformation** from a simple distribution (Gaussian noise) to our complex data distribution. If we can learn this transformation, we can generate new samples by:\n",
    "\n",
    "1. Sample noise\n",
    "2. Apply our learned transformation\n",
    "3. Out comes a realistic image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## Why Flow Matching?\n\nSeveral approaches exist for generative modeling:\n\n| Approach | Core Idea | Challenge |\n|----------|-----------|----------|\n| **GANs** (Generative Adversarial Networks) | Generator fools discriminator | Training instability |\n| **VAEs** (Variational Autoencoders) | Encode/decode through latent space | Blurry outputs |\n| **DDPM** (Denoising Diffusion Probabilistic Models) | Gradually denoise over many steps | Slow sampling |\n| **Flow Matching** | Learn straight paths from noise to data | Simple and fast |\n\nFlow matching has become the preferred choice because:\n\n1. **Simpler mathematics** — no stochastic differential equations\n2. **Faster sampling** — straight paths require fewer steps\n3. **State-of-the-art results** — used in Stable Diffusion 3, Flux, and more"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## The Core Idea\n",
    "\n",
    "Flow matching constructs a continuous path between noise and data:\n",
    "\n",
    "$$x_t = (1-t) \\cdot x_{\\text{data}} + t \\cdot x_{\\text{noise}}$$\n",
    "\n",
    "We train a neural network to predict the velocity along this path. Then to generate:\n",
    "\n",
    "1. Start with pure noise at $t=1$\n",
    "2. Follow the velocity field backward to $t=0$\n",
    "3. Arrive at a realistic image\n",
    "\n",
    "The velocity is constant (straight lines!), which makes everything clean and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## What We'll Build\n\n| Notebook | Topic | What You'll Learn |\n|----------|-------|-------------------|\n| **Flow Matching** | The basics | Linear interpolation, velocity fields, Euler sampling |\n| **Diffusion Transformer** | DiT (Diffusion Transformer) architecture | Patchify, adaLN, transformers for images |\n| **Class Conditioning** | Controlled generation | Classifier-free guidance |\n| **Text Conditioning** | Text-to-image | CLIP (Contrastive Language-Image Pre-training) encoder, cross-attention |\n| **Latent Diffusion** | Scaling up | VAE (Variational Autoencoder) compression, the Stable Diffusion approach |\n\nBy the end, you'll understand how modern image generation works—from the mathematical foundations to the architectural choices that make it practical."
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## Prerequisites\n\nThis section assumes familiarity with:\n\n- **PyTorch** — tensors, modules, training loops\n- **Transformers** — attention, the basics from earlier sections\n- **Basic probability** — distributions, sampling\n\nThe math gets a bit more involved than the language modeling sections (ODEs, or Ordinary Differential Equations, and flow equations), but we'll build intuition step by step."
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This is the final section of the book. By the end, you'll have built transformers for both language and vision—understanding not just *how* they work, but *why* the same architecture succeeds across such different domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  },
  "description": "Introduction to diffusion models and flow matching for generating images from noise."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}