{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# From Noise to Images\n",
    "\n",
    "So far, everything we've built has been about language—predicting tokens, following instructions, reasoning through problems. But transformers aren't limited to text.\n",
    "\n",
    "The same attention mechanism that revolutionized NLP has transformed computer vision. And nowhere is this more visible than in image generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The Trick\n\nEvery image you've ever seen from Stable Diffusion, Midjourney, or DALL-E started as pure random noise. A **generative model** learned to transform that noise into coherent images.\n\nIn this section, we'll build one from scratch."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Problem We're Solving\n",
    "\n",
    "Here's the setup:\n",
    "\n",
    "- We have training data (real images)\n",
    "- We want to sample *new* images from the same distribution\n",
    "- But we don't know the distribution explicitly—we only have examples\n",
    "\n",
    "The strategy: learn a **transformation** from a simple distribution (Gaussian noise) to our complex data distribution. If we can learn this transformation, we can generate new samples by:\n",
    "\n",
    "1. Sample noise\n",
    "2. Apply our learned transformation\n",
    "3. Out comes a realistic image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Why Flow Matching?\n",
    "\n",
    "Several approaches exist for generative modeling:\n",
    "\n",
    "| Approach | Core Idea | Challenge |\n",
    "|----------|-----------|----------|\n",
    "| **GANs** | Generator fools discriminator | Training instability |\n",
    "| **VAEs** | Encode/decode through latent space | Blurry outputs |\n",
    "| **DDPM** | Gradually denoise over many steps | Slow sampling |\n",
    "| **Flow Matching** | Learn straight paths from noise to data | Simple and fast |\n",
    "\n",
    "Flow matching has become the preferred choice because:\n",
    "\n",
    "1. **Simpler mathematics** — no stochastic differential equations\n",
    "2. **Faster sampling** — straight paths require fewer steps\n",
    "3. **State-of-the-art results** — used in Stable Diffusion 3, Flux, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## The Core Idea\n",
    "\n",
    "Flow matching constructs a continuous path between noise and data:\n",
    "\n",
    "$$x_t = (1-t) \\cdot x_{\\text{data}} + t \\cdot x_{\\text{noise}}$$\n",
    "\n",
    "We train a neural network to predict the velocity along this path. Then to generate:\n",
    "\n",
    "1. Start with pure noise at $t=1$\n",
    "2. Follow the velocity field backward to $t=0$\n",
    "3. Arrive at a realistic image\n",
    "\n",
    "The velocity is constant (straight lines!), which makes everything clean and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## What We'll Build\n",
    "\n",
    "| Notebook | Topic | What You'll Learn |\n",
    "|----------|-------|-------------------|\n",
    "| **Flow Matching** | The basics | Linear interpolation, velocity fields, Euler sampling |\n",
    "| **Diffusion Transformer** | DiT architecture | Patchify, adaLN, transformers for images |\n",
    "| **Class Conditioning** | Controlled generation | Classifier-free guidance |\n",
    "| **Text Conditioning** | Text-to-image | CLIP encoder, cross-attention |\n",
    "| **Latent Diffusion** | Scaling up | VAE compression, the Stable Diffusion approach |\n",
    "\n",
    "By the end, you'll understand how modern image generation works—from the mathematical foundations to the architectural choices that make it practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This section assumes familiarity with:\n",
    "\n",
    "- **PyTorch** — tensors, modules, training loops\n",
    "- **Transformers** — attention, the basics from earlier sections\n",
    "- **Basic probability** — distributions, sampling\n",
    "\n",
    "The math gets a bit more involved than the language modeling sections (ODEs, flow equations), but we'll build intuition step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This is the final section of the book. By the end, you'll have built transformers for both language and vision—understanding not just *how* they work, but *why* the same architecture succeeds across such different domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}