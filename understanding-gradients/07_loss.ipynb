{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Loss Calculation\n",
    "\n",
    "**Measuring prediction error with cross-entropy loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. The forward pass is almost done.\n",
    "\n",
    "We've got these nice 16-dimensional vectors representing each token after going through embeddings, attention, FFN, and layer norm. But we still have a problem.\n",
    "\n",
    "**The model isn't actually predicting anything yet.**\n",
    "\n",
    "We have hidden states, not predictions. We need to convert these hidden states into probabilities over our vocabulary. Then we can measure how wrong we are.\n",
    "\n",
    "That's what this step does: **project to vocabulary space** and **compute the loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task: Next-Token Prediction\n",
    "\n",
    "Our model is a language model. Its job is simple: given a sequence of tokens, predict the next token.\n",
    "\n",
    "```\n",
    "Input:  <BOS> I    like transformers <EOS>\n",
    "IDs:    [1,   3,   4,   5,          2]\n",
    "```\n",
    "\n",
    "At each position, we want to predict what comes next:\n",
    "\n",
    "| Position | Current Token | Should Predict |\n",
    "|----------|--------------|----------------|\n",
    "| 0 | `<BOS>` | `I` (token 3) |\n",
    "| 1 | `I` | `like` (token 4) |\n",
    "| 2 | `like` | `transformers` (token 5) |\n",
    "| 3 | `transformers` | `<EOS>` (token 2) |\n",
    "| 4 | `<EOS>` | nothing (end) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "D_FF = 64\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS\n",
    "EPSILON = 1e-5\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n = len(A), len(A[0])\n",
    "    p = len(B[0])\n",
    "    return [[sum(A[i][k] * B[k][j] for k in range(n)) for j in range(p)] for i in range(m)]\n",
    "\n",
    "def transpose(A):\n",
    "    return [[A[i][j] for i in range(len(A))] for j in range(len(A[0]))]\n",
    "\n",
    "def softmax(vec):\n",
    "    max_val = max(vec)\n",
    "    exp_vec = [math.exp(v - max_val) for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def softmax_causal(vec):\n",
    "    max_val = max(v for v in vec if v != float('-inf'))\n",
    "    exp_vec = [math.exp(v - max_val) if v != float('-inf') else 0 for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def layer_norm(x, gamma, beta, epsilon=1e-5):\n",
    "    mean = sum(x) / len(x)\n",
    "    variance = sum((xi - mean)**2 for xi in x) / len(x)\n",
    "    std = math.sqrt(variance + epsilon)\n",
    "    x_norm = [(xi - mean) / std for xi in x]\n",
    "    return [gamma[i] * x_norm[i] + beta[i] for i in range(len(x))]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the full forward pass\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "tokens = [1, 3, 4, 5, 2]\n",
    "seq_len = len(tokens)\n",
    "X = [add_vectors(E_token[tokens[i]], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "# Attention\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "def compute_attention(Q, K, V):\n",
    "    seq_len, d_k = len(Q), len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    scores = matmul(Q, transpose(K))\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j > i:\n",
    "                scaled[i][j] = float('-inf')\n",
    "    weights = [softmax_causal(row) for row in scaled]\n",
    "    return matmul(weights, V)\n",
    "\n",
    "attention_output_all = [compute_attention(Q_all[h], K_all[h], V_all[h]) for h in range(NUM_HEADS)]\n",
    "concat_output = [attention_output_all[0][i] + attention_output_all[1][i] for i in range(seq_len)]\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)\n",
    "multi_head_output = matmul(concat_output, transpose(W_O))\n",
    "\n",
    "# FFN\n",
    "W1 = random_matrix(D_FF, D_MODEL)\n",
    "b1 = random_vector(D_FF)\n",
    "W2 = random_matrix(D_MODEL, D_FF)\n",
    "b2 = random_vector(D_MODEL)\n",
    "hidden = [[sum(multi_head_output[i][k] * W1[j][k] for k in range(D_MODEL)) + b1[j] for j in range(D_FF)] for i in range(seq_len)]\n",
    "activated = [[gelu(h) for h in row] for row in hidden]\n",
    "ffn_output = [[sum(activated[i][k] * W2[j][k] for k in range(D_FF)) + b2[j] for j in range(D_MODEL)] for i in range(seq_len)]\n",
    "\n",
    "# Residual + LayerNorm\n",
    "residual = [add_vectors(multi_head_output[i], ffn_output[i]) for i in range(seq_len)]\n",
    "gamma = [1.0] * D_MODEL\n",
    "beta = [0.0] * D_MODEL\n",
    "layer_norm_output = [layer_norm(residual[i], gamma, beta, EPSILON) for i in range(seq_len)]\n",
    "\n",
    "print(\"Recreated full forward pass through transformer block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project to Vocabulary Space (Logits)\n",
    "\n",
    "Our hidden states are 16-dimensional. Our vocabulary has 6 tokens. We need to map from 16D → 6D.\n",
    "\n",
    "Enter the **language modeling head** (LM head): a simple linear projection.\n",
    "\n",
    "$$\\text{logits} = W_{lm} \\cdot \\text{hidden\\_state}$$\n",
    "\n",
    "These \"logits\" are **unnormalized scores**. Higher scores mean the model thinks that token is more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LM head weight matrix\n",
    "W_lm = random_matrix(VOCAB_SIZE, D_MODEL)  # [6, 16]\n",
    "\n",
    "print(f\"LM Head Weight Matrix W_lm\")\n",
    "print(f\"Shape: [{VOCAB_SIZE}, {D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute logits: hidden_state @ W_lm^T\n",
    "W_lm_T = transpose(W_lm)\n",
    "logits = matmul(layer_norm_output, W_lm_T)\n",
    "\n",
    "print(\"Logits (unnormalized scores)\")\n",
    "print(f\"Shape: [{seq_len}, {VOCAB_SIZE}]\")\n",
    "print()\n",
    "print(f\"{'Position':<12} {'<PAD>':>8} {'<BOS>':>8} {'<EOS>':>8} {'I':>8} {'like':>8} {'trans':>8}\")\n",
    "print(\"-\"*70)\n",
    "for i, row in enumerate(logits):\n",
    "    print(f\"{TOKEN_NAMES[tokens[i]]:<12} {row[0]:>8.4f} {row[1]:>8.4f} {row[2]:>8.4f} {row[3]:>8.4f} {row[4]:>8.4f} {row[5]:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert to Probabilities (Softmax)\n",
    "\n",
    "Logits are scores, but they're not probabilities. They don't sum to 1. Some are negative.\n",
    "\n",
    "**Softmax** fixes this:\n",
    "\n",
    "$$P(\\text{token}_i) = \\frac{\\exp(\\text{logit}_i)}{\\sum_j \\exp(\\text{logit}_j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax to get probabilities\n",
    "probs = [softmax(row) for row in logits]\n",
    "\n",
    "print(\"Probabilities (after softmax)\")\n",
    "print(f\"Shape: [{seq_len}, {VOCAB_SIZE}]\")\n",
    "print()\n",
    "print(f\"{'Position':<12} {'<PAD>':>8} {'<BOS>':>8} {'<EOS>':>8} {'I':>8} {'like':>8} {'trans':>8} {'Sum':>8}\")\n",
    "print(\"-\"*80)\n",
    "for i, row in enumerate(probs):\n",
    "    row_sum = sum(row)\n",
    "    print(f\"{TOKEN_NAMES[tokens[i]]:<12} {row[0]:>8.4f} {row[1]:>8.4f} {row[2]:>8.4f} {row[3]:>8.4f} {row[4]:>8.4f} {row[5]:>8.4f} {row_sum:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute Loss (Cross-Entropy)\n",
    "\n",
    "Now we need to measure how wrong the model is.\n",
    "\n",
    "The metric is **cross-entropy loss**:\n",
    "\n",
    "$$L = -\\log P(\\text{correct\\_token})$$\n",
    "\n",
    "- If model is confident and correct (P = 1.0): loss = 0 → perfect!\n",
    "- If model is uncertain (P = 0.5): loss ≈ 0.69 → okay\n",
    "- If model is wrong and confident (P = 0.1): loss ≈ 2.3 → bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target tokens (what we should predict)\n",
    "# At position i, we predict token i+1\n",
    "targets = [3, 4, 5, 2]  # I, like, transformers, <EOS>\n",
    "\n",
    "print(\"Targets (what the model should predict)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "for i in range(len(targets)):\n",
    "    print(f\"Position {i} ({TOKEN_NAMES[tokens[i]]:12s}) → should predict: {TOKEN_NAMES[targets[i]]} (token {targets[i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-entropy loss\n",
    "losses = []\n",
    "\n",
    "print(\"Cross-Entropy Loss Calculation\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"{'Position':<12} {'Current':<12} {'Target':<12} {'P(target)':>10} {'Loss':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    target = targets[i]\n",
    "    prob_target = probs[i][target]\n",
    "    loss = -math.log(prob_target)\n",
    "    losses.append(loss)\n",
    "    print(f\"{i:<12} {TOKEN_NAMES[tokens[i]]:<12} {TOKEN_NAMES[target]:<12} {prob_target:>10.4f} {loss:>10.4f}\")\n",
    "\n",
    "total_loss = sum(losses)\n",
    "avg_loss = total_loss / len(losses)\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Total':<36} {' ':>10} {total_loss:>10.4f}\")\n",
    "print(f\"{'Average':<36} {' ':>10} {avg_loss:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does This Loss Mean?\n",
    "\n",
    "For reference:\n",
    "- **Random guessing** (uniform over 6 tokens): $-\\log(1/6) \\approx 1.79$\n",
    "- **Perfect prediction**: $-\\log(1.0) = 0.0$\n",
    "\n",
    "Our model's loss is close to random guessing. That's exactly what we'd expect from an **untrained model with random weights**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_loss = -math.log(1/6)\n",
    "print(f\"Our average loss: {avg_loss:.4f}\")\n",
    "print(f\"Random guessing:  {random_loss:.4f}\")\n",
    "print()\n",
    "if avg_loss > random_loss:\n",
    "    print(\"We're slightly worse than random - untrained model, as expected!\")\n",
    "else:\n",
    "    print(\"We're slightly better than random - just luck with initialization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass: Complete!\n",
    "\n",
    "We've computed:\n",
    "\n",
    "1. ✅ **Embeddings** — Converted tokens to vectors\n",
    "2. ✅ **Q/K/V Projections** — Prepared for attention\n",
    "3. ✅ **Attention** — Computed context-aware representations\n",
    "4. ✅ **Multi-head** — Combined multiple attention perspectives\n",
    "5. ✅ **Feed-forward** — Applied non-linear transformations\n",
    "6. ✅ **Layer normalization** — Stabilized activations\n",
    "7. ✅ **Output projection** — Projected to vocabulary space\n",
    "8. ✅ **Loss calculation** — Measured prediction error\n",
    "\n",
    "**The forward pass is done.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next: Backpropagation\n",
    "\n",
    "Now comes the fun part.\n",
    "\n",
    "We know the model is wrong (loss ≈ 1.9). The question is: **how do we fix it?**\n",
    "\n",
    "We need to compute gradients—how much each parameter contributed to the error. Then we'll update those parameters to reduce the loss.\n",
    "\n",
    "This is **backpropagation**: computing gradients by walking backward through the computation graph, applying the chain rule at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store everything for backprop\n",
    "forward_pass_data = {\n",
    "    'tokens': tokens,\n",
    "    'targets': targets,\n",
    "    'X': X,\n",
    "    'layer_norm_output': layer_norm_output,\n",
    "    'logits': logits,\n",
    "    'probs': probs,\n",
    "    'losses': losses,\n",
    "    'avg_loss': avg_loss,\n",
    "    # All weights\n",
    "    'E_token': E_token,\n",
    "    'E_pos': E_pos,\n",
    "    'W_Q': W_Q, 'W_K': W_K, 'W_V': W_V,\n",
    "    'W_O': W_O,\n",
    "    'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,\n",
    "    'W_lm': W_lm,\n",
    "    'gamma': gamma, 'beta': beta\n",
    "}\n",
    "print(f\"Forward pass complete. Loss: {avg_loss:.4f}\")\n",
    "print(\"Data stored for backpropagation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
