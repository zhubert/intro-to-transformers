{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Step of the Forward Pass\n",
    "\n",
    "We've come a long way. Our input text has been tokenized, embedded, processed through attention, transformed by the feed-forward network, and stabilized with layer normalization. Each token now has a 16-dimensional representation that encodes both its identity and its context.\n",
    "\n",
    "But **the model hasn't actually made any predictions yet**.\n",
    "\n",
    "We have hidden states. rich, context-aware vectors. But we need *probabilities*. We need the model to tell us: \"Given everything I've seen so far, what token should come next?\"\n",
    "\n",
    "This notebook covers the final two operations of the forward pass:\n",
    "1. **Output projection**: Convert 16-dimensional hidden states to 6-dimensional logits (one score per vocabulary token)\n",
    "2. **Loss computation**: Measure how wrong our predictions are using cross-entropy\n",
    "\n",
    "The loss is the single number that tells us how badly the model performed. It's what we'll be minimizing during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Language Modeling Task\n",
    "\n",
    "Before we explore the math, let's be crystal clear about what our model is trying to do.\n",
    "\n",
    "A **language model** predicts the next token given all previous tokens. This is called **autoregressive** generation. each prediction depends only on what came before, not what comes after.\n",
    "\n",
    "Our input sequence is:\n",
    "\n",
    "```\n",
    "Text:       <BOS>  I   like  transformers  <EOS>\n",
    "Token IDs:  [1,    3,  4,    5,            2]\n",
    "Positions:  [0,    1,  2,    3,            4]\n",
    "```\n",
    "\n",
    "At each position, the model must predict what comes next:\n",
    "\n",
    "| Position | Input Token | Target (Next Token) |\n",
    "|----------|-------------|--------------------|\n",
    "| 0 | `<BOS>` | `I` (token 3) |\n",
    "| 1 | `I` | `like` (token 4) |\n",
    "| 2 | `like` | `transformers` (token 5) |\n",
    "| 3 | `transformers` | `<EOS>` (token 2) |\n",
    "| 4 | `<EOS>` |. (sequence ended) |\n",
    "\n",
    "Position 4 is the end-of-sequence marker. There's nothing to predict after that. So we have 4 predictions to make and 4 losses to compute.\n",
    "\n",
    "(This is why the causal mask in attention was so important. when predicting at position 2, the model can only see positions 0, 1, and 2. It can't peek ahead at \"transformers\" or \"<EOS>\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.158175Z",
     "iopub.status.busy": "2026-01-22T02:58:45.158175Z",
     "iopub.status.idle": "2026-01-22T02:58:45.164045Z",
     "shell.execute_reply": "2026-01-22T02:58:45.164045Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "D_FF = 64\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS\n",
    "EPSILON = 1e-5\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.164045Z",
     "iopub.status.busy": "2026-01-22T02:58:45.164045Z",
     "iopub.status.idle": "2026-01-22T02:58:45.171637Z",
     "shell.execute_reply": "2026-01-22T02:58:45.171637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n = len(A), len(A[0])\n",
    "    p = len(B[0])\n",
    "    return [[sum(A[i][k] * B[k][j] for k in range(n)) for j in range(p)] for i in range(m)]\n",
    "\n",
    "def transpose(A):\n",
    "    return [[A[i][j] for i in range(len(A))] for j in range(len(A[0]))]\n",
    "\n",
    "def softmax(vec):\n",
    "    max_val = max(vec)\n",
    "    exp_vec = [math.exp(v - max_val) for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def softmax_causal(vec):\n",
    "    max_val = max(v for v in vec if v != float('-inf'))\n",
    "    exp_vec = [math.exp(v - max_val) if v != float('-inf') else 0 for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def layer_norm(x, gamma, beta, epsilon=1e-5):\n",
    "    mean = sum(x) / len(x)\n",
    "    variance = sum((xi - mean)**2 for xi in x) / len(x)\n",
    "    std = math.sqrt(variance + epsilon)\n",
    "    x_norm = [(xi - mean) / std for xi in x]\n",
    "    return [gamma[i] * x_norm[i] + beta[i] for i in range(len(x))]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.171637Z",
     "iopub.status.busy": "2026-01-22T02:58:45.171637Z",
     "iopub.status.idle": "2026-01-22T02:58:45.180753Z",
     "shell.execute_reply": "2026-01-22T02:58:45.180753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated full forward pass through transformer block\n",
      "Hidden states shape: [5, 16]\n"
     ]
    }
   ],
   "source": [
    "# Recreate the full forward pass from previous notebooks\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "tokens = [1, 3, 4, 5, 2]\n",
    "seq_len = len(tokens)\n",
    "X = [add_vectors(E_token[tokens[i]], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "# Attention\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "def compute_attention(Q, K, V):\n",
    "    seq_len, d_k = len(Q), len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    scores = matmul(Q, transpose(K))\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j > i:\n",
    "                scaled[i][j] = float('-inf')\n",
    "    weights = [softmax_causal(row) for row in scaled]\n",
    "    return matmul(weights, V)\n",
    "\n",
    "attention_output_all = [compute_attention(Q_all[h], K_all[h], V_all[h]) for h in range(NUM_HEADS)]\n",
    "concat_output = [attention_output_all[0][i] + attention_output_all[1][i] for i in range(seq_len)]\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)\n",
    "multi_head_output = matmul(concat_output, transpose(W_O))\n",
    "\n",
    "# FFN\n",
    "W1 = random_matrix(D_FF, D_MODEL)\n",
    "b1 = random_vector(D_FF)\n",
    "W2 = random_matrix(D_MODEL, D_FF)\n",
    "b2 = random_vector(D_MODEL)\n",
    "hidden = [[sum(multi_head_output[i][k] * W1[j][k] for k in range(D_MODEL)) + b1[j] for j in range(D_FF)] for i in range(seq_len)]\n",
    "activated = [[gelu(h) for h in row] for row in hidden]\n",
    "ffn_output = [[sum(activated[i][k] * W2[j][k] for k in range(D_FF)) + b2[j] for j in range(D_MODEL)] for i in range(seq_len)]\n",
    "\n",
    "# Residual + LayerNorm\n",
    "residual = [add_vectors(multi_head_output[i], ffn_output[i]) for i in range(seq_len)]\n",
    "gamma = [1.0] * D_MODEL\n",
    "beta = [0.0] * D_MODEL\n",
    "layer_norm_output = [layer_norm(residual[i], gamma, beta, EPSILON) for i in range(seq_len)]\n",
    "\n",
    "print(\"Recreated full forward pass through transformer block\")\n",
    "print(f\"Hidden states shape: [{seq_len}, {D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Output Projection (Hidden States → Logits)\n",
    "\n",
    "Our hidden states are 16-dimensional vectors. But our vocabulary has 6 tokens. We need to convert from `d_model = 16` dimensions to `vocab_size = 6` dimensions.\n",
    "\n",
    "This is done by the **language modeling head** (often called the \"LM head\" or \"output projection\"). A simple linear layer with no bias:\n",
    "\n",
    "$$\\text{logits} = \\text{hidden} \\cdot W_{lm}^T$$\n",
    "\n",
    "Where:\n",
    "- $\\text{hidden}$ has shape `[seq_len, d_model]` = `[5, 16]`\n",
    "- $W_{lm}$ has shape `[vocab_size, d_model]` = `[6, 16]`\n",
    "- $\\text{logits}$ has shape `[seq_len, vocab_size]` = `[5, 6]`\n",
    "\n",
    "**What are logits?**\n",
    "\n",
    "Logits are raw, unnormalized scores. Each logit represents how strongly the model believes in that token being the correct prediction. Higher logits = more confidence. But logits can be negative, and they don't sum to 1. they're not probabilities yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.198750Z",
     "iopub.status.busy": "2026-01-22T02:58:45.198750Z",
     "iopub.status.idle": "2026-01-22T02:58:45.202507Z",
     "shell.execute_reply": "2026-01-22T02:58:45.202507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM Head Weight Matrix W_lm\n",
      "Shape: [6, 16]\n",
      "Parameters: 96 = 96\n"
     ]
    }
   ],
   "source": [
    "# Initialize LM head weight matrix\n",
    "W_lm = random_matrix(VOCAB_SIZE, D_MODEL)  # [6, 16]\n",
    "\n",
    "print(f\"LM Head Weight Matrix W_lm\")\n",
    "print(f\"Shape: [{VOCAB_SIZE}, {D_MODEL}]\")\n",
    "print(f\"Parameters: {VOCAB_SIZE * D_MODEL} = 96\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.202507Z",
     "iopub.status.busy": "2026-01-22T02:58:45.202507Z",
     "iopub.status.idle": "2026-01-22T02:58:45.206533Z",
     "shell.execute_reply": "2026-01-22T02:58:45.206533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits (unnormalized scores)\n",
      "Shape: [5, 6]\n",
      "\n",
      "Position        <PAD>    <BOS>    <EOS>        I     like    trans\n",
      "----------------------------------------------------------------------\n",
      "<BOS>          0.3362  -0.1025  -0.0514  -0.0053   0.2907   0.0848\n",
      "I              0.3332  -0.1114  -0.0743  -0.0182   0.2730   0.0714\n",
      "like           0.3179  -0.1641  -0.0885  -0.0041   0.3290   0.0326\n",
      "transformers   0.3252  -0.1279  -0.0646  -0.0031   0.3202   0.0518\n",
      "<EOS>          0.3209  -0.1388  -0.0673   0.0066   0.3317   0.0356\n"
     ]
    }
   ],
   "source": [
    "# Compute logits: hidden_state @ W_lm^T\n",
    "W_lm_T = transpose(W_lm)\n",
    "logits = matmul(layer_norm_output, W_lm_T)\n",
    "\n",
    "print(\"Logits (unnormalized scores)\")\n",
    "print(f\"Shape: [{seq_len}, {VOCAB_SIZE}]\")\n",
    "print()\n",
    "print(f\"{'Position':<12} {'<PAD>':>8} {'<BOS>':>8} {'<EOS>':>8} {'I':>8} {'like':>8} {'trans':>8}\")\n",
    "print(\"-\"*70)\n",
    "for i, row in enumerate(logits):\n",
    "    print(f\"{TOKEN_NAMES[tokens[i]]:<12} {row[0]:>8.4f} {row[1]:>8.4f} {row[2]:>8.4f} {row[3]:>8.4f} {row[4]:>8.4f} {row[5]:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Logits\n",
    "\n",
    "Each row gives us scores for all 6 vocabulary tokens at that position. Look at position 0 (`<BOS>`):\n",
    "\n",
    "The model needs to predict `I` (token 3) here. But with random weights, the scores are random. The model has no idea yet that `<BOS>` should predict `I`.\n",
    "\n",
    "Let's see a detailed calculation for one logit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.206533Z",
     "iopub.status.busy": "2026-01-22T02:58:45.206533Z",
     "iopub.status.idle": "2026-01-22T02:58:45.210768Z",
     "shell.execute_reply": "2026-01-22T02:58:45.210768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed: Computing logits[0][3] (probability score for 'I' at position 0)\n",
      "======================================================================\n",
      "\n",
      "logits[0][3] = hidden[0] · W_lm[3]\n",
      "\n",
      "hidden[0] (16 dims): [-0.1319, -1.2702, -0.4969,  1.5600,  0.6278,  0.4769, -2.2261,  0.1465,  1.5670,  0.2124, -0.6301, -0.9558,  0.8655, -0.7691,  0.9595,  0.0645]\n",
      "\n",
      "W_lm[3] (16 dims):   [-0.0919,  0.0860, -0.0569,  0.0092, -0.0476, -0.1328, -0.0465,  0.1397, -0.0415, -0.0676, -0.0428, -0.0335,  0.0229, -0.0412, -0.0098, -0.0491]\n",
      "\n",
      "Dot product = -0.005250\n",
      "Actual logits[0][3] = -0.005250\n"
     ]
    }
   ],
   "source": [
    "# Detailed calculation for logits[0][3] (position 0 predicting token 3 \"I\")\n",
    "print(\"Detailed: Computing logits[0][3] (probability score for 'I' at position 0)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"logits[0][3] = hidden[0] · W_lm[3]\")\n",
    "print()\n",
    "print(f\"hidden[0] (16 dims): {format_vector(layer_norm_output[0])}\")\n",
    "print()\n",
    "print(f\"W_lm[3] (16 dims):   {format_vector(W_lm[3])}\")\n",
    "print()\n",
    "dot_product = sum(layer_norm_output[0][j] * W_lm[3][j] for j in range(D_MODEL))\n",
    "print(f\"Dot product = {dot_product:.6f}\")\n",
    "print(f\"Actual logits[0][3] = {logits[0][3]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Softmax (Logits → Probabilities)\n",
    "\n",
    "Logits are useful for computation, but humans think in probabilities and the loss function needs probabilities.\n",
    "\n",
    "**Softmax** converts arbitrary real numbers into a valid probability distribution:\n",
    "\n",
    "$$P(\\text{token}_i) = \\frac{\\exp(\\text{logit}_i)}{\\sum_{j=1}^{V} \\exp(\\text{logit}_j)}$$\n",
    "\n",
    "Where $V$ is the vocabulary size (6 in our case).\n",
    "\n",
    "**Why softmax works:**\n",
    "\n",
    "1. **Exponentiation** ($\\exp$) makes all values positive\n",
    "2. **Dividing by the sum** ensures everything adds to 1\n",
    "3. **Larger logits dominate** because exp grows exponentially\n",
    "\n",
    "If one logit is much larger than the others, it will have probability close to 1. If all logits are similar, probabilities will be uniform.\n",
    "\n",
    "**Numerical stability:**\n",
    "\n",
    "In practice, we subtract the maximum logit before exponentiating:\n",
    "\n",
    "$$P(\\text{token}_i) = \\frac{\\exp(\\text{logit}_i - \\max(\\text{logits}))}{\\sum_{j} \\exp(\\text{logit}_j - \\max(\\text{logits}))}$$\n",
    "\n",
    "This prevents overflow when logits are large. It doesn't change the result because:\n",
    "\n",
    "$$\\frac{\\exp(a - c)}{\\exp(b - c)} = \\frac{\\exp(a)}{\\exp(b)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.210768Z",
     "iopub.status.busy": "2026-01-22T02:58:45.210768Z",
     "iopub.status.idle": "2026-01-22T02:58:45.215118Z",
     "shell.execute_reply": "2026-01-22T02:58:45.215118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed: Softmax for position 0\n",
      "============================================================\n",
      "\n",
      "Logits: [ 0.3362, -0.1025, -0.0514, -0.0053,  0.2907,  0.0848]\n",
      "\n",
      "Step 1: Subtract max for stability\n",
      "  max(logits) = 0.3362\n",
      "  shifted = [ 0.0000, -0.4387, -0.3876, -0.3415, -0.0455, -0.2514]\n",
      "\n",
      "Step 2: Exponentiate\n",
      "  exp(shifted) = [ 1.0000,  0.6449,  0.6787,  0.7107,  0.9555,  0.7777]\n",
      "\n",
      "Step 3: Normalize\n",
      "  sum = 4.7674\n",
      "  probabilities = [ 0.2098,  0.1353,  0.1424,  0.1491,  0.2004,  0.1631]\n",
      "  sum of probs = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Detailed softmax calculation for position 0\n",
    "print(\"Detailed: Softmax for position 0\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Logits: {format_vector(logits[0])}\")\n",
    "print()\n",
    "\n",
    "max_logit = max(logits[0])\n",
    "print(f\"Step 1: Subtract max for stability\")\n",
    "print(f\"  max(logits) = {max_logit:.4f}\")\n",
    "shifted = [l - max_logit for l in logits[0]]\n",
    "print(f\"  shifted = {format_vector(shifted)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Step 2: Exponentiate\")\n",
    "exp_vals = [math.exp(s) for s in shifted]\n",
    "print(f\"  exp(shifted) = {format_vector(exp_vals)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Step 3: Normalize\")\n",
    "sum_exp = sum(exp_vals)\n",
    "print(f\"  sum = {sum_exp:.4f}\")\n",
    "probs_manual = [e / sum_exp for e in exp_vals]\n",
    "print(f\"  probabilities = {format_vector(probs_manual)}\")\n",
    "print(f\"  sum of probs = {sum(probs_manual):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.215118Z",
     "iopub.status.busy": "2026-01-22T02:58:45.215118Z",
     "iopub.status.idle": "2026-01-22T02:58:45.219075Z",
     "shell.execute_reply": "2026-01-22T02:58:45.219075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities (after softmax)\n",
      "Shape: [5, 6]\n",
      "\n",
      "Position        <PAD>    <BOS>    <EOS>        I     like    trans      Sum\n",
      "--------------------------------------------------------------------------------\n",
      "<BOS>          0.2098   0.1353   0.1424   0.1491   0.2004   0.1631   1.0000\n",
      "I              0.2118   0.1358   0.1409   0.1490   0.1994   0.1630   1.0000\n",
      "like           0.2096   0.1294   0.1396   0.1519   0.2119   0.1576   1.0000\n",
      "transformers   0.2088   0.1327   0.1414   0.1504   0.2078   0.1589   1.0000\n",
      "<EOS>          0.2082   0.1315   0.1412   0.1521   0.2105   0.1565   1.0000\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to all positions\n",
    "probs = [softmax(row) for row in logits]\n",
    "\n",
    "print(\"Probabilities (after softmax)\")\n",
    "print(f\"Shape: [{seq_len}, {VOCAB_SIZE}]\")\n",
    "print()\n",
    "print(f\"{'Position':<12} {'<PAD>':>8} {'<BOS>':>8} {'<EOS>':>8} {'I':>8} {'like':>8} {'trans':>8} {'Sum':>8}\")\n",
    "print(\"-\"*80)\n",
    "for i, row in enumerate(probs):\n",
    "    row_sum = sum(row)\n",
    "    print(f\"{TOKEN_NAMES[tokens[i]]:<12} {row[0]:>8.4f} {row[1]:>8.4f} {row[2]:>8.4f} {row[3]:>8.4f} {row[4]:>8.4f} {row[5]:>8.4f} {row_sum:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Probabilities\n",
    "\n",
    "Each row now sums to 1.0000. A valid probability distribution. Look at position 0 (`<BOS>`):\n",
    "\n",
    "The model assigns about equal probability to all tokens. It gives `<PAD>` the highest probability (~21%) and `<BOS>` the lowest (~14%). But it should be predicting `I` with high confidence.\n",
    "\n",
    "This is what an untrained model looks like: random guessing. The probabilities are roughly uniform because the random weights don't encode any useful patterns yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Cross-Entropy Loss\n",
    "\n",
    "Now we need a single number that measures how wrong our predictions are. This is the **loss function**.\n",
    "\n",
    "For language modeling, we use **cross-entropy loss**:\n",
    "\n",
    "$$L = -\\log P(\\text{correct token})$$\n",
    "\n",
    "That's it. Take the probability the model assigned to the correct answer, take its logarithm, and negate it.\n",
    "\n",
    "**Why does this make sense?**\n",
    "\n",
    "- If the model is **confident and correct** ($P = 0.99$): $L = -\\log(0.99) \\approx 0.01$ (low loss, good!)\n",
    "- If the model is **uncertain** ($P = 0.5$): $L = -\\log(0.5) \\approx 0.69$ (medium loss)\n",
    "- If the model is **wrong** ($P = 0.01$): $L = -\\log(0.01) \\approx 4.6$ (high loss, bad!)\n",
    "- If the model is **completely wrong** ($P \\to 0$): $L \\to \\infty$ (catastrophic)\n",
    "\n",
    "The negative log has nice properties:\n",
    "1. It's always positive (since probabilities are between 0 and 1)\n",
    "2. It's 0 when we're perfectly confident and correct\n",
    "3. It goes to infinity as we become more wrong\n",
    "4. It penalizes confident wrong answers more than uncertain ones\n",
    "\n",
    "**Connection to information theory:**\n",
    "\n",
    "Cross-entropy has deep roots in information theory. $-\\log_2 P$ is the number of bits needed to encode an event with probability $P$. Minimizing cross-entropy is equivalent to maximizing the likelihood of the data under the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.219075Z",
     "iopub.status.busy": "2026-01-22T02:58:45.219075Z",
     "iopub.status.idle": "2026-01-22T02:58:45.223021Z",
     "shell.execute_reply": "2026-01-22T02:58:45.223021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets (what the model should predict)\n",
      "============================================================\n",
      "\n",
      "Position 0: <BOS>        → should predict: I (token 3)\n",
      "Position 1: I            → should predict: like (token 4)\n",
      "Position 2: like         → should predict: transformers (token 5)\n",
      "Position 3: transformers → should predict: <EOS> (token 2)\n"
     ]
    }
   ],
   "source": [
    "# Define target tokens (what we should predict)\n",
    "# At position i, we predict token i+1\n",
    "targets = [3, 4, 5, 2]  # I, like, transformers, <EOS>\n",
    "\n",
    "print(\"Targets (what the model should predict)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "for i in range(len(targets)):\n",
    "    print(f\"Position {i}: {TOKEN_NAMES[tokens[i]]:12s} → should predict: {TOKEN_NAMES[targets[i]]} (token {targets[i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.223021Z",
     "iopub.status.busy": "2026-01-22T02:58:45.223021Z",
     "iopub.status.idle": "2026-01-22T02:58:45.226764Z",
     "shell.execute_reply": "2026-01-22T02:58:45.226764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed: Cross-entropy loss for position 0\n",
      "============================================================\n",
      "\n",
      "Current token: <BOS> (position 0)\n",
      "Target token: I (token ID 3)\n",
      "\n",
      "Probability distribution: [ 0.2098,  0.1353,  0.1424,  0.1491,  0.2004,  0.1631]\n",
      "                          <PAD>  <BOS>  <EOS>    I     like  trans\n",
      "\n",
      "P(target) = P('I') = probs[0][3] = 0.149082\n",
      "\n",
      "Loss = -log(0.149082) = 1.903260\n"
     ]
    }
   ],
   "source": [
    "# Detailed cross-entropy calculation for position 0\n",
    "print(\"Detailed: Cross-entropy loss for position 0\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Current token: {TOKEN_NAMES[tokens[0]]} (position 0)\")\n",
    "print(f\"Target token: {TOKEN_NAMES[targets[0]]} (token ID {targets[0]})\")\n",
    "print()\n",
    "print(f\"Probability distribution: {format_vector(probs[0])}\")\n",
    "print(f\"                          <PAD>  <BOS>  <EOS>    I     like  trans\")\n",
    "print()\n",
    "prob_target = probs[0][targets[0]]\n",
    "print(f\"P(target) = P('I') = probs[0][3] = {prob_target:.6f}\")\n",
    "print()\n",
    "loss_0 = -math.log(prob_target)\n",
    "print(f\"Loss = -log({prob_target:.6f}) = {loss_0:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.226764Z",
     "iopub.status.busy": "2026-01-22T02:58:45.226764Z",
     "iopub.status.idle": "2026-01-22T02:58:45.230935Z",
     "shell.execute_reply": "2026-01-22T02:58:45.230935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss Calculation\n",
      "======================================================================\n",
      "\n",
      "Position     Current      Target        P(target)       Loss\n",
      "----------------------------------------------------------------------\n",
      "0            <BOS>        I                0.1491     1.9033\n",
      "1            I            like             0.1994     1.6123\n",
      "2            like         transformers     0.1576     1.8479\n",
      "3            transformers <EOS>            0.1414     1.9560\n",
      "----------------------------------------------------------------------\n",
      "Total loss                                          7.3195\n",
      "Average loss                                        1.8299\n"
     ]
    }
   ],
   "source": [
    "# Compute cross-entropy loss for all positions\n",
    "losses = []\n",
    "\n",
    "print(\"Cross-Entropy Loss Calculation\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"{'Position':<12} {'Current':<12} {'Target':<12} {'P(target)':>10} {'Loss':>10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    target = targets[i]\n",
    "    prob_target = probs[i][target]\n",
    "    loss = -math.log(prob_target)\n",
    "    losses.append(loss)\n",
    "    print(f\"{i:<12} {TOKEN_NAMES[tokens[i]]:<12} {TOKEN_NAMES[target]:<12} {prob_target:>10.4f} {loss:>10.4f}\")\n",
    "\n",
    "total_loss = sum(losses)\n",
    "avg_loss = total_loss / len(losses)\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Total loss':<36} {' ':>10} {total_loss:>10.4f}\")\n",
    "print(f\"{'Average loss':<36} {' ':>10} {avg_loss:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Loss\n",
    "\n",
    "Our average loss is about 1.83. Is that good or bad?\n",
    "\n",
    "**Baseline: random guessing**\n",
    "\n",
    "If the model assigned equal probability to all 6 tokens (uniform distribution), the probability of any token would be $1/6 \\approx 0.167$.\n",
    "\n",
    "The loss for random guessing:\n",
    "$$L_{\\text{random}} = -\\log(1/6) = \\log(6) \\approx 1.79$$\n",
    "\n",
    "**What our loss tells us:**\n",
    "\n",
    "Our model's loss (~1.83) is *slightly worse* than random guessing. That's exactly what we'd expect from an untrained model with random weights. The model hasn't learned anything yet. it's flipping a 6-sided die.\n",
    "\n",
    "**What we want:**\n",
    "\n",
    "A well-trained model should have loss close to 0, meaning it predicts the correct next token with high probability.\n",
    "\n",
    "| Loss Value | Interpretation |\n",
    "|------------|----------------|\n",
    "| ~1.79 | Random guessing (uniform over 6 tokens) |\n",
    "| ~1.0 | Model has learned some patterns |\n",
    "| ~0.5 | Model is fairly confident and usually correct |\n",
    "| ~0.1 | Model is very good at this task |\n",
    "| ~0.0 | Perfect predictions (never happens in practice) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.230935Z",
     "iopub.status.busy": "2026-01-22T02:58:45.230935Z",
     "iopub.status.idle": "2026-01-22T02:58:45.234442Z",
     "shell.execute_reply": "2026-01-22T02:58:45.234442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Comparison\n",
      "========================================\n",
      "Our model's average loss: 1.8299\n",
      "Random guessing loss:     1.7918\n",
      "\n",
      "We're 0.0381 worse than random guessing.\n",
      "This is expected for an untrained model!\n"
     ]
    }
   ],
   "source": [
    "# Compare our loss to random guessing\n",
    "random_loss = -math.log(1/VOCAB_SIZE)\n",
    "\n",
    "print(\"Loss Comparison\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Our model's average loss: {avg_loss:.4f}\")\n",
    "print(f\"Random guessing loss:     {random_loss:.4f}\")\n",
    "print()\n",
    "\n",
    "if avg_loss > random_loss:\n",
    "    diff = avg_loss - random_loss\n",
    "    print(f\"We're {diff:.4f} worse than random guessing.\")\n",
    "    print(\"This is expected for an untrained model!\")\n",
    "else:\n",
    "    diff = random_loss - avg_loss\n",
    "    print(f\"We're {diff:.4f} better than random guessing.\")\n",
    "    print(\"Got lucky with weight initialization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity: A More Intuitive Metric\n",
    "\n",
    "Loss values like 1.83 are hard to interpret. **Perplexity** is a more intuitive alternative.\n",
    "\n",
    "$$\\text{Perplexity} = \\exp(\\text{loss})$$\n",
    "\n",
    "Perplexity can be thought of as \"the effective number of choices the model is considering.\" A perplexity of 6 means the model is as uncertain as if it were choosing uniformly among 6 options.\n",
    "\n",
    "| Loss | Perplexity | Interpretation |\n",
    "|------|------------|----------------|\n",
    "| 1.79 | 6.0 | Random among 6 tokens |\n",
    "| 1.10 | 3.0 | Narrowed to ~3 likely tokens |\n",
    "| 0.69 | 2.0 | Coin flip between 2 tokens |\n",
    "| 0.10 | 1.1 | Very confident, ~1 choice |\n",
    "| 0.00 | 1.0 | Perfect certainty |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.235509Z",
     "iopub.status.busy": "2026-01-22T02:58:45.235509Z",
     "iopub.status.idle": "2026-01-22T02:58:45.237803Z",
     "shell.execute_reply": "2026-01-22T02:58:45.237803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity\n",
      "========================================\n",
      "Our perplexity: 6.23\n",
      "Random perplexity: 6.00\n",
      "\n",
      "The model is as uncertain as if it were choosing\n",
      "uniformly among 6.2 tokens.\n"
     ]
    }
   ],
   "source": [
    "perplexity = math.exp(avg_loss)\n",
    "random_perplexity = math.exp(random_loss)\n",
    "\n",
    "print(\"Perplexity\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Our perplexity: {perplexity:.2f}\")\n",
    "print(f\"Random perplexity: {random_perplexity:.2f}\")\n",
    "print()\n",
    "print(f\"The model is as uncertain as if it were choosing\")\n",
    "print(f\"uniformly among {perplexity:.1f} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Forward Pass is Complete!\n",
    "\n",
    "We've traced the entire forward pass from text to loss:\n",
    "\n",
    "```\n",
    "\"I like transformers\"\n",
    "        ↓\n",
    "    [Tokenization]        → [1, 3, 4, 5, 2]\n",
    "        ↓\n",
    "    [Embeddings]          → [5, 16] matrix (token + position)\n",
    "        ↓\n",
    "    [Q/K/V Projections]   → Query, Key, Value matrices\n",
    "        ↓\n",
    "    [Attention]           → Context-aware representations\n",
    "        ↓\n",
    "    [Multi-head]          → Combined from 2 heads\n",
    "        ↓\n",
    "    [Feed-forward]        → Non-linear transformations\n",
    "        ↓\n",
    "    [Layer norm]          → Stabilized activations\n",
    "        ↓\n",
    "    [Output projection]   → [5, 6] logits\n",
    "        ↓\n",
    "    [Softmax]             → [5, 6] probabilities\n",
    "        ↓\n",
    "    [Cross-entropy]       → Loss = 1.83\n",
    "```\n",
    "\n",
    "The loss (1.83) tells us the model is performing at random-guessing level. That's our starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next: Backpropagation\n",
    "\n",
    "We know the model is wrong. The question now is: **how do we make it less wrong?**\n",
    "\n",
    "We have about 2,600 parameters (embedding matrices, attention weights, FFN weights, layer norm parameters, LM head). Each one contributes somehow to the final loss.\n",
    "\n",
    "**Backpropagation** answers the question: \"For each parameter, if I nudged it slightly, how much would the loss change?\"\n",
    "\n",
    "This \"how much would the loss change\" is the **gradient**. Once we have gradients for all parameters, we can update them to reduce the loss.\n",
    "\n",
    "The next notebook starts the backward pass: computing gradients by walking backward through the computation graph, applying the chain rule at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:45.237803Z",
     "iopub.status.busy": "2026-01-22T02:58:45.237803Z",
     "iopub.status.idle": "2026-01-22T02:58:45.241662Z",
     "shell.execute_reply": "2026-01-22T02:58:45.241662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass complete.\n",
      "Average loss: 1.8299\n",
      "Perplexity: 6.23\n",
      "\n",
      "Data stored for backpropagation.\n"
     ]
    }
   ],
   "source": [
    "# Store everything for backpropagation\n",
    "forward_pass_data = {\n",
    "    'tokens': tokens,\n",
    "    'targets': targets,\n",
    "    'X': X,\n",
    "    'layer_norm_output': layer_norm_output,\n",
    "    'logits': logits,\n",
    "    'probs': probs,\n",
    "    'losses': losses,\n",
    "    'avg_loss': avg_loss,\n",
    "    # All weights\n",
    "    'E_token': E_token,\n",
    "    'E_pos': E_pos,\n",
    "    'W_Q': W_Q, 'W_K': W_K, 'W_V': W_V,\n",
    "    'W_O': W_O,\n",
    "    'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,\n",
    "    'W_lm': W_lm,\n",
    "    'gamma': gamma, 'beta': beta\n",
    "}\n",
    "\n",
    "print(f\"Forward pass complete.\")\n",
    "print(f\"Average loss: {avg_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "print()\n",
    "print(\"Data stored for backpropagation.\")"
   ]
  }
 ],
 "metadata": {
  "description": "Projects to vocabulary, applies softmax, computes cross-entropy loss for next-token prediction with full calculations.",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
