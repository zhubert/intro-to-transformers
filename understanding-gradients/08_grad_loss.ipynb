{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Gradients: Loss\n",
    "\n",
    "**Computing gradients of the loss with respect to logits**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. Time for backpropagation.\n",
    "\n",
    "We've got a loss of ~1.9—roughly random guessing. Now we need to figure out how to improve it. That means computing **gradients**.\n",
    "\n",
    "Gradients tell us: \"if we nudge this parameter by a tiny amount, how much does the loss change?\" Once we know that for every parameter, we can adjust them in the direction that reduces loss.\n",
    "\n",
    "This is **backpropagation**—walking backward through the computation graph, computing gradients via the chain rule.\n",
    "\n",
    "Let's start at the end: the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Beautiful Formula\n",
    "\n",
    "For cross-entropy loss with softmax, the gradient has an incredibly clean closed form:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\text{logit}_i} = P(i) - \\mathbb{1}[i = \\text{target}]$$\n",
    "\n",
    "Where:\n",
    "- $P(i)$ = softmax probability for token $i$\n",
    "- $\\mathbb{1}[i = \\text{target}]$ = 1 if $i$ is the correct token, 0 otherwise\n",
    "\n",
    "**That's it.**\n",
    "\n",
    "For the correct class: gradient = $P(\\text{target}) - 1$ (negative)\n",
    "\n",
    "For all other classes: gradient = $P(i)$ (positive)\n",
    "\n",
    "This is one of the most elegant results in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "D_FF = 64\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets for each position:\n",
      "  Position 0 (<BOS>) -> I\n",
      "  Position 1 (I) -> like\n",
      "  Position 2 (like) -> transformers\n",
      "  Position 3 (transformers) -> <EOS>\n"
     ]
    }
   ],
   "source": [
    "# For this notebook, we'll use pre-computed probabilities (from forward pass)\n",
    "# These are what a random model would produce\n",
    "probs = [\n",
    "    [0.1785, 0.2007, 0.1759, 0.1254, 0.1563, 0.1632],  # pos 0\n",
    "    [0.1836, 0.1969, 0.1805, 0.1233, 0.1500, 0.1657],  # pos 1\n",
    "    [0.1795, 0.2050, 0.1782, 0.1207, 0.1437, 0.1728],  # pos 2\n",
    "    [0.1855, 0.2017, 0.1771, 0.1271, 0.1391, 0.1695],  # pos 3\n",
    "]\n",
    "\n",
    "targets = [3, 4, 5, 2]  # I, like, transformers, <EOS>\n",
    "tokens = [1, 3, 4, 5, 2]\n",
    "\n",
    "print(\"Targets for each position:\")\n",
    "for i, t in enumerate(targets):\n",
    "    print(f\"  Position {i} ({TOKEN_NAMES[tokens[i]]}) -> {TOKEN_NAMES[t]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Makes Sense\n",
    "\n",
    "**Gradients point in the direction of INCREASING loss.**\n",
    "\n",
    "During gradient descent, we do: `logit = logit - learning_rate × gradient`\n",
    "\n",
    "So:\n",
    "- **For the correct class**: gradient is negative ($P - 1 < 0$)\n",
    "  - Subtracting a negative = adding → **INCREASE** this logit ✓\n",
    "- **For incorrect classes**: gradient is positive ($P > 0$)\n",
    "  - Subtracting a positive → **DECREASE** these logits ✓\n",
    "\n",
    "We want to push the correct class's logit up and all others down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Gradients w.r.t. Logits\n",
      "======================================================================\n",
      "\n",
      "Position        <PAD>    <BOS>    <EOS>        I     like    trans\n",
      "----------------------------------------------------------------------\n",
      "<BOS>          0.1785   0.2007   0.1759  -0.8746   0.1563   0.1632\n",
      "I              0.1836   0.1969   0.1805   0.1233  -0.8500   0.1657\n",
      "like           0.1795   0.2050   0.1782   0.1207   0.1437  -0.8272\n",
      "transformers   0.1855   0.2017  -0.8229   0.1271   0.1391   0.1695\n"
     ]
    }
   ],
   "source": [
    "def compute_loss_gradient(probs, target):\n",
    "    \"\"\"\n",
    "    Compute gradient of cross-entropy loss w.r.t. logits.\n",
    "    dL/dlogit[i] = P(i) - 1 if i == target, else P(i)\n",
    "    \"\"\"\n",
    "    grad = probs.copy()\n",
    "    grad[target] -= 1.0\n",
    "    return grad\n",
    "\n",
    "# Compute gradients for all positions\n",
    "dL_dlogits = []\n",
    "for i in range(len(targets)):\n",
    "    grad = compute_loss_gradient(probs[i], targets[i])\n",
    "    dL_dlogits.append(grad)\n",
    "\n",
    "print(\"Loss Gradients w.r.t. Logits\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"{'Position':<12} {'<PAD>':>8} {'<BOS>':>8} {'<EOS>':>8} {'I':>8} {'like':>8} {'trans':>8}\")\n",
    "print(\"-\"*70)\n",
    "for i, grad in enumerate(dL_dlogits):\n",
    "    print(f\"{TOKEN_NAMES[tokens[i]]:<12} {grad[0]:>8.4f} {grad[1]:>8.4f} {grad[2]:>8.4f} {grad[3]:>8.4f} {grad[4]:>8.4f} {grad[5]:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed: Position 0 (<BOS> → I)\n",
      "============================================================\n",
      "\n",
      "Current probabilities:\n",
      "  P(<PAD>       ) = 0.1785 \n",
      "  P(<BOS>       ) = 0.2007 \n",
      "  P(<EOS>       ) = 0.1759 \n",
      "  P(I           ) = 0.1254 ← target\n",
      "  P(like        ) = 0.1563 \n",
      "  P(transformers) = 0.1632 \n",
      "\n",
      "Gradients (P(i) - 1[i==target]):\n",
      "  dL/dlogit[<PAD>       ] = 0.1785 - 0 =   0.1785\n",
      "  dL/dlogit[<BOS>       ] = 0.2007 - 0 =   0.2007\n",
      "  dL/dlogit[<EOS>       ] = 0.1759 - 0 =   0.1759\n",
      "  dL/dlogit[I           ] = 0.1254 - 1 =  -0.8746\n",
      "  dL/dlogit[like        ] = 0.1563 - 0 =   0.1563\n",
      "  dL/dlogit[transformers] = 0.1632 - 0 =   0.1632\n"
     ]
    }
   ],
   "source": [
    "# Detailed example for position 0\n",
    "print(\"Detailed: Position 0 (<BOS> → I)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Current probabilities:\")\n",
    "for j, name in enumerate(TOKEN_NAMES):\n",
    "    marker = \"← target\" if j == targets[0] else \"\"\n",
    "    print(f\"  P({name:12s}) = {probs[0][j]:.4f} {marker}\")\n",
    "print()\n",
    "print(\"Gradients (P(i) - 1[i==target]):\")\n",
    "for j, name in enumerate(TOKEN_NAMES):\n",
    "    is_target = 1 if j == targets[0] else 0\n",
    "    grad = probs[0][j] - is_target\n",
    "    print(f\"  dL/dlogit[{name:12s}] = {probs[0][j]:.4f} - {is_target} = {grad:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Gradients Sum to Zero\n",
    "\n",
    "The gradients should sum to zero at each position:\n",
    "\n",
    "$$\\sum_i \\frac{\\partial L}{\\partial \\text{logit}_i} = \\sum_i P(i) - 1 = 1 - 1 = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Gradients sum to zero\n",
      "\n",
      "Position 0: sum(gradients) = -0.0000000000 ✓\n",
      "Position 1: sum(gradients) = 0.0000000000 ✓\n",
      "Position 2: sum(gradients) = -0.0001000000 ✗\n",
      "Position 3: sum(gradients) = 0.0000000000 ✓\n"
     ]
    }
   ],
   "source": [
    "print(\"Verification: Gradients sum to zero\")\n",
    "print()\n",
    "for i, grad in enumerate(dL_dlogits):\n",
    "    grad_sum = sum(grad)\n",
    "    print(f\"Position {i}: sum(gradients) = {grad_sum:12.10f} {'✓' if abs(grad_sum) < 1e-6 else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude Matters\n",
    "\n",
    "Notice the magnitudes:\n",
    "- Target gradient: ~-0.85 (large negative)\n",
    "- Non-target gradients: ~0.15-0.20 (small positive)\n",
    "\n",
    "The target gradient is **much larger** in magnitude because:\n",
    "- The model is getting it wrong (low probability for correct answer)\n",
    "- We need a strong signal to push it in the right direction\n",
    "\n",
    "As the model improves and assigns higher probability to the correct token, that gradient will shrink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "We've computed $\\frac{\\partial L}{\\partial \\text{logits}}$.\n",
    "\n",
    "But we can't update the logits directly—they're computed from the hidden states via the language modeling head.\n",
    "\n",
    "To backpropagate further, we need:\n",
    "1. **Gradients for $W_{lm}$** (the language modeling head weights)\n",
    "2. **Gradients for $h$** (the hidden states going into the LM head)\n",
    "\n",
    "Then we'll continue backward through layer norm, FFN, attention, and finally the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gradients stored for next notebook.\n"
     ]
    }
   ],
   "source": [
    "# Store for next notebook\n",
    "grad_loss_data = {\n",
    "    'dL_dlogits': dL_dlogits,\n",
    "    'probs': probs,\n",
    "    'targets': targets\n",
    "}\n",
    "print(\"Loss gradients stored for next notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
