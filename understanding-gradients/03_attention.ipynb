{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "**The core mechanism that lets tokens communicate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## This Is the Important Part\n",
    "\n",
    "If transformers have a \"secret sauce,\" this is it.\n",
    "\n",
    "Attention is what lets the model understand context. It's how the word \"bank\" can mean different things in \"river bank\" vs \"bank account.\" It's how pronouns find their antecedents. It's how relationships between distant words get captured.\n",
    "\n",
    "We have our Q, K, V projections from the previous notebook. Now we use them to actually compute attention: how much should each token pay attention to each other token?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Attention Formula\n",
    "\n",
    "The full attention computation is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "That's dense. Let's break it into five steps:\n",
    "\n",
    "1. **Compute raw scores**: $\\text{scores} = Q \\cdot K^T$\n",
    "2. **Scale**: $\\text{scaled} = \\frac{\\text{scores}}{\\sqrt{d_k}}$\n",
    "3. **Apply causal mask**: Set future positions to $-\\infty$\n",
    "4. **Softmax**: Convert to probabilities\n",
    "5. **Weighted sum**: $\\text{output} = \\text{weights} \\cdot V$\n",
    "\n",
    "Each step has a purpose. Let's go through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.656049Z",
     "iopub.status.busy": "2025-12-10T21:16:59.655971Z",
     "iopub.status.idle": "2025-12-10T21:16:59.658217Z",
     "shell.execute_reply": "2025-12-10T21:16:59.657952Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Model dimensions\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS  # 8\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.658945Z",
     "iopub.status.busy": "2025-12-10T21:16:59.658874Z",
     "iopub.status.idle": "2025-12-10T21:16:59.661746Z",
     "shell.execute_reply": "2025-12-10T21:16:59.661455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n, p = len(A), len(A[0]), len(B[0])\n",
    "    return [[sum(A[i][k] * B[k][j] for k in range(n)) for j in range(p)] for i in range(m)]\n",
    "\n",
    "def transpose(A):\n",
    "    \"\"\"Transpose a matrix: swap rows and columns\"\"\"\n",
    "    rows, cols = len(A), len(A[0])\n",
    "    return [[A[i][j] for i in range(rows)] for j in range(cols)]\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    \"\"\"Compute dot product of two vectors\"\"\"\n",
    "    return sum(a * b for a, b in zip(v1, v2))\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.662506Z",
     "iopub.status.busy": "2025-12-10T21:16:59.662431Z",
     "iopub.status.idle": "2025-12-10T21:16:59.665340Z",
     "shell.execute_reply": "2025-12-10T21:16:59.664980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated Q, K, V from previous notebooks\n",
      "Shape of each: [5, 8]\n"
     ]
    }
   ],
   "source": [
    "# Recreate Q, K, V from previous notebooks\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "tokens = [1, 3, 4, 5, 2]  # <BOS>, I, like, transformers, <EOS>\n",
    "seq_len = len(tokens)\n",
    "X = [add_vectors(E_token[tokens[i]], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "# QKV weights and projections\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "print(f\"Recreated Q, K, V from previous notebooks\")\n",
    "print(f\"Shape of each: [{seq_len}, {D_K}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 1: Compute Attention Scores\n",
    "\n",
    "The first step is to compute how well each query matches each key. We do this with a matrix multiplication:\n",
    "\n",
    "$$\\text{scores} = Q \\cdot K^T$$\n",
    "\n",
    "**Shapes:**\n",
    "- $Q$: `[5, 8]` — 5 queries, each 8-dimensional\n",
    "- $K^T$: `[8, 5]` — transpose of K (5 keys, each 8-dimensional)\n",
    "- $\\text{scores}$: `[5, 5]` — score for each (query, key) pair\n",
    "\n",
    "The element $\\text{scores}_{ij}$ is the dot product of query $i$ with key $j$. It measures: \"how well does token $i$'s query match token $j$'s key?\"\n",
    "\n",
    "Higher score = better match = more attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.666191Z",
     "iopub.status.busy": "2025-12-10T21:16:59.666097Z",
     "iopub.status.idle": "2025-12-10T21:16:59.668616Z",
     "shell.execute_reply": "2025-12-10T21:16:59.668352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 0: Attention Scores (Q @ K^T)\n",
      "Shape: [5, 8] @ [8, 5] = [5, 5]\n",
      "\n",
      "Each row is one query; each column is one key.\n",
      "scores[i][j] = how much should token i attend to token j?\n",
      "\n",
      "            <BOS>         I      like  transformers     <EOS>\n",
      "   <BOS> [ -0.0126    0.0213   -0.0152    0.0211   -0.0137]\n",
      "       I [  0.0021   -0.0134    0.0119   -0.0027    0.0091]\n",
      "    like [ -0.0140    0.0097   -0.0039    0.0169   -0.0061]\n",
      "transformers [ -0.0018   -0.0119    0.0046   -0.0016    0.0088]\n",
      "   <EOS> [ -0.0022    0.0084   -0.0022   -0.0016   -0.0069]\n"
     ]
    }
   ],
   "source": [
    "# Work with Head 0 for this walkthrough\n",
    "head = 0\n",
    "Q = Q_all[head]\n",
    "K = K_all[head]\n",
    "V = V_all[head]\n",
    "\n",
    "# Compute scores = Q @ K^T\n",
    "K_T = transpose(K)\n",
    "scores = matmul(Q, K_T)\n",
    "\n",
    "print(f\"HEAD {head}: Attention Scores (Q @ K^T)\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}] @ [{D_K}, {seq_len}] = [{seq_len}, {seq_len}]\")\n",
    "print()\n",
    "print(\"Each row is one query; each column is one key.\")\n",
    "print(\"scores[i][j] = how much should token i attend to token j?\")\n",
    "print()\n",
    "\n",
    "# Print with token labels\n",
    "header = \"         \" + \"  \".join([f\"{TOKEN_NAMES[tokens[j]]:>8}\" for j in range(seq_len)])\n",
    "print(header)\n",
    "for i, row in enumerate(scores):\n",
    "    values = \"  \".join([f\"{v:8.4f}\" for v in row])\n",
    "    print(f\"{TOKEN_NAMES[tokens[i]]:>8} [{values}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### What Does a Score Mean?\n",
    "\n",
    "Let's look at one specific score: how much should \"I\" (position 1) attend to \"\\<BOS\\>\" (position 0)?\n",
    "\n",
    "This is the dot product of \"I\"'s query with \"\\<BOS\\>\"'s key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.669417Z",
     "iopub.status.busy": "2025-12-10T21:16:59.669341Z",
     "iopub.status.idle": "2025-12-10T21:16:59.671595Z",
     "shell.execute_reply": "2025-12-10T21:16:59.671329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing scores[1][0]: how much should 'I' attend to '<BOS>'?\n",
      "============================================================\n",
      "\n",
      "Query for 'I' (Q[1]):\n",
      "  [-0.0997, -0.0394,  0.0301,  0.0469,  0.0628, -0.0026, -0.0506,  0.0320]\n",
      "\n",
      "Key for '<BOS>' (K[0]):\n",
      "  [-0.0090, -0.0398,  0.0085, -0.0527, -0.0375, -0.0001, -0.0328,  0.0792]\n",
      "\n",
      "Score = dot product of these vectors\n",
      "      = (-0.0997 × -0.0090) + (-0.0394 × -0.0398) + (0.0301 × 0.0085) + ...\n",
      "      = 0.0021\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing scores[1][0]: how much should 'I' attend to '<BOS>'?\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Query for 'I' (Q[1]):\")\n",
    "print(f\"  {format_vector(Q[1])}\")\n",
    "print()\n",
    "print(f\"Key for '<BOS>' (K[0]):\")\n",
    "print(f\"  {format_vector(K[0])}\")\n",
    "print()\n",
    "print(\"Score = dot product of these vectors\")\n",
    "\n",
    "# Show the dot product calculation\n",
    "score = dot_product(Q[1], K[0])\n",
    "terms = [f\"({Q[1][i]:.4f} × {K[0][i]:.4f})\" for i in range(3)]\n",
    "print(f\"      = {' + '.join(terms)} + ...\")\n",
    "print(f\"      = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 2: Scale the Scores\n",
    "\n",
    "We divide all scores by $\\sqrt{d_k} = \\sqrt{8} \\approx 2.83$:\n",
    "\n",
    "$$\\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}}$$\n",
    "\n",
    "**Why scale?**\n",
    "\n",
    "Dot products of high-dimensional vectors can be large. If $d_k = 8$ and each element is around 0.1, the dot product could be around $8 \\times 0.1 \\times 0.1 = 0.08$. That's fine.\n",
    "\n",
    "But as $d_k$ grows (say, to 64 or 128), dot products grow proportionally. Large inputs to softmax push it toward extreme values—one position gets weight ~1.0, everything else gets ~0.0. The gradients become tiny, training stalls.\n",
    "\n",
    "Dividing by $\\sqrt{d_k}$ keeps the variance of the scores roughly constant regardless of $d_k$. The softmax stays in a \"healthy\" range where gradients flow well.\n",
    "\n",
    "(This is one of those details that seems minor but matters a lot in practice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.672287Z",
     "iopub.status.busy": "2025-12-10T21:16:59.672221Z",
     "iopub.status.idle": "2025-12-10T21:16:59.674234Z",
     "shell.execute_reply": "2025-12-10T21:16:59.673966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling factor: sqrt(8) = 2.8284\n",
      "\n",
      "Scaled Scores (scores / 2.8284)\n",
      "\n",
      "            <BOS>         I      like  transformers     <EOS>\n",
      "   <BOS> [ -0.0045    0.0075   -0.0054    0.0075   -0.0048]\n",
      "       I [  0.0007   -0.0047    0.0042   -0.0009    0.0032]\n",
      "    like [ -0.0049    0.0034   -0.0014    0.0060   -0.0021]\n",
      "transformers [ -0.0006   -0.0042    0.0016   -0.0006    0.0031]\n",
      "   <EOS> [ -0.0008    0.0030   -0.0008   -0.0006   -0.0024]\n"
     ]
    }
   ],
   "source": [
    "scale = math.sqrt(D_K)\n",
    "print(f\"Scaling factor: sqrt({D_K}) = {scale:.4f}\")\n",
    "print()\n",
    "\n",
    "scaled_scores = [[s / scale for s in row] for row in scores]\n",
    "\n",
    "print(f\"Scaled Scores (scores / {scale:.4f})\")\n",
    "print()\n",
    "header = \"         \" + \"  \".join([f\"{TOKEN_NAMES[tokens[j]]:>8}\" for j in range(seq_len)])\n",
    "print(header)\n",
    "for i, row in enumerate(scaled_scores):\n",
    "    values = \"  \".join([f\"{v:8.4f}\" for v in row])\n",
    "    print(f\"{TOKEN_NAMES[tokens[i]]:>8} [{values}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 3: Apply the Causal Mask\n",
    "\n",
    "This is a decoder-only model (like GPT). It generates text left-to-right, one token at a time. When predicting the next token, it can only see previous tokens—not future ones.\n",
    "\n",
    "We enforce this with a **causal mask**: set scores for future positions to $-\\infty$.\n",
    "\n",
    "$$\\text{masked}_{ij} = \\begin{cases}\n",
    "\\text{scaled}_{ij} & \\text{if } j \\leq i \\\\\n",
    "-\\infty & \\text{if } j > i\n",
    "\\end{cases}$$\n",
    "\n",
    "Why $-\\infty$? Because $e^{-\\infty} = 0$. When we apply softmax, positions with $-\\infty$ will get weight 0. They contribute nothing.\n",
    "\n",
    "**The mask pattern:**\n",
    "```\n",
    "Position:      0    1    2    3    4\n",
    "0 (<BOS>)    [ ok  -∞   -∞   -∞   -∞ ]  can only see itself\n",
    "1 (I)        [ ok   ok  -∞   -∞   -∞ ]  can see 0, 1\n",
    "2 (like)     [ ok   ok   ok  -∞   -∞ ]  can see 0, 1, 2\n",
    "3 (trans.)   [ ok   ok   ok   ok  -∞ ]  can see 0, 1, 2, 3\n",
    "4 (<EOS>)    [ ok   ok   ok   ok   ok ]  can see all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.674898Z",
     "iopub.status.busy": "2025-12-10T21:16:59.674834Z",
     "iopub.status.idle": "2025-12-10T21:16:59.676987Z",
     "shell.execute_reply": "2025-12-10T21:16:59.676743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Scores (future positions set to -inf)\n",
      "\n",
      "            <BOS>         I      like  transformers     <EOS>\n",
      "   <BOS> [ -0.0045,     -inf,     -inf,     -inf,     -inf]\n",
      "       I [  0.0007,  -0.0047,     -inf,     -inf,     -inf]\n",
      "    like [ -0.0049,   0.0034,  -0.0014,     -inf,     -inf]\n",
      "transformers [ -0.0006,  -0.0042,   0.0016,  -0.0006,     -inf]\n",
      "   <EOS> [ -0.0008,   0.0030,  -0.0008,  -0.0006,  -0.0024]\n"
     ]
    }
   ],
   "source": [
    "# Apply causal mask\n",
    "masked_scores = []\n",
    "for i in range(seq_len):\n",
    "    row = []\n",
    "    for j in range(seq_len):\n",
    "        if j <= i:  # Can attend to this position\n",
    "            row.append(scaled_scores[i][j])\n",
    "        else:  # Future position - mask it out\n",
    "            row.append(float('-inf'))\n",
    "    masked_scores.append(row)\n",
    "\n",
    "print(\"Masked Scores (future positions set to -inf)\")\n",
    "print()\n",
    "header = \"         \" + \"  \".join([f\"{TOKEN_NAMES[tokens[j]]:>8}\" for j in range(seq_len)])\n",
    "print(header)\n",
    "for i, row in enumerate(masked_scores):\n",
    "    values = []\n",
    "    for v in row:\n",
    "        if v == float('-inf'):\n",
    "            values.append(\"    -inf\")\n",
    "        else:\n",
    "            values.append(f\"{v:8.4f}\")\n",
    "    print(f\"{TOKEN_NAMES[tokens[i]]:>8} [{', '.join(values)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 4: Softmax\n",
    "\n",
    "Now we convert scores to probabilities using **softmax**:\n",
    "\n",
    "$$\\text{weight}_i = \\frac{e^{\\text{score}_i}}{\\sum_j e^{\\text{score}_j}}$$\n",
    "\n",
    "Softmax does three things:\n",
    "1. **Exponentiation** — makes all values positive\n",
    "2. **Normalization** — makes them sum to 1\n",
    "3. **Amplification** — larger scores get disproportionately larger weights\n",
    "\n",
    "The result is a probability distribution over positions. Higher scores → higher weights → more attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.677812Z",
     "iopub.status.busy": "2025-12-10T21:16:59.677734Z",
     "iopub.status.idle": "2025-12-10T21:16:59.679571Z",
     "shell.execute_reply": "2025-12-10T21:16:59.679288Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(vec):\n",
    "    \"\"\"\n",
    "    Compute softmax of a vector, handling -inf values.\n",
    "    \n",
    "    We subtract the max for numerical stability (doesn't change the result,\n",
    "    but prevents overflow when exponentiating large numbers).\n",
    "    \"\"\"\n",
    "    # Find max of non-inf values\n",
    "    finite_vals = [v for v in vec if v != float('-inf')]\n",
    "    max_val = max(finite_vals) if finite_vals else 0\n",
    "    \n",
    "    # Exponentiate (shifted by max)\n",
    "    exp_vec = []\n",
    "    for v in vec:\n",
    "        if v == float('-inf'):\n",
    "            exp_vec.append(0.0)  # e^(-inf) = 0\n",
    "        else:\n",
    "            exp_vec.append(math.exp(v - max_val))\n",
    "    \n",
    "    # Normalize\n",
    "    total = sum(exp_vec)\n",
    "    return [e / total for e in exp_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.680420Z",
     "iopub.status.busy": "2025-12-10T21:16:59.680334Z",
     "iopub.status.idle": "2025-12-10T21:16:59.683192Z",
     "shell.execute_reply": "2025-12-10T21:16:59.682850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Softmax for position 1 ('I')\n",
      "============================================================\n",
      "\n",
      "Masked scores: [0.000737600323286676, -0.004733186959169455] (only positions 0,1 are visible)\n",
      "\n",
      "Step 1: Exponentiate (subtracting max=0.0007 for stability)\n",
      "  exp(0.0007 - 0.0007) = exp(0.0000) = 1.0000\n",
      "  exp(-0.0047 - 0.0007) = exp(-0.0055) = 0.9945\n",
      "\n",
      "Step 2: Sum = 1.0000 + 0.9945 = 1.9945\n",
      "\n",
      "Step 3: Normalize\n",
      "  weight[0] = 1.0000 / 1.9945 = 0.5014\n",
      "  weight[1] = 0.9945 / 1.9945 = 0.4986\n",
      "\n",
      "Sum of weights: 1.0000 (should be 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Let's trace through softmax for position 1 (\"I\")\n",
    "print(\"Example: Softmax for position 1 ('I')\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Masked scores: {masked_scores[1][:2]} (only positions 0,1 are visible)\")\n",
    "print()\n",
    "\n",
    "s0, s1 = masked_scores[1][0], masked_scores[1][1]\n",
    "print(f\"Step 1: Exponentiate (subtracting max={max(s0,s1):.4f} for stability)\")\n",
    "exp_0 = math.exp(s0 - max(s0, s1))\n",
    "exp_1 = math.exp(s1 - max(s0, s1))\n",
    "print(f\"  exp({s0:.4f} - {max(s0,s1):.4f}) = exp({s0 - max(s0,s1):.4f}) = {exp_0:.4f}\")\n",
    "print(f\"  exp({s1:.4f} - {max(s0,s1):.4f}) = exp({s1 - max(s0,s1):.4f}) = {exp_1:.4f}\")\n",
    "print()\n",
    "\n",
    "total = exp_0 + exp_1\n",
    "print(f\"Step 2: Sum = {exp_0:.4f} + {exp_1:.4f} = {total:.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Step 3: Normalize\")\n",
    "print(f\"  weight[0] = {exp_0:.4f} / {total:.4f} = {exp_0/total:.4f}\")\n",
    "print(f\"  weight[1] = {exp_1:.4f} / {total:.4f} = {exp_1/total:.4f}\")\n",
    "print()\n",
    "print(f\"Sum of weights: {exp_0/total + exp_1/total:.4f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.683891Z",
     "iopub.status.busy": "2025-12-10T21:16:59.683821Z",
     "iopub.status.idle": "2025-12-10T21:16:59.685884Z",
     "shell.execute_reply": "2025-12-10T21:16:59.685614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights (after softmax)\n",
      "\n",
      "Each row sums to 1.0. These are the 'attention probabilities'.\n",
      "\n",
      "            <BOS>         I      like  transformers     <EOS>\n",
      "   <BOS> [  1.0000    0.0000    0.0000    0.0000    0.0000]  sum=1.0000\n",
      "       I [  0.5014    0.4986    0.0000    0.0000    0.0000]  sum=1.0000\n",
      "    like [  0.3320    0.3348    0.3332    0.0000    0.0000]  sum=1.0000\n",
      "transformers [  0.2501    0.2492    0.2506    0.2501    0.0000]  sum=1.0000\n",
      "   <EOS> [  0.1999    0.2007    0.1999    0.2000    0.1996]  sum=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to all rows\n",
    "attention_weights = [softmax(row) for row in masked_scores]\n",
    "\n",
    "print(\"Attention Weights (after softmax)\")\n",
    "print()\n",
    "print(\"Each row sums to 1.0. These are the 'attention probabilities'.\")\n",
    "print()\n",
    "header = \"         \" + \"  \".join([f\"{TOKEN_NAMES[tokens[j]]:>8}\" for j in range(seq_len)])\n",
    "print(header)\n",
    "for i, row in enumerate(attention_weights):\n",
    "    values = \"  \".join([f\"{v:8.4f}\" for v in row])\n",
    "    row_sum = sum(row)\n",
    "    print(f\"{TOKEN_NAMES[tokens[i]]:>8} [{values}]  sum={row_sum:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Interpreting the Weights\n",
    "\n",
    "Look at what we computed:\n",
    "\n",
    "- **Position 0 (`<BOS>`)**: 100% attention to itself. It has no choice—it can only see itself.\n",
    "- **Position 1 (`I`)**: About 50-50 between `<BOS>` and itself. \n",
    "- **Later positions**: Spread attention more evenly across all visible positions.\n",
    "\n",
    "The weights are nearly uniform because our model is untrained—the Q and K projections are random noise. In a trained model, you'd see much more interesting patterns:\n",
    "- Verbs attending strongly to their subjects\n",
    "- Pronouns attending to their antecedents\n",
    "- Related concepts clustering together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Step 5: Weighted Sum of Values\n",
    "\n",
    "Finally, we use the attention weights to compute a weighted combination of values:\n",
    "\n",
    "$$\\text{output} = \\text{weights} \\cdot V$$\n",
    "\n",
    "**Shapes:**\n",
    "- $\\text{weights}$: `[5, 5]` — attention from each position to each position\n",
    "- $V$: `[5, 8]` — value vector for each position\n",
    "- $\\text{output}$: `[5, 8]` — new representation for each position\n",
    "\n",
    "Each output vector is a weighted average of value vectors, where the weights come from the attention.\n",
    "\n",
    "This is how information flows: token $i$ gathers information from other tokens by taking a weighted sum of their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.686561Z",
     "iopub.status.busy": "2025-12-10T21:16:59.686490Z",
     "iopub.status.idle": "2025-12-10T21:16:59.688309Z",
     "shell.execute_reply": "2025-12-10T21:16:59.688050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output for Head 0\n",
      "Shape: [5, 5] @ [5, 8] = [5, 8]\n",
      "\n",
      "  output[0] = [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737]  # <BOS>\n",
      "  output[1] = [ 0.0683,  0.0368, -0.0263, -0.0574,  0.0152, -0.0174, -0.0084, -0.0760]  # I\n",
      "  output[2] = [ 0.0247,  0.0789,  0.0074, -0.0635,  0.0180, -0.0098, -0.0184, -0.0173]  # like\n",
      "  output[3] = [ 0.0254,  0.0511, -0.0182, -0.0322,  0.0103, -0.0126, -0.0282,  0.0018]  # transformers\n",
      "  output[4] = [ 0.0325,  0.0367, -0.0202, -0.0262,  0.0188, -0.0040, -0.0321,  0.0167]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Compute attention output\n",
    "attention_output = matmul(attention_weights, V)\n",
    "\n",
    "print(f\"Attention Output for Head {head}\")\n",
    "print(f\"Shape: [{seq_len}, {seq_len}] @ [{seq_len}, {D_K}] = [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(attention_output):\n",
    "    print(f\"  output[{i}] = {format_vector(row)}  # {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.689076Z",
     "iopub.status.busy": "2025-12-10T21:16:59.689006Z",
     "iopub.status.idle": "2025-12-10T21:16:59.691207Z",
     "shell.execute_reply": "2025-12-10T21:16:59.690904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed: Computing output for position 1 ('I')\n",
      "============================================================\n",
      "\n",
      "output[1] = sum of (attention_weight × value) for each visible position\n",
      "\n",
      "Attention weights: 0.5014 to '<BOS>', 0.4986 to 'I'\n",
      "\n",
      "V[0] (value for '<BOS>'): [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737]\n",
      "V[1] (value for 'I'):     [ 0.0565,  0.0479, -0.0409, -0.0089, -0.0037,  0.0547, -0.0085, -0.0782]\n",
      "\n",
      "output[1] = 0.5014 × V[0] + 0.4986 × V[1]\n",
      "\n",
      "Result: [ 0.0683,  0.0368, -0.0263, -0.0574,  0.0152, -0.0174, -0.0084, -0.0760]\n"
     ]
    }
   ],
   "source": [
    "# Detailed calculation for position 1\n",
    "print(\"Detailed: Computing output for position 1 ('I')\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"output[1] = sum of (attention_weight × value) for each visible position\")\n",
    "print()\n",
    "\n",
    "w0, w1 = attention_weights[1][0], attention_weights[1][1]\n",
    "print(f\"Attention weights: {w0:.4f} to '<BOS>', {w1:.4f} to 'I'\")\n",
    "print()\n",
    "print(f\"V[0] (value for '<BOS>'): {format_vector(V[0])}\")\n",
    "print(f\"V[1] (value for 'I'):     {format_vector(V[1])}\")\n",
    "print()\n",
    "print(f\"output[1] = {w0:.4f} × V[0] + {w1:.4f} × V[1]\")\n",
    "print()\n",
    "\n",
    "# Compute manually\n",
    "manual_output = [w0 * V[0][d] + w1 * V[1][d] for d in range(D_K)]\n",
    "print(f\"Result: {format_vector(manual_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## The Complete Attention Function\n",
    "\n",
    "Let's wrap all five steps into a single function and run it for both heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.691854Z",
     "iopub.status.busy": "2025-12-10T21:16:59.691785Z",
     "iopub.status.idle": "2025-12-10T21:16:59.693747Z",
     "shell.execute_reply": "2025-12-10T21:16:59.693521Z"
    }
   },
   "outputs": [],
   "source": [
    "def attention(Q, K, V, causal=True):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix [seq_len, d_k]\n",
    "        K: Key matrix [seq_len, d_k]\n",
    "        V: Value matrix [seq_len, d_k]\n",
    "        causal: If True, apply causal mask (can't attend to future)\n",
    "    \n",
    "    Returns:\n",
    "        output: [seq_len, d_k] - weighted sum of values\n",
    "        weights: [seq_len, seq_len] - attention weights\n",
    "    \"\"\"\n",
    "    seq_len = len(Q)\n",
    "    d_k = len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    \n",
    "    # Step 1: scores = Q @ K^T\n",
    "    K_T = transpose(K)\n",
    "    scores = matmul(Q, K_T)\n",
    "    \n",
    "    # Step 2: Scale\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    \n",
    "    # Step 3: Causal mask\n",
    "    if causal:\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                if j > i:\n",
    "                    scaled[i][j] = float('-inf')\n",
    "    \n",
    "    # Step 4: Softmax\n",
    "    weights = [softmax(row) for row in scaled]\n",
    "    \n",
    "    # Step 5: Weighted sum\n",
    "    output = matmul(weights, V)\n",
    "    \n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.694466Z",
     "iopub.status.busy": "2025-12-10T21:16:59.694396Z",
     "iopub.status.idle": "2025-12-10T21:16:59.696394Z",
     "shell.execute_reply": "2025-12-10T21:16:59.696165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 0: Attention Weights\n",
      "  [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]  # <BOS>\n",
      "  [0.5014, 0.4986, 0.0000, 0.0000, 0.0000]  # I\n",
      "  [0.3320, 0.3348, 0.3332, 0.0000, 0.0000]  # like\n",
      "  [0.2501, 0.2492, 0.2506, 0.2501, 0.0000]  # transformers\n",
      "  [0.1999, 0.2007, 0.1999, 0.2000, 0.1996]  # <EOS>\n",
      "\n",
      "HEAD 1: Attention Weights\n",
      "  [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]  # <BOS>\n",
      "  [0.5009, 0.4991, 0.0000, 0.0000, 0.0000]  # I\n",
      "  [0.3342, 0.3337, 0.3322, 0.0000, 0.0000]  # like\n",
      "  [0.2514, 0.2494, 0.2510, 0.2482, 0.0000]  # transformers\n",
      "  [0.1999, 0.1997, 0.2001, 0.2000, 0.2003]  # <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute attention for both heads\n",
    "attention_output_all = []\n",
    "attention_weights_all = []\n",
    "\n",
    "for h in range(NUM_HEADS):\n",
    "    output, weights = attention(Q_all[h], K_all[h], V_all[h])\n",
    "    attention_output_all.append(output)\n",
    "    attention_weights_all.append(weights)\n",
    "    \n",
    "    print(f\"HEAD {h}: Attention Weights\")\n",
    "    for i, row in enumerate(weights):\n",
    "        values = \", \".join([f\"{v:.4f}\" for v in row])\n",
    "        print(f\"  [{values}]  # {TOKEN_NAMES[tokens[i]]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## What We've Computed\n",
    "\n",
    "For each head, we now have:\n",
    "\n",
    "| What | Shape | Meaning |\n",
    "|------|-------|--------|\n",
    "| Attention weights | [5, 5] | How much each position attends to each other |\n",
    "| Attention output | [5, 8] | New representation incorporating context |\n",
    "\n",
    "The output for each position is now a mixture of information from other positions. Token representations are no longer independent—they've started to incorporate context.\n",
    "\n",
    "This is the power of attention: each token's representation can now depend on the entire sequence (up to its position)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "We have attention outputs from two heads, each with shape `[5, 8]`. But our model expects `d_model = 16` dimensions.\n",
    "\n",
    "Next, we'll:\n",
    "1. **Concatenate** the head outputs: `[5, 8]` + `[5, 8]` → `[5, 16]`\n",
    "2. **Project** through an output matrix to mix information across heads\n",
    "\n",
    "This is the \"multi\" in multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:59.697139Z",
     "iopub.status.busy": "2025-12-10T21:16:59.697072Z",
     "iopub.status.idle": "2025-12-10T21:16:59.698673Z",
     "shell.execute_reply": "2025-12-10T21:16:59.698390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention computation complete. Ready for multi-head combination.\n"
     ]
    }
   ],
   "source": [
    "# Store for next notebook\n",
    "attention_data = {\n",
    "    'attention_weights': attention_weights_all,\n",
    "    'attention_output': attention_output_all,\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'Q': Q_all,\n",
    "    'K': K_all,\n",
    "    'V': V_all\n",
    "}\n",
    "print(\"Attention computation complete. Ready for multi-head combination.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
