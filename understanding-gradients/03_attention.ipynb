{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Attention Mechanism\n\n**Computing attention scores and weighted combinations of values**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, this is it. The attention mechanism itself.\n",
    "\n",
    "This is the core innovation that makes transformers so powerful (and why they've basically taken over NLP, computer vision, and... well, everything)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Attention?\n",
    "\n",
    "Attention allows each token to look at other tokens in the sequence and decide how much to focus on each one. This creates context-aware representations where each token's output depends on the entire sequence (up to its position, anyway).\n",
    "\n",
    "**The intuition:**\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What do I offer?\"\n",
    "- **Value (V)**: \"What information do I provide?\"\n",
    "\n",
    "For each token, we compute how well its query matches every key, then use those match scores to create a weighted combination of values. Simple idea, powerful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention Algorithm\n",
    "\n",
    "The attention mechanism consists of 5 steps:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "1. **Compute attention scores**: $\\text{scores} = QK^T$\n",
    "2. **Scale**: $\\text{scaled} = \\frac{\\text{scores}}{\\sqrt{d_k}}$\n",
    "3. **Apply causal mask**: Set future positions to $-\\infty$\n",
    "4. **Softmax**: Convert to probabilities\n",
    "5. **Weighted sum**: $\\text{output} = \\text{weights} \\cdot V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Set seed for reproducibility (same as previous notebooks)\n",
    "random.seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS  # 8\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n = len(A), len(A[0])\n",
    "    p = len(B[0])\n",
    "    result = [[0] * p for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(n))\n",
    "    return result\n",
    "\n",
    "def transpose(A):\n",
    "    \"\"\"Transpose matrix A\"\"\"\n",
    "    rows, cols = len(A), len(A[0])\n",
    "    return [[A[i][j] for i in range(rows)] for j in range(cols)]\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    \"\"\"Compute dot product of two vectors\"\"\"\n",
    "    return sum(a * b for a, b in zip(v1, v2))\n",
    "\n",
    "def softmax(vec):\n",
    "    \"\"\"Compute softmax of a vector (handles -inf for masking)\"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    max_val = max(v for v in vec if v != float('-inf'))\n",
    "    exp_vec = [math.exp(v - max_val) if v != float('-inf') else 0 for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated Q, K, V from previous notebooks\n",
      "Each has shape [5, 8]\n"
     ]
    }
   ],
   "source": [
    "# Recreate embeddings and QKV from previous notebooks\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "\n",
    "tokens = [1, 3, 4, 5, 2]  # <BOS>, I, like, transformers, <EOS>\n",
    "seq_len = len(tokens)\n",
    "\n",
    "token_embeddings = [E_token[token_id] for token_id in tokens]\n",
    "X = [add_vectors(token_embeddings[i], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "# QKV weight matrices\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "print(\"Recreated Q, K, V from previous notebooks\")\n",
    "print(f\"Each has shape [{seq_len}, {D_K}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute Attention Scores\n",
    "\n",
    "Multiply queries by keys (transposed) to get a matrix of \"compatibility scores\":\n",
    "\n",
    "$$\\text{scores} = QK^T$$\n",
    "\n",
    "**Shapes:**\n",
    "- $Q$: $[5, 8]$ (5 tokens, 8 dimensions per head)\n",
    "- $K^T$: $[8, 5]$ (transposed from $[5, 8]$)\n",
    "- $\\text{scores}$: $[5, 5]$ (each token attending to each token)\n",
    "\n",
    "Each element $\\text{scores}_{ij}$ represents how much token $i$ should attend to token $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 0 - Attention Scores (Q @ K^T)\n",
      "Shape: [5, 8] @ [8, 5] = [5, 5]\n",
      "\n",
      "  [-0.0126,  0.0213, -0.0152,  0.0211, -0.0137]  # pos 0: <BOS>\n",
      "  [ 0.0021, -0.0134,  0.0119, -0.0027,  0.0091]  # pos 1: I\n",
      "  [-0.0140,  0.0097, -0.0039,  0.0169, -0.0061]  # pos 2: like\n",
      "  [-0.0018, -0.0119,  0.0046, -0.0016,  0.0088]  # pos 3: transformers\n",
      "  [-0.0022,  0.0084, -0.0022, -0.0016, -0.0069]  # pos 4: <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Compute attention scores for Head 0\n",
    "head = 0\n",
    "Q = Q_all[head]\n",
    "K = K_all[head]\n",
    "V = V_all[head]\n",
    "\n",
    "# scores = Q @ K^T\n",
    "K_T = transpose(K)\n",
    "scores = matmul(Q, K_T)\n",
    "\n",
    "print(f\"HEAD {head} - Attention Scores (Q @ K^T)\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}] @ [{D_K}, {seq_len}] = [{seq_len}, {seq_len}]\")\n",
    "print()\n",
    "for i, row in enumerate(scores):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Calculation: Position 1 attending to Position 0\n",
    "\n",
    "Let's see how the score for \"I\" attending to \"\\<BOS\\>\" is computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing score[1, 0] - how much 'I' attends to '<BOS>'\n",
      "============================================================\n",
      "\n",
      "Q[0][1] (query for 'I'):\n",
      "  [-0.0997, -0.0394,  0.0301,  0.0469,  0.0628, -0.0026, -0.0506,  0.0320]\n",
      "\n",
      "K[0][0] (key for '<BOS>'):\n",
      "  [-0.0090, -0.0398,  0.0085, -0.0527, -0.0375, -0.0001, -0.0328,  0.0792]\n",
      "\n",
      "score[1, 0] = Q[0][1] · K[0][0] (dot product)\n",
      "           = 0.0021\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing score[1, 0] - how much 'I' attends to '<BOS>'\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Q[0][1] (query for 'I'):\")\n",
    "print(f\"  {format_vector(Q[1])}\")\n",
    "print()\n",
    "print(f\"K[0][0] (key for '<BOS>'):\")\n",
    "print(f\"  {format_vector(K[0])}\")\n",
    "print()\n",
    "print(\"score[1, 0] = Q[0][1] · K[0][0] (dot product)\")\n",
    "score_1_0 = dot_product(Q[1], K[0])\n",
    "print(f\"           = {score_1_0:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Scale the Scores\n",
    "\n",
    "Divide by $\\sqrt{d_k}$ to prevent very large values:\n",
    "\n",
    "$$\\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}} = \\frac{\\text{scores}}{\\sqrt{8}} = \\frac{\\text{scores}}{2.8284}$$\n",
    "\n",
    "**Why scale?** Without scaling, the dot products can grow large in magnitude, which pushes softmax into regions with very small gradients. This makes training unstable. The scaling factor $\\sqrt{d_k}$ keeps the scores in a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale factor: sqrt(8) = 2.8284\n",
      "\n",
      "Scaled Scores (scores / 2.8284)\n",
      "  [-0.0045,  0.0075, -0.0054,  0.0075, -0.0048]  # pos 0: <BOS>\n",
      "  [ 0.0007, -0.0047,  0.0042, -0.0009,  0.0032]  # pos 1: I\n",
      "  [-0.0049,  0.0034, -0.0014,  0.0060, -0.0021]  # pos 2: like\n",
      "  [-0.0006, -0.0042,  0.0016, -0.0006,  0.0031]  # pos 3: transformers\n",
      "  [-0.0008,  0.0030, -0.0008, -0.0006, -0.0024]  # pos 4: <EOS>\n"
     ]
    }
   ],
   "source": [
    "scale = math.sqrt(D_K)\n",
    "print(f\"Scale factor: sqrt({D_K}) = {scale:.4f}\")\n",
    "print()\n",
    "\n",
    "scaled_scores = [[s / scale for s in row] for row in scores]\n",
    "\n",
    "print(f\"Scaled Scores (scores / {scale:.4f})\")\n",
    "for i, row in enumerate(scaled_scores):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply Causal Mask\n",
    "\n",
    "For autoregressive (decoder-only) transformers, each position can only attend to **previous positions** (including itself). We set future positions to $-\\infty$:\n",
    "\n",
    "$$\\text{masked\\_scores}_{ij} = \\begin{cases}\n",
    "\\text{scaled\\_scores}_{ij} & \\text{if } j \\leq i \\\\\n",
    "-\\infty & \\text{if } j > i\n",
    "\\end{cases}$$\n",
    "\n",
    "**Mask pattern:**\n",
    "```\n",
    "Position:     0  1  2  3  4\n",
    "0 (<BOS>)   [ ✓  ✗  ✗  ✗  ✗ ]  can only see itself\n",
    "1 (I)       [ ✓  ✓  ✗  ✗  ✗ ]  can see 0, 1\n",
    "2 (like)    [ ✓  ✓  ✓  ✗  ✗ ]  can see 0, 1, 2\n",
    "3 (trans.)  [ ✓  ✓  ✓  ✓  ✗ ]  can see 0, 1, 2, 3\n",
    "4 (<EOS>)   [ ✓  ✓  ✓  ✓  ✓ ]  can see all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Scores (future positions set to -inf)\n",
      "  [-0.0045,    -inf,    -inf,    -inf,    -inf]  # pos 0: <BOS>\n",
      "  [ 0.0007, -0.0047,    -inf,    -inf,    -inf]  # pos 1: I\n",
      "  [-0.0049,  0.0034, -0.0014,    -inf,    -inf]  # pos 2: like\n",
      "  [-0.0006, -0.0042,  0.0016, -0.0006,    -inf]  # pos 3: transformers\n",
      "  [-0.0008,  0.0030, -0.0008, -0.0006, -0.0024]  # pos 4: <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Apply causal mask\n",
    "masked_scores = []\n",
    "for i in range(seq_len):\n",
    "    row = []\n",
    "    for j in range(seq_len):\n",
    "        if j <= i:\n",
    "            row.append(scaled_scores[i][j])\n",
    "        else:\n",
    "            row.append(float('-inf'))\n",
    "    masked_scores.append(row)\n",
    "\n",
    "print(\"Masked Scores (future positions set to -inf)\")\n",
    "for i, row in enumerate(masked_scores):\n",
    "    row_str = [f\"{v:7.4f}\" if v != float('-inf') else \"   -inf\" for v in row]\n",
    "    print(f\"  [{', '.join(row_str)}]  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Softmax\n",
    "\n",
    "Convert scores to probabilities (each row sums to 1):\n",
    "\n",
    "$$\\text{weights}_{ij} = \\frac{e^{\\text{masked\\_scores}_{ij}}}{\\sum_{k=1}^{n} e^{\\text{masked\\_scores}_{ik}}}$$\n",
    "\n",
    "**What is softmax?**\n",
    "\n",
    "The softmax function converts a vector of arbitrary real numbers into a probability distribution. All values end up between 0 and 1, and they sum to 1.\n",
    "\n",
    "It's called \"soft\" max because it emphasizes the largest values while still keeping smaller values non-zero (unlike a \"hard\" max that just picks the biggest and zeroes everything else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Softmax for position 1 ('I')\n",
      "============================================================\n",
      "\n",
      "Masked scores for pos 1: [0.000737600323286676, -0.004733186959169455] (only first 2 visible)\n",
      "\n",
      "exp(0.0007) = 1.0007\n",
      "exp(-0.0047) = 0.9953\n",
      "sum = 1.9960\n",
      "\n",
      "weight[1,0] = 1.0007 / 1.9960 = 0.5014\n",
      "weight[1,1] = 0.9953 / 1.9960 = 0.4986\n",
      "\n",
      "Sum of weights: 1.0000 (should be 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Example: softmax on position 1's scores\n",
    "print(\"Example: Softmax for position 1 ('I')\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Masked scores for pos 1: {masked_scores[1][:2]} (only first 2 visible)\")\n",
    "print()\n",
    "\n",
    "# Manual calculation\n",
    "s0, s1 = masked_scores[1][0], masked_scores[1][1]\n",
    "exp_0 = math.exp(s0)\n",
    "exp_1 = math.exp(s1)\n",
    "sum_exp = exp_0 + exp_1\n",
    "\n",
    "print(f\"exp({s0:.4f}) = {exp_0:.4f}\")\n",
    "print(f\"exp({s1:.4f}) = {exp_1:.4f}\")\n",
    "print(f\"sum = {sum_exp:.4f}\")\n",
    "print()\n",
    "print(f\"weight[1,0] = {exp_0:.4f} / {sum_exp:.4f} = {exp_0/sum_exp:.4f}\")\n",
    "print(f\"weight[1,1] = {exp_1:.4f} / {sum_exp:.4f} = {exp_1/sum_exp:.4f}\")\n",
    "print()\n",
    "print(f\"Sum of weights: {exp_0/sum_exp + exp_1/sum_exp:.4f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights (after softmax)\n",
      "  [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000]  # pos 0: <BOS>\n",
      "  [ 0.5014,  0.4986,  0.0000,  0.0000,  0.0000]  # pos 1: I\n",
      "  [ 0.3320,  0.3348,  0.3332,  0.0000,  0.0000]  # pos 2: like\n",
      "  [ 0.2501,  0.2492,  0.2506,  0.2501,  0.0000]  # pos 3: transformers\n",
      "  [ 0.1999,  0.2007,  0.1999,  0.2000,  0.1996]  # pos 4: <EOS>\n",
      "\n",
      "Interpretation:\n",
      "- Position 0 (<BOS>) attends 100% to itself\n",
      "- Position 1 (I) attends ~50% to <BOS>, ~50% to itself\n",
      "- Later positions spread attention more evenly\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to all rows\n",
    "attention_weights = [softmax(row) for row in masked_scores]\n",
    "\n",
    "print(\"Attention Weights (after softmax)\")\n",
    "for i, row in enumerate(attention_weights):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")\n",
    "    \n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Position 0 (<BOS>) attends 100% to itself\")\n",
    "print(\"- Position 1 (I) attends ~50% to <BOS>, ~50% to itself\")\n",
    "print(\"- Later positions spread attention more evenly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compute Weighted Sum of Values\n",
    "\n",
    "Multiply attention weights by values to get the final output:\n",
    "\n",
    "$$\\text{output} = \\text{weights} \\cdot V$$\n",
    "\n",
    "Each output vector is a weighted combination of all the value vectors that this position can attend to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output for Head 0\n",
      "Shape: [5, 5] @ [5, 8] = [5, 8]\n",
      "\n",
      "  [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737]  # pos 0: <BOS>\n",
      "  [ 0.0683,  0.0368, -0.0263, -0.0574,  0.0152, -0.0174, -0.0084, -0.0760]  # pos 1: I\n",
      "  [ 0.0247,  0.0789,  0.0074, -0.0635,  0.0180, -0.0098, -0.0184, -0.0173]  # pos 2: like\n",
      "  [ 0.0254,  0.0511, -0.0182, -0.0322,  0.0103, -0.0126, -0.0282,  0.0018]  # pos 3: transformers\n",
      "  [ 0.0325,  0.0367, -0.0202, -0.0262,  0.0188, -0.0040, -0.0321,  0.0167]  # pos 4: <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Compute attention output\n",
    "attention_output = matmul(attention_weights, V)\n",
    "\n",
    "print(f\"Attention Output for Head {head}\")\n",
    "print(f\"Shape: [{seq_len}, {seq_len}] @ [{seq_len}, {D_K}] = [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(attention_output):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed: Output for position 1 ('I')\n",
      "============================================================\n",
      "\n",
      "Weights: [0.5013676934094159, 0.4986323065905841] (attending to pos 0 and 1)\n",
      "V[0] (value for <BOS>): [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737]\n",
      "V[1] (value for I):     [ 0.0565,  0.0479, -0.0409, -0.0089, -0.0037,  0.0547, -0.0085, -0.0782]\n",
      "\n",
      "output[1] = 0.5003 × V[0] + 0.4997 × V[1]\n",
      "\n",
      "Result: [ 0.0683,  0.0368, -0.0263, -0.0574,  0.0152, -0.0174, -0.0084, -0.0760]\n"
     ]
    }
   ],
   "source": [
    "# Detailed calculation for position 1\n",
    "print(\"Detailed: Output for position 1 ('I')\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Weights: {attention_weights[1][:2]} (attending to pos 0 and 1)\")\n",
    "print(f\"V[0] (value for <BOS>): {format_vector(V[0])}\")\n",
    "print(f\"V[1] (value for I):     {format_vector(V[1])}\")\n",
    "print()\n",
    "print(\"output[1] = 0.5003 × V[0] + 0.4997 × V[1]\")\n",
    "print()\n",
    "w0, w1 = attention_weights[1][0], attention_weights[1][1]\n",
    "manual_output = [w0 * V[0][d] + w1 * V[1][d] for d in range(D_K)]\n",
    "print(f\"Result: {format_vector(manual_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Attention for Both Heads\n",
    "\n",
    "Let's compute attention for both heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HEAD 0 - Attention Weights\n",
      "  [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000]  # pos 0: <BOS>\n",
      "  [ 0.5014,  0.4986,  0.0000,  0.0000,  0.0000]  # pos 1: I\n",
      "  [ 0.3320,  0.3348,  0.3332,  0.0000,  0.0000]  # pos 2: like\n",
      "  [ 0.2501,  0.2492,  0.2506,  0.2501,  0.0000]  # pos 3: transformers\n",
      "  [ 0.1999,  0.2007,  0.1999,  0.2000,  0.1996]  # pos 4: <EOS>\n",
      "\n",
      "HEAD 1 - Attention Weights\n",
      "  [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000]  # pos 0: <BOS>\n",
      "  [ 0.5009,  0.4991,  0.0000,  0.0000,  0.0000]  # pos 1: I\n",
      "  [ 0.3342,  0.3337,  0.3322,  0.0000,  0.0000]  # pos 2: like\n",
      "  [ 0.2514,  0.2494,  0.2510,  0.2482,  0.0000]  # pos 3: transformers\n",
      "  [ 0.1999,  0.1997,  0.2001,  0.2000,  0.2003]  # pos 4: <EOS>\n"
     ]
    }
   ],
   "source": [
    "def compute_attention(Q, K, V, mask=True):\n",
    "    \"\"\"Compute scaled dot-product attention\"\"\"\n",
    "    seq_len = len(Q)\n",
    "    d_k = len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    \n",
    "    # Step 1: Q @ K^T\n",
    "    K_T = transpose(K)\n",
    "    scores = matmul(Q, K_T)\n",
    "    \n",
    "    # Step 2: Scale\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    \n",
    "    # Step 3: Causal mask\n",
    "    if mask:\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                if j > i:\n",
    "                    scaled[i][j] = float('-inf')\n",
    "    \n",
    "    # Step 4: Softmax\n",
    "    weights = [softmax(row) for row in scaled]\n",
    "    \n",
    "    # Step 5: Weighted sum\n",
    "    output = matmul(weights, V)\n",
    "    \n",
    "    return weights, output\n",
    "\n",
    "# Compute for both heads\n",
    "attention_weights_all = []\n",
    "attention_output_all = []\n",
    "\n",
    "for h in range(NUM_HEADS):\n",
    "    weights, output = compute_attention(Q_all[h], K_all[h], V_all[h])\n",
    "    attention_weights_all.append(weights)\n",
    "    attention_output_all.append(output)\n",
    "    \n",
    "    print(f\"\\nHEAD {h} - Attention Weights\")\n",
    "    for i, row in enumerate(weights):\n",
    "        print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HEAD 0 - Attention Output\n",
      "Shape: [5, 8]\n",
      "  [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737]  # pos 0: <BOS>\n",
      "  [ 0.0683,  0.0368, -0.0263, -0.0574,  0.0152, -0.0174, -0.0084, -0.0760]  # pos 1: I\n",
      "  [ 0.0247,  0.0789,  0.0074, -0.0635,  0.0180, -0.0098, -0.0184, -0.0173]  # pos 2: like\n",
      "  [ 0.0254,  0.0511, -0.0182, -0.0322,  0.0103, -0.0126, -0.0282,  0.0018]  # pos 3: transformers\n",
      "  [ 0.0325,  0.0367, -0.0202, -0.0262,  0.0188, -0.0040, -0.0321,  0.0167]  # pos 4: <EOS>\n",
      "\n",
      "HEAD 1 - Attention Output\n",
      "Shape: [5, 8]\n",
      "  [ 0.0107, -0.0291, -0.0100, -0.0312,  0.0214,  0.0372,  0.0105,  0.0279]  # pos 0: <BOS>\n",
      "  [-0.0199, -0.0151,  0.0026,  0.0107,  0.0091, -0.0204, -0.0320, -0.0193]  # pos 1: I\n",
      "  [-0.0320, -0.0102,  0.0178, -0.0153,  0.0433,  0.0026,  0.0002, -0.0198]  # pos 2: like\n",
      "  [-0.0111, -0.0085,  0.0093,  0.0101,  0.0440,  0.0237,  0.0056, -0.0311]  # pos 3: transformers\n",
      "  [-0.0119, -0.0013, -0.0069,  0.0016,  0.0480,  0.0233,  0.0096, -0.0121]  # pos 4: <EOS>\n"
     ]
    }
   ],
   "source": [
    "for h in range(NUM_HEADS):\n",
    "    print(f\"\\nHEAD {h} - Attention Output\")\n",
    "    print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "    for i, row in enumerate(attention_output_all[h]):\n",
    "        print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Attention Patterns\n",
    "\n",
    "The attention weights tell us what each token is \"looking at\":\n",
    "\n",
    "- **Position 0 (`<BOS>`)** can only attend to itself (100%)\n",
    "- **Position 1 (`I`)** attends ~50% to `<BOS>` and ~50% to itself\n",
    "- **Later positions** spread attention more evenly across all previous tokens\n",
    "\n",
    "The attention is spread almost equally because our weights are randomly initialized—the model hasn't learned anything meaningful yet.\n",
    "\n",
    "In a trained model, you'd see more interesting patterns:\n",
    "- Verbs attending strongly to their subjects\n",
    "- Pronouns attending to their antecedents\n",
    "- Related concepts attending to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "We've got attention outputs from both heads. Now we need to combine them:\n",
    "1. **Concatenate** the outputs from both heads\n",
    "2. **Project** the concatenated result back to $d_{model}$ dimensions\n",
    "3. Apply **residual connections** and **layer normalization**\n",
    "\n",
    "Then it's on to the feed-forward network. We're making progress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention data stored for next notebook.\n"
     ]
    }
   ],
   "source": [
    "# Store for next notebook\n",
    "attention_data = {\n",
    "    'attention_weights': attention_weights_all,\n",
    "    'attention_output': attention_output_all,\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'Q': Q_all,\n",
    "    'K': K_all,\n",
    "    'V': V_all\n",
    "}\n",
    "print(\"Attention data stored for next notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}