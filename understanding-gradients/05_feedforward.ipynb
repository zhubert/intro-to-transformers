{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feed-Forward Network\n",
    "\n",
    "**Position-wise transformations to add non-linearity and expressiveness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, multi-head attention is behind us. Now for something... simpler?\n",
    "\n",
    "The feed-forward network (FFN).\n",
    "\n",
    "Here's the deal: attention lets tokens talk to each other, which is great. But it's all linear transformations and weighted sums. We need some **non-linearity** in here—some way for the model to learn complex, non-linear relationships.\n",
    "\n",
    "That's where the FFN comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Feed-Forward Network?\n",
    "\n",
    "It's just a two-layer fully connected neural network. Applied independently to each position.\n",
    "\n",
    "That's it. No attention, no looking at other tokens. Just:\n",
    "1. **Expand** the representation to a higher dimension\n",
    "2. **Apply a non-linear activation** (GELU in our case)\n",
    "3. **Project** back down to the original dimension\n",
    "\n",
    "Think of it as giving each token's representation some \"personal processing time\" to transform itself in complex, non-linear ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture\n",
    "\n",
    "$$\\text{FFN}(x) = W_2 \\cdot \\text{GELU}(W_1 \\cdot x + b_1) + b_2$$\n",
    "\n",
    "Breaking it down:\n",
    "- **$W_1$**: Weights for the first layer, shape $[d_{ff}, d_{model}] = [64, 16]$ (expansion)\n",
    "- **$b_1$**: Bias for the first layer, shape $[d_{ff}] = [64]$\n",
    "- **GELU**: Gaussian Error Linear Unit activation\n",
    "- **$W_2$**: Weights for the second layer, shape $[d_{model}, d_{ff}] = [16, 64]$ (projection)\n",
    "- **$b_2$**: Bias for the second layer, shape $[d_{model}] = [16]$\n",
    "\n",
    "**Why expand to 64 dimensions?** The standard ratio in transformers is $d_{ff} = 4 \\times d_{model}$. The expansion gives the model more \"room\" to represent complex transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "D_FF = 64  # 4 * D_MODEL\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n = len(A), len(A[0])\n",
    "    p = len(B[0])\n",
    "    return [[sum(A[i][k] * B[k][j] for k in range(n)) for j in range(p)] for i in range(m)]\n",
    "\n",
    "def transpose(A):\n",
    "    return [[A[i][j] for i in range(len(A))] for j in range(len(A[0]))]\n",
    "\n",
    "def softmax(vec):\n",
    "    max_val = max(v for v in vec if v != float('-inf'))\n",
    "    exp_vec = [math.exp(v - max_val) if v != float('-inf') else 0 for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"GELU activation: x * Φ(x) using tanh approximation\"\"\"\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate multi-head attention output from previous notebooks\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "tokens = [1, 3, 4, 5, 2]\n",
    "seq_len = len(tokens)\n",
    "X = [add_vectors(E_token[tokens[i]], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "# QKV and attention\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "def compute_attention(Q, K, V):\n",
    "    seq_len, d_k = len(Q), len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    scores = matmul(Q, transpose(K))\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j > i:\n",
    "                scaled[i][j] = float('-inf')\n",
    "    weights = [softmax(row) for row in scaled]\n",
    "    return matmul(weights, V)\n",
    "\n",
    "attention_output_all = [compute_attention(Q_all[h], K_all[h], V_all[h]) for h in range(NUM_HEADS)]\n",
    "concat_output = [attention_output_all[0][i] + attention_output_all[1][i] for i in range(seq_len)]\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)\n",
    "multi_head_output = matmul(concat_output, transpose(W_O))\n",
    "\n",
    "print(\"Recreated multi-head attention output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GELU Activation\n",
    "\n",
    "We're using GELU (Gaussian Error Linear Unit) instead of the classic ReLU.\n",
    "\n",
    "$$\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)\\right)\\right)$$\n",
    "\n",
    "**Why GELU instead of ReLU?**\n",
    "\n",
    "ReLU just zeros out negative values: $\\text{ReLU}(x) = \\max(0, x)$. It's simple, but it's a hard cutoff.\n",
    "\n",
    "GELU is smoother. It still emphasizes positive values, but it doesn't completely kill negative ones—they get gently suppressed instead. This smoothness helps with gradient flow during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GELU vs ReLU\n",
    "print(\"GELU vs ReLU comparison\")\n",
    "print(\"=\"*40)\n",
    "print(f\"{'x':>8} | {'ReLU':>8} | {'GELU':>8}\")\n",
    "print(\"-\"*40)\n",
    "for x in [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]:\n",
    "    relu = max(0, x)\n",
    "    gelu_val = gelu(x)\n",
    "    print(f\"{x:>8.1f} | {relu:>8.4f} | {gelu_val:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FFN weights\n",
    "W1 = random_matrix(D_FF, D_MODEL)   # [64, 16] - expansion\n",
    "b1 = random_vector(D_FF)            # [64]\n",
    "W2 = random_matrix(D_MODEL, D_FF)   # [16, 64] - projection\n",
    "b2 = random_vector(D_MODEL)         # [16]\n",
    "\n",
    "print(f\"FFN Weight Shapes:\")\n",
    "print(f\"  W1: [{D_FF}, {D_MODEL}] (expansion)\")\n",
    "print(f\"  b1: [{D_FF}]\")\n",
    "print(f\"  W2: [{D_MODEL}, {D_FF}] (projection)\")\n",
    "print(f\"  b2: [{D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: First Linear Layer (Expansion)\n",
    "\n",
    "$$\\text{hidden} = W_1 \\cdot x + b_1$$\n",
    "\n",
    "We're expanding from 16 dimensions to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute first linear layer for all positions\n",
    "# hidden = input @ W1^T + b1\n",
    "W1_T = transpose(W1)\n",
    "hidden = matmul(multi_head_output, W1_T)\n",
    "hidden = [[hidden[i][j] + b1[j] for j in range(D_FF)] for i in range(seq_len)]\n",
    "\n",
    "print(f\"Hidden layer (after first linear)\")\n",
    "print(f\"Shape: [{seq_len}, {D_FF}]\")\n",
    "print()\n",
    "print(f\"First 8 values for position 0 (<BOS>):\")\n",
    "print(f\"  {format_vector(hidden[0][:8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: GELU Activation\n",
    "\n",
    "Apply GELU element-wise to the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GELU activation\n",
    "activated = [[gelu(h) for h in row] for row in hidden]\n",
    "\n",
    "print(f\"After GELU activation\")\n",
    "print()\n",
    "print(f\"Example for position 0, first 8 values:\")\n",
    "print(f\"  Before: {format_vector(hidden[0][:8])}\")\n",
    "print(f\"  After:  {format_vector(activated[0][:8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Second Linear Layer (Projection)\n",
    "\n",
    "$$\\text{output} = W_2 \\cdot \\text{activated} + b_2$$\n",
    "\n",
    "We're projecting back down from 64 dimensions to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute second linear layer\n",
    "# output = activated @ W2^T + b2\n",
    "W2_T = transpose(W2)\n",
    "ffn_output = matmul(activated, W2_T)\n",
    "ffn_output = [[ffn_output[i][j] + b2[j] for j in range(D_MODEL)] for i in range(seq_len)]\n",
    "\n",
    "print(f\"FFN Output\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(ffn_output):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before and After\n",
    "\n",
    "Let's compare position 1 (`I`) before and after the FFN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Position 1 ('I') - Before and After FFN\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Before FFN (multi-head output):\")\n",
    "print(f\"  {format_vector(multi_head_output[1])}\")\n",
    "print()\n",
    "print(f\"After FFN:\")\n",
    "print(f\"  {format_vector(ffn_output[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the Point?\n",
    "\n",
    "Here's what the FFN accomplishes:\n",
    "\n",
    "1. **Non-linearity**: Attention is all linear operations. The FFN adds crucial non-linear transformations via GELU.\n",
    "\n",
    "2. **Position-wise processing**: Each token gets its own transformation, independent of others. Attention mixed information *between* tokens; FFN processes each token *individually*.\n",
    "\n",
    "3. **Expressiveness**: The expansion to 64 dimensions gives the model more capacity to represent complex functions.\n",
    "\n",
    "4. **Feature transformation**: The FFN can learn to emphasize certain features, suppress others, create new combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "We just replaced the attention output with the FFN output. But that means we **lost** all the information from attention. That's... not great.\n",
    "\n",
    "Enter: **residual connections** and **layer normalization**.\n",
    "\n",
    "Instead of just using the FFN output, we're going to *add* it to the original input. That way we keep the old information while adding new transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store for next notebook\n",
    "ffn_data = {\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'multi_head_output': multi_head_output,\n",
    "    'ffn_output': ffn_output,\n",
    "    'W1': W1, 'b1': b1,\n",
    "    'W2': W2, 'b2': b2\n",
    "}\n",
    "print(\"FFN data stored for next notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
