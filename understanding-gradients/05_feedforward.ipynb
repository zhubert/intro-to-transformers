{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Feed-Forward Network\n",
    "\n",
    "**Adding non-linearity through position-wise transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why We Need Non-Linearity\n",
    "\n",
    "Multi-head attention is powerful, but here's the thing: it's all linear operations.\n",
    "\n",
    "Matrix multiplications, weighted sums, projections—these are all linear. And stacking linear operations on top of linear operations just gives you... more linear operations. A thousand linear layers can be collapsed into a single linear layer.\n",
    "\n",
    "To learn truly complex functions, we need **non-linearity**. That's where the feed-forward network comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## What the FFN Does\n",
    "\n",
    "The feed-forward network (FFN) is surprisingly simple: a two-layer neural network applied independently to each position.\n",
    "\n",
    "$$\\text{FFN}(x) = W_2 \\cdot \\text{GELU}(W_1 \\cdot x + b_1) + b_2$$\n",
    "\n",
    "**The architecture:**\n",
    "1. **Expand**: Project from 16 dimensions to 64 dimensions (4× expansion)\n",
    "2. **Activate**: Apply GELU non-linearity\n",
    "3. **Project**: Bring back down to 16 dimensions\n",
    "\n",
    "Why the expansion? More dimensions = more room for complex transformations. The 4× ratio ($d_{ff} = 4 \\times d_{model}$) is standard in transformers.\n",
    "\n",
    "**Key insight**: The FFN is applied *independently* to each position. No cross-position interaction here. Attention lets tokens communicate; the FFN lets each token process what it's learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.244631Z",
     "iopub.status.busy": "2025-12-10T21:17:03.244507Z",
     "iopub.status.idle": "2025-12-10T21:17:03.246720Z",
     "shell.execute_reply": "2025-12-10T21:17:03.246469Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "D_FF = 64  # 4 * D_MODEL, standard expansion ratio\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.247662Z",
     "iopub.status.busy": "2025-12-10T21:17:03.247591Z",
     "iopub.status.idle": "2025-12-10T21:17:03.250641Z",
     "shell.execute_reply": "2025-12-10T21:17:03.250386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n, p = len(A), len(A[0]), len(B[0])\n",
    "    return [[sum(A[i][k] * B[k][j] for k in range(n)) for j in range(p)] for i in range(m)]\n",
    "\n",
    "def transpose(A):\n",
    "    return [[A[i][j] for i in range(len(A))] for j in range(len(A[0]))]\n",
    "\n",
    "def softmax(vec):\n",
    "    max_val = max(v for v in vec if v != float('-inf'))\n",
    "    exp_vec = [math.exp(v - max_val) if v != float('-inf') else 0 for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.251477Z",
     "iopub.status.busy": "2025-12-10T21:17:03.251403Z",
     "iopub.status.idle": "2025-12-10T21:17:03.255247Z",
     "shell.execute_reply": "2025-12-10T21:17:03.254985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated multi-head attention output\n",
      "Shape: [5, 16]\n"
     ]
    }
   ],
   "source": [
    "# Recreate multi-head attention output from previous notebooks\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "tokens = [1, 3, 4, 5, 2]\n",
    "seq_len = len(tokens)\n",
    "X = [add_vectors(E_token[tokens[i]], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "def compute_attention(Q, K, V):\n",
    "    seq_len, d_k = len(Q), len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    scores = matmul(Q, transpose(K))\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j > i:\n",
    "                scaled[i][j] = float('-inf')\n",
    "    weights = [softmax(row) for row in scaled]\n",
    "    return matmul(weights, V)\n",
    "\n",
    "attention_output_all = [compute_attention(Q_all[h], K_all[h], V_all[h]) for h in range(NUM_HEADS)]\n",
    "concat_output = [attention_output_all[0][i] + attention_output_all[1][i] for i in range(seq_len)]\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)\n",
    "multi_head_output = matmul(concat_output, transpose(W_O))\n",
    "\n",
    "print(\"Recreated multi-head attention output\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## The GELU Activation Function\n",
    "\n",
    "We're using GELU (Gaussian Error Linear Unit) as our non-linearity. The exact formula is:\n",
    "\n",
    "$$\\text{GELU}(x) = x \\cdot \\Phi(x)$$\n",
    "\n",
    "Where $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution. In practice, we use a fast approximation:\n",
    "\n",
    "$$\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)\\right)\\right)$$\n",
    "\n",
    "**Why GELU instead of ReLU?**\n",
    "\n",
    "ReLU just zeros out negatives: $\\text{ReLU}(x) = \\max(0, x)$. Simple, but it creates a hard cutoff—dead neurons that never recover.\n",
    "\n",
    "GELU is smoother. It still emphasizes positive values, but negative values get gently suppressed rather than killed entirely. This smoothness helps gradients flow better during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.256098Z",
     "iopub.status.busy": "2025-12-10T21:17:03.256033Z",
     "iopub.status.idle": "2025-12-10T21:17:03.258076Z",
     "shell.execute_reply": "2025-12-10T21:17:03.257827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU vs ReLU\n",
      "=============================================\n",
      "       x |       ReLU |       GELU\n",
      "---------------------------------------------\n",
      "    -2.0 |     0.0000 |    -0.0454\n",
      "    -1.0 |     0.0000 |    -0.1588\n",
      "    -0.5 |     0.0000 |    -0.1543\n",
      "     0.0 |     0.0000 |     0.0000\n",
      "     0.5 |     0.5000 |     0.3457\n",
      "     1.0 |     1.0000 |     0.8412\n",
      "     2.0 |     2.0000 |     1.9546\n",
      "\n",
      "Notice: GELU doesn't completely kill negative values.\n",
      "At x=-1, ReLU gives 0, but GELU gives -0.159.\n"
     ]
    }
   ],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"GELU activation using tanh approximation\"\"\"\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "# Compare GELU vs ReLU\n",
    "print(\"GELU vs ReLU\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"{'x':>8} | {'ReLU':>10} | {'GELU':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for x in [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]:\n",
    "    relu = max(0, x)\n",
    "    gelu_val = gelu(x)\n",
    "    print(f\"{x:>8.1f} | {relu:>10.4f} | {gelu_val:>10.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Notice: GELU doesn't completely kill negative values.\")\n",
    "print(\"At x=-1, ReLU gives 0, but GELU gives -0.159.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## FFN Weights\n",
    "\n",
    "The FFN has four sets of learnable parameters:\n",
    "\n",
    "| Parameter | Shape | Purpose |\n",
    "|-----------|-------|--------|\n",
    "| $W_1$ | [64, 16] | Expansion weights |\n",
    "| $b_1$ | [64] | Expansion bias |\n",
    "| $W_2$ | [16, 64] | Projection weights |\n",
    "| $b_2$ | [16] | Projection bias |\n",
    "\n",
    "**Total: 64×16 + 64 + 16×64 + 16 = 2,128 parameters**\n",
    "\n",
    "That's more than attention! The FFN is actually where most of the parameters live in a transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.258798Z",
     "iopub.status.busy": "2025-12-10T21:17:03.258732Z",
     "iopub.status.idle": "2025-12-10T21:17:03.260990Z",
     "shell.execute_reply": "2025-12-10T21:17:03.260748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN Parameters:\n",
      "  W1: [64, 16] = 1024 values\n",
      "  b1: [64] = 64 values\n",
      "  W2: [16, 64] = 1024 values\n",
      "  b2: [16] = 16 values\n",
      "  Total: 2128 parameters\n"
     ]
    }
   ],
   "source": [
    "# Initialize FFN weights\n",
    "W1 = random_matrix(D_FF, D_MODEL)   # [64, 16] - expansion\n",
    "b1 = random_vector(D_FF)            # [64]\n",
    "W2 = random_matrix(D_MODEL, D_FF)   # [16, 64] - projection\n",
    "b2 = random_vector(D_MODEL)         # [16]\n",
    "\n",
    "print(f\"FFN Parameters:\")\n",
    "print(f\"  W1: [{D_FF}, {D_MODEL}] = {D_FF * D_MODEL} values\")\n",
    "print(f\"  b1: [{D_FF}] = {D_FF} values\")\n",
    "print(f\"  W2: [{D_MODEL}, {D_FF}] = {D_MODEL * D_FF} values\")\n",
    "print(f\"  b2: [{D_MODEL}] = {D_MODEL} values\")\n",
    "print(f\"  Total: {D_FF * D_MODEL + D_FF + D_MODEL * D_FF + D_MODEL} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 1: Expansion Layer\n",
    "\n",
    "First, we expand from 16 dimensions to 64 dimensions:\n",
    "\n",
    "$$\\text{hidden} = W_1 \\cdot x + b_1$$\n",
    "\n",
    "For each position, we take the 16-dimensional vector, multiply by $W_1^T$ (shape `[16, 64]`), and add bias $b_1$. Result: 64 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.261732Z",
     "iopub.status.busy": "2025-12-10T21:17:03.261666Z",
     "iopub.status.idle": "2025-12-10T21:17:03.263766Z",
     "shell.execute_reply": "2025-12-10T21:17:03.263511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer (after expansion)\n",
      "Shape: [5, 64]\n",
      "\n",
      "Position 0 (<BOS>), first 8 of 64 dimensions:\n",
      "  [ 0.0000,  0.0439,  0.0457,  0.1031, -0.0962, -0.0283,  0.0890,  0.0303]...\n"
     ]
    }
   ],
   "source": [
    "# Compute first linear layer: hidden = input @ W1^T + b1\n",
    "W1_T = transpose(W1)\n",
    "hidden = matmul(multi_head_output, W1_T)\n",
    "hidden = [[hidden[i][j] + b1[j] for j in range(D_FF)] for i in range(seq_len)]\n",
    "\n",
    "print(f\"Hidden layer (after expansion)\")\n",
    "print(f\"Shape: [{seq_len}, {D_FF}]\")\n",
    "print()\n",
    "print(f\"Position 0 (<BOS>), first 8 of 64 dimensions:\")\n",
    "print(f\"  {format_vector(hidden[0][:8])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 2: GELU Activation\n",
    "\n",
    "Apply GELU element-wise to all 64 dimensions at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.264504Z",
     "iopub.status.busy": "2025-12-10T21:17:03.264425Z",
     "iopub.status.idle": "2025-12-10T21:17:03.266386Z",
     "shell.execute_reply": "2025-12-10T21:17:03.266140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After GELU activation\n",
      "\n",
      "Position 0, first 4 dimensions:\n",
      "  Before GELU: [ 0.0000,  0.0439,  0.0457,  0.1031]\n",
      "  After GELU:  [ 0.0000,  0.0227,  0.0237,  0.0558]\n",
      "\n",
      "Values shrink (especially negatives) but maintain sign.\n"
     ]
    }
   ],
   "source": [
    "# Apply GELU activation element-wise\n",
    "activated = [[gelu(h) for h in row] for row in hidden]\n",
    "\n",
    "print(f\"After GELU activation\")\n",
    "print()\n",
    "print(f\"Position 0, first 4 dimensions:\")\n",
    "print(f\"  Before GELU: {format_vector(hidden[0][:4])}\")\n",
    "print(f\"  After GELU:  {format_vector(activated[0][:4])}\")\n",
    "print()\n",
    "print(\"Values shrink (especially negatives) but maintain sign.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 3: Projection Layer\n",
    "\n",
    "Project back from 64 dimensions to 16 dimensions:\n",
    "\n",
    "$$\\text{output} = W_2 \\cdot \\text{activated} + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.267043Z",
     "iopub.status.busy": "2025-12-10T21:17:03.266979Z",
     "iopub.status.idle": "2025-12-10T21:17:03.269371Z",
     "shell.execute_reply": "2025-12-10T21:17:03.269108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN Output\n",
      "Shape: [5, 16]\n",
      "\n",
      "  [ 0.0043, -0.0896,  0.0020,  0.2294,  0.1020,  0.0966, -0.2073,  0.0574,  0.1951,  0.0692, -0.0388, -0.0762,  0.1390, -0.0384,  0.1633,  0.0529]  # <BOS>\n",
      "  [ 0.0012, -0.0877, -0.0015,  0.2298,  0.0984,  0.0971, -0.2083,  0.0581,  0.1963,  0.0669, -0.0434, -0.0800,  0.1372, -0.0373,  0.1639,  0.0528]  # I\n",
      "  [-0.0003, -0.0905,  0.0001,  0.2295,  0.0975,  0.0969, -0.2105,  0.0582,  0.1989,  0.0687, -0.0433, -0.0817,  0.1337, -0.0350,  0.1647,  0.0542]  # like\n",
      "  [ 0.0001, -0.0893, -0.0010,  0.2295,  0.0969,  0.0972, -0.2107,  0.0590,  0.1985,  0.0678, -0.0429, -0.0819,  0.1327, -0.0335,  0.1639,  0.0539]  # transformers\n",
      "  [-0.0004, -0.0894, -0.0002,  0.2300,  0.0976,  0.0970, -0.2113,  0.0588,  0.1994,  0.0691, -0.0428, -0.0819,  0.1326, -0.0337,  0.1642,  0.0539]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Compute second linear layer: output = activated @ W2^T + b2\n",
    "W2_T = transpose(W2)\n",
    "ffn_output = matmul(activated, W2_T)\n",
    "ffn_output = [[ffn_output[i][j] + b2[j] for j in range(D_MODEL)] for i in range(seq_len)]\n",
    "\n",
    "print(f\"FFN Output\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(ffn_output):\n",
    "    print(f\"  {format_vector(row)}  # {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Before and After\n",
    "\n",
    "Let's compare what went into the FFN (multi-head attention output) with what came out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.270161Z",
     "iopub.status.busy": "2025-12-10T21:17:03.270063Z",
     "iopub.status.idle": "2025-12-10T21:17:03.271990Z",
     "shell.execute_reply": "2025-12-10T21:17:03.271742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 1 ('I') - Before and After FFN\n",
      "======================================================================\n",
      "\n",
      "Before FFN (attention output):\n",
      "  [ 0.0269,  0.0066,  0.0113, -0.0154,  0.0114,  0.0032, -0.0065, -0.0108,  0.0190, -0.0091,  0.0180,  0.0097, -0.0075,  0.0061, -0.0079,  0.0110]\n",
      "\n",
      "After FFN:\n",
      "  [ 0.0012, -0.0877, -0.0015,  0.2298,  0.0984,  0.0971, -0.2083,  0.0581,  0.1963,  0.0669, -0.0434, -0.0800,  0.1372, -0.0373,  0.1639,  0.0528]\n",
      "\n",
      "The FFN has transformed the representation through\n",
      "expansion → non-linearity → projection.\n"
     ]
    }
   ],
   "source": [
    "print(\"Position 1 ('I') - Before and After FFN\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Before FFN (attention output):\")\n",
    "print(f\"  {format_vector(multi_head_output[1])}\")\n",
    "print()\n",
    "print(f\"After FFN:\")\n",
    "print(f\"  {format_vector(ffn_output[1])}\")\n",
    "print()\n",
    "print(\"The FFN has transformed the representation through\")\n",
    "print(\"expansion → non-linearity → projection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## What the FFN Accomplishes\n",
    "\n",
    "The FFN serves several purposes:\n",
    "\n",
    "1. **Non-linearity**: Attention is linear; GELU adds the non-linear transformations needed to learn complex functions.\n",
    "\n",
    "2. **Position-wise processing**: Each token gets independent processing time. Attention mixed information *between* tokens; FFN lets each token *digest* what it learned.\n",
    "\n",
    "3. **Feature transformation**: The expansion to 64 dimensions gives the model room to create new feature combinations, emphasize important patterns, and suppress noise.\n",
    "\n",
    "4. **Memory storage**: Research suggests FFNs act as key-value memories, storing factual knowledge learned during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "There's a problem: we just *replaced* the attention output with the FFN output. All that information from attention is gone!\n",
    "\n",
    "That's where **residual connections** come in. Instead of replacing, we'll *add* the FFN output to the original input. This way:\n",
    "- The original information is preserved\n",
    "- The FFN learns to compute *changes* rather than complete replacements\n",
    "- Gradients can flow directly through the residual path\n",
    "\n",
    "We'll also apply **layer normalization** to keep activations stable. That's the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:03.272675Z",
     "iopub.status.busy": "2025-12-10T21:17:03.272611Z",
     "iopub.status.idle": "2025-12-10T21:17:03.274095Z",
     "shell.execute_reply": "2025-12-10T21:17:03.273859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN complete. Ready for layer normalization.\n"
     ]
    }
   ],
   "source": [
    "# Store for next notebook\n",
    "ffn_data = {\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'multi_head_output': multi_head_output,\n",
    "    'ffn_output': ffn_output,\n",
    "    'W1': W1, 'b1': b1,\n",
    "    'W2': W2, 'b2': b2\n",
    "}\n",
    "print(\"FFN complete. Ready for layer normalization.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
