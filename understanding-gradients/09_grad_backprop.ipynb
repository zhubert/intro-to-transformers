{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Backpropagation Through the Network\n",
    "\n",
    "**Computing gradients for all layers via the chain rule**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the loss gradients with respect to logits. Now we need to propagate these gradients backward through every layer:\n",
    "\n",
    "1. **Output layer** → gradients for W_lm and hidden states\n",
    "2. **Layer norm** → gradients for gamma, beta, and pre-norm activations\n",
    "3. **FFN** → gradients for W1, b1, W2, b2\n",
    "4. **Multi-head attention** → gradients for W_Q, W_K, W_V, W_O\n",
    "5. **Embeddings** → gradients for E_token and E_pos\n",
    "\n",
    "The key tool is the **chain rule**: if $L$ depends on $y$ which depends on $x$, then:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "D_FF = 64\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Output Layer Gradients\n",
    "\n",
    "The output layer computes: $\\text{logits} = h \\cdot W_{lm}^T$\n",
    "\n",
    "We need:\n",
    "- $\\frac{\\partial L}{\\partial W_{lm}}$ to update the weights\n",
    "- $\\frac{\\partial L}{\\partial h}$ to continue backprop\n",
    "\n",
    "For a linear layer $y = x \\cdot W^T$:\n",
    "- $\\frac{\\partial L}{\\partial W} = (\\frac{\\partial L}{\\partial y})^T \\cdot x$\n",
    "- $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated values (in practice, these come from forward pass)\n",
    "seq_len = 4  # Only positions with targets\n",
    "\n",
    "# Loss gradients from previous notebook\n",
    "dL_dlogits = [\n",
    "    [0.1785, 0.2007, 0.1759, -0.8746, 0.1563, 0.1632],\n",
    "    [0.1836, 0.1969, 0.1805, 0.1233, -0.8500, 0.1657],\n",
    "    [0.1795, 0.2050, 0.1782, 0.1207, 0.1437, -0.8272],\n",
    "    [0.1855, 0.2017, -0.8229, 0.1271, 0.1391, 0.1695],\n",
    "]\n",
    "\n",
    "# Random hidden states and W_lm (for demonstration)\n",
    "h = [random_vector(D_MODEL) for _ in range(seq_len)]\n",
    "W_lm = random_matrix(VOCAB_SIZE, D_MODEL)\n",
    "\n",
    "print(f\"dL_dlogits shape: [{seq_len}, {VOCAB_SIZE}]\")\n",
    "print(f\"h shape: [{seq_len}, {D_MODEL}]\")\n",
    "print(f\"W_lm shape: [{VOCAB_SIZE}, {D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient for W_lm\n",
    "# dL_dW_lm[i][j] = sum over positions of dL_dlogits[pos][i] * h[pos][j]\n",
    "dL_dW_lm = [[0.0] * D_MODEL for _ in range(VOCAB_SIZE)]\n",
    "for pos in range(seq_len):\n",
    "    for i in range(VOCAB_SIZE):\n",
    "        for j in range(D_MODEL):\n",
    "            dL_dW_lm[i][j] += dL_dlogits[pos][i] * h[pos][j]\n",
    "\n",
    "print(\"Gradient for W_lm (first row):\")\n",
    "print(f\"  {format_vector(dL_dW_lm[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient for hidden states\n",
    "# dL_dh[pos][j] = sum over vocab of dL_dlogits[pos][i] * W_lm[i][j]\n",
    "dL_dh = [[0.0] * D_MODEL for _ in range(seq_len)]\n",
    "for pos in range(seq_len):\n",
    "    for j in range(D_MODEL):\n",
    "        for i in range(VOCAB_SIZE):\n",
    "            dL_dh[pos][j] += dL_dlogits[pos][i] * W_lm[i][j]\n",
    "\n",
    "print(\"Gradient for hidden states (position 0):\")\n",
    "print(f\"  {format_vector(dL_dh[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Layer Norm Gradients\n",
    "\n",
    "Layer norm is: $y = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta$\n",
    "\n",
    "The gradients involve:\n",
    "- $\\frac{\\partial L}{\\partial \\gamma}$ and $\\frac{\\partial L}{\\partial \\beta}$ for parameters\n",
    "- $\\frac{\\partial L}{\\partial x}$ which requires the Jacobian of layer norm\n",
    "\n",
    "The Jacobian is complex because normalizing each element affects the mean and variance, which affects all other elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gamma and beta, the gradients are simpler\n",
    "# dL_dgamma[j] = sum over positions of dL_dy[pos][j] * x_norm[pos][j]\n",
    "# dL_dbeta[j] = sum over positions of dL_dy[pos][j]\n",
    "\n",
    "# Simulated normalized values\n",
    "x_norm = [random_vector(D_MODEL) for _ in range(seq_len)]\n",
    "\n",
    "dL_dgamma = [0.0] * D_MODEL\n",
    "dL_dbeta = [0.0] * D_MODEL\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    for j in range(D_MODEL):\n",
    "        dL_dgamma[j] += dL_dh[pos][j] * x_norm[pos][j]\n",
    "        dL_dbeta[j] += dL_dh[pos][j]\n",
    "\n",
    "print(\"Gradient for gamma (first 8 values):\")\n",
    "print(f\"  {format_vector(dL_dgamma[:8])}\")\n",
    "print()\n",
    "print(\"Gradient for beta (first 8 values):\")\n",
    "print(f\"  {format_vector(dL_dbeta[:8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: FFN Gradients\n",
    "\n",
    "The FFN computes:\n",
    "1. $h_1 = x \\cdot W_1^T + b_1$\n",
    "2. $h_2 = \\text{GELU}(h_1)$\n",
    "3. $y = h_2 \\cdot W_2^T + b_2$\n",
    "\n",
    "We backprop through each in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU derivative\n",
    "def gelu_derivative(x):\n",
    "    \"\"\"Derivative of GELU activation\"\"\"\n",
    "    # Approximation of GELU derivative\n",
    "    cdf = 0.5 * (1 + math.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))\n",
    "    pdf = math.exp(-x**2 / 2) / math.sqrt(2 * math.pi)\n",
    "    return cdf + x * pdf\n",
    "\n",
    "# Example\n",
    "print(\"GELU derivatives at sample points:\")\n",
    "for x in [-1.0, 0.0, 1.0]:\n",
    "    print(f\"  GELU'({x:4.1f}) = {gelu_derivative(x):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2 gradients: dL_dW2[i][j] = sum of dL_dy[pos][i] * h2[pos][j]\n",
    "# b2 gradients: dL_db2[i] = sum of dL_dy[pos][i]\n",
    "\n",
    "# Simulated values\n",
    "h2 = [random_vector(D_FF) for _ in range(seq_len)]  # After GELU\n",
    "dL_dy = dL_dh  # Gradient flowing in (after accounting for residual)\n",
    "\n",
    "W2 = random_matrix(D_MODEL, D_FF)\n",
    "\n",
    "dL_dW2 = [[0.0] * D_FF for _ in range(D_MODEL)]\n",
    "dL_db2 = [0.0] * D_MODEL\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    for i in range(D_MODEL):\n",
    "        dL_db2[i] += dL_dy[pos][i]\n",
    "        for j in range(D_FF):\n",
    "            dL_dW2[i][j] += dL_dy[pos][i] * h2[pos][j]\n",
    "\n",
    "print(f\"dL_dW2 shape: [{D_MODEL}, {D_FF}]\")\n",
    "print(f\"dL_db2 shape: [{D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Attention Gradients\n",
    "\n",
    "This is the most complex part. We need gradients for:\n",
    "- $W_Q$, $W_K$, $W_V$ (per head)\n",
    "- $W_O$ (output projection)\n",
    "\n",
    "The attention computation involves:\n",
    "1. Q, K, V projections\n",
    "2. Scaled dot-product attention with softmax\n",
    "3. Concatenation and output projection\n",
    "\n",
    "Each step requires careful application of the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified example: gradient for W_O\n",
    "# W_O projects concatenated attention outputs back to d_model\n",
    "\n",
    "concat_attn = [random_vector(D_MODEL) for _ in range(seq_len)]\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)\n",
    "\n",
    "# dL_dW_O[i][j] = sum of dL_dattn[pos][i] * concat[pos][j]\n",
    "dL_dW_O = [[0.0] * D_MODEL for _ in range(D_MODEL)]\n",
    "for pos in range(seq_len):\n",
    "    for i in range(D_MODEL):\n",
    "        for j in range(D_MODEL):\n",
    "            dL_dW_O[i][j] += dL_dh[pos][i] * concat_attn[pos][j]\n",
    "\n",
    "print(f\"dL_dW_O shape: [{D_MODEL}, {D_MODEL}]\")\n",
    "print(f\"First row: {format_vector(dL_dW_O[0][:8])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Embedding Gradients\n",
    "\n",
    "Finally, we compute gradients for the embeddings:\n",
    "- $E_{token}$: token embeddings\n",
    "- $E_{pos}$: position embeddings\n",
    "\n",
    "The embedding lookup is essentially indexing, so gradients flow back only to the embeddings that were actually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient for token embeddings\n",
    "# Only tokens that appeared in the sequence receive gradients\n",
    "tokens_used = [1, 3, 4, 5, 2]  # BOS, I, like, transformers, EOS\n",
    "\n",
    "dL_dE_token = [[0.0] * D_MODEL for _ in range(VOCAB_SIZE)]\n",
    "\n",
    "# Simulated gradient flowing into embeddings\n",
    "dL_dX = [random_vector(D_MODEL) for _ in range(5)]\n",
    "\n",
    "for pos, token_id in enumerate(tokens_used):\n",
    "    for j in range(D_MODEL):\n",
    "        dL_dE_token[token_id][j] += dL_dX[pos][j]\n",
    "\n",
    "print(\"Token embedding gradients:\")\n",
    "for i, name in enumerate(TOKEN_NAMES):\n",
    "    norm = sum(g**2 for g in dL_dE_token[i]) ** 0.5\n",
    "    if norm > 0:\n",
    "        print(f\"  {name:12s}: gradient norm = {norm:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {name:12s}: no gradient (not used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Chain Rule at Work\n",
    "\n",
    "We've traced gradients backward through:\n",
    "\n",
    "1. **Loss → Logits**: Simple formula $P(i) - \\mathbb{1}[i=\\text{target}]$\n",
    "2. **Logits → Hidden states**: Linear layer backprop\n",
    "3. **Layer norm**: Jacobian through normalization\n",
    "4. **FFN**: Two linear layers + GELU derivative\n",
    "5. **Attention**: Complex but systematic chain rule application\n",
    "6. **Embeddings**: Gradient accumulation for used tokens\n",
    "\n",
    "Every parameter now has a gradient telling us how to reduce the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "We have gradients for all ~2,600 parameters. Now we use the **AdamW optimizer** to actually update them.\n",
    "\n",
    "AdamW combines:\n",
    "- Adaptive learning rates per parameter\n",
    "- Momentum to smooth updates\n",
    "- Weight decay for regularization\n",
    "\n",
    "That's the final step in our training loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
