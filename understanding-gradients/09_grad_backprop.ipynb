{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Chain Rule: Our Only Tool\n",
    "\n",
    "In the last notebook, we computed $\\frac{\\partial L}{\\partial \\text{logits}}$. How the loss depends on the raw prediction scores. But we can't modify logits directly. They're computed from hidden states, which are computed from the FFN, which depends on attention, which depends on embeddings.\n",
    "\n",
    "To train the model, we need gradients for all the *learnable parameters*:\n",
    "- Token embeddings ($E_{token}$)\n",
    "- Position embeddings ($E_{pos}$)\n",
    "- Q, K, V projection matrices ($W_Q$, $W_K$, $W_V$ for each head)\n",
    "- Output projection ($W_O$)\n",
    "- FFN weights ($W_1$, $b_1$, $W_2$, $b_2$)\n",
    "- Layer norm parameters ($\\gamma$, $\\beta$)\n",
    "- Language modeling head ($W_{lm}$)\n",
    "\n",
    "**The chain rule** is how we get there. If the loss $L$ depends on $y$, and $y$ depends on $x$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "We already have $\\frac{\\partial L}{\\partial y}$ from the previous layer. We just need to compute the local derivative $\\frac{\\partial y}{\\partial x}$ and multiply.\n",
    "\n",
    "This notebook works backward through every layer, computing gradients as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.438544Z",
     "iopub.status.busy": "2025-12-10T21:17:10.438471Z",
     "iopub.status.idle": "2025-12-10T21:17:10.440449Z",
     "shell.execute_reply": "2025-12-10T21:17:10.440169Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "D_FF = 64\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS  # 8\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.441208Z",
     "iopub.status.busy": "2025-12-10T21:17:10.441128Z",
     "iopub.status.idle": "2025-12-10T21:17:10.443371Z",
     "shell.execute_reply": "2025-12-10T21:17:10.443068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def zeros_matrix(rows, cols):\n",
    "    return [[0.0] * cols for _ in range(rows)]\n",
    "\n",
    "def zeros_vector(size):\n",
    "    return [0.0] * size\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Output Layer (LM Head) Gradients\n",
    "\n",
    "The output layer computes:\n",
    "\n",
    "$$\\text{logits} = h \\cdot W_{lm}^T$$\n",
    "\n",
    "Where:\n",
    "- $h$ has shape `[seq_len, d_model]` = `[4, 16]` (hidden states going into prediction)\n",
    "- $W_{lm}$ has shape `[vocab_size, d_model]` = `[6, 16]`\n",
    "- $\\text{logits}$ has shape `[seq_len, vocab_size]` = `[4, 6]`\n",
    "\n",
    "We have $\\frac{\\partial L}{\\partial \\text{logits}}$ from the previous notebook. We need:\n",
    "1. $\\frac{\\partial L}{\\partial W_{lm}}$. to update the weights\n",
    "2. $\\frac{\\partial L}{\\partial h}$. to continue backpropagating\n",
    "\n",
    "**Gradient for a linear layer:**\n",
    "\n",
    "For $y = x \\cdot W^T$ (a general linear layer):\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\left(\\frac{\\partial L}{\\partial y}\\right)^T \\cdot x$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot W$$\n",
    "\n",
    "These formulas come from matrix calculus. The key insight is that in a linear layer, input and output are connected through the weight matrix in a way that's \"symmetric\" for forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.444148Z",
     "iopub.status.busy": "2025-12-10T21:17:10.444062Z",
     "iopub.status.idle": "2025-12-10T21:17:10.446404Z",
     "shell.execute_reply": "2025-12-10T21:17:10.446154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  dL_dlogits: [4, 6]\n",
      "  h:          [4, 16]\n",
      "  W_lm:       [6, 16]\n"
     ]
    }
   ],
   "source": [
    "# We have 4 positions that make predictions (positions 0-3 predict tokens 1-4)\n",
    "seq_len = 4\n",
    "\n",
    "# Loss gradients from previous notebook\n",
    "dL_dlogits = [\n",
    "    [0.1785, 0.2007, 0.1759, -0.8746, 0.1563, 0.1632],  # position 0: should predict I\n",
    "    [0.1836, 0.1969, 0.1805, 0.1233, -0.8500, 0.1657],  # position 1: should predict like\n",
    "    [0.1795, 0.2050, 0.1782, 0.1207, 0.1437, -0.8272],  # position 2: should predict transformers\n",
    "    [0.1855, 0.2017, -0.8229, 0.1271, 0.1391, 0.1695],  # position 3: should predict <EOS>\n",
    "]\n",
    "\n",
    "# Simulated hidden states (would come from forward pass)\n",
    "h = [random_vector(D_MODEL) for _ in range(seq_len)]\n",
    "\n",
    "# Simulated W_lm (would be initialized at model creation)\n",
    "W_lm = random_matrix(VOCAB_SIZE, D_MODEL)\n",
    "\n",
    "print(f\"Shapes:\")\n",
    "print(f\"  dL_dlogits: [{seq_len}, {VOCAB_SIZE}]\")\n",
    "print(f\"  h:          [{seq_len}, {D_MODEL}]\")\n",
    "print(f\"  W_lm:       [{VOCAB_SIZE}, {D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.463439Z",
     "iopub.status.busy": "2025-12-10T21:17:10.463368Z",
     "iopub.status.idle": "2025-12-10T21:17:10.465333Z",
     "shell.execute_reply": "2025-12-10T21:17:10.465012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for W_lm (dL/dW_lm)\n",
      "Shape: [6, 16]\n",
      "\n",
      "First row (gradient for <PAD> token's projection):\n",
      "  [-0.0567, -0.0032, -0.0264,  0.0356,  0.0148, -0.0320,  0.0002, -0.0341,  0.0249,  0.0188,  0.0225,  0.0555,  0.0640, -0.0139, -0.0175, -0.0472]\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient for W_lm\n",
    "# dL_dW_lm[i][j] = sum over positions of dL_dlogits[pos][i] * h[pos][j]\n",
    "\n",
    "dL_dW_lm = zeros_matrix(VOCAB_SIZE, D_MODEL)\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    for i in range(VOCAB_SIZE):  # vocabulary index\n",
    "        for j in range(D_MODEL):  # embedding dimension\n",
    "            dL_dW_lm[i][j] += dL_dlogits[pos][i] * h[pos][j]\n",
    "\n",
    "print(\"Gradient for W_lm (dL/dW_lm)\")\n",
    "print(f\"Shape: [{VOCAB_SIZE}, {D_MODEL}]\")\n",
    "print()\n",
    "print(\"First row (gradient for <PAD> token's projection):\")\n",
    "print(f\"  {format_vector(dL_dW_lm[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.466061Z",
     "iopub.status.busy": "2025-12-10T21:17:10.465978Z",
     "iopub.status.idle": "2025-12-10T21:17:10.468044Z",
     "shell.execute_reply": "2025-12-10T21:17:10.467794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for hidden states (dL/dh)\n",
      "Shape: [4, 16]\n",
      "\n",
      "Position 0 gradient:\n",
      "  [ 0.0903, -0.0471, -0.1804,  0.0674,  0.0800, -0.1086,  0.1034,  0.0616,  0.0108,  0.1447,  0.0161,  0.1209, -0.1277,  0.0034, -0.1746,  0.0298]\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient for hidden states (to continue backprop)\n",
    "# dL_dh[pos][j] = sum over vocab of dL_dlogits[pos][i] * W_lm[i][j]\n",
    "\n",
    "dL_dh = zeros_matrix(seq_len, D_MODEL)\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    for j in range(D_MODEL):  # embedding dimension\n",
    "        for i in range(VOCAB_SIZE):  # vocabulary index\n",
    "            dL_dh[pos][j] += dL_dlogits[pos][i] * W_lm[i][j]\n",
    "\n",
    "print(\"Gradient for hidden states (dL/dh)\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "print(\"Position 0 gradient:\")\n",
    "print(f\"  {format_vector(dL_dh[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Layer Normalization Gradients\n",
    "\n",
    "Layer norm is more complex because normalizing one element affects the mean and variance, which affects *all* elements. The formula is:\n",
    "\n",
    "$$y = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "Where $\\mu$ and $\\sigma$ are computed from $x$ itself.\n",
    "\n",
    "**Gradients for parameters:**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\gamma_j} = \\sum_{\\text{positions}} \\frac{\\partial L}{\\partial y_j} \\cdot \\hat{x}_j$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\beta_j} = \\sum_{\\text{positions}} \\frac{\\partial L}{\\partial y_j}$$\n",
    "\n",
    "Where $\\hat{x} = \\frac{x - \\mu}{\\sigma}$ is the normalized input.\n",
    "\n",
    "**Gradient for input (to continue backprop):**\n",
    "\n",
    "This involves the Jacobian of layer norm, which is a bit involved because changing one input element affects $\\mu$ and $\\sigma$. The full derivation is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{\\gamma_i}{\\sigma} \\left( \\frac{\\partial L}{\\partial y_i} - \\frac{1}{d} \\sum_j \\frac{\\partial L}{\\partial y_j} - \\frac{\\hat{x}_i}{d} \\sum_j \\frac{\\partial L}{\\partial y_j} \\hat{x}_j \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.468705Z",
     "iopub.status.busy": "2025-12-10T21:17:10.468641Z",
     "iopub.status.idle": "2025-12-10T21:17:10.470896Z",
     "shell.execute_reply": "2025-12-10T21:17:10.470679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Norm Gradients\n",
      "==================================================\n",
      "\n",
      "Gradient for gamma (first 8 of 16 values):\n",
      "  [ 0.0225, -0.0240, -0.0296,  0.0103,  0.0170, -0.0249,  0.0179, -0.0028]\n",
      "\n",
      "Gradient for beta (first 8 of 16 values):\n",
      "  [-0.0287, -0.0846, -0.2322, -0.0055, -0.1733,  0.0561,  0.0873,  0.1953]\n"
     ]
    }
   ],
   "source": [
    "# Simulated normalized values (would come from forward pass)\n",
    "x_norm = [random_vector(D_MODEL) for _ in range(seq_len)]  # x_hat = (x - mean) / std\n",
    "\n",
    "# Initial gamma = 1, beta = 0 (standard initialization)\n",
    "gamma = [1.0] * D_MODEL\n",
    "beta = [0.0] * D_MODEL\n",
    "\n",
    "# Compute gradients for gamma and beta\n",
    "dL_dgamma = zeros_vector(D_MODEL)\n",
    "dL_dbeta = zeros_vector(D_MODEL)\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    for j in range(D_MODEL):\n",
    "        dL_dgamma[j] += dL_dh[pos][j] * x_norm[pos][j]\n",
    "        dL_dbeta[j] += dL_dh[pos][j]\n",
    "\n",
    "print(\"Layer Norm Gradients\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"Gradient for gamma (first 8 of 16 values):\")\n",
    "print(f\"  {format_vector(dL_dgamma[:8])}\")\n",
    "print()\n",
    "print(\"Gradient for beta (first 8 of 16 values):\")\n",
    "print(f\"  {format_vector(dL_dbeta[:8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feed-Forward Network Gradients\n",
    "\n",
    "The FFN computes:\n",
    "\n",
    "$$\\text{FFN}(x) = W_2 \\cdot \\text{GELU}(W_1 \\cdot x + b_1) + b_2$$\n",
    "\n",
    "Breaking this into steps:\n",
    "1. $h_1 = W_1 \\cdot x + b_1$. Linear projection (expand to 64 dims)\n",
    "2. $h_2 = \\text{GELU}(h_1)$. Activation function\n",
    "3. $y = W_2 \\cdot h_2 + b_2$. Linear projection (back to 16 dims)\n",
    "\n",
    "We backprop through each in reverse order.\n",
    "\n",
    "**GELU derivative:**\n",
    "\n",
    "GELU is defined as $\\text{GELU}(x) = x \\cdot \\Phi(x)$ where $\\Phi$ is the standard Gaussian CDF.\n",
    "\n",
    "Its derivative is:\n",
    "\n",
    "$$\\text{GELU}'(x) = \\Phi(x) + x \\cdot \\phi(x)$$\n",
    "\n",
    "Where $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}$ is the Gaussian PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.471656Z",
     "iopub.status.busy": "2025-12-10T21:17:10.471592Z",
     "iopub.status.idle": "2025-12-10T21:17:10.473771Z",
     "shell.execute_reply": "2025-12-10T21:17:10.473534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Derivative Values\n",
      "========================================\n",
      "\n",
      "       x      GELU(x)     GELU'(x)\n",
      "----------------------------------------\n",
      "    -2.0      -0.0454      -0.0853\n",
      "    -1.0      -0.1588      -0.0832\n",
      "    -0.5      -0.1543       0.1325\n",
      "     0.0       0.0000       0.5000\n",
      "     0.5       0.3457       0.8675\n",
      "     1.0       0.8412       1.0832\n",
      "     2.0       1.9546       1.0853\n"
     ]
    }
   ],
   "source": [
    "def gelu_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of GELU activation function.\n",
    "    GELU(x) = x * Phi(x) where Phi is standard Gaussian CDF.\n",
    "    GELU'(x) = Phi(x) + x * phi(x) where phi is Gaussian PDF.\n",
    "    \"\"\"\n",
    "    # Gaussian CDF approximation (same as in forward GELU)\n",
    "    cdf = 0.5 * (1 + math.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))\n",
    "    # Gaussian PDF\n",
    "    pdf = math.exp(-x**2 / 2) / math.sqrt(2 * math.pi)\n",
    "    return cdf + x * pdf\n",
    "\n",
    "# Show GELU derivative behavior\n",
    "print(\"GELU Derivative Values\")\n",
    "print(\"=\"*40)\n",
    "print()\n",
    "print(f\"{'x':>8} {'GELU(x)':>12} {'GELU\\'(x)':>12}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "for x in [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]:\n",
    "    print(f\"{x:>8.1f} {gelu(x):>12.4f} {gelu_derivative(x):>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.474402Z",
     "iopub.status.busy": "2025-12-10T21:17:10.474336Z",
     "iopub.status.idle": "2025-12-10T21:17:10.476933Z",
     "shell.execute_reply": "2025-12-10T21:17:10.476599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN Shapes:\n",
      "  W1: [64, 16] (expand)\n",
      "  W2: [16, 64] (project)\n",
      "  h1, h2: [4, 64]\n"
     ]
    }
   ],
   "source": [
    "# Simulated values from forward pass\n",
    "h1 = [random_vector(D_FF) for _ in range(seq_len)]   # Before GELU [4, 64]\n",
    "h2 = [[gelu(val) for val in row] for row in h1]      # After GELU [4, 64]\n",
    "x_ffn = [random_vector(D_MODEL) for _ in range(seq_len)]  # Input to FFN [4, 16]\n",
    "\n",
    "# Weights (would be initialized at model creation)\n",
    "W1 = random_matrix(D_FF, D_MODEL)    # [64, 16]\n",
    "W2 = random_matrix(D_MODEL, D_FF)    # [16, 64]\n",
    "b1 = random_vector(D_FF)             # [64]\n",
    "b2 = random_vector(D_MODEL)          # [16]\n",
    "\n",
    "# Gradient flowing in (after residual connection handling)\n",
    "dL_dy = dL_dh  # [4, 16]\n",
    "\n",
    "print(\"FFN Shapes:\")\n",
    "print(f\"  W1: [{D_FF}, {D_MODEL}] (expand)\")\n",
    "print(f\"  W2: [{D_MODEL}, {D_FF}] (project)\")\n",
    "print(f\"  h1, h2: [{seq_len}, {D_FF}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.477673Z",
     "iopub.status.busy": "2025-12-10T21:17:10.477579Z",
     "iopub.status.idle": "2025-12-10T21:17:10.480126Z",
     "shell.execute_reply": "2025-12-10T21:17:10.479887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN Second Layer Gradients\n",
      "  dL_dW2 shape: [16, 64]\n",
      "  dL_db2 shape: [16]\n",
      "\n",
      "  dL_db2 (first 8): [-0.0287, -0.0846, -0.2322, -0.0055, -0.1733,  0.0561,  0.0873,  0.1953]\n"
     ]
    }
   ],
   "source": [
    "# Step 3a: Gradient for W2 and b2 (second linear layer)\n",
    "# y = h2 @ W2^T + b2\n",
    "\n",
    "dL_dW2 = zeros_matrix(D_MODEL, D_FF)\n",
    "dL_db2 = zeros_vector(D_MODEL)\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    for i in range(D_MODEL):\n",
    "        dL_db2[i] += dL_dy[pos][i]\n",
    "        for j in range(D_FF):\n",
    "            dL_dW2[i][j] += dL_dy[pos][i] * h2[pos][j]\n",
    "\n",
    "print(\"FFN Second Layer Gradients\")\n",
    "print(f\"  dL_dW2 shape: [{D_MODEL}, {D_FF}]\")\n",
    "print(f\"  dL_db2 shape: [{D_MODEL}]\")\n",
    "print()\n",
    "print(f\"  dL_db2 (first 8): {format_vector(dL_db2[:8])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.480824Z",
     "iopub.status.busy": "2025-12-10T21:17:10.480757Z",
     "iopub.status.idle": "2025-12-10T21:17:10.483400Z",
     "shell.execute_reply": "2025-12-10T21:17:10.483147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backprop through GELU\n",
      "  dL_dh2 shape: [4, 64]\n",
      "  dL_dh1 shape: [4, 64]\n"
     ]
    }
   ],
   "source": [
    "# Step 3b: Gradient flowing to h2 (for backprop through GELU)\n",
    "# dL_dh2[pos][j] = sum_i dL_dy[pos][i] * W2[i][j]\n",
    "\n",
    "dL_dh2 = zeros_matrix(seq_len, D_FF)\n",
    "for pos in range(seq_len):\n",
    "    for j in range(D_FF):\n",
    "        for i in range(D_MODEL):\n",
    "            dL_dh2[pos][j] += dL_dy[pos][i] * W2[i][j]\n",
    "\n",
    "# Step 3c: Backprop through GELU (element-wise)\n",
    "# dL_dh1[pos][j] = dL_dh2[pos][j] * GELU'(h1[pos][j])\n",
    "\n",
    "dL_dh1 = zeros_matrix(seq_len, D_FF)\n",
    "for pos in range(seq_len):\n",
    "    for j in range(D_FF):\n",
    "        dL_dh1[pos][j] = dL_dh2[pos][j] * gelu_derivative(h1[pos][j])\n",
    "\n",
    "print(\"Backprop through GELU\")\n",
    "print(f\"  dL_dh2 shape: [{seq_len}, {D_FF}]\")\n",
    "print(f\"  dL_dh1 shape: [{seq_len}, {D_FF}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.484029Z",
     "iopub.status.busy": "2025-12-10T21:17:10.483963Z",
     "iopub.status.idle": "2025-12-10T21:17:10.486293Z",
     "shell.execute_reply": "2025-12-10T21:17:10.486035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN First Layer Gradients\n",
      "  dL_dW1 shape: [64, 16]\n",
      "  dL_db1 shape: [64]\n",
      "\n",
      "  dL_db1 (first 8 of 64): [-0.0364,  0.0281,  0.0095, -0.0106, -0.0364,  0.0285, -0.0004, -0.0165]\n"
     ]
    }
   ],
   "source": [
    "# Step 3d: Gradient for W1 and b1 (first linear layer)\n",
    "# h1 = x @ W1^T + b1\n",
    "\n",
    "dL_dW1 = zeros_matrix(D_FF, D_MODEL)\n",
    "dL_db1 = zeros_vector(D_FF)\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    for i in range(D_FF):\n",
    "        dL_db1[i] += dL_dh1[pos][i]\n",
    "        for j in range(D_MODEL):\n",
    "            dL_dW1[i][j] += dL_dh1[pos][i] * x_ffn[pos][j]\n",
    "\n",
    "print(\"FFN First Layer Gradients\")\n",
    "print(f\"  dL_dW1 shape: [{D_FF}, {D_MODEL}]\")\n",
    "print(f\"  dL_db1 shape: [{D_FF}]\")\n",
    "print()\n",
    "print(f\"  dL_db1 (first 8 of 64): {format_vector(dL_db1[:8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Attention Gradients\n",
    "\n",
    "This is the most complex part. The attention mechanism involves:\n",
    "1. **Q, K, V projections**: $Q = X \\cdot W_Q$, $K = X \\cdot W_K$, $V = X \\cdot W_V$\n",
    "2. **Attention scores**: $\\text{scores} = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}$\n",
    "3. **Softmax**: $\\text{weights} = \\text{softmax}(\\text{scores})$\n",
    "4. **Weighted values**: $\\text{output} = \\text{weights} \\cdot V$\n",
    "5. **Output projection**: $\\text{multi\\_head} = \\text{concat} \\cdot W_O$\n",
    "\n",
    "We need to backprop through all of these. Let's start with the output projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.486886Z",
     "iopub.status.busy": "2025-12-10T21:17:10.486822Z",
     "iopub.status.idle": "2025-12-10T21:17:10.488894Z",
     "shell.execute_reply": "2025-12-10T21:17:10.488669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Projection Gradient\n",
      "  dL_dW_O shape: [16, 16]\n",
      "  First row: [-0.0054, -0.0005, -0.0072, -0.0334,  0.0020,  0.0262, -0.0032,  0.0370]...\n"
     ]
    }
   ],
   "source": [
    "# Simulated concatenated attention output\n",
    "concat_attn = [random_vector(D_MODEL) for _ in range(seq_len)]\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)\n",
    "\n",
    "# Gradient for W_O\n",
    "# multi_head = concat @ W_O^T\n",
    "# dL_dW_O[i][j] = sum_pos dL_dmh[pos][i] * concat[pos][j]\n",
    "\n",
    "dL_dW_O = zeros_matrix(D_MODEL, D_MODEL)\n",
    "for pos in range(seq_len):\n",
    "    for i in range(D_MODEL):\n",
    "        for j in range(D_MODEL):\n",
    "            dL_dW_O[i][j] += dL_dh[pos][i] * concat_attn[pos][j]\n",
    "\n",
    "print(\"Output Projection Gradient\")\n",
    "print(f\"  dL_dW_O shape: [{D_MODEL}, {D_MODEL}]\")\n",
    "print(f\"  First row: {format_vector(dL_dW_O[0][:8])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.489525Z",
     "iopub.status.busy": "2025-12-10T21:17:10.489461Z",
     "iopub.status.idle": "2025-12-10T21:17:10.491296Z",
     "shell.execute_reply": "2025-12-10T21:17:10.491032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for concatenated attention output\n",
      "  Shape: [4, 16]\n"
     ]
    }
   ],
   "source": [
    "# Gradient flowing to concatenated attention\n",
    "dL_dconcat = zeros_matrix(seq_len, D_MODEL)\n",
    "for pos in range(seq_len):\n",
    "    for j in range(D_MODEL):\n",
    "        for i in range(D_MODEL):\n",
    "            dL_dconcat[pos][j] += dL_dh[pos][i] * W_O[i][j]\n",
    "\n",
    "print(\"Gradient for concatenated attention output\")\n",
    "print(f\"  Shape: [{seq_len}, {D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Weight Gradients\n",
    "\n",
    "For the Q, K, V projection matrices, we need to backprop through:\n",
    "1. The weighted sum: $\\text{output} = \\text{weights} \\cdot V$\n",
    "2. The softmax operation\n",
    "3. The scaled dot product: $\\text{scores} = Q \\cdot K^T / \\sqrt{d_k}$\n",
    "4. The linear projections\n",
    "\n",
    "This is where backpropagation gets intricate. Each head has its own $W_Q$, $W_K$, $W_V$, and we need gradients for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.492148Z",
     "iopub.status.busy": "2025-12-10T21:17:10.492071Z",
     "iopub.status.idle": "2025-12-10T21:17:10.494650Z",
     "shell.execute_reply": "2025-12-10T21:17:10.494401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Head 0 Matrices\n",
      "  W_Q: [16, 8]\n",
      "  W_K: [16, 8]\n",
      "  W_V: [16, 8]\n",
      "  Q, K, V: [4, 8]\n"
     ]
    }
   ],
   "source": [
    "# Simulated values for one attention head\n",
    "# In a full implementation, we'd do this for each head\n",
    "\n",
    "X = [random_vector(D_MODEL) for _ in range(seq_len)]  # Input\n",
    "W_Q_head0 = random_matrix(D_MODEL, D_K)  # [16, 8]\n",
    "W_K_head0 = random_matrix(D_MODEL, D_K)  # [16, 8]\n",
    "W_V_head0 = random_matrix(D_MODEL, D_K)  # [16, 8]\n",
    "\n",
    "# Q, K, V for head 0 (computed in forward pass)\n",
    "Q = [[sum(X[i][k] * W_Q_head0[k][j] for k in range(D_MODEL)) for j in range(D_K)] for i in range(seq_len)]\n",
    "K = [[sum(X[i][k] * W_K_head0[k][j] for k in range(D_MODEL)) for j in range(D_K)] for i in range(seq_len)]\n",
    "V = [[sum(X[i][k] * W_V_head0[k][j] for k in range(D_MODEL)) for j in range(D_K)] for i in range(seq_len)]\n",
    "\n",
    "print(\"Attention Head 0 Matrices\")\n",
    "print(f\"  W_Q: [{D_MODEL}, {D_K}]\")\n",
    "print(f\"  W_K: [{D_MODEL}, {D_K}]\")\n",
    "print(f\"  W_V: [{D_MODEL}, {D_K}]\")\n",
    "print(f\"  Q, K, V: [{seq_len}, {D_K}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.495359Z",
     "iopub.status.busy": "2025-12-10T21:17:10.495294Z",
     "iopub.status.idle": "2025-12-10T21:17:10.497162Z",
     "shell.execute_reply": "2025-12-10T21:17:10.496941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for W_Q (head 0)\n",
      "  Shape: [16, 8]\n",
      "  First row: [-0.0017, -0.0116, -0.0035,  0.0058,  0.0073, -0.0046, -0.0093, -0.0092]\n"
     ]
    }
   ],
   "source": [
    "# Simplified gradient computation for W_Q (head 0)\n",
    "# In full backprop, we'd compute this through the attention mechanism\n",
    "\n",
    "# For demonstration, assume we have dL_dQ (gradient flowing into Q)\n",
    "dL_dQ = [random_vector(D_K) for _ in range(seq_len)]  # [4, 8]\n",
    "\n",
    "# Gradient for W_Q: dL_dW_Q[i][j] = sum_pos dL_dQ[pos][j] * X[pos][i]\n",
    "dL_dW_Q = zeros_matrix(D_MODEL, D_K)\n",
    "for pos in range(seq_len):\n",
    "    for i in range(D_MODEL):\n",
    "        for j in range(D_K):\n",
    "            dL_dW_Q[i][j] += dL_dQ[pos][j] * X[pos][i]\n",
    "\n",
    "print(\"Gradient for W_Q (head 0)\")\n",
    "print(f\"  Shape: [{D_MODEL}, {D_K}]\")\n",
    "print(f\"  First row: {format_vector(dL_dW_Q[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Embedding Gradients\n",
    "\n",
    "Finally, we compute gradients for the embedding matrices.\n",
    "\n",
    "**Token embeddings** ($E_{token}$):\n",
    "\n",
    "The embedding lookup is just indexing: $e = E_{token}[\\text{token\\_id}]$. So the gradient only flows to the rows that were actually used.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial E_{token}[i]} = \\sum_{\\text{positions where token } i \\text{ appears}} \\frac{\\partial L}{\\partial X[\\text{position}]}$$\n",
    "\n",
    "**Position embeddings** ($E_{pos}$):\n",
    "\n",
    "Same idea. gradient accumulates for each position that was used.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial E_{pos}[p]} = \\frac{\\partial L}{\\partial X[p]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.497939Z",
     "iopub.status.busy": "2025-12-10T21:17:10.497875Z",
     "iopub.status.idle": "2025-12-10T21:17:10.500033Z",
     "shell.execute_reply": "2025-12-10T21:17:10.499782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embedding Gradients\n",
      "==================================================\n",
      "\n",
      "  <PAD>       : no gradient (token not used)\n",
      "  <BOS>       : ||gradient|| = 0.4340\n",
      "  <EOS>       : ||gradient|| = 0.3268\n",
      "  I           : ||gradient|| = 0.2963\n",
      "  like        : ||gradient|| = 0.3597\n",
      "  transformers: ||gradient|| = 0.3583\n"
     ]
    }
   ],
   "source": [
    "# Tokens used in our sequence\n",
    "tokens_used = [1, 3, 4, 5, 2]  # <BOS>, I, like, transformers, <EOS>\n",
    "full_seq_len = 5  # Including the last position\n",
    "\n",
    "# Gradient flowing into embeddings (would come from backprop through attention)\n",
    "dL_dX = [random_vector(D_MODEL) for _ in range(full_seq_len)]\n",
    "\n",
    "# Gradient for token embeddings\n",
    "dL_dE_token = zeros_matrix(VOCAB_SIZE, D_MODEL)\n",
    "\n",
    "for pos, token_id in enumerate(tokens_used):\n",
    "    for j in range(D_MODEL):\n",
    "        dL_dE_token[token_id][j] += dL_dX[pos][j]\n",
    "\n",
    "print(\"Token Embedding Gradients\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "for i, name in enumerate(TOKEN_NAMES):\n",
    "    grad_norm = math.sqrt(sum(g**2 for g in dL_dE_token[i]))\n",
    "    if grad_norm > 0:\n",
    "        print(f\"  {name:12s}: ||gradient|| = {grad_norm:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {name:12s}: no gradient (token not used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.500758Z",
     "iopub.status.busy": "2025-12-10T21:17:10.500694Z",
     "iopub.status.idle": "2025-12-10T21:17:10.502517Z",
     "shell.execute_reply": "2025-12-10T21:17:10.502275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Embedding Gradients\n",
      "==================================================\n",
      "\n",
      "  Position 0: ||gradient|| = 0.4340\n",
      "  Position 1: ||gradient|| = 0.2963\n",
      "  Position 2: ||gradient|| = 0.3597\n",
      "  Position 3: ||gradient|| = 0.3583\n",
      "  Position 4: ||gradient|| = 0.3268\n"
     ]
    }
   ],
   "source": [
    "# Gradient for position embeddings\n",
    "dL_dE_pos = zeros_matrix(MAX_SEQ_LEN, D_MODEL)\n",
    "\n",
    "for pos in range(full_seq_len):\n",
    "    for j in range(D_MODEL):\n",
    "        dL_dE_pos[pos][j] = dL_dX[pos][j]\n",
    "\n",
    "print(\"Position Embedding Gradients\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "for pos in range(full_seq_len):\n",
    "    grad_norm = math.sqrt(sum(g**2 for g in dL_dE_pos[pos]))\n",
    "    print(f\"  Position {pos}: ||gradient|| = {grad_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: All Gradients Computed\n",
    "\n",
    "We've traced the chain rule backward through the entire network:\n",
    "\n",
    "| Layer | Parameters | Gradient Shape | Purpose |\n",
    "|-------|------------|----------------|--------|\n",
    "| **LM Head** | $W_{lm}$ | [6, 16] | Predict next token |\n",
    "| **Layer Norm** | $\\gamma$, $\\beta$ | [16], [16] | Normalize activations |\n",
    "| **FFN** | $W_2$, $b_2$ | [16, 64], [16] | Project back |\n",
    "| | $W_1$, $b_1$ | [64, 16], [64] | Expand to hidden |\n",
    "| **Attention** | $W_O$ | [16, 16] | Output projection |\n",
    "| | $W_Q$ (\u00d72 heads) | [16, 8] | Query projection |\n",
    "| | $W_K$ (\u00d72 heads) | [16, 8] | Key projection |\n",
    "| | $W_V$ (\u00d72 heads) | [16, 8] | Value projection |\n",
    "| **Embeddings** | $E_{token}$ | [6, 16] | Token vectors |\n",
    "| | $E_{pos}$ | [5, 16] | Position vectors |\n",
    "\n",
    "Total: ~2,600 parameters, each with its own gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Insight: Local Computation, Global Effect\n",
    "\n",
    "Backpropagation is effective because each layer only needs to know:\n",
    "1. **What it computed** during the forward pass\n",
    "2. **The gradient flowing in** from the layer above\n",
    "\n",
    "It doesn't need to know about the loss function, the other layers, or anything else. Each layer computes its local derivatives and passes the gradient backward.\n",
    "\n",
    "Yet when we're done, every parameter has a gradient that tells us exactly how it contributed to the final loss. even parameters that are 10 layers removed from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next: The Optimizer\n",
    "\n",
    "We have gradients for all ~2,600 parameters. The gradient tells us which direction reduces the loss.\n",
    "\n",
    "The simplest approach would be **gradient descent**:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{\\partial L}{\\partial \\theta}$$\n",
    "\n",
    "But modern transformers use **AdamW**, which is much more sophisticated:\n",
    "- **Adaptive learning rates**: Each parameter gets its own learning rate based on gradient history\n",
    "- **Momentum**: Smooth out noisy gradients by averaging over time\n",
    "- **Weight decay**: Regularize by shrinking weights toward zero\n",
    "\n",
    "The next notebook implements AdamW and completes our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:10.503205Z",
     "iopub.status.busy": "2025-12-10T21:17:10.503134Z",
     "iopub.status.idle": "2025-12-10T21:17:10.504762Z",
     "shell.execute_reply": "2025-12-10T21:17:10.504511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backpropagation Complete\n",
      "==================================================\n",
      "\n",
      "Gradients computed for:\n",
      "  - LM head (96 params)\n",
      "  - Layer norm (32 params)\n",
      "  - FFN (2,128 params)\n",
      "  - Attention (1,024 params)\n",
      "  - Embeddings (~176 params)\n",
      "\n",
      "Ready for optimization step.\n"
     ]
    }
   ],
   "source": [
    "print(\"Backpropagation Complete\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"Gradients computed for:\")\n",
    "print(\"  - LM head (96 params)\")\n",
    "print(\"  - Layer norm (32 params)\")\n",
    "print(\"  - FFN (2,128 params)\")\n",
    "print(\"  - Attention (1,024 params)\")\n",
    "print(\"  - Embeddings (~176 params)\")\n",
    "print()\n",
    "print(\"Ready for optimization step.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Backpropagates gradients through entire network layer by layer, computing parameter gradients for all weights."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}