{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization & Embeddings\n",
    "\n",
    "**Converting text into numerical representations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the thing about neural networks: they only understand numbers.\n",
    "\n",
    "Words, sentences, paragraphs? Meaningless. But vectors of floating-point numbers? Now we're talking.\n",
    "\n",
    "So the first step in any language model is converting text into numbers the model can actually process. This happens in two stages: **tokenization** (breaking text into tokens) and **embedding** (converting those tokens into vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Vocabulary\n",
    "\n",
    "For this project, we're using a tiny vocabulary with just 6 tokens:\n",
    "\n",
    "| Token ID | Token | Description |\n",
    "|----------|-------|-------------|\n",
    "| 0 | `<PAD>` | Padding token (for sequences of different lengths) |\n",
    "| 1 | `<BOS>` | Beginning of sequence marker |\n",
    "| 2 | `<EOS>` | End of sequence marker |\n",
    "| 3 | `I` | Content word |\n",
    "| 4 | `like` | Content word |\n",
    "| 5 | `transformers` | Content word |\n",
    "\n",
    "In real language models, vocabularies contain thousands or tens of thousands of tokens (words, subwords, or characters). GPT-3, for instance, has 50,257 tokens. Ours has 6. This makes the calculations actually manageable while still demonstrating all the key concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "MAX_SEQ_LEN = 5\n",
    "\n",
    "# Token names for pretty printing\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Text\n",
    "\n",
    "We'll train our model on a single sentence:\n",
    "\n",
    "```\n",
    "\"I like transformers\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Process\n",
    "\n",
    "To convert our text into token IDs, we:\n",
    "\n",
    "1. Add a `<BOS>` (beginning of sequence) token at the start\n",
    "2. Convert each word to its token ID\n",
    "3. Add an `<EOS>` (end of sequence) token at the end\n",
    "\n",
    "**Result:**\n",
    "\n",
    "```\n",
    "Text:      <BOS>  I     like  transformers  <EOS>\n",
    "Token IDs: [1,    3,    4,    5,            2]\n",
    "```\n",
    "\n",
    "Our sequence has **length 5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our input sequence\n",
    "tokens = [1, 3, 4, 5, 2]  # <BOS>, I, like, transformers, <EOS>\n",
    "seq_len = len(tokens)\n",
    "\n",
    "print(f\"Input sequence: {tokens}\")\n",
    "print(f\"As text: {' '.join(TOKEN_NAMES[t] for t in tokens)}\")\n",
    "print(f\"Sequence length: {seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling Task\n",
    "\n",
    "In a decoder-only transformer (like GPT), the game is simple: predict the next token.\n",
    "\n",
    "At each position, we look at all the previous tokens and try to guess what comes next. We use a **causal mask** to enforce this \"no peeking at the future\" rule:\n",
    "\n",
    "| Position | Input tokens | Predict |\n",
    "|----------|--------------|--------|\n",
    "| 0 | `<BOS>` | `I` (token 3) |\n",
    "| 1 | `<BOS> I` | `like` (token 4) |\n",
    "| 2 | `<BOS> I like` | `transformers` (token 5) |\n",
    "| 3 | `<BOS> I like transformers` | `<EOS>` (token 2) |\n",
    "\n",
    "Position 4 (`<EOS>`) doesn't need to predict anything—it marks the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings: Tokens to Vectors\n",
    "\n",
    "Okay, so we have token IDs. But remember, the model needs actual vectors—continuous representations it can do math with.\n",
    "\n",
    "We use two types of embeddings to create these vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Token Embeddings\n",
    "\n",
    "Each token gets its own embedding vector. With `d_model = 16`, that means each of our 6 tokens maps to a 16-dimensional vector.\n",
    "\n",
    "We have a **token embedding matrix** of shape `[vocab_size, d_model]` = `[6, 16]`:\n",
    "\n",
    "```\n",
    "E_token = [\n",
    "  [e₀,₀,  e₀,₁,  ..., e₀,₁₅],   ← embedding for token 0 (<PAD>)\n",
    "  [e₁,₀,  e₁,₁,  ..., e₁,₁₅],   ← embedding for token 1 (<BOS>)\n",
    "  [e₂,₀,  e₂,₁,  ..., e₂,₁₅],   ← embedding for token 2 (<EOS>)\n",
    "  [e₃,₀,  e₃,₁,  ..., e₃,₁₅],   ← embedding for token 3 (I)\n",
    "  [e₄,₀,  e₄,₁,  ..., e₄,₁₅],   ← embedding for token 4 (like)\n",
    "  [e₅,₀,  e₅,₁,  ..., e₅,₁₅],   ← embedding for token 5 (transformers)\n",
    "]\n",
    "```\n",
    "\n",
    "To get the embedding for token ID `i`, we simply look up row `i` of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_vector(size, scale=0.1):\n",
    "    \"\"\"Generate a random vector with values ~ N(0, scale^2)\"\"\"\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    \"\"\"Format vector as string with specified decimal places\"\"\"\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\"\n",
    "\n",
    "# Initialize token embedding matrix\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "\n",
    "print(f\"Token Embedding Matrix E_token\")\n",
    "print(f\"Shape: [{VOCAB_SIZE}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(E_token):\n",
    "    print(f\"  Token {i} ({TOKEN_NAMES[i]:12s}): {format_vector(row)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Position Embeddings\n",
    "\n",
    "Here's a weird thing about transformers: they have no built-in sense of order.\n",
    "\n",
    "Unlike RNNs (which process sequences left-to-right) or CNNs (which have spatial structure), the attention mechanism treats the input like a bag of tokens. It has no idea that \"I\" comes before \"like\" unless we explicitly tell it.\n",
    "\n",
    "So we need position embeddings. We use **learned position embeddings** (there are other approaches like sinusoidal encoding, but learned embeddings work great).\n",
    "\n",
    "We have a **position embedding matrix** of shape `[max_seq_len, d_model]` = `[5, 16]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize position embedding matrix\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "\n",
    "print(f\"Position Embedding Matrix E_pos\")\n",
    "print(f\"Shape: [{MAX_SEQ_LEN}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(E_pos):\n",
    "    print(f\"  Position {i}: {format_vector(row)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combined Embeddings\n",
    "\n",
    "For each token at position `i`, we add the token embedding and position embedding:\n",
    "\n",
    "$$\\text{embedding}[i] = \\text{token\\_embedding}[\\text{token\\_id}[i]] + \\text{position\\_embedding}[i]$$\n",
    "\n",
    "For our sequence `[1, 3, 4, 5, 2]`:\n",
    "\n",
    "```\n",
    "Position 0: embedding[0] = E_token[1] + E_pos[0]  (for <BOS>)\n",
    "Position 1: embedding[1] = E_token[3] + E_pos[1]  (for I)\n",
    "Position 2: embedding[2] = E_token[4] + E_pos[2]  (for like)\n",
    "Position 3: embedding[3] = E_token[5] + E_pos[3]  (for transformers)\n",
    "Position 4: embedding[4] = E_token[2] + E_pos[4]  (for <EOS>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vectors(v1, v2):\n",
    "    \"\"\"Element-wise addition of two vectors\"\"\"\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "# Look up token embeddings for our sequence\n",
    "token_embeddings = [E_token[token_id] for token_id in tokens]\n",
    "\n",
    "# Add position embeddings to get combined embeddings\n",
    "X = [add_vectors(token_embeddings[i], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "print(\"Computing combined embeddings...\")\n",
    "print(\"X[i] = E_token[token_id[i]] + E_pos[i]\")\n",
    "print()\n",
    "\n",
    "for i in range(seq_len):\n",
    "    token_name = TOKEN_NAMES[tokens[i]]\n",
    "    print(f\"Position {i} ('{token_name}'):\")\n",
    "    print(f\"  Token embedding:    {format_vector(token_embeddings[i])}\")\n",
    "    print(f\"  Position embedding: {format_vector(E_pos[i])}\")\n",
    "    print(f\"  Combined X[{i}]:       {format_vector(X[i])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result: Matrix X\n",
    "\n",
    "The result is a matrix $X$ of shape `[seq_len, d_model]` = `[5, 16]`, where each row is the 16-dimensional embedding for one token in our sequence.\n",
    "\n",
    "**This matrix X is the input to our transformer block.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL COMBINED EMBEDDINGS MATRIX X\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}] (seq_len, d_model)\")\n",
    "print()\n",
    "print(\"X =\")\n",
    "for i, row in enumerate(X):\n",
    "    token_name = TOKEN_NAMES[tokens[i]]\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {token_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "These embeddings will flow through our transformer block, where the real magic happens:\n",
    "1. Query, Key, Value projections (splitting into attention heads)\n",
    "2. Self-attention scores (figuring out which tokens should attend to which)\n",
    "3. Multi-head attention (combining information from multiple attention heads)\n",
    "4. Feed-forward transformations (processing the attended representations)\n",
    "\n",
    "Let's move on to the QKV projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store variables for use in subsequent notebooks\n",
    "# (In a real setting, you'd save these or pass them along)\n",
    "embedding_data = {\n",
    "    'X': X,\n",
    "    'E_token': E_token,\n",
    "    'E_pos': E_pos,\n",
    "    'tokens': tokens,\n",
    "    'TOKEN_NAMES': TOKEN_NAMES,\n",
    "    'VOCAB_SIZE': VOCAB_SIZE,\n",
    "    'D_MODEL': D_MODEL,\n",
    "    'MAX_SEQ_LEN': MAX_SEQ_LEN\n",
    "}\n",
    "print(\"Embedding data stored for next notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
