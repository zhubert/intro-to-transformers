{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tokenization & Embeddings\n",
    "\n",
    "**Converting text into vectors the model can understand**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Fundamental Problem\n",
    "\n",
    "Neural networks only understand numbers.\n",
    "\n",
    "Not words. Not characters. Not meaning. Just floating-point numbers arranged in vectors and matrices.\n",
    "\n",
    "So before a transformer can do anything with text, we need to convert it into numbers. But not just any numbers—we need representations that capture something useful about what words *mean* and how they *relate* to each other.\n",
    "\n",
    "This notebook covers that conversion: from the string \"I like transformers\" to a matrix of numbers the model can actually process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Tokenization (Text → Integer IDs)\n",
    "\n",
    "The first step is simple: break text into pieces and assign each piece a number.\n",
    "\n",
    "These pieces are called **tokens**. In real models, tokens might be words, parts of words (\"un\" + \"believe\" + \"able\"), or even individual characters. GPT-3 uses about 50,000 different tokens. We'll use 6.\n",
    "\n",
    "Our vocabulary:\n",
    "\n",
    "| Token ID | Token | Purpose |\n",
    "|----------|-------|--------|\n",
    "| 0 | `<PAD>` | Padding (for batching sequences of different lengths) |\n",
    "| 1 | `<BOS>` | Beginning of sequence marker |\n",
    "| 2 | `<EOS>` | End of sequence marker |\n",
    "| 3 | `I` | The word \"I\" |\n",
    "| 4 | `like` | The word \"like\" |\n",
    "| 5 | `transformers` | The word \"transformers\" |\n",
    "\n",
    "The special tokens (`<PAD>`, `<BOS>`, `<EOS>`) are conventions that help the model understand sequence boundaries. `<BOS>` says \"a new sequence starts here.\" `<EOS>` says \"the sequence ends here.\" `<PAD>` fills in gaps when sequences have different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T00:44:46.174288Z",
     "iopub.status.busy": "2025-12-09T00:44:46.174206Z",
     "iopub.status.idle": "2025-12-09T00:44:46.176149Z",
     "shell.execute_reply": "2025-12-09T00:44:46.175892Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Model dimensions\n",
    "VOCAB_SIZE = 6      # Number of unique tokens\n",
    "D_MODEL = 16        # Embedding dimension (size of each token's vector)\n",
    "MAX_SEQ_LEN = 5     # Maximum sequence length\n",
    "\n",
    "# Human-readable token names\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Tokenizing Our Input\n",
    "\n",
    "Let's convert \"I like transformers\" into token IDs:\n",
    "\n",
    "```\n",
    "Original text:   \"I like transformers\"\n",
    "With markers:    <BOS> I like transformers <EOS>\n",
    "Token IDs:       [1,   3, 4,   5,           2]\n",
    "```\n",
    "\n",
    "We add `<BOS>` at the start and `<EOS>` at the end. The result is a list of 5 integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T00:44:46.177005Z",
     "iopub.status.busy": "2025-12-09T00:44:46.176937Z",
     "iopub.status.idle": "2025-12-09T00:44:46.178654Z",
     "shell.execute_reply": "2025-12-09T00:44:46.178407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 3, 4, 5, 2]\n",
      "As text: <BOS> I like transformers <EOS>\n",
      "Sequence length: 5\n"
     ]
    }
   ],
   "source": [
    "# Our tokenized input\n",
    "tokens = [1, 3, 4, 5, 2]  # <BOS>, I, like, transformers, <EOS>\n",
    "seq_len = len(tokens)\n",
    "\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"As text: {' '.join(TOKEN_NAMES[t] for t in tokens)}\")\n",
    "print(f\"Sequence length: {seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## The Language Modeling Task\n",
    "\n",
    "Before we go further, let's be clear about what the model is trying to do.\n",
    "\n",
    "A language model predicts the next token. At each position, it looks at all the previous tokens and guesses what comes next. This is called **autoregressive** generation—each prediction depends only on what came before.\n",
    "\n",
    "For our sequence:\n",
    "\n",
    "| Position | Sees | Must Predict |\n",
    "|----------|------|-------------|\n",
    "| 0 | `<BOS>` | `I` |\n",
    "| 1 | `<BOS> I` | `like` |\n",
    "| 2 | `<BOS> I like` | `transformers` |\n",
    "| 3 | `<BOS> I like transformers` | `<EOS>` |\n",
    "\n",
    "Position 4 is `<EOS>`, which marks the end—nothing to predict.\n",
    "\n",
    "This is why we need a **causal mask** later (we'll see it in the attention notebook). The model at position 2 shouldn't be able to peek at position 3 and 4—that would be cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 2: Token Embeddings (IDs → Vectors)\n",
    "\n",
    "Okay, we have integers. But integers are terrible representations for learning.\n",
    "\n",
    "Why? Because integers imply ordering and distance that doesn't exist. Token 5 isn't \"bigger\" than token 3. Token 4 isn't \"between\" tokens 3 and 5 in any meaningful way. The numbering is arbitrary.\n",
    "\n",
    "What we need is a **continuous vector** for each token—something we can do math with, something where distance and direction have meaning.\n",
    "\n",
    "Enter: **embeddings**.\n",
    "\n",
    "### The Embedding Matrix\n",
    "\n",
    "We create a matrix $E_{token}$ of shape `[vocab_size, d_model]` = `[6, 16]`. Each row is a 16-dimensional vector representing one token:\n",
    "\n",
    "$$E_{token} = \\begin{bmatrix} \n",
    "- & \\text{embedding for token 0 (<PAD>)} & - \\\\\n",
    "- & \\text{embedding for token 1 (<BOS>)} & - \\\\\n",
    "- & \\text{embedding for token 2 (<EOS>)} & - \\\\\n",
    "- & \\text{embedding for token 3 (I)} & - \\\\\n",
    "- & \\text{embedding for token 4 (like)} & - \\\\\n",
    "- & \\text{embedding for token 5 (transformers)} & -\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "To get the embedding for token ID $i$, we just look up row $i$. That's it. Embeddings are just table lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T00:44:46.179346Z",
     "iopub.status.busy": "2025-12-09T00:44:46.179280Z",
     "iopub.status.idle": "2025-12-09T00:44:46.181649Z",
     "shell.execute_reply": "2025-12-09T00:44:46.181386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embedding Matrix E_token\n",
      "Shape: [6, 16]\n",
      "\n",
      "  Token 0 (<PAD>       ): [-0.0144, -0.0173, -0.0111,  0.0702, -0.0128, -0.1497,  0.0332, -0.0267, -0.0217,  0.0116,  0.0232,  0.1164,  0.0657,  0.0111, -0.0738, -0.1015]\n",
      "  Token 1 (<BOS>       ): [ 0.0246,  0.1311,  0.0042, -0.0106,  0.0532, -0.1454, -0.0312,  0.0490,  0.0873, -0.0241,  0.0377,  0.0248,  0.0782, -0.1113,  0.0568, -0.1515]\n",
      "  Token 2 (<EOS>       ): [-0.2620, -0.0607, -0.0916,  0.0876,  0.0664, -0.1219,  0.0847, -0.1002, -0.0086, -0.0294,  0.0114,  0.0819,  0.0638,  0.0350,  0.0650,  0.0478]\n",
      "  Token 3 (I           ): [-0.0627, -0.0717, -0.0470,  0.0499, -0.0250,  0.2336, -0.0819, -0.1099,  0.0768,  0.1422,  0.0506,  0.0836,  0.1426, -0.0094, -0.1423, -0.0532]\n",
      "  Token 4 (like        ): [ 0.0953, -0.1444,  0.0034,  0.0253, -0.0316,  0.0724,  0.0581,  0.2321,  0.0620, -0.0609, -0.0562, -0.0832,  0.0952, -0.0567, -0.0070,  0.0749]\n",
      "  Token 5 (transformers): [-0.0723, -0.0294, -0.1841, -0.1082, -0.0568,  0.0416,  0.1193, -0.0018,  0.0261,  0.0168,  0.1085,  0.0893,  0.0274, -0.1011,  0.0903,  0.0381]\n"
     ]
    }
   ],
   "source": [
    "def random_vector(size, scale=0.1):\n",
    "    \"\"\"Generate a random vector with values drawn from N(0, scale^2)\"\"\"\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    \"\"\"Format a vector as a readable string\"\"\"\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\"\n",
    "\n",
    "# Initialize token embedding matrix with small random values\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "\n",
    "print(f\"Token Embedding Matrix E_token\")\n",
    "print(f\"Shape: [{VOCAB_SIZE}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(E_token):\n",
    "    print(f\"  Token {i} ({TOKEN_NAMES[i]:12s}): {format_vector(row)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Why Random Initialization?\n",
    "\n",
    "These embeddings start as random noise. They don't encode any meaning yet.\n",
    "\n",
    "That's fine. During training, the model will adjust these vectors so that:\n",
    "- Similar words end up with similar embeddings\n",
    "- The vectors capture useful features for the prediction task\n",
    "- Relationships between words are encoded in the geometry (e.g., king - man + woman ≈ queen)\n",
    "\n",
    "We initialize with small random values (scale=0.1) to break symmetry and avoid numerical issues at the start of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 3: Position Embeddings (Where in the Sequence?)\n",
    "\n",
    "Here's something weird about transformers: they have no built-in sense of order.\n",
    "\n",
    "Think about it. The attention mechanism (which we'll see later) compares every token to every other token simultaneously. It's not processing left-to-right like a human reading. It's looking at all tokens at once.\n",
    "\n",
    "This means: without something extra, the model has no idea that \"I\" comes before \"like\" which comes before \"transformers.\" It would process \"transformers like I\" exactly the same way.\n",
    "\n",
    "That's... bad. Word order matters.\n",
    "\n",
    "### The Solution: Add Position Information\n",
    "\n",
    "We give each position its own embedding vector, then **add** it to the token embedding:\n",
    "\n",
    "$$\\text{input}_i = E_{token}[\\text{token}_i] + E_{pos}[i]$$\n",
    "\n",
    "The position embedding matrix $E_{pos}$ has shape `[max_seq_len, d_model]` = `[5, 16]`. Each row is a unique vector for that position.\n",
    "\n",
    "After adding position embeddings:\n",
    "- The word \"I\" at position 1 has a different representation than \"I\" at position 3\n",
    "- The model can learn that certain patterns occur at certain positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T00:44:46.182321Z",
     "iopub.status.busy": "2025-12-09T00:44:46.182254Z",
     "iopub.status.idle": "2025-12-09T00:44:46.184017Z",
     "shell.execute_reply": "2025-12-09T00:44:46.183766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Embedding Matrix E_pos\n",
      "Shape: [5, 16]\n",
      "\n",
      "  Position 0: [ 0.1227, -0.0030,  0.1953, -0.0359,  0.1593,  0.0115, -0.0516, -0.1128, -0.0151,  0.1423,  0.0816,  0.0689, -0.2376,  0.0711,  0.0556, -0.0550]\n",
      "  Position 1: [-0.0627, -0.0002,  0.1725, -0.1055, -0.0428,  0.1362, -0.0446, -0.0364,  0.0098, -0.1241,  0.0220, -0.1210,  0.0885,  0.0003,  0.2283,  0.0281]\n",
      "  Position 2: [ 0.1366, -0.1303, -0.0122,  0.0323,  0.1746, -0.1681,  0.0991,  0.0591,  0.1534,  0.0712,  0.0052, -0.0522, -0.1248,  0.0195, -0.0192,  0.2020]\n",
      "  Position 3: [-0.0611,  0.0320, -0.1569, -0.0395,  0.0261,  0.0824,  0.1448, -0.0044, -0.1117,  0.0458,  0.0517,  0.0492, -0.0700,  0.1133,  0.0088,  0.0700]\n",
      "  Position 4: [ 0.1274,  0.0609,  0.0287,  0.2153,  0.0244, -0.0296,  0.0112,  0.1483,  0.0119,  0.0519,  0.1196, -0.0513, -0.1727,  0.0299,  0.0230, -0.0608]\n"
     ]
    }
   ],
   "source": [
    "# Initialize position embedding matrix\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "\n",
    "print(f\"Position Embedding Matrix E_pos\")\n",
    "print(f\"Shape: [{MAX_SEQ_LEN}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(E_pos):\n",
    "    print(f\"  Position {i}: {format_vector(row)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Why Add Instead of Concatenate?\n",
    "\n",
    "You might wonder: why add the position embedding to the token embedding? Why not concatenate them (stick them side by side)?\n",
    "\n",
    "Concatenation would work, but it has a cost: it would increase the dimension. If token embeddings are 16-dimensional and position embeddings are 16-dimensional, concatenation gives you 32-dimensional vectors. Every subsequent layer would need to be larger.\n",
    "\n",
    "Addition is cheaper. We keep the same dimension, and the model learns to \"share\" the 16 dimensions between token identity and position information.\n",
    "\n",
    "(There's also a deeper reason: addition lets the model learn to ignore position when it doesn't matter, by having position embeddings that are nearly orthogonal to the directions the model cares about. But that's getting into the weeds.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Putting It Together: The Input Matrix X\n",
    "\n",
    "Now we can build the actual input to our transformer.\n",
    "\n",
    "For each position $i$ in our sequence:\n",
    "1. Look up the token embedding: $E_{token}[\\text{tokens}[i]]$\n",
    "2. Look up the position embedding: $E_{pos}[i]$\n",
    "3. Add them together: $X[i] = E_{token}[\\text{tokens}[i]] + E_{pos}[i]$\n",
    "\n",
    "Let's compute this for our sequence `[1, 3, 4, 5, 2]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T00:44:46.184676Z",
     "iopub.status.busy": "2025-12-09T00:44:46.184612Z",
     "iopub.status.idle": "2025-12-09T00:44:46.187097Z",
     "shell.execute_reply": "2025-12-09T00:44:46.186845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing input embeddings X = E_token[token_id] + E_pos[position]\n",
      "================================================================================\n",
      "\n",
      "Position 0: token '<BOS>' (ID 1)\n",
      "  Token embedding E_token[1]:\n",
      "    [ 0.0246,  0.1311,  0.0042, -0.0106,  0.0532, -0.1454, -0.0312,  0.0490,  0.0873, -0.0241,  0.0377,  0.0248,  0.0782, -0.1113,  0.0568, -0.1515]\n",
      "  Position embedding E_pos[0]:\n",
      "    [ 0.1227, -0.0030,  0.1953, -0.0359,  0.1593,  0.0115, -0.0516, -0.1128, -0.0151,  0.1423,  0.0816,  0.0689, -0.2376,  0.0711,  0.0556, -0.0550]\n",
      "  Sum (X[0]):\n",
      "    [ 0.1473,  0.1281,  0.1995, -0.0465,  0.2125, -0.1338, -0.0829, -0.0638,  0.0722,  0.1183,  0.1193,  0.0937, -0.1594, -0.0402,  0.1124, -0.2064]\n",
      "\n",
      "Position 1: token 'I' (ID 3)\n",
      "  Token embedding E_token[3]:\n",
      "    [-0.0627, -0.0717, -0.0470,  0.0499, -0.0250,  0.2336, -0.0819, -0.1099,  0.0768,  0.1422,  0.0506,  0.0836,  0.1426, -0.0094, -0.1423, -0.0532]\n",
      "  Position embedding E_pos[1]:\n",
      "    [-0.0627, -0.0002,  0.1725, -0.1055, -0.0428,  0.1362, -0.0446, -0.0364,  0.0098, -0.1241,  0.0220, -0.1210,  0.0885,  0.0003,  0.2283,  0.0281]\n",
      "  Sum (X[1]):\n",
      "    [-0.1254, -0.0720,  0.1255, -0.0556, -0.0678,  0.3698, -0.1265, -0.1463,  0.0866,  0.0181,  0.0726, -0.0374,  0.2312, -0.0091,  0.0860, -0.0251]\n",
      "\n",
      "Position 2: token 'like' (ID 4)\n",
      "  Token embedding E_token[4]:\n",
      "    [ 0.0953, -0.1444,  0.0034,  0.0253, -0.0316,  0.0724,  0.0581,  0.2321,  0.0620, -0.0609, -0.0562, -0.0832,  0.0952, -0.0567, -0.0070,  0.0749]\n",
      "  Position embedding E_pos[2]:\n",
      "    [ 0.1366, -0.1303, -0.0122,  0.0323,  0.1746, -0.1681,  0.0991,  0.0591,  0.1534,  0.0712,  0.0052, -0.0522, -0.1248,  0.0195, -0.0192,  0.2020]\n",
      "  Sum (X[2]):\n",
      "    [ 0.2319, -0.2747, -0.0089,  0.0576,  0.1430, -0.0957,  0.1571,  0.2913,  0.2154,  0.0103, -0.0510, -0.1353, -0.0296, -0.0371, -0.0262,  0.2770]\n",
      "\n",
      "Position 3: token 'transformers' (ID 5)\n",
      "  Token embedding E_token[5]:\n",
      "    [-0.0723, -0.0294, -0.1841, -0.1082, -0.0568,  0.0416,  0.1193, -0.0018,  0.0261,  0.0168,  0.1085,  0.0893,  0.0274, -0.1011,  0.0903,  0.0381]\n",
      "  Position embedding E_pos[3]:\n",
      "    [-0.0611,  0.0320, -0.1569, -0.0395,  0.0261,  0.0824,  0.1448, -0.0044, -0.1117,  0.0458,  0.0517,  0.0492, -0.0700,  0.1133,  0.0088,  0.0700]\n",
      "  Sum (X[3]):\n",
      "    [-0.1334,  0.0027, -0.3410, -0.1478, -0.0307,  0.1240,  0.2642, -0.0063, -0.0856,  0.0626,  0.1602,  0.1385, -0.0427,  0.0122,  0.0991,  0.1081]\n",
      "\n",
      "Position 4: token '<EOS>' (ID 2)\n",
      "  Token embedding E_token[2]:\n",
      "    [-0.2620, -0.0607, -0.0916,  0.0876,  0.0664, -0.1219,  0.0847, -0.1002, -0.0086, -0.0294,  0.0114,  0.0819,  0.0638,  0.0350,  0.0650,  0.0478]\n",
      "  Position embedding E_pos[4]:\n",
      "    [ 0.1274,  0.0609,  0.0287,  0.2153,  0.0244, -0.0296,  0.0112,  0.1483,  0.0119,  0.0519,  0.1196, -0.0513, -0.1727,  0.0299,  0.0230, -0.0608]\n",
      "  Sum (X[4]):\n",
      "    [-0.1346,  0.0002, -0.0629,  0.3029,  0.0908, -0.1515,  0.0959,  0.0481,  0.0032,  0.0225,  0.1310,  0.0306, -0.1088,  0.0649,  0.0880, -0.0130]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_vectors(v1, v2):\n",
    "    \"\"\"Element-wise addition of two vectors\"\"\"\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "# Look up token embeddings for our sequence\n",
    "token_embeddings = [E_token[token_id] for token_id in tokens]\n",
    "\n",
    "# Add position embeddings\n",
    "X = [add_vectors(token_embeddings[i], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "print(\"Computing input embeddings X = E_token[token_id] + E_pos[position]\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for i in range(seq_len):\n",
    "    token_id = tokens[i]\n",
    "    token_name = TOKEN_NAMES[token_id]\n",
    "    print(f\"Position {i}: token '{token_name}' (ID {token_id})\")\n",
    "    print(f\"  Token embedding E_token[{token_id}]:\")\n",
    "    print(f\"    {format_vector(token_embeddings[i])}\")\n",
    "    print(f\"  Position embedding E_pos[{i}]:\")\n",
    "    print(f\"    {format_vector(E_pos[i])}\")\n",
    "    print(f\"  Sum (X[{i}]):\")\n",
    "    print(f\"    {format_vector(X[i])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### The Resulting Matrix\n",
    "\n",
    "We now have our input matrix $X$ with shape `[seq_len, d_model]` = `[5, 16]`.\n",
    "\n",
    "Each row is a 16-dimensional vector representing one token at one position. This matrix is the input to the transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T00:44:46.187753Z",
     "iopub.status.busy": "2025-12-09T00:44:46.187687Z",
     "iopub.status.idle": "2025-12-09T00:44:46.189328Z",
     "shell.execute_reply": "2025-12-09T00:44:46.189098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL INPUT MATRIX X\n",
      "================================================================================\n",
      "Shape: [5, 16]\n",
      "\n",
      "  X[0] = [ 0.1473,  0.1281,  0.1995, -0.0465,  0.2125, -0.1338, -0.0829, -0.0638,  0.0722,  0.1183,  0.1193,  0.0937, -0.1594, -0.0402,  0.1124, -0.2064]  # <BOS>\n",
      "  X[1] = [-0.1254, -0.0720,  0.1255, -0.0556, -0.0678,  0.3698, -0.1265, -0.1463,  0.0866,  0.0181,  0.0726, -0.0374,  0.2312, -0.0091,  0.0860, -0.0251]  # I\n",
      "  X[2] = [ 0.2319, -0.2747, -0.0089,  0.0576,  0.1430, -0.0957,  0.1571,  0.2913,  0.2154,  0.0103, -0.0510, -0.1353, -0.0296, -0.0371, -0.0262,  0.2770]  # like\n",
      "  X[3] = [-0.1334,  0.0027, -0.3410, -0.1478, -0.0307,  0.1240,  0.2642, -0.0063, -0.0856,  0.0626,  0.1602,  0.1385, -0.0427,  0.0122,  0.0991,  0.1081]  # transformers\n",
      "  X[4] = [-0.1346,  0.0002, -0.0629,  0.3029,  0.0908, -0.1515,  0.0959,  0.0481,  0.0032,  0.0225,  0.1310,  0.0306, -0.1088,  0.0649,  0.0880, -0.0130]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL INPUT MATRIX X\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(X):\n",
    "    token_name = TOKEN_NAMES[tokens[i]]\n",
    "    print(f\"  X[{i}] = {format_vector(row)}  # {token_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## What We've Built\n",
    "\n",
    "Let's recap the transformation:\n",
    "\n",
    "```\n",
    "\"I like transformers\"\n",
    "        ↓ tokenization\n",
    "[1, 3, 4, 5, 2]                    # 5 integers\n",
    "        ↓ token embeddings\n",
    "[5, 16] matrix                     # 5 rows × 16 columns\n",
    "        ↓ add position embeddings\n",
    "[5, 16] matrix (X)                 # final input\n",
    "```\n",
    "\n",
    "We've gone from a string to a matrix of continuous values. The model can now:\n",
    "- Do math with these vectors (add, multiply, take dot products)\n",
    "- Learn to adjust the embeddings during training\n",
    "- Use the position information to understand word order\n",
    "\n",
    "This matrix $X$ will flow through the rest of the transformer: attention, feed-forward layers, layer normalization, and finally produce predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "The input matrix $X$ is ready. Now comes the core of the transformer: **self-attention**.\n",
    "\n",
    "Before we can compute attention, we need to project $X$ into three different representations:\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What do I contain?\"\n",
    "- **Value (V)**: \"What information should I pass along?\"\n",
    "\n",
    "These projections are called **QKV projections**, and they're the subject of the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T00:44:46.189970Z",
     "iopub.status.busy": "2025-12-09T00:44:46.189908Z",
     "iopub.status.idle": "2025-12-09T00:44:46.191403Z",
     "shell.execute_reply": "2025-12-09T00:44:46.191182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding data ready for the next notebook.\n"
     ]
    }
   ],
   "source": [
    "# Store data for use in subsequent notebooks\n",
    "embedding_data = {\n",
    "    'X': X,\n",
    "    'E_token': E_token,\n",
    "    'E_pos': E_pos,\n",
    "    'tokens': tokens,\n",
    "    'TOKEN_NAMES': TOKEN_NAMES,\n",
    "    'VOCAB_SIZE': VOCAB_SIZE,\n",
    "    'D_MODEL': D_MODEL,\n",
    "    'MAX_SEQ_LEN': MAX_SEQ_LEN\n",
    "}\n",
    "print(\"Embedding data ready for the next notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
