{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# QKV Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Core Idea of Attention\n",
    "\n",
    "Attention lets each token gather information from other tokens.\n",
    "\n",
    "When processing the word \"like\" in \"I like transformers,\" the model might want to know: what's the subject? What's the object? What came before? Attention lets the model look at other positions and pull in relevant information.\n",
    "\n",
    "But how does a token decide which other tokens are \"relevant\"? That's where Query, Key, and Value come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Database Analogy\n",
    "\n",
    "Think of attention like a fuzzy database lookup:\n",
    "\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What do I contain?\" (the label or tag)\n",
    "- **Value (V)**: \"What information should I return?\" (the actual content)\n",
    "\n",
    "In a normal database, you query with an exact key and get back the matching value. In attention, you query with a vector, compare it to all keys, and get back a *weighted combination* of all values. weighted by how well each key matches your query.\n",
    "\n",
    "The \"fuzzy\" part is crucial. There's no exact match. Every key contributes something; good matches contribute more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Why Three Separate Projections?\n",
    "\n",
    "You might wonder: why not just use the embeddings directly? Why create separate Q, K, V representations?\n",
    "\n",
    "Here's the insight: **what you're looking for might be different from what you contain.**\n",
    "\n",
    "Consider the word \"it\" in \"The cat sat on the mat. It was tired.\"\n",
    "- As a **query**, \"it\" is looking for its antecedent (what does \"it\" refer to?)\n",
    "- As a **key**, \"it\" is saying \"I'm a pronoun that could be referenced\"\n",
    "- As a **value**, \"it\" contains information about being a subject, being tired, etc.\n",
    "\n",
    "These are different roles. The same token needs to express different things depending on whether it's doing the looking (query) or being looked at (key/value).\n",
    "\n",
    "Separate projections let the model learn these different roles independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Multi-Head Attention: Multiple Perspectives\n",
    "\n",
    "We're using **multi-head attention** with 2 heads. What does that mean?\n",
    "\n",
    "Each head is an independent attention mechanism with its own Q, K, V projections. Different heads can learn to focus on different things:\n",
    "- Head 0 might learn syntactic patterns (subject-verb relationships)\n",
    "- Head 1 might learn semantic patterns (related concepts)\n",
    "\n",
    "It's like having multiple experts examine the same data from different angles.\n",
    "\n",
    "**Our architecture:**\n",
    "- `d_model = 16` (embedding dimension)\n",
    "- `num_heads = 2`\n",
    "- `d_k = d_model / num_heads = 8` (dimension per head)\n",
    "\n",
    "Each head projects from 16 dimensions down to 8 dimensions. Later, we'll concatenate the 2 heads back to 16 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.850033Z",
     "iopub.status.busy": "2025-12-10T21:16:57.849948Z",
     "iopub.status.idle": "2025-12-10T21:16:57.852458Z",
     "shell.execute_reply": "2025-12-10T21:16:57.852149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension (d_model): 16\n",
      "Number of attention heads: 2\n",
      "Dimension per head (d_k): 8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility (same as previous notebook)\n",
    "random.seed(42)\n",
    "\n",
    "# Model dimensions\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS  # 8 dimensions per head\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]\n",
    "\n",
    "print(f\"Embedding dimension (d_model): {D_MODEL}\")\n",
    "print(f\"Number of attention heads: {NUM_HEADS}\")\n",
    "print(f\"Dimension per head (d_k): {D_K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.853238Z",
     "iopub.status.busy": "2025-12-10T21:16:57.853144Z",
     "iopub.status.idle": "2025-12-10T21:16:57.855327Z",
     "shell.execute_reply": "2025-12-10T21:16:57.855040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    \"\"\"Generate a random vector with values drawn from N(0, scale^2)\"\"\"\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    \"\"\"Generate a random matrix with values drawn from N(0, scale^2)\"\"\"\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    \"\"\"Element-wise addition of two vectors\"\"\"\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    \"\"\"Format a vector as a readable string\"\"\"\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.856133Z",
     "iopub.status.busy": "2025-12-10T21:16:57.856064Z",
     "iopub.status.idle": "2025-12-10T21:16:57.858102Z",
     "shell.execute_reply": "2025-12-10T21:16:57.857836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix X recreated from previous notebook\n",
      "Shape: [5, 16]\n"
     ]
    }
   ],
   "source": [
    "# Recreate embeddings from previous notebook (same random seed ensures same values)\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "\n",
    "tokens = [1, 3, 4, 5, 2]  # <BOS>, I, like, transformers, <EOS>\n",
    "seq_len = len(tokens)\n",
    "\n",
    "# Compute input embeddings X\n",
    "token_embeddings = [E_token[token_id] for token_id in tokens]\n",
    "X = [add_vectors(token_embeddings[i], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "print(f\"Input matrix X recreated from previous notebook\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## The Projection Weights\n",
    "\n",
    "For each attention head, we have three weight matrices:\n",
    "\n",
    "- $W_Q$: Query projection, shape `[d_model, d_k]` = `[16, 8]`\n",
    "- $W_K$: Key projection, shape `[d_model, d_k]` = `[16, 8]`\n",
    "- $W_V$: Value projection, shape `[d_model, d_k]` = `[16, 8]`\n",
    "\n",
    "Each matrix projects from 16 dimensions to 8 dimensions. These weights are learned during training. the model figures out what projections are useful for the prediction task.\n",
    "\n",
    "With 2 heads, we have 6 weight matrices total (3 per head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.858840Z",
     "iopub.status.busy": "2025-12-10T21:16:57.858771Z",
     "iopub.status.idle": "2025-12-10T21:16:57.860878Z",
     "shell.execute_reply": "2025-12-10T21:16:57.860603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 2 heads, each with:\n",
      "  W_Q: [16, 8]\n",
      "  W_K: [16, 8]\n",
      "  W_V: [16, 8]\n",
      "\n",
      "Total weight matrices: 6\n"
     ]
    }
   ],
   "source": [
    "# Initialize weight matrices for each head\n",
    "W_Q = []  # Query weights\n",
    "W_K = []  # Key weights\n",
    "W_V = []  # Value weights\n",
    "\n",
    "for head in range(NUM_HEADS):\n",
    "    W_Q.append(random_matrix(D_MODEL, D_K))  # [16, 8]\n",
    "    W_K.append(random_matrix(D_MODEL, D_K))  # [16, 8]\n",
    "    W_V.append(random_matrix(D_MODEL, D_K))  # [16, 8]\n",
    "\n",
    "print(f\"Initialized {NUM_HEADS} heads, each with:\")\n",
    "print(f\"  W_Q: [{D_MODEL}, {D_K}]\")\n",
    "print(f\"  W_K: [{D_MODEL}, {D_K}]\")\n",
    "print(f\"  W_V: [{D_MODEL}, {D_K}]\")\n",
    "print(f\"\\nTotal weight matrices: {NUM_HEADS * 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Matrix Multiplication: A Quick Review\n",
    "\n",
    "The projection operation is just matrix multiplication. Let's make sure we understand exactly what that means.\n",
    "\n",
    "When we multiply matrices $A$ and $B$:\n",
    "- $A$ has shape `[m, n]`\n",
    "- $B$ has shape `[n, p]`\n",
    "- Result has shape `[m, p]`\n",
    "\n",
    "**The key rule**: number of columns in $A$ must equal number of rows in $B$.\n",
    "\n",
    "Each element of the result is a **dot product**:\n",
    "\n",
    "$$\\text{result}_{ij} = \\sum_{k=0}^{n-1} A_{ik} \\cdot B_{kj}$$\n",
    "\n",
    "In words: take row $i$ from $A$, take column $j$ from $B$, multiply element-wise, sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.861608Z",
     "iopub.status.busy": "2025-12-10T21:16:57.861539Z",
     "iopub.status.idle": "2025-12-10T21:16:57.863969Z",
     "shell.execute_reply": "2025-12-10T21:16:57.863721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: [2, 3] @ [3, 2] = [2, 2]\n",
      "A = [[1, 2, 3], [4, 5, 6]]\n",
      "B = [[1, 4], [2, 5], [3, 6]]\n",
      "A @ B = [[14, 32], [32, 77]]\n",
      "\n",
      "Verification:\n",
      "  result[0][0] = 1*1 + 2*2 + 3*3 = 14\n",
      "  result[0][1] = 1*4 + 2*5 + 3*6 = 32\n"
     ]
    }
   ],
   "source": [
    "def matmul(A, B):\n",
    "    \"\"\"\n",
    "    Multiply matrices A @ B.\n",
    "    A has shape [m, n], B has shape [n, p], result has shape [m, p].\n",
    "    \"\"\"\n",
    "    m = len(A)       # number of rows in A\n",
    "    n = len(A[0])    # number of columns in A (= rows in B)\n",
    "    p = len(B[0])    # number of columns in B\n",
    "    \n",
    "    # Initialize result matrix with zeros\n",
    "    result = [[0.0] * p for _ in range(m)]\n",
    "    \n",
    "    # Compute each element\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            # Dot product of row i from A and column j from B\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(n))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Quick example to verify\n",
    "A = [[1, 2, 3], [4, 5, 6]]  # [2, 3]\n",
    "B = [[1, 4], [2, 5], [3, 6]]  # [3, 2]\n",
    "result = matmul(A, B)  # Should be [2, 2]\n",
    "\n",
    "print(\"Example: [2, 3] @ [3, 2] = [2, 2]\")\n",
    "print(f\"A = {A}\")\n",
    "print(f\"B = {B}\")\n",
    "print(f\"A @ B = {result}\")\n",
    "print()\n",
    "print(\"Verification:\")\n",
    "print(f\"  result[0][0] = 1*1 + 2*2 + 3*3 = {1*1 + 2*2 + 3*3}\")\n",
    "print(f\"  result[0][1] = 1*4 + 2*5 + 3*6 = {1*4 + 2*5 + 3*6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Computing Q, K, V\n",
    "\n",
    "Now we can compute the projections. For each head:\n",
    "\n",
    "$$Q = X \\cdot W_Q \\quad \\text{shape: } [5, 16] \\times [16, 8] = [5, 8]$$\n",
    "$$K = X \\cdot W_K \\quad \\text{shape: } [5, 16] \\times [16, 8] = [5, 8]$$\n",
    "$$V = X \\cdot W_V \\quad \\text{shape: } [5, 16] \\times [16, 8] = [5, 8]$$\n",
    "\n",
    "Each row of $Q$ is the query vector for one token. Same for $K$ and $V$.\n",
    "\n",
    "Let's compute them for both heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.864633Z",
     "iopub.status.busy": "2025-12-10T21:16:57.864560Z",
     "iopub.status.idle": "2025-12-10T21:16:57.866560Z",
     "shell.execute_reply": "2025-12-10T21:16:57.866324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Q, K, V for 2 heads\n",
      "Each Q, K, V has shape [5, 8]\n"
     ]
    }
   ],
   "source": [
    "# Compute Q, K, V for each head\n",
    "Q_all = []  # Will hold Q matrices for each head\n",
    "K_all = []  # Will hold K matrices for each head\n",
    "V_all = []  # Will hold V matrices for each head\n",
    "\n",
    "for head in range(NUM_HEADS):\n",
    "    Q = matmul(X, W_Q[head])  # [5, 16] @ [16, 8] = [5, 8]\n",
    "    K = matmul(X, W_K[head])  # [5, 16] @ [16, 8] = [5, 8]\n",
    "    V = matmul(X, W_V[head])  # [5, 16] @ [16, 8] = [5, 8]\n",
    "    \n",
    "    Q_all.append(Q)\n",
    "    K_all.append(K)\n",
    "    V_all.append(V)\n",
    "\n",
    "print(f\"Computed Q, K, V for {NUM_HEADS} heads\")\n",
    "print(f\"Each Q, K, V has shape [{seq_len}, {D_K}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Detailed Example: Computing One Query Vector\n",
    "\n",
    "Let's trace through exactly how we compute the query vector for position 0 (`<BOS>`) in head 0.\n",
    "\n",
    "We're computing:\n",
    "$$Q[0] = X[0] \\cdot W_Q[0]$$\n",
    "\n",
    "Where:\n",
    "- $X[0]$ is a 16-dimensional vector (the embedding for `<BOS>`)\n",
    "- $W_Q[0]$ is a `[16, 8]` matrix\n",
    "- $Q[0]$ is an 8-dimensional vector (the query for `<BOS>`)\n",
    "\n",
    "Each element of $Q[0]$ is a dot product between $X[0]$ and one column of $W_Q[0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.867494Z",
     "iopub.status.busy": "2025-12-10T21:16:57.867420Z",
     "iopub.status.idle": "2025-12-10T21:16:57.869894Z",
     "shell.execute_reply": "2025-12-10T21:16:57.869596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Q[0] for Head 0 (query for <BOS>)\n",
      "======================================================================\n",
      "\n",
      "Input: X[0] (embedding for <BOS>), shape [16]\n",
      "  [ 0.1473,  0.1281,  0.1995, -0.0465,  0.2125, -0.1338, -0.0829, -0.0638,  0.0722,  0.1183,  0.1193,  0.0937, -0.1594, -0.0402,  0.1124, -0.2064]\n",
      "\n",
      "Weight: W_Q[0], shape [16, 8]\n",
      "  (16 rows, 8 columns - too big to print fully)\n",
      "\n",
      "Output: Q[0] = X[0] @ W_Q[0], shape [8]\n",
      "\n",
      "Q[0][0] = X[0] · W_Q[0][:, 0]  (dot product with column 0)\n",
      "       = (0.1473 × 0.0871) + (0.1281 × -0.0745) + (0.1995 × 0.0003) + ...\n",
      "       = -0.0179\n",
      "\n",
      "Q[0][1] = X[0] · W_Q[0][:, 1]  (dot product with column 1)\n",
      "       = (0.1473 × 0.0608) + (0.1281 × 0.0523) + (0.1995 × 0.2266) + ...\n",
      "       = 0.1390\n",
      "\n",
      "Full result: Q[0] = [-0.0179,  0.1390, -0.1115,  0.0441, -0.0565, -0.0221,  0.1540, -0.0131]\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing Q[0] for Head 0 (query for <BOS>)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Input: X[0] (embedding for <BOS>), shape [16]\")\n",
    "print(f\"  {format_vector(X[0])}\")\n",
    "print()\n",
    "print(f\"Weight: W_Q[0], shape [16, 8]\")\n",
    "print(f\"  (16 rows, 8 columns - too big to print fully)\")\n",
    "print()\n",
    "print(f\"Output: Q[0] = X[0] @ W_Q[0], shape [8]\")\n",
    "print()\n",
    "\n",
    "# Show detailed calculation for first two output dimensions\n",
    "for j in range(2):\n",
    "    print(f\"Q[0][{j}] = X[0] · W_Q[0][:, {j}]  (dot product with column {j})\")\n",
    "    \n",
    "    # Get column j of W_Q[0]\n",
    "    col_j = [W_Q[0][i][j] for i in range(D_MODEL)]\n",
    "    \n",
    "    # Show first few terms\n",
    "    terms = [f\"({X[0][i]:.4f} × {col_j[i]:.4f})\" for i in range(3)]\n",
    "    print(f\"       = {' + '.join(terms)} + ...\")\n",
    "    \n",
    "    # Compute actual value\n",
    "    value = sum(X[0][i] * col_j[i] for i in range(D_MODEL))\n",
    "    print(f\"       = {value:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Full result: Q[0] = {format_vector(Q_all[0][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Head 0: All Q, K, V Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.870617Z",
     "iopub.status.busy": "2025-12-10T21:16:57.870547Z",
     "iopub.status.idle": "2025-12-10T21:16:57.872265Z",
     "shell.execute_reply": "2025-12-10T21:16:57.872025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 0: Query Matrix Q\n",
      "Shape: [5, 8]\n",
      "\n",
      "  Q[0] = [-0.0179,  0.1390, -0.1115,  0.0441, -0.0565, -0.0221,  0.1540, -0.0131]  # <BOS>\n",
      "  Q[1] = [-0.0997, -0.0394,  0.0301,  0.0469,  0.0628, -0.0026, -0.0506,  0.0320]  # I\n",
      "  Q[2] = [-0.0154,  0.0507, -0.0404,  0.0923,  0.0319, -0.0150,  0.0833, -0.0375]  # like\n",
      "  Q[3] = [ 0.0012, -0.0905,  0.0421,  0.0099,  0.1038,  0.0244, -0.0546, -0.0397]  # transformers\n",
      "  Q[4] = [ 0.0812,  0.0104,  0.0022,  0.0003, -0.0376,  0.0182,  0.0318, -0.0184]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "head = 0\n",
    "print(f\"HEAD {head}: Query Matrix Q\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(Q_all[head]):\n",
    "    print(f\"  Q[{i}] = {format_vector(row)}  # {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.872943Z",
     "iopub.status.busy": "2025-12-10T21:16:57.872875Z",
     "iopub.status.idle": "2025-12-10T21:16:57.874532Z",
     "shell.execute_reply": "2025-12-10T21:16:57.874282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 0: Key Matrix K\n",
      "Shape: [5, 8]\n",
      "\n",
      "  K[0] = [ 0.0817,  0.0209,  0.0114,  0.0069,  0.0258,  0.0144, -0.0401,  0.0410]  # <BOS>\n",
      "  K[1] = [-0.0228,  0.0577, -0.0045, -0.0131,  0.0082, -0.0335,  0.0272,  0.0137]  # I\n",
      "  K[2] = [ 0.0675, -0.0504, -0.1121,  0.0738,  0.0479, -0.1313,  0.0103,  0.0228]  # like\n",
      "  K[3] = [-0.1202,  0.1335,  0.0520,  0.0626, -0.0597,  0.0077,  0.0658, -0.0298]  # transformers\n",
      "  K[4] = [ 0.0189, -0.0549,  0.0358, -0.0400, -0.0008,  0.0210,  0.0411, -0.0375]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(f\"HEAD {head}: Key Matrix K\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(K_all[head]):\n",
    "    print(f\"  K[{i}] = {format_vector(row)}  # {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.875125Z",
     "iopub.status.busy": "2025-12-10T21:16:57.875057Z",
     "iopub.status.idle": "2025-12-10T21:16:57.876647Z",
     "shell.execute_reply": "2025-12-10T21:16:57.876425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 0: Value Matrix V\n",
      "Shape: [5, 8]\n",
      "\n",
      "  V[0] = [-0.0090, -0.0398,  0.0085, -0.0527, -0.0375, -0.0001, -0.0328,  0.0792]  # <BOS>\n",
      "  V[1] = [ 0.0048,  0.0079,  0.0088,  0.0217, -0.1038, -0.0268,  0.0811, -0.1041]  # I\n",
      "  V[2] = [ 0.0390,  0.1344,  0.0726,  0.0888,  0.0703,  0.1238, -0.1341,  0.1226]  # like\n",
      "  V[3] = [-0.0103,  0.0407, -0.0746,  0.0207,  0.0585, -0.0899,  0.0405, -0.0838]  # transformers\n",
      "  V[4] = [-0.0202, -0.0619, -0.0048, -0.0391,  0.0689, -0.0415, -0.0032,  0.0630]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(f\"HEAD {head}: Value Matrix V\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(V_all[head]):\n",
    "    print(f\"  V[{i}] = {format_vector(row)}  # {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Head 1: Different Projections, Different Representation\n",
    "\n",
    "Head 1 has its own weight matrices, so it produces completely different Q, K, V representations from the same input. This is the \"multi\" in multi-head attention. multiple parallel views of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.877351Z",
     "iopub.status.busy": "2025-12-10T21:16:57.877282Z",
     "iopub.status.idle": "2025-12-10T21:16:57.878944Z",
     "shell.execute_reply": "2025-12-10T21:16:57.878702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 1: Query Matrix Q\n",
      "Shape: [5, 8]\n",
      "\n",
      "  Q[0] = [-0.0801,  0.0205, -0.0577,  0.0358,  0.0203, -0.0472,  0.1419,  0.0332]  # <BOS>\n",
      "  Q[1] = [ 0.0791,  0.0428, -0.0408,  0.0261, -0.0520, -0.0152, -0.0639, -0.0355]  # I\n",
      "  Q[2] = [-0.0232,  0.0231, -0.0204,  0.0449,  0.0019,  0.0651,  0.0958, -0.0080]  # like\n",
      "  Q[3] = [ 0.0913,  0.0219,  0.0457, -0.0627,  0.0176, -0.1209, -0.1008, -0.0297]  # transformers\n",
      "  Q[4] = [ 0.0314, -0.0331, -0.0224, -0.0109, -0.0103, -0.0073,  0.0198, -0.0383]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "head = 1\n",
    "print(f\"HEAD {head}: Query Matrix Q\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(Q_all[head]):\n",
    "    print(f\"  Q[{i}] = {format_vector(row)}  # {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.879589Z",
     "iopub.status.busy": "2025-12-10T21:16:57.879522Z",
     "iopub.status.idle": "2025-12-10T21:16:57.881161Z",
     "shell.execute_reply": "2025-12-10T21:16:57.880922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 1: Key Matrix K\n",
      "Shape: [5, 8]\n",
      "\n",
      "  K[0] = [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737]  # <BOS>\n",
      "  K[1] = [ 0.0565,  0.0479, -0.0409, -0.0089, -0.0037,  0.0547, -0.0085, -0.0782]  # I\n",
      "  K[2] = [-0.0624,  0.1632,  0.0750, -0.0765,  0.0238,  0.0042, -0.0385,  0.1000]  # like\n",
      "  K[3] = [ 0.0276, -0.0325, -0.0956,  0.0622, -0.0129, -0.0202, -0.0572,  0.0587]  # transformers\n",
      "  K[4] = [ 0.0606, -0.0210, -0.0280, -0.0021,  0.0533,  0.0302, -0.0483,  0.0772]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(f\"HEAD {head}: Key Matrix K\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(K_all[head]):\n",
    "    print(f\"  K[{i}] = {format_vector(row)}  # {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.881796Z",
     "iopub.status.busy": "2025-12-10T21:16:57.881729Z",
     "iopub.status.idle": "2025-12-10T21:16:57.883388Z",
     "shell.execute_reply": "2025-12-10T21:16:57.883125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 1: Value Matrix V\n",
      "Shape: [5, 8]\n",
      "\n",
      "  V[0] = [ 0.0107, -0.0291, -0.0100, -0.0312,  0.0214,  0.0372,  0.0105,  0.0279]  # <BOS>\n",
      "  V[1] = [-0.0506, -0.0011,  0.0151,  0.0528, -0.0033, -0.0783, -0.0746, -0.0666]  # I\n",
      "  V[2] = [-0.0562, -0.0003,  0.0484, -0.0677,  0.1120,  0.0491,  0.0651, -0.0207]  # like\n",
      "  V[3] = [ 0.0521, -0.0033, -0.0165,  0.0878,  0.0455,  0.0866,  0.0211, -0.0656]  # transformers\n",
      "  V[4] = [-0.0155,  0.0273, -0.0714, -0.0334,  0.0643,  0.0217,  0.0260,  0.0643]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(f\"HEAD {head}: Value Matrix V\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(V_all[head]):\n",
    "    print(f\"  V[{i}] = {format_vector(row)}  # {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## What We've Computed\n",
    "\n",
    "Starting from input $X$ `[5, 16]`, we now have for each head:\n",
    "\n",
    "| Matrix | Shape | Meaning |\n",
    "|--------|-------|--------|\n",
    "| Q | [5, 8] | What each token is looking for |\n",
    "| K | [5, 8] | What each token offers as a match |\n",
    "| V | [5, 8] | What information each token carries |\n",
    "\n",
    "These are the building blocks for attention. In the next notebook, we'll use Q and K to compute attention scores (how much should each token attend to each other token?), then use those scores to take weighted combinations of V."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "We have Q, K, V. Now comes the actual attention computation:\n",
    "\n",
    "1. **Attention scores**: $\\text{scores} = Q \\cdot K^T$ (how well does each query match each key?)\n",
    "2. **Scaling**: Divide by $\\sqrt{d_k}$ (we'll explain why)\n",
    "3. **Masking**: Prevent tokens from attending to future positions\n",
    "4. **Softmax**: Convert scores to probabilities\n",
    "5. **Weighted sum**: $\\text{output} = \\text{weights} \\cdot V$\n",
    "\n",
    "This is where tokens actually start \"talking\" to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:16:57.884062Z",
     "iopub.status.busy": "2025-12-10T21:16:57.883996Z",
     "iopub.status.idle": "2025-12-10T21:16:57.885540Z",
     "shell.execute_reply": "2025-12-10T21:16:57.885299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKV projections complete. Ready for attention computation.\n"
     ]
    }
   ],
   "source": [
    "# Store for next notebook\n",
    "qkv_data = {\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'W_Q': W_Q,\n",
    "    'W_K': W_K,\n",
    "    'W_V': W_V,\n",
    "    'Q': Q_all,\n",
    "    'K': K_all,\n",
    "    'V': V_all,\n",
    "    'D_MODEL': D_MODEL,\n",
    "    'D_K': D_K,\n",
    "    'NUM_HEADS': NUM_HEADS\n",
    "}\n",
    "print(\"QKV projections complete. Ready for attention computation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Projects input embeddings into Query, Key, and Value matrices for attention computation using weight matrices."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
