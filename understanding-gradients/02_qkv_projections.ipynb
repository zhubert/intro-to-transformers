{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. QKV Projections\n",
    "\n",
    "**Transforming embeddings into Query, Key, and Value representations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got our embedding matrix `X`. Now things get interesting.\n",
    "\n",
    "The next step is to compute **Query (Q)**, **Key (K)**, and **Value (V)** projections. These are the fundamental building blocks of the attention mechanism (and where a lot of the magic happens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Q, K, V?\n",
    "\n",
    "The attention mechanism lets each token decide how much attention to pay to every other token in the sequence. Think of it like a database lookup, but fuzzy and learned:\n",
    "\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What do I contain?\"\n",
    "- **Value (V)**: \"What information do I have to offer?\"\n",
    "\n",
    "We take each token's embedding and project it into these three different representations using learned weight matrices. Same input, three different views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Structure\n",
    "\n",
    "Our model uses **multi-head attention** with `num_heads = 2`. This means we compute attention independently in 2 different subspaces, then combine the results.\n",
    "\n",
    "**Architecture:**\n",
    "- **d_model:** 16 (embedding dimension)\n",
    "- **num_heads:** 2\n",
    "- **d_k:** d_model / num_heads = 8 (dimension per head)\n",
    "\n",
    "Each head has its own set of weight matrices that project the 16-dimensional embeddings into 8-dimensional Q, K, V representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility (same as previous notebook)\n",
    "random.seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS  # 8\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]\n",
    "\n",
    "print(f\"d_model: {D_MODEL}\")\n",
    "print(f\"num_heads: {NUM_HEADS}\")\n",
    "print(f\"d_k (dimension per head): {D_K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    \"\"\"Generate a random vector with values ~ N(0, scale^2)\"\"\"\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    \"\"\"Generate a random matrix with values ~ N(0, scale^2)\"\"\"\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    \"\"\"Element-wise addition of two vectors\"\"\"\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    \"\"\"Multiply matrices A @ B where A is [m, n] and B is [n, p]\"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    p = len(B[0])\n",
    "    result = [[0] * p for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(n))\n",
    "    return result\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    \"\"\"Format vector as string with specified decimal places\"\"\"\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate embeddings from previous notebook (same random seed)\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "\n",
    "tokens = [1, 3, 4, 5, 2]  # <BOS>, I, like, transformers, <EOS>\n",
    "seq_len = len(tokens)\n",
    "\n",
    "token_embeddings = [E_token[token_id] for token_id in tokens]\n",
    "X = [add_vectors(token_embeddings[i], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "print(f\"Embedding matrix X: [{seq_len}, {D_MODEL}]\")\n",
    "print(\"(Recreated from previous notebook with same random seed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Matrices\n",
    "\n",
    "For each head, we need three weight matrices:\n",
    "\n",
    "**For Head 0:**\n",
    "- **W_Q[0]:** Query weight matrix `[16, 8]`\n",
    "- **W_K[0]:** Key weight matrix `[16, 8]`\n",
    "- **W_V[0]:** Value weight matrix `[16, 8]`\n",
    "\n",
    "**For Head 1:**\n",
    "- **W_Q[1]:** Query weight matrix `[16, 8]`\n",
    "- **W_K[1]:** Key weight matrix `[16, 8]`\n",
    "- **W_V[1]:** Value weight matrix `[16, 8]`\n",
    "\n",
    "These matrices are initialized with small random values and are learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weight matrices for each head\n",
    "W_Q = []\n",
    "W_K = []\n",
    "W_V = []\n",
    "\n",
    "for head in range(NUM_HEADS):\n",
    "    W_Q.append(random_matrix(D_MODEL, D_K))  # [16, 8]\n",
    "    W_K.append(random_matrix(D_MODEL, D_K))  # [16, 8]\n",
    "    W_V.append(random_matrix(D_MODEL, D_K))  # [16, 8]\n",
    "    \n",
    "print(f\"Initialized weight matrices for {NUM_HEADS} heads\")\n",
    "print(f\"Each W_Q, W_K, W_V has shape [{D_MODEL}, {D_K}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication Basics\n",
    "\n",
    "Before we dive into the actual Q, K, V calculations, let's review **matrix multiplication**. It's the core operation we'll be using... basically everywhere in this project.\n",
    "\n",
    "### How Matrix Multiplication Works\n",
    "\n",
    "When we multiply two matrices `A @ B`:\n",
    "- **A** has shape `[m, n]` (m rows, n columns)\n",
    "- **B** has shape `[n, p]` (n rows, p columns)\n",
    "- The result has shape `[m, p]` (m rows, p columns)\n",
    "\n",
    "**Key requirement:** The number of columns in A must equal the number of rows in B.\n",
    "\n",
    "**The operation:** To compute element `[i, j]` in the result:\n",
    "\n",
    "$$\\text{result}_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj}$$\n",
    "\n",
    "In other words, we take row $i$ from A, column $j$ from B, multiply corresponding elements, and sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example of matrix multiplication\n",
    "A = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]\n",
    "B = [\n",
    "    [1, 4],\n",
    "    [2, 5],\n",
    "    [3, 6]\n",
    "]\n",
    "\n",
    "print(\"Example: [2,3] @ [3,2] = [2,2]\")\n",
    "print()\n",
    "print(\"A =\", A)\n",
    "print(\"B =\", B)\n",
    "print()\n",
    "result = matmul(A, B)\n",
    "print(\"Result[0,0] = (1*1) + (2*2) + (3*3) =\", 1*1 + 2*2 + 3*3)\n",
    "print(\"Result[0,1] = (1*4) + (2*5) + (3*6) =\", 1*4 + 2*5 + 3*6)\n",
    "print(\"Result[1,0] = (4*1) + (5*2) + (6*3) =\", 4*1 + 5*2 + 6*3)\n",
    "print(\"Result[1,1] = (4*4) + (5*5) + (6*6) =\", 4*4 + 5*5 + 6*6)\n",
    "print()\n",
    "print(\"Result =\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Projection Operation\n",
    "\n",
    "Now we can compute Q, K, V for each head using matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{head} &= X W_Q^{(head)} \\\\\n",
    "K_{head} &= X W_K^{(head)} \\\\\n",
    "V_{head} &= X W_V^{(head)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `X` has shape `[seq_len, d_model]` = `[5, 16]`\n",
    "- `W_Q[head]`, `W_K[head]`, `W_V[head]` have shape `[d_model, d_k]` = `[16, 8]`\n",
    "- `Q[head]`, `K[head]`, `V[head]` have shape `[seq_len, d_k]` = `[5, 8]`\n",
    "\n",
    "Each row of the result represents the Q/K/V vector for one token position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Q, K, V for each head\n",
    "Q_all = []\n",
    "K_all = []\n",
    "V_all = []\n",
    "\n",
    "for head in range(NUM_HEADS):\n",
    "    Q = matmul(X, W_Q[head])  # [5, 16] @ [16, 8] = [5, 8]\n",
    "    K = matmul(X, W_K[head])\n",
    "    V = matmul(X, W_V[head])\n",
    "    \n",
    "    Q_all.append(Q)\n",
    "    K_all.append(K)\n",
    "    V_all.append(V)\n",
    "\n",
    "print(f\"Computed Q, K, V for {NUM_HEADS} heads\")\n",
    "print(f\"Each Q, K, V has shape [{seq_len}, {D_K}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Calculation Example\n",
    "\n",
    "Let's walk through computing `Q[0][0]`â€”the query vector for the first token `<BOS>` in head 0.\n",
    "\n",
    "For each output dimension `j` (0 to 7), we compute:\n",
    "```\n",
    "Q[0][0][j] = sum(X[0][i] * W_Q[0][i][j] for i in range(16))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed calculation for Q[0][0] (query for <BOS> in head 0)\n",
    "print(\"Computing Q[0][0] - Query for <BOS> in Head 0\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Input: X[0] (embedding for <BOS>):\")\n",
    "print(f\"  {format_vector(X[0])}\")\n",
    "print()\n",
    "print(f\"Operation: Q[0][0] = X[0] @ W_Q[0]\")\n",
    "print(f\"  [{1}, {D_MODEL}] @ [{D_MODEL}, {D_K}] = [{1}, {D_K}]\")\n",
    "print()\n",
    "\n",
    "# Show detailed computation for first output dimension\n",
    "print(\"Example: Computing Q[0][0][0] (first dimension):\")\n",
    "terms = [f\"({X[0][i]:.4f} * {W_Q[0][i][0]:.4f})\" for i in range(4)]\n",
    "print(f\"  = {' + '.join(terms)} + ...\")\n",
    "val = sum(X[0][i] * W_Q[0][i][0] for i in range(D_MODEL))\n",
    "print(f\"  = {val:.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Result: Q[0][0] = {format_vector(Q_all[0][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head 0 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEAD 0 - Query Matrix Q[0]\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(Q_all[0]):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEAD 0 - Key Matrix K[0]\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(K_all[0]):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEAD 0 - Value Matrix V[0]\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(V_all[0]):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head 1 Results\n",
    "\n",
    "The second head uses different weight matrices, producing different Q, K, V representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEAD 1 - Query Matrix Q[1]\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(Q_all[1]):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEAD 1 - Key Matrix K[1]\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(K_all[1]):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEAD 1 - Value Matrix V[1]\")\n",
    "print(f\"Shape: [{seq_len}, {D_K}]\")\n",
    "print()\n",
    "for i, row in enumerate(V_all[1]):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Multiple Heads?\n",
    "\n",
    "Notice that Head 0 and Head 1 produce completely different Q, K, V representations for the same input. This is the power of multi-head attention.\n",
    "\n",
    "Different heads can learn to focus on different types of relationships:\n",
    "\n",
    "- **Head 0** might learn to focus on syntactic relationships (like subject-verb agreement)\n",
    "- **Head 1** might learn to focus on semantic relationships (like related concepts)\n",
    "\n",
    "It's like having multiple experts examining the same data from different perspectives. Later, we'll combine the outputs from both heads to get a richer, more nuanced representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "Now that we have Q, K, V for both heads, we can compute the actual attention mechanism:\n",
    "1. **Attention scores**: How much should each token attend to every other token?\n",
    "2. **Attention weights**: Normalized scores (using softmax)\n",
    "3. **Attention output**: Weighted combination of value vectors\n",
    "\n",
    "This is where the \"attention\" actually happens. Let's dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store for next notebook\n",
    "qkv_data = {\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'W_Q': W_Q,\n",
    "    'W_K': W_K,\n",
    "    'W_V': W_V,\n",
    "    'Q': Q_all,\n",
    "    'K': K_all,\n",
    "    'V': V_all,\n",
    "    'D_MODEL': D_MODEL,\n",
    "    'D_K': D_K,\n",
    "    'NUM_HEADS': NUM_HEADS\n",
    "}\n",
    "print(\"QKV data stored for next notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
