{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdamW Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Step: Actually Learning\n",
    "\n",
    "We've traced the entire forward pass through our tiny transformer. We've computed the loss. We've backpropagated gradients through every layer, all the way back to the embeddings.\n",
    "\n",
    "Now we have ~2,600 gradients, one for each parameter in the model. Each gradient tells us: \"If you increase this parameter slightly, the loss will change by approximately this much.\"\n",
    "\n",
    "The question is: **what do we do with these gradients?**\n",
    "\n",
    "The simplest answer would be plain **stochastic gradient descent (SGD)**:\n",
    "\n",
    "$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial \\theta}$$\n",
    "\n",
    "Subtract the gradient (scaled by a learning rate $\\eta$), and you're done.\n",
    "\n",
    "But SGD has problems. Modern transformers use something much more sophisticated: **AdamW**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why SGD Isn't Enough\n",
    "\n",
    "Plain gradient descent has several failure modes:\n",
    "\n",
    "**1. Same learning rate for all parameters**\n",
    "\n",
    "Some parameters need large updates (they're far from optimal). Others need tiny updates (they're already close). SGD uses the same learning rate for everyone, which means you have to choose a rate that's safe for the most sensitive parameters, and that's too slow for the rest.\n",
    "\n",
    "**2. Noisy gradients cause oscillation**\n",
    "\n",
    "Gradients computed on mini-batches are noisy estimates of the true gradient. SGD follows each noisy gradient directly, which can cause the optimization to zigzag back and forth instead of heading straight toward the minimum.\n",
    "\n",
    "**3. Sensitive to learning rate**\n",
    "\n",
    "Too high a learning rate and training diverges. Too low and it takes forever. The right learning rate depends on the loss surface, which changes during training.\n",
    "\n",
    "**AdamW** addresses all of these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What AdamW Does\n",
    "\n",
    "AdamW combines three powerful ideas:\n",
    "\n",
    "**1. Adaptive learning rates (from Adam)**\n",
    "\n",
    "Each parameter gets its own effective learning rate, based on the history of its gradients. Parameters with consistently large gradients get smaller learning rates (we're already moving fast). Parameters with small gradients get larger learning rates (we need to push harder).\n",
    "\n",
    "**2. Momentum (from Adam)**\n",
    "\n",
    "Instead of following each noisy gradient directly, we track an exponential moving average of past gradients. This smooths out the noise and helps the optimizer maintain direction even when individual gradients are noisy.\n",
    "\n",
    "**3. Weight decay (the \"W\" in AdamW)**\n",
    "\n",
    "Regularization that shrinks weights toward zero. This prevents overfitting by penalizing large weights. AdamW applies weight decay *directly* to parameters, separate from the gradient update. (The original Adam paper applied weight decay through the gradient, which interacted poorly with the adaptive learning rates.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.262363Z",
     "iopub.status.busy": "2025-12-10T21:17:12.262278Z",
     "iopub.status.idle": "2025-12-10T21:17:12.273939Z",
     "shell.execute_reply": "2025-12-10T21:17:12.273680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW Hyperparameters\n",
      "==================================================\n",
      "  learning_rate (\u03b1) = 0.001\n",
      "  beta1 (\u03b2\u2081)        = 0.9\n",
      "  beta2 (\u03b2\u2082)        = 0.999\n",
      "  epsilon (\u03b5)       = 1e-08\n",
      "  weight_decay (\u03bb)  = 0.01\n",
      "\n",
      "These values are standard. Most transformer training\n",
      "uses exactly these numbers.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# AdamW hyperparameters (standard values from the literature)\n",
    "learning_rate = 0.001    # \u03b1 (alpha): base learning rate\n",
    "beta1 = 0.9              # \u03b2\u2081: decay rate for first moment (momentum)\n",
    "beta2 = 0.999            # \u03b2\u2082: decay rate for second moment (adaptive LR)\n",
    "epsilon = 1e-8           # \u03b5: small constant for numerical stability\n",
    "weight_decay = 0.01      # \u03bb: weight decay coefficient\n",
    "\n",
    "print(\"AdamW Hyperparameters\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  learning_rate (\u03b1) = {learning_rate}\")\n",
    "print(f\"  beta1 (\u03b2\u2081)        = {beta1}\")\n",
    "print(f\"  beta2 (\u03b2\u2082)        = {beta2}\")\n",
    "print(f\"  epsilon (\u03b5)       = {epsilon}\")\n",
    "print(f\"  weight_decay (\u03bb)  = {weight_decay}\")\n",
    "print()\n",
    "print(\"These values are standard. Most transformer training\")\n",
    "print(\"uses exactly these numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AdamW Algorithm\n",
    "\n",
    "For each parameter $\\theta$, AdamW maintains two **state variables** across training steps:\n",
    "\n",
    "- $m$ (first moment estimate): An exponential moving average of the gradient (this is momentum)\n",
    "- $v$ (second moment estimate): An exponential moving average of the squared gradient (this enables adaptive learning rates)\n",
    "\n",
    "At each time step $t$, given gradient $g_t = \\frac{\\partial L}{\\partial \\theta}$:\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1: Update biased moment estimates**\n",
    "\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\n",
    "\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$$\n",
    "\n",
    "The first moment $m$ accumulates gradient direction (momentum). The second moment $v$ accumulates gradient magnitude.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Bias correction**\n",
    "\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "\n",
    "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "Since $m$ and $v$ are initialized to zero, they're biased toward zero in early steps. This correction compensates for that bias.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: Weight decay**\n",
    "\n",
    "$$\\theta \\leftarrow \\theta \\cdot (1 - \\alpha \\cdot \\lambda)$$\n",
    "\n",
    "Shrink the parameter toward zero by a small amount. This is regularization.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 4: Parameter update**\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "Update the parameter using the bias-corrected moments. The $\\sqrt{\\hat{v}_t}$ in the denominator creates the adaptive learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.290923Z",
     "iopub.status.busy": "2025-12-10T21:17:12.290818Z",
     "iopub.status.idle": "2025-12-10T21:17:12.293084Z",
     "shell.execute_reply": "2025-12-10T21:17:12.292809Z"
    }
   },
   "outputs": [],
   "source": [
    "def adamw_update(theta, gradient, m, v, t, \n",
    "                 lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, wd=0.01):\n",
    "    \"\"\"\n",
    "    Perform one AdamW update step for a single parameter.\n",
    "    \n",
    "    Args:\n",
    "        theta: Current parameter value\n",
    "        gradient: Gradient of loss w.r.t. this parameter\n",
    "        m: First moment estimate (momentum)\n",
    "        v: Second moment estimate (for adaptive LR)\n",
    "        t: Time step (starts at 1, increments each update)\n",
    "        lr: Base learning rate (alpha)\n",
    "        beta1: First moment decay rate\n",
    "        beta2: Second moment decay rate\n",
    "        eps: Small constant for numerical stability\n",
    "        wd: Weight decay coefficient\n",
    "    \n",
    "    Returns:\n",
    "        new_theta: Updated parameter value\n",
    "        new_m: Updated first moment\n",
    "        new_v: Updated second moment\n",
    "    \"\"\"\n",
    "    # Step 1: Update biased moments\n",
    "    m_new = beta1 * m + (1 - beta1) * gradient\n",
    "    v_new = beta2 * v + (1 - beta2) * gradient**2\n",
    "    \n",
    "    # Step 2: Bias correction\n",
    "    m_hat = m_new / (1 - beta1**t)\n",
    "    v_hat = v_new / (1 - beta2**t)\n",
    "    \n",
    "    # Step 3: Weight decay (applied before gradient update)\n",
    "    theta_decayed = theta * (1 - lr * wd)\n",
    "    \n",
    "    # Step 4: Parameter update\n",
    "    theta_new = theta_decayed - lr * m_hat / (math.sqrt(v_hat) + eps)\n",
    "    \n",
    "    return theta_new, m_new, v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example: Updating One Parameter\n",
    "\n",
    "Let's walk through a complete AdamW update for a single parameter. We'll update the first element of the `<BOS>` token's embedding.\n",
    "\n",
    "Suppose:\n",
    "- Current value: $\\theta = 0.024634$\n",
    "- Gradient: $g = -0.352893$ (negative means increasing $\\theta$ would *decrease* loss)\n",
    "- This is time step $t = 1$ (first update)\n",
    "- Moments initialized to zero: $m_0 = 0$, $v_0 = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.294013Z",
     "iopub.status.busy": "2025-12-10T21:17:12.293930Z",
     "iopub.status.idle": "2025-12-10T21:17:12.295983Z",
     "shell.execute_reply": "2025-12-10T21:17:12.295707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State\n",
      "==================================================\n",
      "  Parameter \u03b8     = 0.024634\n",
      "  Gradient g      = -0.352893\n",
      "  First moment m  = 0.000000\n",
      "  Second moment v = 0.000000\n",
      "  Time step t     = 1\n"
     ]
    }
   ],
   "source": [
    "# Initial state for our example parameter\n",
    "theta = 0.024634      # Current value of E_token[<BOS>][0]\n",
    "g = -0.352893         # Gradient (negative = we should increase theta)\n",
    "m_prev = 0.0          # First moment (initialized to 0)\n",
    "v_prev = 0.0          # Second moment (initialized to 0)\n",
    "t = 1                 # Time step (first update)\n",
    "\n",
    "print(\"Initial State\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Parameter \u03b8     = {theta:.6f}\")\n",
    "print(f\"  Gradient g      = {g:.6f}\")\n",
    "print(f\"  First moment m  = {m_prev:.6f}\")\n",
    "print(f\"  Second moment v = {v_prev:.6f}\")\n",
    "print(f\"  Time step t     = {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.296774Z",
     "iopub.status.busy": "2025-12-10T21:17:12.296709Z",
     "iopub.status.idle": "2025-12-10T21:17:12.299042Z",
     "shell.execute_reply": "2025-12-10T21:17:12.298827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Update Biased Moment Estimates\n",
      "============================================================\n",
      "\n",
      "First moment (momentum):\n",
      "  m\u2081 = \u03b2\u2081 \u00d7 m\u2080 + (1 - \u03b2\u2081) \u00d7 g\n",
      "     = 0.9 \u00d7 0.0 + 0.1 \u00d7 -0.352893\n",
      "     = -0.035289\n",
      "\n",
      "Second moment (for adaptive LR):\n",
      "  v\u2081 = \u03b2\u2082 \u00d7 v\u2080 + (1 - \u03b2\u2082) \u00d7 g\u00b2\n",
      "     = 0.999 \u00d7 0.0 + 0.001 \u00d7 (-0.352893)\u00b2\n",
      "     = 0.999 \u00d7 0.0 + 0.001 \u00d7 0.124533\n",
      "     = 0.000124533\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Update biased moments\n",
    "m_new = beta1 * m_prev + (1 - beta1) * g\n",
    "v_new = beta2 * v_prev + (1 - beta2) * g**2\n",
    "\n",
    "print(\"Step 1: Update Biased Moment Estimates\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"First moment (momentum):\")\n",
    "print(f\"  m\u2081 = \u03b2\u2081 \u00d7 m\u2080 + (1 - \u03b2\u2081) \u00d7 g\")\n",
    "print(f\"     = {beta1} \u00d7 {m_prev} + {1-beta1:.1f} \u00d7 {g:.6f}\")\n",
    "print(f\"     = {m_new:.6f}\")\n",
    "print()\n",
    "print(\"Second moment (for adaptive LR):\")\n",
    "print(f\"  v\u2081 = \u03b2\u2082 \u00d7 v\u2080 + (1 - \u03b2\u2082) \u00d7 g\u00b2\")\n",
    "print(f\"     = {beta2} \u00d7 {v_prev} + {1-beta2:.3f} \u00d7 ({g:.6f})\u00b2\")\n",
    "print(f\"     = {beta2} \u00d7 {v_prev} + {1-beta2:.3f} \u00d7 {g**2:.6f}\")\n",
    "print(f\"     = {v_new:.9f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.299874Z",
     "iopub.status.busy": "2025-12-10T21:17:12.299792Z",
     "iopub.status.idle": "2025-12-10T21:17:12.302176Z",
     "shell.execute_reply": "2025-12-10T21:17:12.301904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Bias Correction\n",
      "============================================================\n",
      "\n",
      "Bias-corrected first moment:\n",
      "  m\u0302\u2081 = m\u2081 / (1 - \u03b2\u2081^t)\n",
      "     = -0.035289 / (1 - 0.9^1)\n",
      "     = -0.035289 / 0.1\n",
      "     = -0.352893\n",
      "\n",
      "Bias-corrected second moment:\n",
      "  v\u0302\u2081 = v\u2081 / (1 - \u03b2\u2082^t)\n",
      "     = 0.000124533 / (1 - 0.999^1)\n",
      "     = 0.000124533 / 0.001\n",
      "     = 0.124533\n",
      "\n",
      "Note: At t=1, m\u0302 equals the gradient (-0.352893 \u2248 -0.352893).\n",
      "This is expected. We haven't accumulated any history yet.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Bias correction\n",
    "m_hat = m_new / (1 - beta1**t)\n",
    "v_hat = v_new / (1 - beta2**t)\n",
    "\n",
    "print(\"Step 2: Bias Correction\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Bias-corrected first moment:\")\n",
    "print(f\"  m\u0302\u2081 = m\u2081 / (1 - \u03b2\u2081^t)\")\n",
    "print(f\"     = {m_new:.6f} / (1 - {beta1}^{t})\")\n",
    "print(f\"     = {m_new:.6f} / {1 - beta1**t:.1f}\")\n",
    "print(f\"     = {m_hat:.6f}\")\n",
    "print()\n",
    "print(\"Bias-corrected second moment:\")\n",
    "print(f\"  v\u0302\u2081 = v\u2081 / (1 - \u03b2\u2082^t)\")\n",
    "print(f\"     = {v_new:.9f} / (1 - {beta2}^{t})\")\n",
    "print(f\"     = {v_new:.9f} / {1 - beta2**t:.3f}\")\n",
    "print(f\"     = {v_hat:.6f}\")\n",
    "print()\n",
    "print(f\"Note: At t=1, m\u0302 equals the gradient ({m_hat:.6f} \u2248 {g:.6f}).\")\n",
    "print(f\"This is expected. We haven't accumulated any history yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.302881Z",
     "iopub.status.busy": "2025-12-10T21:17:12.302801Z",
     "iopub.status.idle": "2025-12-10T21:17:12.304821Z",
     "shell.execute_reply": "2025-12-10T21:17:12.304585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Weight Decay\n",
      "============================================================\n",
      "\n",
      "  \u03b8_decayed = \u03b8 \u00d7 (1 - \u03b1 \u00d7 \u03bb)\n",
      "           = 0.024634 \u00d7 (1 - 0.001 \u00d7 0.01)\n",
      "           = 0.024634 \u00d7 0.99999\n",
      "           = 0.024633754\n",
      "\n",
      "Weight shrinkage: 0.000000246\n",
      "(Tiny! Weight decay is a gentle regularization.)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Weight decay\n",
    "decay_factor = 1 - learning_rate * weight_decay\n",
    "theta_decayed = theta * decay_factor\n",
    "\n",
    "print(\"Step 3: Weight Decay\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"  \u03b8_decayed = \u03b8 \u00d7 (1 - \u03b1 \u00d7 \u03bb)\")\n",
    "print(f\"           = {theta:.6f} \u00d7 (1 - {learning_rate} \u00d7 {weight_decay})\")\n",
    "print(f\"           = {theta:.6f} \u00d7 {decay_factor:.5f}\")\n",
    "print(f\"           = {theta_decayed:.9f}\")\n",
    "print()\n",
    "print(f\"Weight shrinkage: {theta - theta_decayed:.9f}\")\n",
    "print(f\"(Tiny! Weight decay is a gentle regularization.)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.305422Z",
     "iopub.status.busy": "2025-12-10T21:17:12.305358Z",
     "iopub.status.idle": "2025-12-10T21:17:12.307803Z",
     "shell.execute_reply": "2025-12-10T21:17:12.307559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Parameter Update\n",
      "============================================================\n",
      "\n",
      "Compute the adaptive learning rate:\n",
      "  Effective LR = \u03b1 / (\u221av\u0302 + \u03b5)\n",
      "              = 0.001 / (\u221a0.124533 + 1e-08)\n",
      "              = 0.001 / (0.352893 + 1e-08)\n",
      "              = 0.001 / 0.352893\n",
      "              = 0.002834\n",
      "\n",
      "This is 2.83\u00d7 the base learning rate!\n",
      "\n",
      "Compute the update:\n",
      "  Update = \u03b1 \u00d7 m\u0302 / (\u221av\u0302 + \u03b5)\n",
      "         = 0.001 \u00d7 -0.352893 / 0.352893\n",
      "         = -0.001000\n",
      "\n",
      "Apply to parameter:\n",
      "  \u03b8_new = \u03b8_decayed - update\n",
      "        = 0.024634 - (-0.001000)\n",
      "        = 0.025634\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Parameter update\n",
    "denominator = math.sqrt(v_hat) + epsilon\n",
    "adaptive_lr = learning_rate / denominator\n",
    "update_amount = learning_rate * m_hat / denominator\n",
    "theta_new = theta_decayed - update_amount\n",
    "\n",
    "print(\"Step 4: Parameter Update\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Compute the adaptive learning rate:\")\n",
    "print(f\"  Effective LR = \u03b1 / (\u221av\u0302 + \u03b5)\")\n",
    "print(f\"              = {learning_rate} / (\u221a{v_hat:.6f} + {epsilon})\")\n",
    "print(f\"              = {learning_rate} / ({math.sqrt(v_hat):.6f} + {epsilon})\")\n",
    "print(f\"              = {learning_rate} / {denominator:.6f}\")\n",
    "print(f\"              = {adaptive_lr:.6f}\")\n",
    "print()\n",
    "print(f\"This is {adaptive_lr/learning_rate:.2f}\u00d7 the base learning rate!\")\n",
    "print()\n",
    "print(\"Compute the update:\")\n",
    "print(f\"  Update = \u03b1 \u00d7 m\u0302 / (\u221av\u0302 + \u03b5)\")\n",
    "print(f\"         = {learning_rate} \u00d7 {m_hat:.6f} / {denominator:.6f}\")\n",
    "print(f\"         = {update_amount:.6f}\")\n",
    "print()\n",
    "print(\"Apply to parameter:\")\n",
    "print(f\"  \u03b8_new = \u03b8_decayed - update\")\n",
    "print(f\"        = {theta_decayed:.6f} - ({update_amount:.6f})\")\n",
    "print(f\"        = {theta_new:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.308441Z",
     "iopub.status.busy": "2025-12-10T21:17:12.308373Z",
     "iopub.status.idle": "2025-12-10T21:17:12.310158Z",
     "shell.execute_reply": "2025-12-10T21:17:12.309924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUMMARY: One AdamW Update\n",
      "============================================================\n",
      "\n",
      "Parameter: E_token[<BOS>][0]\n",
      "  Before:  \u03b8 = 0.024634\n",
      "  After:   \u03b8 = 0.025634\n",
      "  Change:      +0.001000\n",
      "\n",
      "The gradient was negative (-0.352893), meaning:\n",
      "  'Increasing \u03b8 would decrease the loss.'\n",
      "So AdamW increased \u03b8. The model learned slightly!\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY: One AdamW Update\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Parameter: E_token[<BOS>][0]\")\n",
    "print(f\"  Before:  \u03b8 = {theta:.6f}\")\n",
    "print(f\"  After:   \u03b8 = {theta_new:.6f}\")\n",
    "print(f\"  Change:      {theta_new - theta:+.6f}\")\n",
    "print()\n",
    "print(f\"The gradient was negative ({g:.6f}), meaning:\")\n",
    "print(f\"  'Increasing \u03b8 would decrease the loss.'\")\n",
    "print(f\"So AdamW increased \u03b8. The model learned slightly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the Adaptive Learning Rate Matters\n",
    "\n",
    "Notice that our effective learning rate was ~2.83\u00d7 the base learning rate. This happened because:\n",
    "\n",
    "$$\\text{Effective LR} = \\frac{\\alpha}{\\sqrt{\\hat{v}} + \\epsilon} = \\frac{0.001}{0.353 + 10^{-8}} \\approx 0.00283$$\n",
    "\n",
    "The denominator $\\sqrt{\\hat{v}}$ is the RMS (root mean square) of recent gradients for this parameter.\n",
    "\n",
    "- **Large gradients** \u2192 Large $\\hat{v}$ \u2192 Small effective LR \u2192 Cautious updates\n",
    "- **Small gradients** \u2192 Small $\\hat{v}$ \u2192 Large effective LR \u2192 Aggressive updates\n",
    "\n",
    "This automatically adapts to each parameter's gradient scale. Parameters deep in the network often have smaller gradients (due to the chain rule), but Adam compensates by giving them larger effective learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.310889Z",
     "iopub.status.busy": "2025-12-10T21:17:12.310797Z",
     "iopub.status.idle": "2025-12-10T21:17:12.313088Z",
     "shell.execute_reply": "2025-12-10T21:17:12.312836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Gradient Magnitude Affects Learning Rate\n",
      "============================================================\n",
      "\n",
      "  |gradient|     v\u0302 (\u2248g\u00b2)    Effective LR   Multiplier\n",
      "------------------------------------------------------------\n",
      "       0.001     0.000001        0.999990       999.99\u00d7\n",
      "       0.010     0.000100        0.100000       100.00\u00d7\n",
      "       0.100     0.010000        0.010000        10.00\u00d7\n",
      "       0.500     0.250000        0.002000         2.00\u00d7\n",
      "       1.000     1.000000        0.001000         1.00\u00d7\n",
      "       5.000    25.000000        0.000200         0.20\u00d7\n",
      "\n",
      "Small gradients get large multipliers; large gradients get small ones.\n",
      "This is what makes Adam 'adaptive'.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate how v_hat affects the effective learning rate\n",
    "print(\"How Gradient Magnitude Affects Learning Rate\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"{'|gradient|':>12} {'v\u0302 (\u2248g\u00b2)':>12} {'Effective LR':>15} {'Multiplier':>12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "test_grads = [0.001, 0.01, 0.1, 0.5, 1.0, 5.0]\n",
    "for g_test in test_grads:\n",
    "    v_test = g_test**2  # Simplified: v \u2248 g\u00b2 at t=1\n",
    "    eff_lr = learning_rate / (math.sqrt(v_test) + epsilon)\n",
    "    mult = eff_lr / learning_rate\n",
    "    print(f\"{g_test:>12.3f} {v_test:>12.6f} {eff_lr:>15.6f} {mult:>12.2f}\u00d7\")\n",
    "\n",
    "print()\n",
    "print(\"Small gradients get large multipliers; large gradients get small ones.\")\n",
    "print(\"This is what makes Adam 'adaptive'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating All Parameters\n",
    "\n",
    "We apply this same AdamW update to **every single parameter** in the model. Let's count them:\n",
    "\n",
    "| Component | Shape | Count |\n",
    "|-----------|-------|-------|\n",
    "| Token embeddings $E_{token}$ | [6, 16] | 96 |\n",
    "| Position embeddings $E_{pos}$ | [5, 16] | 80 |\n",
    "| Attention $W_Q$ (\u00d72 heads) | [16, 8] \u00d7 2 | 256 |\n",
    "| Attention $W_K$ (\u00d72 heads) | [16, 8] \u00d7 2 | 256 |\n",
    "| Attention $W_V$ (\u00d72 heads) | [16, 8] \u00d7 2 | 256 |\n",
    "| Attention $W_O$ | [16, 16] | 256 |\n",
    "| FFN $W_1$ | [64, 16] | 1,024 |\n",
    "| FFN $b_1$ | [64] | 64 |\n",
    "| FFN $W_2$ | [16, 64] | 1,024 |\n",
    "| FFN $b_2$ | [16] | 16 |\n",
    "| Layer norm $\\gamma$ | [16] | 16 |\n",
    "| Layer norm $\\beta$ | [16] | 16 |\n",
    "| LM head $W_{lm}$ | [6, 16] | 96 |\n",
    "| **Total** | | **~3,456** |\n",
    "\n",
    "Each parameter has its own $m$ and $v$ state, which means AdamW needs to store 2\u00d7 as many values as there are parameters. This is the memory cost of adaptive optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.313906Z",
     "iopub.status.busy": "2025-12-10T21:17:12.313837Z",
     "iopub.status.idle": "2025-12-10T21:17:12.316020Z",
     "shell.execute_reply": "2025-12-10T21:17:12.315732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Count\n",
      "========================================\n",
      "  Token embeddings         :     96\n",
      "  Position embeddings      :     80\n",
      "  W_Q (2 heads)            :    256\n",
      "  W_K (2 heads)            :    256\n",
      "  W_V (2 heads)            :    256\n",
      "  W_O                      :    256\n",
      "  FFN W_1                  :  1,024\n",
      "  FFN b_1                  :     64\n",
      "  FFN W_2                  :  1,024\n",
      "  FFN b_2                  :     16\n",
      "  Layer norm \u03b3             :     16\n",
      "  Layer norm \u03b2             :     16\n",
      "  LM head W_lm             :     96\n",
      "----------------------------------------\n",
      "  Total                    :  3,456\n",
      "\n",
      "AdamW state (m and v): 6,912 values\n"
     ]
    }
   ],
   "source": [
    "# Parameter count breakdown\n",
    "param_counts = {\n",
    "    \"Token embeddings\": 6 * 16,\n",
    "    \"Position embeddings\": 5 * 16,\n",
    "    \"W_Q (2 heads)\": 2 * 16 * 8,\n",
    "    \"W_K (2 heads)\": 2 * 16 * 8,\n",
    "    \"W_V (2 heads)\": 2 * 16 * 8,\n",
    "    \"W_O\": 16 * 16,\n",
    "    \"FFN W_1\": 64 * 16,\n",
    "    \"FFN b_1\": 64,\n",
    "    \"FFN W_2\": 16 * 64,\n",
    "    \"FFN b_2\": 16,\n",
    "    \"Layer norm \u03b3\": 16,\n",
    "    \"Layer norm \u03b2\": 16,\n",
    "    \"LM head W_lm\": 6 * 16,\n",
    "}\n",
    "\n",
    "print(\"Parameter Count\")\n",
    "print(\"=\"*40)\n",
    "total = 0\n",
    "for name, count in param_counts.items():\n",
    "    print(f\"  {name:25s}: {count:>6,}\")\n",
    "    total += count\n",
    "print(\"-\"*40)\n",
    "print(f\"  {'Total':25s}: {total:>6,}\")\n",
    "print()\n",
    "print(f\"AdamW state (m and v): {2 * total:,} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Training Loop\n",
    "\n",
    "We've now traced through **one complete training step**:\n",
    "\n",
    "1. **Forward pass** (Notebooks 01-07)\n",
    "   - Tokenize input \u2192 Embeddings \u2192 Attention \u2192 FFN \u2192 Layer Norm \u2192 Logits \u2192 Loss\n",
    "\n",
    "2. **Backward pass** (Notebooks 08-09)\n",
    "   - Compute $\\frac{\\partial L}{\\partial \\text{logits}}$ \u2192 Backprop through each layer \u2192 Gradients for all parameters\n",
    "\n",
    "3. **Optimization** (This notebook)\n",
    "   - Update each parameter using AdamW\n",
    "\n",
    "In pseudocode:\n",
    "\n",
    "```python\n",
    "# Initialize model parameters randomly\n",
    "# Initialize Adam state (m=0, v=0 for each parameter)\n",
    "t = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in training_data:\n",
    "        t += 1\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model.forward(batch)\n",
    "        loss = cross_entropy(predictions, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        gradients = model.backward(loss)\n",
    "        \n",
    "        # Optimization\n",
    "        for param in model.parameters:\n",
    "            param.value, param.m, param.v = adamw_update(\n",
    "                param.value, param.gradient, param.m, param.v, t\n",
    "            )\n",
    "```\n",
    "\n",
    "Repeat this loop millions of times. Each iteration, the loss gets smaller. The model gets better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Up\n",
    "\n",
    "Our tiny model has ~3,500 parameters. Real language models have:\n",
    "\n",
    "| Model | Parameters |\n",
    "|-------|------------|\n",
    "| GPT-2 Small | 124 million |\n",
    "| GPT-2 Large | 774 million |\n",
    "| GPT-3 | 175 billion |\n",
    "| LLaMA 2 70B | 70 billion |\n",
    "| Claude 3 | Unknown (but large) |\n",
    "\n",
    "The math is identical. Every one of those billions of parameters goes through the same AdamW update we just computed. The difference is scale, and the engineering required to parallelize the computation across thousands of GPUs.\n",
    "\n",
    "But the algorithm is exactly what we've shown: forward pass, backward pass, AdamW update. Repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We've Accomplished\n",
    "\n",
    "We calculated, by hand with explicit numbers, a complete training step through a transformer:\n",
    "\n",
    "**Forward pass:**\n",
    "- Tokenization and embedding lookup\n",
    "- Q/K/V projections for attention\n",
    "- Scaled dot-product attention with causal masking\n",
    "- Multi-head concatenation and output projection\n",
    "- Feed-forward network with GELU activation\n",
    "- Layer normalization with residual connections\n",
    "- Output projection and softmax\n",
    "- Cross-entropy loss computation\n",
    "\n",
    "**Backward pass:**\n",
    "- Gradient of loss with respect to logits\n",
    "- Backpropagation through every layer using the chain rule\n",
    "- Gradients for all ~3,500 parameters\n",
    "\n",
    "**Optimization:**\n",
    "- AdamW update with momentum, adaptive learning rates, and weight decay\n",
    "\n",
    "Nothing was hidden. No mystery. Just math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Thoughts\n",
    "\n",
    "You've made it through the entire pipeline.\n",
    "\n",
    "You've seen every matrix multiplication, every activation function, every gradient calculation, every weight update. You understand how a transformer processes text, why attention works, what backpropagation computes, and how optimization nudges parameters toward better predictions.\n",
    "\n",
    "When someone says \"a transformer learns by gradient descent,\" you now know **exactly** what that means, down to the individual floating-point operations.\n",
    "\n",
    "This is the difference between knowing *about* something and truly understanding it. You don't just know that transformers use attention mechanisms. You've computed the attention scores yourself. You don't just know that neural networks use backpropagation. You've traced the chain rule through every layer.\n",
    "\n",
    "The next time you use GPT, Claude, or any language model, remember: under the hood, it's doing exactly what we did here. Billions of times larger. Trillions of times repeated. But the same fundamental math.\n",
    "\n",
    "Transformers aren't mysterious. They're just math.\n",
    "\n",
    "And now you understand the math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:12.316789Z",
     "iopub.status.busy": "2025-12-10T21:17:12.316697Z",
     "iopub.status.idle": "2025-12-10T21:17:12.318400Z",
     "shell.execute_reply": "2025-12-10T21:17:12.318108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step Complete\n",
      "==================================================\n",
      "\n",
      "  Forward pass:  \u2713\n",
      "  Backward pass: \u2713\n",
      "  Optimization:  \u2713\n",
      "\n",
      "One training iteration done.\n",
      "Repeat ~billions of times for a real LLM.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Step Complete\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"  Forward pass:  \u2713\")\n",
    "print(\"  Backward pass: \u2713\")\n",
    "print(\"  Optimization:  \u2713\")\n",
    "print()\n",
    "print(\"One training iteration done.\")\n",
    "print(\"Repeat ~billions of times for a real LLM.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Updates all model parameters using AdamW optimizer with momentum, adaptive learning rates, and weight decay."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}