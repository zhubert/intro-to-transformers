{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Why Multiple Heads?\n",
    "\n",
    "We just computed attention separately for two heads. Each head has its own Q, K, V projections, so each head learns to look at the data differently.\n",
    "\n",
    "Think of it like having multiple experts examine the same text:\n",
    "- One head might specialize in syntactic relationships (subject-verb, noun-adjective)\n",
    "- Another might focus on semantic relationships (related concepts, coreference)\n",
    "- Another might track positional patterns (nearby words, document structure)\n",
    "\n",
    "In practice, what heads learn is emergent. we don't design them for specific tasks. But research has shown that different heads genuinely specialize. Some learn to track quotation marks. Others focus on rare words. Others capture long-range dependencies.\n",
    "\n",
    "The multi-head mechanism lets the model attend to information from different representation subspaces simultaneously. It's one of the key innovations that made transformers so effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Multi-Head Process\n",
    "\n",
    "We have attention outputs from 2 heads, each with shape `[5, 8]`. To get back to `d_model = 16`:\n",
    "\n",
    "1. **Concatenate** the head outputs: `[5, 8]` and `[5, 8]` → `[5, 16]`\n",
    "2. **Project** through output matrix $W_O$: `[5, 16]` @ `[16, 16]` → `[5, 16]`\n",
    "\n",
    "$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_0, \\text{head}_1) \\cdot W_O$$\n",
    "\n",
    "The concatenation is straightforward. Just stack the vectors side by side.\n",
    "\n",
    "The output projection through $W_O$ is important: it lets the model mix information across heads. Without it, the heads would remain completely independent. With it, the model can learn combinations like \"if head 0 sees X and head 1 sees Y, output Z.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.457090Z",
     "iopub.status.busy": "2025-12-10T21:17:01.457005Z",
     "iopub.status.idle": "2025-12-10T21:17:01.459103Z",
     "shell.execute_reply": "2025-12-10T21:17:01.458800Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS  # 8\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.459982Z",
     "iopub.status.busy": "2025-12-10T21:17:01.459869Z",
     "iopub.status.idle": "2025-12-10T21:17:01.463337Z",
     "shell.execute_reply": "2025-12-10T21:17:01.463064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n, p = len(A), len(A[0]), len(B[0])\n",
    "    return [[sum(A[i][k] * B[k][j] for k in range(n)) for j in range(p)] for i in range(m)]\n",
    "\n",
    "def transpose(A):\n",
    "    rows, cols = len(A), len(A[0])\n",
    "    return [[A[i][j] for i in range(rows)] for j in range(cols)]\n",
    "\n",
    "def softmax(vec):\n",
    "    finite_vals = [v for v in vec if v != float('-inf')]\n",
    "    max_val = max(finite_vals) if finite_vals else 0\n",
    "    exp_vec = [math.exp(v - max_val) if v != float('-inf') else 0.0 for v in vec]\n",
    "    total = sum(exp_vec)\n",
    "    return [e / total for e in exp_vec]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.464034Z",
     "iopub.status.busy": "2025-12-10T21:17:01.463962Z",
     "iopub.status.idle": "2025-12-10T21:17:01.467775Z",
     "shell.execute_reply": "2025-12-10T21:17:01.467468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated attention outputs from previous notebooks\n",
      "Head 0 output shape: [5, 8]\n",
      "Head 1 output shape: [5, 8]\n"
     ]
    }
   ],
   "source": [
    "# Recreate everything from previous notebooks\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "tokens = [1, 3, 4, 5, 2]\n",
    "seq_len = len(tokens)\n",
    "X = [add_vectors(E_token[tokens[i]], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "def attention(Q, K, V):\n",
    "    seq_len, d_k = len(Q), len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    scores = matmul(Q, transpose(K))\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j > i:\n",
    "                scaled[i][j] = float('-inf')\n",
    "    weights = [softmax(row) for row in scaled]\n",
    "    return matmul(weights, V), weights\n",
    "\n",
    "attention_output_all = []\n",
    "for h in range(NUM_HEADS):\n",
    "    output, _ = attention(Q_all[h], K_all[h], V_all[h])\n",
    "    attention_output_all.append(output)\n",
    "\n",
    "print(f\"Recreated attention outputs from previous notebooks\")\n",
    "print(f\"Head 0 output shape: [{len(attention_output_all[0])}, {len(attention_output_all[0][0])}]\")\n",
    "print(f\"Head 1 output shape: [{len(attention_output_all[1])}, {len(attention_output_all[1][0])}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 1: Concatenate Head Outputs\n",
    "\n",
    "Each head produced an output of shape `[5, 8]`. We concatenate them along the last dimension:\n",
    "\n",
    "$$\\text{concat}[i] = [\\text{head}_0[i] \\;\\|\\; \\text{head}_1[i]]$$\n",
    "\n",
    "For each position $i$, we take the 8-dimensional vector from head 0 and the 8-dimensional vector from head 1, and stick them together to get a 16-dimensional vector.\n",
    "\n",
    "Result shape: `[5, 16]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.468486Z",
     "iopub.status.busy": "2025-12-10T21:17:01.468413Z",
     "iopub.status.idle": "2025-12-10T21:17:01.470314Z",
     "shell.execute_reply": "2025-12-10T21:17:01.470044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Attention Outputs\n",
      "Shape: [5, 16]\n",
      "\n",
      "  pos 0 (<BOS>       ): [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737,  0.0107, -0.0291, -0.0100, -0.0312,  0.0214,  0.0372,  0.0105,  0.0279]\n",
      "  pos 1 (I           ): [ 0.0683,  0.0368, -0.0263, -0.0574,  0.0152, -0.0174, -0.0084, -0.0760, -0.0199, -0.0151,  0.0026,  0.0107,  0.0091, -0.0204, -0.0320, -0.0193]\n",
      "  pos 2 (like        ): [ 0.0247,  0.0789,  0.0074, -0.0635,  0.0180, -0.0098, -0.0184, -0.0173, -0.0320, -0.0102,  0.0178, -0.0153,  0.0433,  0.0026,  0.0002, -0.0198]\n",
      "  pos 3 (transformers): [ 0.0254,  0.0511, -0.0182, -0.0322,  0.0103, -0.0126, -0.0282,  0.0018, -0.0111, -0.0085,  0.0093,  0.0101,  0.0440,  0.0237,  0.0056, -0.0311]\n",
      "  pos 4 (<EOS>       ): [ 0.0325,  0.0367, -0.0202, -0.0262,  0.0188, -0.0040, -0.0321,  0.0167, -0.0119, -0.0013, -0.0069,  0.0016,  0.0480,  0.0233,  0.0096, -0.0121]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate head outputs\n",
    "concat_output = []\n",
    "for i in range(seq_len):\n",
    "    # Concatenate head 0 and head 1 outputs for position i\n",
    "    concat_row = attention_output_all[0][i] + attention_output_all[1][i]\n",
    "    concat_output.append(concat_row)\n",
    "\n",
    "print(\"Concatenated Attention Outputs\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(concat_output):\n",
    "    print(f\"  pos {i} ({TOKEN_NAMES[tokens[i]]:12s}): {format_vector(row)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.470953Z",
     "iopub.status.busy": "2025-12-10T21:17:01.470887Z",
     "iopub.status.idle": "2025-12-10T21:17:01.472637Z",
     "shell.execute_reply": "2025-12-10T21:17:01.472400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed: Concatenation for position 0 (<BOS>)\n",
      "======================================================================\n",
      "\n",
      "Head 0 output (8 dims): [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737]\n",
      "\n",
      "Head 1 output (8 dims): [ 0.0107, -0.0291, -0.0100, -0.0312,  0.0214,  0.0372,  0.0105,  0.0279]\n",
      "\n",
      "Concatenated (16 dims): [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737,  0.0107, -0.0291, -0.0100, -0.0312,  0.0214,  0.0372,  0.0105,  0.0279]\n",
      "\n",
      "First 8 elements come from head 0, last 8 from head 1.\n"
     ]
    }
   ],
   "source": [
    "# Show the concatenation for position 0 in detail\n",
    "print(\"Detailed: Concatenation for position 0 (<BOS>)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Head 0 output (8 dims): {format_vector(attention_output_all[0][0])}\")\n",
    "print()\n",
    "print(f\"Head 1 output (8 dims): {format_vector(attention_output_all[1][0])}\")\n",
    "print()\n",
    "print(f\"Concatenated (16 dims): {format_vector(concat_output[0])}\")\n",
    "print()\n",
    "print(\"First 8 elements come from head 0, last 8 from head 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 2: Output Projection\n",
    "\n",
    "The concatenated output is `[5, 16]`. We project it through the output weight matrix $W_O$:\n",
    "\n",
    "$$\\text{multi\\_head\\_output} = \\text{concat} \\cdot W_O^T$$\n",
    "\n",
    "Where $W_O$ has shape `[d_model, d_model]` = `[16, 16]`.\n",
    "\n",
    "**Why this projection?**\n",
    "\n",
    "Without $W_O$, the heads would be completely separate. The first 8 dimensions would always come from head 0, the last 8 from head 1. There's no interaction.\n",
    "\n",
    "With $W_O$, each output dimension becomes a learned combination of *all* dimensions from *all* heads. The model can learn:\n",
    "- \"If head 0 found a verb and head 1 found the subject, strengthen the connection\"\n",
    "- \"Suppress noise when heads disagree\"\n",
    "- \"Combine syntactic and semantic signals into a unified representation\"\n",
    "\n",
    "This mixing is crucial for multi-head attention's power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.473251Z",
     "iopub.status.busy": "2025-12-10T21:17:01.473187Z",
     "iopub.status.idle": "2025-12-10T21:17:01.474781Z",
     "shell.execute_reply": "2025-12-10T21:17:01.474543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Projection Matrix W_O\n",
      "Shape: [16, 16]\n",
      "\n",
      "(16×16 = 256 learnable parameters for mixing heads)\n"
     ]
    }
   ],
   "source": [
    "# Initialize output projection matrix\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)  # [16, 16]\n",
    "\n",
    "print(f\"Output Projection Matrix W_O\")\n",
    "print(f\"Shape: [{D_MODEL}, {D_MODEL}]\")\n",
    "print()\n",
    "print(\"(16×16 = 256 learnable parameters for mixing heads)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.475364Z",
     "iopub.status.busy": "2025-12-10T21:17:01.475301Z",
     "iopub.status.idle": "2025-12-10T21:17:01.477178Z",
     "shell.execute_reply": "2025-12-10T21:17:01.476943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention Output\n",
      "Shape: [5, 16] @ [16, 16] = [5, 16]\n",
      "\n",
      "  pos 0 (<BOS>       ): [ 0.0334,  0.0033, -0.0041, -0.0073,  0.0185,  0.0074,  0.0169,  0.0107,  0.0277,  0.0060,  0.0222,  0.0241,  0.0074,  0.0067, -0.0067,  0.0063]\n",
      "  pos 1 (I           ): [ 0.0269,  0.0066,  0.0113, -0.0154,  0.0114,  0.0032, -0.0065, -0.0108,  0.0190, -0.0091,  0.0180,  0.0097, -0.0075,  0.0061, -0.0079,  0.0110]\n",
      "  pos 2 (like        ): [ 0.0085,  0.0086,  0.0159, -0.0177,  0.0026,  0.0205, -0.0057, -0.0055,  0.0059, -0.0043,  0.0007, -0.0053,  0.0075, -0.0012, -0.0043, -0.0016]\n",
      "  pos 3 (transformers): [ 0.0064, -0.0084,  0.0092, -0.0173,  0.0068,  0.0119, -0.0100, -0.0027,  0.0027, -0.0073,  0.0036, -0.0076,  0.0022, -0.0070, -0.0095, -0.0070]\n",
      "  pos 4 (<EOS>       ): [ 0.0039, -0.0068,  0.0098, -0.0136,  0.0031,  0.0090, -0.0086, -0.0027, -0.0003, -0.0044, -0.0029, -0.0062,  0.0060, -0.0048, -0.0036, -0.0115]\n"
     ]
    }
   ],
   "source": [
    "# Apply output projection: concat @ W_O^T\n",
    "W_O_T = transpose(W_O)\n",
    "multi_head_output = matmul(concat_output, W_O_T)\n",
    "\n",
    "print(\"Multi-Head Attention Output\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}] @ [{D_MODEL}, {D_MODEL}] = [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(multi_head_output):\n",
    "    print(f\"  pos {i} ({TOKEN_NAMES[tokens[i]]:12s}): {format_vector(row)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.477793Z",
     "iopub.status.busy": "2025-12-10T21:17:01.477717Z",
     "iopub.status.idle": "2025-12-10T21:17:01.479895Z",
     "shell.execute_reply": "2025-12-10T21:17:01.479667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed: Computing output[0][0]\n",
      "======================================================================\n",
      "\n",
      "output[0][0] = concat[0] · W_O[:, 0]\n",
      "\n",
      "concat[0] (16 dims): [ 0.0800,  0.0257, -0.0117, -0.1056,  0.0339, -0.0891, -0.0083, -0.0737,  0.0107, -0.0291, -0.0100, -0.0312,  0.0214,  0.0372,  0.0105,  0.0279]\n",
      "\n",
      "W_O[:, 0] (16 dims): [-0.0262, -0.0621,  0.1544, -0.1022,  0.0636,  0.1289, -0.0064, -0.1005, -0.0360, -0.0405,  0.0004,  0.1525, -0.0449, -0.0366,  0.0846,  0.0188]\n",
      "\n",
      "Dot product = -0.001460\n",
      "Actual output[0][0] = 0.033421\n"
     ]
    }
   ],
   "source": [
    "# Detailed calculation for one element\n",
    "print(\"Detailed: Computing output[0][0]\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"output[0][0] = concat[0] · W_O[:, 0]\")\n",
    "print()\n",
    "print(f\"concat[0] (16 dims): {format_vector(concat_output[0])}\")\n",
    "print()\n",
    "col_0 = [W_O[i][0] for i in range(D_MODEL)]\n",
    "print(f\"W_O[:, 0] (16 dims): {format_vector(col_0)}\")\n",
    "print()\n",
    "\n",
    "result = sum(concat_output[0][j] * col_0[j] for j in range(D_MODEL))\n",
    "print(f\"Dot product = {result:.6f}\")\n",
    "print(f\"Actual output[0][0] = {multi_head_output[0][0]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Comparing Input and Output\n",
    "\n",
    "Let's compare what went into multi-head attention (the original embeddings $X$) with what came out.\n",
    "\n",
    "The output has the same shape as the input `[5, 16]`, but each vector now incorporates information from other positions via attention. The representation for \"like\" now contains information about \"I\" and \"\\<BOS\\>\". it's no longer just about the word \"like\" in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.480510Z",
     "iopub.status.busy": "2025-12-10T21:17:01.480442Z",
     "iopub.status.idle": "2025-12-10T21:17:01.482541Z",
     "shell.execute_reply": "2025-12-10T21:17:01.482291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: Input vs Multi-Head Attention Output\n",
      "================================================================================\n",
      "\n",
      "Position 0 (<BOS>)\n",
      "  Input X:  [ 0.1473,  0.1281,  0.1995, -0.0465,  0.2125, -0.1338, -0.0829, -0.0638,  0.0722,  0.1183,  0.1193,  0.0937, -0.1594, -0.0402,  0.1124, -0.2064]\n",
      "  Output:   [ 0.0334,  0.0033, -0.0041, -0.0073,  0.0185,  0.0074,  0.0169,  0.0107,  0.0277,  0.0060,  0.0222,  0.0241,  0.0074,  0.0067, -0.0067,  0.0063]\n",
      "  Magnitude: 0.5278 → 0.0637\n",
      "\n",
      "Position 1 (I)\n",
      "  Input X:  [-0.1254, -0.0720,  0.1255, -0.0556, -0.0678,  0.3698, -0.1265, -0.1463,  0.0866,  0.0181,  0.0726, -0.0374,  0.2312, -0.0091,  0.0860, -0.0251]\n",
      "  Output:   [ 0.0269,  0.0066,  0.0113, -0.0154,  0.0114,  0.0032, -0.0065, -0.0108,  0.0190, -0.0091,  0.0180,  0.0097, -0.0075,  0.0061, -0.0079,  0.0110]\n",
      "  Magnitude: 0.5427 → 0.0507\n",
      "\n",
      "Position 2 (like)\n",
      "  Input X:  [ 0.2319, -0.2747, -0.0089,  0.0576,  0.1430, -0.0957,  0.1571,  0.2913,  0.2154,  0.0103, -0.0510, -0.1353, -0.0296, -0.0371, -0.0262,  0.2770]\n",
      "  Output:   [ 0.0085,  0.0086,  0.0159, -0.0177,  0.0026,  0.0205, -0.0057, -0.0055,  0.0059, -0.0043,  0.0007, -0.0053,  0.0075, -0.0012, -0.0043, -0.0016]\n",
      "  Magnitude: 0.6472 → 0.0369\n",
      "\n",
      "Position 3 (transformers)\n",
      "  Input X:  [-0.1334,  0.0027, -0.3410, -0.1478, -0.0307,  0.1240,  0.2642, -0.0063, -0.0856,  0.0626,  0.1602,  0.1385, -0.0427,  0.0122,  0.0991,  0.1081]\n",
      "  Output:   [ 0.0064, -0.0084,  0.0092, -0.0173,  0.0068,  0.0119, -0.0100, -0.0027,  0.0027, -0.0073,  0.0036, -0.0076,  0.0022, -0.0070, -0.0095, -0.0070]\n",
      "  Magnitude: 0.5671 → 0.0334\n",
      "\n",
      "Position 4 (<EOS>)\n",
      "  Input X:  [-0.1346,  0.0002, -0.0629,  0.3029,  0.0908, -0.1515,  0.0959,  0.0481,  0.0032,  0.0225,  0.1310,  0.0306, -0.1088,  0.0649,  0.0880, -0.0130]\n",
      "  Output:   [ 0.0039, -0.0068,  0.0098, -0.0136,  0.0031,  0.0090, -0.0086, -0.0027, -0.0003, -0.0044, -0.0029, -0.0062,  0.0060, -0.0048, -0.0036, -0.0115]\n",
      "  Magnitude: 0.4462 → 0.0280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparison: Input vs Multi-Head Attention Output\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "for i in range(seq_len):\n",
    "    print(f\"Position {i} ({TOKEN_NAMES[tokens[i]]})\")\n",
    "    print(f\"  Input X:  {format_vector(X[i])}\")\n",
    "    print(f\"  Output:   {format_vector(multi_head_output[i])}\")\n",
    "    \n",
    "    # Compute magnitude change\n",
    "    mag_in = math.sqrt(sum(x**2 for x in X[i]))\n",
    "    mag_out = math.sqrt(sum(x**2 for x in multi_head_output[i]))\n",
    "    print(f\"  Magnitude: {mag_in:.4f} → {mag_out:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Parameter Count\n",
    "\n",
    "Multi-head attention has a lot of parameters:\n",
    "\n",
    "| Component | Shape | Count |\n",
    "|-----------|-------|-------|\n",
    "| $W_Q$ (per head) | [16, 8] × 2 heads | 256 |\n",
    "| $W_K$ (per head) | [16, 8] × 2 heads | 256 |\n",
    "| $W_V$ (per head) | [16, 8] × 2 heads | 256 |\n",
    "| $W_O$ | [16, 16] | 256 |\n",
    "| **Total** | | **1,024** |\n",
    "\n",
    "That's 1,024 parameters just for attention in one transformer block. In GPT-3 with d_model=12,288 and 96 heads, the attention parameters per layer are about 150 million."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## What Multi-Head Attention Accomplished\n",
    "\n",
    "We started with embeddings where each position was independent. After multi-head attention:\n",
    "\n",
    "- Each position's representation incorporates information from previous positions\n",
    "- Two different \"perspectives\" (heads) contributed to this mixing\n",
    "- The output projection combined these perspectives into a unified representation\n",
    "\n",
    "The model hasn't changed the number of positions. we still have 5 vectors of dimension 16. But the *content* of those vectors is now context-dependent. This is the fundamental innovation of attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "Multi-head attention is done. But we're not finished with the transformer block.\n",
    "\n",
    "Next comes the **feed-forward network (FFN)**. A simple two-layer neural network applied to each position independently. Where attention lets positions *communicate*, the FFN lets each position *process* the information it's gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:01.483205Z",
     "iopub.status.busy": "2025-12-10T21:17:01.483134Z",
     "iopub.status.idle": "2025-12-10T21:17:01.484607Z",
     "shell.execute_reply": "2025-12-10T21:17:01.484371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention complete. Ready for feed-forward network.\n"
     ]
    }
   ],
   "source": [
    "# Store for next notebook\n",
    "multi_head_data = {\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'multi_head_output': multi_head_output,\n",
    "    'W_O': W_O,\n",
    "    'concat_output': concat_output\n",
    "}\n",
    "print(\"Multi-head attention complete. Ready for feed-forward network.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Concatenates outputs from multiple attention heads and projects back to model dimension with explicit calculations."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
