{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multi-Head Attention\n",
    "\n",
    "**Combining multiple attention heads into a unified representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. We've got attention outputs from both heads. Now what?\n",
    "\n",
    "Time to combine them.\n",
    "\n",
    "This is the \"multi-head\" part of multi-head attention (shocking, I know). Each head's been looking at the sequence through its own lens, learning different patterns and relationships. Now we need to merge these perspectives into a single, unified representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Multiple Heads?\n",
    "\n",
    "Think of it like having multiple experts examine the same data. Each one notices different things.\n",
    "\n",
    "In a trained model, different heads genuinely specialize:\n",
    "- **Head 0** might focus on local patterns (adjacent words, nearby relationships)\n",
    "- **Head 1** might capture long-range dependencies (distant relationships, document structure)\n",
    "\n",
    "Some might learn syntactic patterns—subject-verb agreement, grammatical structure. Others capture semantic relationships—what concepts are related, what words mean together.\n",
    "\n",
    "Our model isn't trained yet (obviously), so the heads haven't learned these specializations. But the architecture is ready for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "\n",
    "The process is pretty straightforward:\n",
    "\n",
    "1. **Concatenate** the outputs from all heads\n",
    "2. **Project** the concatenated result through a learned linear transformation\n",
    "\n",
    "That's it. Two steps.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_0, \\text{head}_1) W_O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS  # 8\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (same as previous notebooks)\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n = len(A), len(A[0])\n",
    "    p = len(B[0])\n",
    "    return [[sum(A[i][k] * B[k][j] for k in range(n)) for j in range(p)] for i in range(m)]\n",
    "\n",
    "def transpose(A):\n",
    "    return [[A[i][j] for i in range(len(A))] for j in range(len(A[0]))]\n",
    "\n",
    "def softmax(vec):\n",
    "    max_val = max(v for v in vec if v != float('-inf'))\n",
    "    exp_vec = [math.exp(v - max_val) if v != float('-inf') else 0 for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate everything from previous notebooks\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "tokens = [1, 3, 4, 5, 2]\n",
    "seq_len = len(tokens)\n",
    "X = [add_vectors(E_token[tokens[i]], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "# QKV weights and projections\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "# Compute attention for each head\n",
    "def compute_attention(Q, K, V):\n",
    "    seq_len, d_k = len(Q), len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    scores = matmul(Q, transpose(K))\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j > i:\n",
    "                scaled[i][j] = float('-inf')\n",
    "    weights = [softmax(row) for row in scaled]\n",
    "    return matmul(weights, V)\n",
    "\n",
    "attention_output_all = [compute_attention(Q_all[h], K_all[h], V_all[h]) for h in range(NUM_HEADS)]\n",
    "print(\"Recreated attention outputs from previous notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Concatenate Head Outputs\n",
    "\n",
    "Each head produced an output of shape $[5, 8]$ (5 tokens, 8 dimensions per head). We concatenate along the feature dimension to get $[5, 16]$.\n",
    "\n",
    "We literally just stick the vectors together, end to end. Head 0's 8 dimensions followed by Head 1's 8 dimensions = 16 dimensions total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate head outputs\n",
    "concat_output = []\n",
    "for i in range(seq_len):\n",
    "    # Concatenate head 0 output with head 1 output for each position\n",
    "    concat_output.append(attention_output_all[0][i] + attention_output_all[1][i])\n",
    "\n",
    "print(\"Concatenated Output\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(concat_output):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the concatenation for position 0\n",
    "print(\"Example: Position 0 (<BOS>)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Head 0 output: {format_vector(attention_output_all[0][0])}\")\n",
    "print(f\"Head 1 output: {format_vector(attention_output_all[1][0])}\")\n",
    "print()\n",
    "print(f\"Concatenated:  {format_vector(concat_output[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Output Projection\n",
    "\n",
    "Now we've got 16-dimensional vectors, and we need to project them using a learned weight matrix $W_O$.\n",
    "\n",
    "**Wait—why project if we're already at the right dimension?**\n",
    "\n",
    "Even though the dimensions match, the projection serves a critical purpose: it lets the model learn how to **mix information** from different heads.\n",
    "\n",
    "Without this projection, Head 0 and Head 1 would operate completely independently. The projection matrix $W_O$ allows the model to learn optimal combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize output projection matrix\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)  # [16, 16]\n",
    "\n",
    "print(f\"Output Projection Matrix W_O\")\n",
    "print(f\"Shape: [{D_MODEL}, {D_MODEL}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply output projection: output = concat @ W_O^T\n",
    "W_O_T = transpose(W_O)\n",
    "multi_head_output = matmul(concat_output, W_O_T)\n",
    "\n",
    "print(\"Multi-Head Attention Output\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(multi_head_output):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Have We Accomplished?\n",
    "\n",
    "Starting from the original embeddings, we've now:\n",
    "\n",
    "1. **Projected** into queries, keys, and values for each head\n",
    "2. **Computed attention** in each head independently\n",
    "3. **Combined** the heads through concatenation and projection\n",
    "\n",
    "Each token's representation now contains:\n",
    "- Information from other tokens it attended to\n",
    "- Patterns detected by multiple attention heads\n",
    "- A richer, more context-aware representation than the original embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before and after for position 1\n",
    "print(\"Position 1 ('I') - Before and After Attention\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Original embedding X[1]:\")\n",
    "print(f\"  {format_vector(X[1])}\")\n",
    "print()\n",
    "print(f\"After multi-head attention:\")\n",
    "print(f\"  {format_vector(multi_head_output[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions At Each Stage\n",
    "\n",
    "| Stage | Shape | Description |\n",
    "|-------|-------|-------------|\n",
    "| Input $X$ | $[5, 16]$ | Original embeddings |\n",
    "| After Q/K/V projection (per head) | $[5, 8]$ | Each head projects to smaller dimension |\n",
    "| Attention weights (per head) | $[5, 5]$ | How much each position attends to others |\n",
    "| Attention output (per head) | $[5, 8]$ | Weighted sum of values |\n",
    "| After concatenation | $[5, 16]$ | Heads combined side-by-side |\n",
    "| After output projection | $[5, 16]$ | Final multi-head attention output |\n",
    "\n",
    "Notice we start at $[5, 16]$ and end at $[5, 16]$. The attention mechanism is a transformation that preserves the shape while enriching the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "Multi-head attention is done! But we're not finished with the transformer block yet:\n",
    "\n",
    "1. **Feed-forward network** — Apply position-wise non-linear transformations\n",
    "2. **Residual connections** — Add the original input back to prevent information loss\n",
    "3. **Layer normalization** — Stabilize the activations\n",
    "\n",
    "Then we'll project to vocabulary and compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store for next notebook\n",
    "multi_head_data = {\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'multi_head_output': multi_head_output,\n",
    "    'W_O': W_O\n",
    "}\n",
    "print(\"Multi-head data stored for next notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
