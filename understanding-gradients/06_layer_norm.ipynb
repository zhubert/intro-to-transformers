{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Two Problems to Solve\n",
    "\n",
    "We've computed the FFN output. But we have two problems:\n",
    "\n",
    "**Problem 1: Information loss**\n",
    "\n",
    "We *replaced* the attention output with the FFN output. All that careful work computing Q, K, V, attention scores.. gone. The FFN output doesn't preserve the original information.\n",
    "\n",
    "**Problem 2: Unstable activations**\n",
    "\n",
    "Deep neural networks have a tendency for activations to drift. Some dimensions grow huge, others shrink to near-zero. As you stack more layers, small imbalances compound. Gradients explode or vanish. Training becomes a nightmare.\n",
    "\n",
    "Two techniques solve these problems: **residual connections** and **layer normalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Residual Connections\n",
    "\n",
    "The fix for information loss is simple: **add** instead of replace.\n",
    "\n",
    "$$\\text{output} = \\text{input} + \\text{FFN}(\\text{input})$$\n",
    "\n",
    "This is a **residual connection** (or skip connection). Instead of learning the full transformation, the FFN only needs to learn the *change* (the residual).\n",
    "\n",
    "Benefits:\n",
    "- **No information loss**: The original input is preserved\n",
    "- **Easier learning**: Learning deltas is easier than learning full transformations\n",
    "- **Better gradients**: During backprop, gradients can flow directly through the addition\n",
    "\n",
    "Residual connections were the key innovation that made very deep networks trainable (ResNet, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Even with residual connections, activations can drift. Layer normalization fixes this by normalizing each position's vector to have:\n",
    "- Mean = 0\n",
    "- Variance = 1\n",
    "\n",
    "The formula:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean of $x$ across dimensions\n",
    "- $\\sigma^2$ = variance of $x$ across dimensions\n",
    "- $\\epsilon$ = small constant for numerical stability (prevents division by zero)\n",
    "- $\\gamma$ = learnable scale parameter (initialized to 1)\n",
    "- $\\beta$ = learnable shift parameter (initialized to 0)\n",
    "- $\\odot$ = element-wise multiplication\n",
    "\n",
    "**Why learnable $\\gamma$ and $\\beta$?**\n",
    "\n",
    "If we only normalized, we'd force every position to have exactly mean 0 and variance 1. That's too restrictive. sometimes non-zero means are useful!\n",
    "\n",
    "The learnable parameters let the model *choose* to denormalize if it helps. At initialization ($\\gamma=1$, $\\beta=0$), it's standard normalization. During training, the model learns the optimal scale and shift for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.037090Z",
     "iopub.status.busy": "2025-12-10T21:17:05.037013Z",
     "iopub.status.idle": "2025-12-10T21:17:05.039063Z",
     "shell.execute_reply": "2025-12-10T21:17:05.038780Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "D_FF = 64\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS\n",
    "EPSILON = 1e-5  # Small constant to prevent division by zero\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.039858Z",
     "iopub.status.busy": "2025-12-10T21:17:05.039792Z",
     "iopub.status.idle": "2025-12-10T21:17:05.043099Z",
     "shell.execute_reply": "2025-12-10T21:17:05.042831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n, p = len(A), len(A[0]), len(B[0])\n",
    "    return [[sum(A[i][k] * B[k][j] for k in range(n)) for j in range(p)] for i in range(m)]\n",
    "\n",
    "def transpose(A):\n",
    "    return [[A[i][j] for i in range(len(A))] for j in range(len(A[0]))]\n",
    "\n",
    "def softmax(vec):\n",
    "    max_val = max(v for v in vec if v != float('-inf'))\n",
    "    exp_vec = [math.exp(v - max_val) if v != float('-inf') else 0 for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.043870Z",
     "iopub.status.busy": "2025-12-10T21:17:05.043803Z",
     "iopub.status.idle": "2025-12-10T21:17:05.049315Z",
     "shell.execute_reply": "2025-12-10T21:17:05.049074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated multi-head attention and FFN outputs\n"
     ]
    }
   ],
   "source": [
    "# Recreate everything from previous notebooks\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "tokens = [1, 3, 4, 5, 2]\n",
    "seq_len = len(tokens)\n",
    "X = [add_vectors(E_token[tokens[i]], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "# Attention\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "def compute_attention(Q, K, V):\n",
    "    seq_len, d_k = len(Q), len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    scores = matmul(Q, transpose(K))\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j > i:\n",
    "                scaled[i][j] = float('-inf')\n",
    "    weights = [softmax(row) for row in scaled]\n",
    "    return matmul(weights, V)\n",
    "\n",
    "attention_output_all = [compute_attention(Q_all[h], K_all[h], V_all[h]) for h in range(NUM_HEADS)]\n",
    "concat_output = [attention_output_all[0][i] + attention_output_all[1][i] for i in range(seq_len)]\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)\n",
    "multi_head_output = matmul(concat_output, transpose(W_O))\n",
    "\n",
    "# FFN\n",
    "W1 = random_matrix(D_FF, D_MODEL)\n",
    "b1 = random_vector(D_FF)\n",
    "W2 = random_matrix(D_MODEL, D_FF)\n",
    "b2 = random_vector(D_MODEL)\n",
    "hidden = matmul(multi_head_output, transpose(W1))\n",
    "hidden = [[hidden[i][j] + b1[j] for j in range(D_FF)] for i in range(seq_len)]\n",
    "activated = [[gelu(h) for h in row] for row in hidden]\n",
    "ffn_output = matmul(activated, transpose(W2))\n",
    "ffn_output = [[ffn_output[i][j] + b2[j] for j in range(D_MODEL)] for i in range(seq_len)]\n",
    "\n",
    "print(\"Recreated multi-head attention and FFN outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 1: Residual Connection\n",
    "\n",
    "Add the attention output (input to FFN) to the FFN output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.050068Z",
     "iopub.status.busy": "2025-12-10T21:17:05.050001Z",
     "iopub.status.idle": "2025-12-10T21:17:05.051843Z",
     "shell.execute_reply": "2025-12-10T21:17:05.051591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Connection: input + FFN(input)\n",
      "======================================================================\n",
      "\n",
      "Example for position 0 (<BOS>):\n",
      "  Attention output: [ 0.0334,  0.0033, -0.0041, -0.0073,  0.0185,  0.0074,  0.0169,  0.0107,  0.0277,  0.0060,  0.0222,  0.0241,  0.0074,  0.0067, -0.0067,  0.0063]\n",
      "  + FFN output:     [ 0.0043, -0.0896,  0.0020,  0.2294,  0.1020,  0.0966, -0.2073,  0.0574,  0.1951,  0.0692, -0.0388, -0.0762,  0.1390, -0.0384,  0.1633,  0.0529]\n",
      "  = Residual:       [ 0.0377, -0.0863, -0.0020,  0.2221,  0.1205,  0.1041, -0.1904,  0.0681,  0.2228,  0.0752, -0.0165, -0.0520,  0.1464, -0.0317,  0.1566,  0.0591]\n"
     ]
    }
   ],
   "source": [
    "# Compute residual: attention_output + FFN(attention_output)\n",
    "residual = [add_vectors(multi_head_output[i], ffn_output[i]) for i in range(seq_len)]\n",
    "\n",
    "print(\"Residual Connection: input + FFN(input)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Example for position 0 (<BOS>):\")\n",
    "print(f\"  Attention output: {format_vector(multi_head_output[0])}\")\n",
    "print(f\"  + FFN output:     {format_vector(ffn_output[0])}\")\n",
    "print(f\"  = Residual:       {format_vector(residual[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 2: Layer Normalization\n",
    "\n",
    "Now we normalize the residual. For each position:\n",
    "\n",
    "**Compute mean:**\n",
    "$$\\mu = \\frac{1}{d_{model}} \\sum_{i=0}^{15} x_i$$\n",
    "\n",
    "**Compute variance:**\n",
    "$$\\sigma^2 = \\frac{1}{d_{model}} \\sum_{i=0}^{15} (x_i - \\mu)^2$$\n",
    "\n",
    "**Normalize:**\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "**Scale and shift:**\n",
    "$$y_i = \\gamma_i \\cdot \\hat{x}_i + \\beta_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.052565Z",
     "iopub.status.busy": "2025-12-10T21:17:05.052498Z",
     "iopub.status.idle": "2025-12-10T21:17:05.054730Z",
     "shell.execute_reply": "2025-12-10T21:17:05.054460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer norm parameters:\n",
      "  gamma (scale): initialized to 1.0 for all 16 dimensions\n",
      "  beta (shift):  initialized to 0.0 for all 16 dimensions\n",
      "\n",
      "With these initial values, LayerNorm just normalizes (no scaling or shifting).\n"
     ]
    }
   ],
   "source": [
    "def layer_norm(x, gamma, beta, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Apply layer normalization to a single vector.\n",
    "    \n",
    "    Args:\n",
    "        x: Input vector [d_model]\n",
    "        gamma: Scale parameters [d_model]\n",
    "        beta: Shift parameters [d_model]\n",
    "        epsilon: Small constant for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Normalized vector, mean, variance\n",
    "    \"\"\"\n",
    "    # Compute mean across dimensions\n",
    "    mean = sum(x) / len(x)\n",
    "    \n",
    "    # Compute variance across dimensions\n",
    "    variance = sum((xi - mean)**2 for xi in x) / len(x)\n",
    "    \n",
    "    # Normalize (with epsilon for numerical stability)\n",
    "    std = math.sqrt(variance + epsilon)\n",
    "    x_norm = [(xi - mean) / std for xi in x]\n",
    "    \n",
    "    # Scale and shift\n",
    "    output = [gamma[i] * x_norm[i] + beta[i] for i in range(len(x))]\n",
    "    \n",
    "    return output, mean, variance\n",
    "\n",
    "# Initialize gamma and beta (learnable parameters)\n",
    "gamma = [1.0] * D_MODEL  # Scale, initialized to 1\n",
    "beta = [0.0] * D_MODEL   # Shift, initialized to 0\n",
    "\n",
    "print(f\"Layer norm parameters:\")\n",
    "print(f\"  gamma (scale): initialized to 1.0 for all {D_MODEL} dimensions\")\n",
    "print(f\"  beta (shift):  initialized to 0.0 for all {D_MODEL} dimensions\")\n",
    "print()\n",
    "print(f\"With these initial values, LayerNorm just normalizes (no scaling or shifting).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.055390Z",
     "iopub.status.busy": "2025-12-10T21:17:05.055322Z",
     "iopub.status.idle": "2025-12-10T21:17:05.057674Z",
     "shell.execute_reply": "2025-12-10T21:17:05.057457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed: Layer Norm for position 0 (<BOS>)\n",
      "============================================================\n",
      "\n",
      "Input: [ 0.0377, -0.0863, -0.0020,  0.2221,  0.1205,  0.1041, -0.1904,  0.0681,  0.2228,  0.0752, -0.0165, -0.0520,  0.1464, -0.0317,  0.1566,  0.0591]\n",
      "\n",
      "Step 1: Compute mean\n",
      "  μ = sum(x) / 16 = 0.052101\n",
      "\n",
      "Step 2: Compute variance\n",
      "  σ² = sum((x - μ)²) / 16 = 0.011860\n",
      "\n",
      "Step 3: Standard deviation (with epsilon for stability)\n",
      "  σ = sqrt(0.011860 + 1e-05) = 0.108949\n",
      "\n",
      "Step 4: Normalize each element\n",
      "  x̂[0] = (0.0377 - 0.0521) / 0.1089 = -0.1319\n",
      "  x̂[1] = (-0.0863 - 0.0521) / 0.1089 = -1.2702\n",
      "  x̂[2] = (-0.0020 - 0.0521) / 0.1089 = -0.4969\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "# Detailed calculation for position 0\n",
    "x = residual[0]\n",
    "mean = sum(x) / D_MODEL\n",
    "variance = sum((xi - mean)**2 for xi in x) / D_MODEL\n",
    "std = math.sqrt(variance + EPSILON)\n",
    "\n",
    "print(\"Detailed: Layer Norm for position 0 (<BOS>)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Input: {format_vector(x)}\")\n",
    "print()\n",
    "print(f\"Step 1: Compute mean\")\n",
    "print(f\"  μ = sum(x) / {D_MODEL} = {mean:.6f}\")\n",
    "print()\n",
    "print(f\"Step 2: Compute variance\")\n",
    "print(f\"  σ² = sum((x - μ)²) / {D_MODEL} = {variance:.6f}\")\n",
    "print()\n",
    "print(f\"Step 3: Standard deviation (with epsilon for stability)\")\n",
    "print(f\"  σ = sqrt({variance:.6f} + {EPSILON}) = {std:.6f}\")\n",
    "print()\n",
    "print(f\"Step 4: Normalize each element\")\n",
    "for i in range(3):\n",
    "    norm_val = (x[i] - mean) / std\n",
    "    print(f\"  x̂[{i}] = ({x[i]:.4f} - {mean:.4f}) / {std:.4f} = {norm_val:.4f}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.058392Z",
     "iopub.status.busy": "2025-12-10T21:17:05.058328Z",
     "iopub.status.idle": "2025-12-10T21:17:05.060133Z",
     "shell.execute_reply": "2025-12-10T21:17:05.059904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Norm Output\n",
      "Shape: [5, 16]\n",
      "\n",
      "  [-0.1319, -1.2702, -0.4969,  1.5600,  0.6278,  0.4769, -2.2261,  0.1465,  1.5670,  0.2124, -0.6301, -0.9558,  0.8655, -0.7691,  0.9595,  0.0645]  # <BOS>\n",
      "  [-0.1476, -1.1384, -0.3134,  1.5437,  0.5944,  0.5080, -2.3531,  0.0272,  1.5526,  0.1223, -0.6328, -1.0414,  0.7749, -0.6856,  1.0133,  0.1760]  # I\n",
      "  [-0.2966, -1.0966, -0.2274,  1.5115,  0.5190,  0.6730, -2.2896,  0.0987,  1.4495,  0.2025, -0.7481, -1.1420,  0.8840, -0.6908,  1.0545,  0.0984]  # like\n",
      "  [-0.2772, -1.1953, -0.2618,  1.5366,  0.5803,  0.6281, -2.2785,  0.1627,  1.4394,  0.1992, -0.6806, -1.1222,  0.8545, -0.6914,  1.0266,  0.0796]  # transformers\n",
      "  [-0.3033, -1.1778, -0.2499,  1.5637,  0.5494,  0.5963, -2.2625,  0.1577,  1.4126,  0.2331, -0.7348, -1.1068,  0.8813, -0.6716,  1.0751,  0.0376]  # <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Apply layer norm to all positions\n",
    "layer_norm_output = []\n",
    "\n",
    "for i in range(seq_len):\n",
    "    output, _, _ = layer_norm(residual[i], gamma, beta, EPSILON)\n",
    "    layer_norm_output.append(output)\n",
    "\n",
    "print(\"Layer Norm Output\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(layer_norm_output):\n",
    "    print(f\"  {format_vector(row)}  # {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Verification: Did It Work?\n",
    "\n",
    "Layer norm should give us mean ≈ 0 and variance ≈ 1 for each position. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.060847Z",
     "iopub.status.busy": "2025-12-10T21:17:05.060784Z",
     "iopub.status.idle": "2025-12-10T21:17:05.062706Z",
     "shell.execute_reply": "2025-12-10T21:17:05.062461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Statistics after LayerNorm\n",
      "============================================================\n",
      "\n",
      "Position 0 (<BOS>       ): mean =  -0.000000, variance = 0.999158\n",
      "Position 1 (I           ): mean =  -0.000000, variance = 0.999175\n",
      "Position 2 (like        ): mean =  -0.000000, variance = 0.999211\n",
      "Position 3 (transformers): mean =  -0.000000, variance = 0.999223\n",
      "Position 4 (<EOS>       ): mean =   0.000000, variance = 0.999231\n",
      "\n",
      "Mean ≈ 0, variance ≈ 1 for all positions. Layer norm worked!\n",
      "\n",
      "(Variance is 0.999 instead of 1.0 because we divide by n, not n-1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Verification: Statistics after LayerNorm\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "for i in range(seq_len):\n",
    "    x = layer_norm_output[i]\n",
    "    mean = sum(x) / len(x)\n",
    "    var = sum((xi - mean)**2 for xi in x) / len(x)\n",
    "    print(f\"Position {i} ({TOKEN_NAMES[tokens[i]]:12s}): mean = {mean:10.6f}, variance = {var:.6f}\")\n",
    "\n",
    "print()\n",
    "print(\"Mean ≈ 0, variance ≈ 1 for all positions. Layer norm worked!\")\n",
    "print()\n",
    "print(\"(Variance is 0.999 instead of 1.0 because we divide by n, not n-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Why Epsilon Matters\n",
    "\n",
    "That tiny $\\epsilon = 10^{-5}$ prevents disaster.\n",
    "\n",
    "If variance happened to be exactly zero (all values identical), we'd divide by zero. Even if variance is just very small, division by a tiny number creates huge values and numerical instability.\n",
    "\n",
    "Adding epsilon ensures we never divide by anything smaller than $\\sqrt{10^{-5}} \\approx 0.003$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Before and After\n",
    "\n",
    "Let's see how the representation changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.063325Z",
     "iopub.status.busy": "2025-12-10T21:17:05.063260Z",
     "iopub.status.idle": "2025-12-10T21:17:05.064971Z",
     "shell.execute_reply": "2025-12-10T21:17:05.064728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 1 ('I') - Before and After LayerNorm\n",
      "======================================================================\n",
      "\n",
      "Before (residual):\n",
      "  [ 0.0281, -0.0810,  0.0098,  0.2144,  0.1098,  0.1003, -0.2148,  0.0474,  0.2153,  0.0578, -0.0253, -0.0703,  0.1297, -0.0311,  0.1560,  0.0637]\n",
      "\n",
      "After (layer norm):\n",
      "  [-0.1476, -1.1384, -0.3134,  1.5437,  0.5944,  0.5080, -2.3531,  0.0272,  1.5526,  0.1223, -0.6328, -1.0414,  0.7749, -0.6856,  1.0133,  0.1760]\n",
      "\n",
      "The values are scaled and shifted, but relative relationships preserved.\n",
      "Large values stay large, small values stay small.\n"
     ]
    }
   ],
   "source": [
    "print(\"Position 1 ('I') - Before and After LayerNorm\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Before (residual):\")\n",
    "print(f\"  {format_vector(residual[1])}\")\n",
    "print()\n",
    "print(f\"After (layer norm):\")\n",
    "print(f\"  {format_vector(layer_norm_output[1])}\")\n",
    "print()\n",
    "print(\"The values are scaled and shifted, but relative relationships preserved.\")\n",
    "print(\"Large values stay large, small values stay small.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## The Complete Transformer Block\n",
    "\n",
    "We just finished one complete transformer block! We computed:\n",
    "\n",
    "```\n",
    "Input X [5, 16]\n",
    "    ↓\n",
    "Multi-Head Attention\n",
    "    ↓\n",
    "+ Residual Connection (add X back)\n",
    "    ↓\n",
    "Layer Norm\n",
    "    ↓\n",
    "Feed-Forward Network\n",
    "    ↓\n",
    "+ Residual Connection (add previous output back)\n",
    "    ↓\n",
    "Layer Norm\n",
    "    ↓\n",
    "Output [5, 16]\n",
    "```\n",
    "\n",
    "(Note: We simplified by doing attention → FFN → residual → layer norm. Real transformers often interleave differently, but the concepts are the same.)\n",
    "\n",
    "In GPT-3, this block is repeated 96 times. Each block refines the representation further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "The transformer block is done! We have 16-dimensional vectors for each position.\n",
    "\n",
    "But these aren't predictions yet. We need to:\n",
    "1. **Project to vocabulary space**: Convert 16D → 6D (one score per token in our vocabulary)\n",
    "2. **Apply softmax**: Convert scores to probabilities\n",
    "3. **Compute loss**: Measure how wrong our predictions are\n",
    "\n",
    "That's the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:17:05.065585Z",
     "iopub.status.busy": "2025-12-10T21:17:05.065523Z",
     "iopub.status.idle": "2025-12-10T21:17:05.066996Z",
     "shell.execute_reply": "2025-12-10T21:17:05.066744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer block complete. Ready for output projection and loss.\n"
     ]
    }
   ],
   "source": [
    "# Store for next notebook\n",
    "layer_norm_data = {\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'layer_norm_output': layer_norm_output,\n",
    "    'gamma': gamma,\n",
    "    'beta': beta\n",
    "}\n",
    "print(\"Transformer block complete. Ready for output projection and loss.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Normalizes activations to zero mean and unit variance, applies learned scale and shift, includes residual connections."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
