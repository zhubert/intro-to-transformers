{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Layer Normalization\n",
    "\n",
    "**Stabilizing activations and adding residual connections**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. We've got our feed-forward network output. Are we done?\n",
    "\n",
    "Not quite.\n",
    "\n",
    "See, we just **replaced** the attention output with the FFN output. That means we lost all the information from attention. All that careful work computing Q, K, V, attention scores, multi-head combinations... gone.\n",
    "\n",
    "That's not ideal.\n",
    "\n",
    "Plus, deep neural networks have this annoying tendency where activations can grow or shrink out of control as you stack more layers. Small errors compound, gradients explode or vanish, and training becomes a nightmare.\n",
    "\n",
    "Two techniques solve these problems: **residual connections** and **layer normalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Residual Connections\n",
    "\n",
    "The fix is beautifully simple: **add** the FFN output to the attention output instead of replacing it.\n",
    "\n",
    "$$\\text{residual} = \\text{attention\\_output} + \\text{FFN}(\\text{attention\\_output})$$\n",
    "\n",
    "This is called a **residual connection** (or skip connection). The idea: Let the FFN learn the **change** to make, not the entire new representation.\n",
    "\n",
    "Benefits:\n",
    "- **No information loss** — original attention output is preserved\n",
    "- **Easier learning** — the FFN only needs to learn deltas\n",
    "- **Better gradients** — during backprop, gradients can flow directly through the residual path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Layer Normalization\n",
    "\n",
    "Even with residual connections, activations can drift over time. One dimension might grow huge, another shrink to near-zero.\n",
    "\n",
    "**Layer normalization** solves this by normalizing each position's activations to have:\n",
    "- Mean = 0\n",
    "- Variance = 1\n",
    "\n",
    "Then it applies learned scale ($\\gamma$) and shift ($\\beta$) parameters to restore expressiveness.\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 6\n",
    "D_MODEL = 16\n",
    "D_FF = 64\n",
    "MAX_SEQ_LEN = 5\n",
    "NUM_HEADS = 2\n",
    "D_K = D_MODEL // NUM_HEADS\n",
    "EPSILON = 1e-5  # For numerical stability\n",
    "\n",
    "TOKEN_NAMES = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"I\", \"like\", \"transformers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (same as before)\n",
    "def random_vector(size, scale=0.1):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def random_matrix(rows, cols, scale=0.1):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def add_vectors(v1, v2):\n",
    "    return [a + b for a, b in zip(v1, v2)]\n",
    "\n",
    "def matmul(A, B):\n",
    "    m, n = len(A), len(A[0])\n",
    "    p = len(B[0])\n",
    "    return [[sum(A[i][k] * B[k][j] for k in range(n)) for j in range(p)] for i in range(m)]\n",
    "\n",
    "def transpose(A):\n",
    "    return [[A[i][j] for i in range(len(A))] for j in range(len(A[0]))]\n",
    "\n",
    "def softmax(vec):\n",
    "    max_val = max(v for v in vec if v != float('-inf'))\n",
    "    exp_vec = [math.exp(v - max_val) if v != float('-inf') else 0 for v in vec]\n",
    "    sum_exp = sum(exp_vec)\n",
    "    return [e / sum_exp for e in exp_vec]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def format_vector(vec, decimals=4):\n",
    "    return \"[\" + \", \".join([f\"{v:7.{decimals}f}\" for v in vec]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate everything from previous notebooks\n",
    "E_token = [random_vector(D_MODEL) for _ in range(VOCAB_SIZE)]\n",
    "E_pos = [random_vector(D_MODEL) for _ in range(MAX_SEQ_LEN)]\n",
    "tokens = [1, 3, 4, 5, 2]\n",
    "seq_len = len(tokens)\n",
    "X = [add_vectors(E_token[tokens[i]], E_pos[i]) for i in range(seq_len)]\n",
    "\n",
    "# Attention\n",
    "W_Q = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_K = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "W_V = [random_matrix(D_MODEL, D_K) for _ in range(NUM_HEADS)]\n",
    "Q_all = [matmul(X, W_Q[h]) for h in range(NUM_HEADS)]\n",
    "K_all = [matmul(X, W_K[h]) for h in range(NUM_HEADS)]\n",
    "V_all = [matmul(X, W_V[h]) for h in range(NUM_HEADS)]\n",
    "\n",
    "def compute_attention(Q, K, V):\n",
    "    seq_len, d_k = len(Q), len(Q[0])\n",
    "    scale = math.sqrt(d_k)\n",
    "    scores = matmul(Q, transpose(K))\n",
    "    scaled = [[s / scale for s in row] for row in scores]\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j > i:\n",
    "                scaled[i][j] = float('-inf')\n",
    "    weights = [softmax(row) for row in scaled]\n",
    "    return matmul(weights, V)\n",
    "\n",
    "attention_output_all = [compute_attention(Q_all[h], K_all[h], V_all[h]) for h in range(NUM_HEADS)]\n",
    "concat_output = [attention_output_all[0][i] + attention_output_all[1][i] for i in range(seq_len)]\n",
    "W_O = random_matrix(D_MODEL, D_MODEL)\n",
    "multi_head_output = matmul(concat_output, transpose(W_O))\n",
    "\n",
    "# FFN\n",
    "W1 = random_matrix(D_FF, D_MODEL)\n",
    "b1 = random_vector(D_FF)\n",
    "W2 = random_matrix(D_MODEL, D_FF)\n",
    "b2 = random_vector(D_MODEL)\n",
    "W1_T = transpose(W1)\n",
    "hidden = matmul(multi_head_output, W1_T)\n",
    "hidden = [[hidden[i][j] + b1[j] for j in range(D_FF)] for i in range(seq_len)]\n",
    "activated = [[gelu(h) for h in row] for row in hidden]\n",
    "W2_T = transpose(W2)\n",
    "ffn_output = matmul(activated, W2_T)\n",
    "ffn_output = [[ffn_output[i][j] + b2[j] for j in range(D_MODEL)] for i in range(seq_len)]\n",
    "\n",
    "print(\"Recreated multi-head attention and FFN outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Add Residual Connection\n",
    "\n",
    "Just add the attention output and FFN output element-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residual: attention output + FFN output\n",
    "residual = [add_vectors(multi_head_output[i], ffn_output[i]) for i in range(seq_len)]\n",
    "\n",
    "print(\"Residual Connection\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Example for position 0 (<BOS>):\")\n",
    "print(f\"  Attention output: {format_vector(multi_head_output[0])}\")\n",
    "print(f\"  FFN output:       {format_vector(ffn_output[0])}\")\n",
    "print(f\"  Residual (sum):   {format_vector(residual[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute Mean and Variance\n",
    "\n",
    "For each position, compute statistics across the D_MODEL dimension:\n",
    "\n",
    "$$\\mu = \\frac{1}{d_{model}} \\sum_{i=0}^{15} x_i$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{d_{model}} \\sum_{i=0}^{15} (x_i - \\mu)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma, beta, epsilon=1e-5):\n",
    "    \"\"\"Apply layer normalization to a single vector\"\"\"\n",
    "    # Compute mean\n",
    "    mean = sum(x) / len(x)\n",
    "    \n",
    "    # Compute variance\n",
    "    variance = sum((xi - mean)**2 for xi in x) / len(x)\n",
    "    \n",
    "    # Normalize\n",
    "    std = math.sqrt(variance + epsilon)\n",
    "    x_norm = [(xi - mean) / std for xi in x]\n",
    "    \n",
    "    # Scale and shift\n",
    "    output = [gamma[i] * x_norm[i] + beta[i] for i in range(len(x))]\n",
    "    \n",
    "    return output, mean, variance\n",
    "\n",
    "# Initialize gamma and beta (learnable parameters)\n",
    "gamma = [1.0] * D_MODEL  # Scale, initialized to 1\n",
    "beta = [0.0] * D_MODEL   # Shift, initialized to 0\n",
    "\n",
    "print(f\"Layer norm parameters:\")\n",
    "print(f\"  gamma (scale): {gamma[:4]}... (all 1.0)\")\n",
    "print(f\"  beta (shift):  {beta[:4]}... (all 0.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed calculation for position 0\n",
    "x = residual[0]\n",
    "mean = sum(x) / D_MODEL\n",
    "variance = sum((xi - mean)**2 for xi in x) / D_MODEL\n",
    "std = math.sqrt(variance + EPSILON)\n",
    "\n",
    "print(\"Detailed calculation for position 0 (<BOS>)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Input (residual): {format_vector(x)}\")\n",
    "print()\n",
    "print(f\"Mean: {mean:.6f}\")\n",
    "print(f\"Variance: {variance:.6f}\")\n",
    "print(f\"Std (with epsilon): {std:.6f}\")\n",
    "print()\n",
    "\n",
    "# Normalize first few values\n",
    "print(\"Normalization examples:\")\n",
    "for i in range(3):\n",
    "    norm_val = (x[i] - mean) / std\n",
    "    print(f\"  x[{i}] = ({x[i]:.4f} - {mean:.4f}) / {std:.4f} = {norm_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply layer norm to all positions\n",
    "layer_norm_output = []\n",
    "stats = []\n",
    "\n",
    "for i in range(seq_len):\n",
    "    output, mean, var = layer_norm(residual[i], gamma, beta, EPSILON)\n",
    "    layer_norm_output.append(output)\n",
    "    stats.append((mean, var))\n",
    "\n",
    "print(\"Layer Norm Output\")\n",
    "print(f\"Shape: [{seq_len}, {D_MODEL}]\")\n",
    "print()\n",
    "for i, row in enumerate(layer_norm_output):\n",
    "    print(f\"  {format_vector(row)}  # pos {i}: {TOKEN_NAMES[tokens[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Did It Work?\n",
    "\n",
    "Let's check that layer normalization actually did what it promised. Each position should now have:\n",
    "- Mean ≈ 0\n",
    "- Variance ≈ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verification: Mean and Variance after LayerNorm\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "for i in range(seq_len):\n",
    "    x = layer_norm_output[i]\n",
    "    mean = sum(x) / len(x)\n",
    "    var = sum((xi - mean)**2 for xi in x) / len(x)\n",
    "    print(f\"Position {i} ({TOKEN_NAMES[tokens[i]]:12s}): mean = {mean:9.6f}, variance = {var:.6f}\")\n",
    "\n",
    "print()\n",
    "print(\"Mean is ~0, variance is ~1. Layer norm worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before and After\n",
    "\n",
    "Let's compare position 1 (`I`) before and after layer normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Position 1 ('I') - Before and After LayerNorm\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Before (residual):\")\n",
    "print(f\"  {format_vector(residual[1])}\")\n",
    "print()\n",
    "print(f\"After (layer norm):\")\n",
    "print(f\"  {format_vector(layer_norm_output[1])}\")\n",
    "print()\n",
    "print(\"The magnitudes changed (normalized), but relative relationships preserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Transformer Block: Complete!\n",
    "\n",
    "We just finished a complete transformer block!\n",
    "\n",
    "In a real transformer, you'd stack multiple blocks. GPT-3 has 96 of these blocks. Each block is:\n",
    "\n",
    "1. Multi-head attention\n",
    "2. Residual + Layer norm\n",
    "3. Feed-forward network\n",
    "4. Residual + Layer norm\n",
    "\n",
    "We're only using **one block** to keep things manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "The transformer block is done. Now we need to convert these 16-dimensional vectors into actual predictions.\n",
    "\n",
    "How do we predict the next token?\n",
    "\n",
    "We'll project these vectors into vocabulary space using a **language modeling head**, then compute the **loss** to see how wrong we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store for next notebook\n",
    "layer_norm_data = {\n",
    "    'X': X,\n",
    "    'tokens': tokens,\n",
    "    'layer_norm_output': layer_norm_output,\n",
    "    'gamma': gamma,\n",
    "    'beta': beta\n",
    "}\n",
    "print(\"Layer norm data stored for next notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
