{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Reasoning with Transformers\n",
    "\n",
    "Language models generate one token at a time, left to right, with no scratch paper. For simple factual recall, this works fine. But for multi-step problems—math, logic, planning—the model needs to *compute* the answer, not just recall it.\n",
    "\n",
    "The breakthrough: if models generate intermediate reasoning steps before the final answer, their accuracy on hard problems improves dramatically. A model that \"thinks out loud\" can solve problems that stump models forced to answer immediately.\n",
    "\n",
    "This section covers the techniques that make this possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Problem with Instant Answers\n",
    "\n",
    "Transformers are autoregressive. They generate one token at a time, left to right, based on everything that came before.\n",
    "\n",
    "When asked a question, the model must *commit* to its answer immediately. There's no scratch paper. No thinking time. Just token after token.\n",
    "\n",
    "For simple questions, this works:\n",
    "\n",
    "- \"What's the capital of France?\" → \"Paris\" ✓\n",
    "- \"Who wrote Hamlet?\" → \"Shakespeare\" ✓\n",
    "\n",
    "But for anything requiring multiple steps, performance degrades quickly.\n",
    "\n",
    "Consider: \"If a train leaves Chicago at 9am going 60mph, and another train leaves New York at 10am going 80mph, when do they meet?\"\n",
    "\n",
    "The model can't just *know* the answer. It needs to:\n",
    "1. Figure out the distance between the cities\n",
    "2. Account for the time offset\n",
    "3. Set up the equations\n",
    "4. Solve them\n",
    "\n",
    "If it tries to output the answer immediately, it's guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Solution: Think Out Loud\n",
    "\n",
    "The key insight that changed the field:\n",
    "\n",
    "**The model's reasoning ability is bottlenecked by output length, not parameter count.**\n",
    "\n",
    "A smaller model that \"thinks\" for 1,000 tokens can outperform a larger model that answers in 10 tokens.\n",
    "\n",
    "When the model generates intermediate reasoning steps, it can:\n",
    "\n",
    "1. **Break down complex problems** into manageable pieces\n",
    "2. **Use its own output as working memory** (transformers don't have scratch space—but they can read what they've already written)\n",
    "3. **Catch and correct mistakes** before committing to a final answer\n",
    "\n",
    "This is called **Chain-of-Thought** reasoning. It's the foundation of everything in this section.\n",
    "\n",
    "```\n",
    "Without CoT:\n",
    "Q: What's 17 × 24?\n",
    "A: 408  ← Just guessing. Often wrong.\n",
    "\n",
    "With CoT:\n",
    "Q: What's 17 × 24?\n",
    "A: Let me work through this step by step.\n",
    "   17 × 24 = 17 × (20 + 4)\n",
    "           = 17 × 20 + 17 × 4\n",
    "           = 340 + 68\n",
    "           = 408  ← Worked it out. Actually correct!\n",
    "```\n",
    "\n",
    "Same model. Same parameters. But by generating intermediate steps, it can actually *compute* the answer instead of guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The Paradigm Shift: Test-Time Compute\n",
    "\n",
    "For years, the recipe for better AI was simple: train bigger models on more data.\n",
    "\n",
    "More parameters → better performance. More training data → better performance.\n",
    "\n",
    "But we're hitting walls. Data is finite. Training compute is expensive. Diminishing returns set in.\n",
    "\n",
    "A new idea has emerged: **test-time compute scaling**.\n",
    "\n",
    "Instead of making the model bigger, let it **think longer**.\n",
    "\n",
    "The math is compelling. Researchers at Google found that on reasoning problems, using extra compute at inference time can outperform a model that's **14x larger**. (That's the \"Scaling LLM Test-Time Compute\" paper from 2024.)\n",
    "\n",
    "This is the approach powering models like OpenAI's o1, DeepSeek-R1, and Google's Gemini 2.0 Flash Thinking. They don't just generate answers—they *reason* first.\n",
    "\n",
    "We're going to build it ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## What We'll Build\n",
    "\n",
    "This section takes you from prompting tricks all the way to training your own reasoning model.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    PROMPTING-BASED REASONING                        │\n",
    "│  No training required — just clever prompts                         │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  Chain-of-Thought      │ \"Let's think step by step\"                 │\n",
    "│  Self-Consistency      │ Sample many chains, vote on the answer     │\n",
    "│  Tree of Thoughts      │ Explore multiple paths, backtrack          │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    VERIFICATION & SEARCH                            │\n",
    "│  Train a reward model to guide reasoning                            │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  Process Reward Model  │ Score each reasoning step                  │\n",
    "│  Best-of-N Sampling    │ Generate N solutions, pick the best        │\n",
    "│  Monte Carlo Search    │ Tree search with learned heuristics        │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    TRAINING REASONING MODELS                        │\n",
    "│  RL-based techniques for learning to reason                         │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│  Budget Forcing        │ Force longer thinking with \"Wait\" tokens   │\n",
    "│  GRPO                  │ RL without a critic (DeepSeek's approach)  │\n",
    "│  Distillation          │ Transfer reasoning to smaller models       │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Each notebook builds on the last. By the end, you'll understand exactly how models like o1 and DeepSeek-R1 work—not just conceptually, but with working code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Scope and Limitations\n",
    "\n",
    "OpenAI's o1 was trained on massive amounts of compute with carefully curated data. We're not going to replicate that.\n",
    "\n",
    "What we *are* going to do:\n",
    "\n",
    "1. **Understand the principles** — What makes these techniques work?\n",
    "2. **Implement them from scratch** — Real code, not hand-wavy pseudocode\n",
    "3. **See them in action** — On problems small enough to run on your laptop\n",
    "4. **Build intuition** — So you can apply these ideas to your own projects\n",
    "\n",
    "The goal is understanding, not production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This section assumes you've worked through the earlier parts of this book, or have equivalent knowledge:\n",
    "\n",
    "- **Transformers** — How attention and generation work\n",
    "- **Fine-tuning** — SFT, reward models, the basics of RLHF\n",
    "- **PyTorch** — Comfortable with tensors, autograd, training loops\n",
    "\n",
    "If you're coming from the \"Fine-Tuning a Transformer\" section, you're in great shape. We'll build directly on those concepts—especially reward modeling, which becomes *process* reward modeling in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:22:28.638621Z",
     "iopub.status.busy": "2025-12-10T21:22:28.638532Z",
     "iopub.status.idle": "2025-12-10T21:22:30.414505Z",
     "shell.execute_reply": "2025-12-10T21:22:30.414108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251124+rocm7.1\n",
      "CUDA available: True\n",
      "CUDA device: Radeon RX 7900 XTX\n",
      "\n",
      "All set! Let's teach some transformers to think.\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure we have what we need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"MPS (Apple Silicon) available\")\n",
    "else:\n",
    "    print(\"Running on CPU (slower but still works!)\")\n",
    "\n",
    "print(\"\\nAll set! Let's teach some transformers to think.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## The Roadmap\n",
    "\n",
    "| Notebook | Topic | Key Idea |\n",
    "|----------|-------|----------|\n",
    "| 01 | Chain-of-Thought | Prompting models to show their work |\n",
    "| 02 | Self-Consistency | Sampling multiple chains, majority voting |\n",
    "| 03 | Tree of Thoughts | Exploring and pruning reasoning paths |\n",
    "| 04 | Process Reward Models | Scoring individual reasoning steps |\n",
    "| 05 | Best-of-N with Verification | Using PRMs to select best solutions |\n",
    "| 06 | Monte Carlo Tree Search | Search algorithms for reasoning |\n",
    "| 07 | Budget Forcing | Controlling reasoning length with \"Wait\" tokens |\n",
    "| 08 | GRPO Training | RL without a critic (DeepSeek's method) |\n",
    "| 09 | Reasoning Distillation | Transferring reasoning to smaller models |\n",
    "\n",
    "Each notebook is self-contained, but they build on each other conceptually. Going in order is recommended.\n",
    "\n",
    "Let's start with the technique that kicked off this field: Chain-of-Thought prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## References\n\nThis section draws on a lot of recent research. Here are the key papers if you want to go deeper:\n\n**Foundational:**\n- Wei et al. (2022) — [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n- Kojima et al. (2022) — [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) (the \"Let's think step by step\" paper)\n\n**Verification:**\n- Lightman et al. (2023) — [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050) (Process Reward Models)\n- Wang et al. (2023) — [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935)\n\n**Search:**\n- Yao et al. (2023) — [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)\n\n**Training Reasoning Models:**\n- OpenAI (2024) — [Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/) (o1 announcement)\n- DeepSeek (2025) — [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)\n- Muennighoff et al. (2025) — [s1: Simple Test-Time Scaling](https://arxiv.org/abs/2501.19393)\n\n**Scaling:**\n- Snell et al. (2024) — [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)\n\nDon't worry about reading all of these. We'll cover the key ideas as we go."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "description": "Introduction to reasoning techniques that improve transformer problem-solving through structured thinking."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}