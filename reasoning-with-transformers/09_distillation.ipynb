{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Reasoning Distillation\n",
    "\n",
    "We've built powerful reasoning systems: chain-of-thought, process reward models, MCTS, GRPO. But these techniques work best with large models (70B+). What about reasoning on a phone? In a browser? On a tiny GPU?\n",
    "\n",
    "Distillation transfers the reasoning *patterns* from a large teacher to a small student."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The DeepSeek Discovery\n",
    "\n",
    "From the DeepSeek-R1 paper:\n",
    "\n",
    "> The reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.\n",
    "\n",
    "In other words:\n",
    "- Train a 70B model to reason with RL → good reasoning emerges\n",
    "- Distill to a 7B model → 7B inherits the reasoning patterns\n",
    "- Result: 7B with distillation > 7B with direct RL\n",
    "\n",
    "The small model can't discover complex reasoning patterns on its own, but it *can* learn to imitate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Types of Distillation\n",
    "\n",
    "### 1. Standard Knowledge Distillation\n",
    "Train student to match teacher's *output distributions*.\n",
    "\n",
    "$$\\mathcal{L}_{\\text{KD}} = \\text{KL}(P_{\\text{teacher}} || P_{\\text{student}})$$\n",
    "\n",
    "### 2. Reasoning Trace Distillation\n",
    "Train student on teacher's *step-by-step solutions*.\n",
    "\n",
    "$$\\mathcal{L}_{\\text{trace}} = -\\log P_{\\text{student}}(\\text{reasoning trace})$$\n",
    "\n",
    "### 3. Behavioral Cloning\n",
    "Just train student to produce the same final outputs.\n",
    "\n",
    "For reasoning, **trace distillation** works best. The student learns not just *what* answer to give, but *how* to think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:57:46.941996Z",
     "iopub.status.busy": "2026-01-22T02:57:46.941996Z",
     "iopub.status.idle": "2026-01-22T02:57:53.393296Z",
     "shell.execute_reply": "2026-01-22T02:57:53.393296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded on cuda\n",
      "Teacher: Qwen2.5-1.5B-Instruct\n",
      "Student: Qwen2.5-0.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "# Load teacher and student models\n",
    "# Teacher: larger model, Student: smaller model\n",
    "print(\"Loading models...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", dtype=\"auto\")\n",
    "student = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=\"auto\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "teacher = teacher.to(device)\n",
    "student = student.to(device)\n",
    "\n",
    "teacher.eval()  # Teacher is frozen\n",
    "# Student will be trained\n",
    "\n",
    "print(f\"Loaded on {device}\")\n",
    "print(f\"Teacher: Qwen2.5-1.5B-Instruct\")\n",
    "print(f\"Student: Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Generating Teacher Reasoning Traces\n",
    "\n",
    "First, we need to collect high-quality reasoning traces from the teacher model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ogughrkqfq",
   "metadata": {},
   "source": [
    "## What's a Trace?\n",
    "\n",
    "A **reasoning trace** is the complete step-by-step solution a model generates when solving a problem. Not just the final answer—the whole internal monologue.\n",
    "\n",
    "For example, when asked \"What is 15 + 28?\", a trace might look like:\n",
    "\n",
    "```\n",
    "Step 1: I'll break this into parts: 15 = 10 + 5\n",
    "Step 2: Add the tens: 10 + 20 = 30\n",
    "Step 3: Add the ones: 5 + 8 = 13\n",
    "Step 4: Combine: 30 + 13 = 43\n",
    "```\n",
    "\n",
    "The trace is everything from \"Step 1\" to \"43\". It's the *how*, not just the *what*.\n",
    "\n",
    "In chain-of-thought prompting, we showed models a few example traces and asked them to produce their own. In distillation, we're taking thousands of high-quality traces from a smart teacher model and training a smaller student to reproduce that same reasoning style.\n",
    "\n",
    "Think of it like showing a student worked examples in a textbook. The trace is the worked example—every intermediate calculation, every logical step laid out explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:57:53.393296Z",
     "iopub.status.busy": "2026-01-22T02:57:53.393296Z",
     "iopub.status.idle": "2026-01-22T02:58:17.676292Z",
     "shell.execute_reply": "2026-01-22T02:58:17.676292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating teacher traces...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 6 traces\n",
      "\n",
      "Example trace:\n",
      "============================================================\n",
      "Problem: What is 15 + 28?\n",
      "\n",
      "Solution: Let me solve this step by step.\n",
      "Step 1: First, I need to add the units digits. In this case, we have 5 and 8. If we add these together, we get 13.\n",
      "Step 2: Now, let's consider what happens when we carry over a digit. Since we are adding two single-digit numbers, t...\n"
     ]
    }
   ],
   "source": [
    "def generate_teacher_traces(teacher, tokenizer, problems: List[str],\n",
    "                            n_per_problem: int = 3,\n",
    "                            max_tokens: int = 150) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Generate reasoning traces from the teacher model.\n",
    "    \n",
    "    We'll generate multiple traces per problem and filter for correctness.\n",
    "    \"\"\"\n",
    "    traces = []\n",
    "    \n",
    "    for problem in problems:\n",
    "        prompt = f\"Problem: {problem}\\n\\nSolution: Let me solve this step by step.\\n\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        for _ in range(n_per_problem):\n",
    "            with torch.no_grad():\n",
    "                outputs = teacher.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            trace = full_text[len(prompt):]\n",
    "            \n",
    "            traces.append({\n",
    "                \"problem\": problem,\n",
    "                \"prompt\": prompt,\n",
    "                \"trace\": trace,\n",
    "                \"full_text\": prompt + trace\n",
    "            })\n",
    "    \n",
    "    return traces\n",
    "\n",
    "\n",
    "# Generate some traces\n",
    "problems = [\n",
    "    \"What is 15 + 28?\",\n",
    "    \"If a train travels 60 miles in 2 hours, what is its speed?\",\n",
    "    \"A store has 50 items. They sell 20% of them. How many are left?\",\n",
    "]\n",
    "\n",
    "print(\"Generating teacher traces...\")\n",
    "teacher_traces = generate_teacher_traces(teacher, tokenizer, problems, n_per_problem=2)\n",
    "\n",
    "print(f\"\\nGenerated {len(teacher_traces)} traces\")\n",
    "print(\"\\nExample trace:\")\n",
    "print(\"=\"*60)\n",
    "print(teacher_traces[0][\"full_text\"][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Trace Distillation Loss\n",
    "\n",
    "The simplest form: train the student to produce the same reasoning traces as the teacher.\n",
    "\n",
    "This is just supervised fine-tuning on teacher-generated data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:17.678035Z",
     "iopub.status.busy": "2026-01-22T02:58:17.678035Z",
     "iopub.status.idle": "2026-01-22T02:58:17.725140Z",
     "shell.execute_reply": "2026-01-22T02:58:17.725140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distillation loss: 1.2435\n"
     ]
    }
   ],
   "source": [
    "def compute_trace_distillation_loss(student, tokenizer, \n",
    "                                     trace: dict) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute loss for matching a teacher trace.\n",
    "    \n",
    "    This is just cross-entropy on the reasoning steps.\n",
    "    \"\"\"\n",
    "    full_text = trace[\"full_text\"]\n",
    "    prompt_len = len(tokenizer(trace[\"prompt\"])[\"input_ids\"])\n",
    "    \n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = student(**inputs, labels=inputs[\"input_ids\"])\n",
    "    \n",
    "    # We only care about loss on the reasoning trace, not the prompt\n",
    "    # In practice, we'd mask the prompt tokens\n",
    "    # For simplicity, we'll use the full loss here\n",
    "    \n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "# Test\n",
    "loss = compute_trace_distillation_loss(student, tokenizer, teacher_traces[0])\n",
    "print(f\"Distillation loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4wepz3mr8z",
   "metadata": {},
   "source": [
    "This loss (~0.97) is actually pretty good for a first attempt! Remember, this is cross-entropy—it's measuring how confident the student is about each next token. A loss of 0 would mean perfect certainty (impossible), while a loss around 1 means the student is reasonably confident but still learning.\n",
    "\n",
    "For context: random guessing across a 32,000-token vocabulary would give you a loss around 10. So 0.97 means the student already has a decent prior—it's not flailing randomly. After training on many examples, we'd expect this to drop closer to 0.5 or lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Token-Level Knowledge Distillation\n",
    "\n",
    "A more sophisticated approach: match the teacher's probability distribution at each token position.\n",
    "\n",
    "$$\\mathcal{L}_{\\text{KD}} = \\sum_t \\text{KL}\\left( \\frac{P_T(y_t|y_{<t})}{\\tau} \\bigg|\\bigg| \\frac{P_S(y_t|y_{<t})}{\\tau} \\right)$$\n",
    "\n",
    "Where $\\tau$ is a temperature that softens the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:17.726883Z",
     "iopub.status.busy": "2026-01-22T02:58:17.726883Z",
     "iopub.status.idle": "2026-01-22T02:58:17.810844Z",
     "shell.execute_reply": "2026-01-22T02:58:17.810844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge distillation losses:\n",
      "  KL loss: 60.5000\n",
      "  Hard target loss: 1.2422\n",
      "  Total: 121.5000\n"
     ]
    }
   ],
   "source": [
    "def knowledge_distillation_loss(teacher, student, tokenizer,\n",
    "                                 text: str, temperature: float = 2.0,\n",
    "                                 alpha: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Token-level knowledge distillation.\n",
    "    \n",
    "    Combines:\n",
    "    1. KL divergence from teacher distributions\n",
    "    2. Hard target cross-entropy\n",
    "    \n",
    "    Args:\n",
    "        teacher: Teacher model (frozen)\n",
    "        student: Student model (training)\n",
    "        tokenizer: Tokenizer\n",
    "        text: Text to distill on\n",
    "        temperature: Softening temperature\n",
    "        alpha: Weight on distillation vs. hard targets\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get teacher logits\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(**inputs)\n",
    "        teacher_logits = teacher_outputs.logits\n",
    "    \n",
    "    # Get student logits\n",
    "    student_outputs = student(**inputs)\n",
    "    student_logits = student_outputs.logits\n",
    "    \n",
    "    # Soft targets (temperature-scaled)\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss = F.kl_div(\n",
    "        student_log_probs[:, :-1, :],  # Predict next token\n",
    "        teacher_probs[:, :-1, :],\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    # Hard target loss (standard cross-entropy)\n",
    "    hard_loss = F.cross_entropy(\n",
    "        student_logits[:, :-1, :].reshape(-1, student_logits.size(-1)),\n",
    "        inputs[\"input_ids\"][:, 1:].reshape(-1)\n",
    "    )\n",
    "    \n",
    "    # Combined loss\n",
    "    # Scale KL by T^2 (standard practice)\n",
    "    total_loss = alpha * (temperature ** 2) * kl_loss + (1 - alpha) * hard_loss\n",
    "    \n",
    "    return total_loss, kl_loss, hard_loss\n",
    "\n",
    "\n",
    "# Test\n",
    "test_text = teacher_traces[0][\"full_text\"]\n",
    "total, kl, hard = knowledge_distillation_loss(\n",
    "    teacher, student, tokenizer, test_text\n",
    ")\n",
    "\n",
    "print(f\"Knowledge distillation losses:\")\n",
    "print(f\"  KL loss: {kl.item():.4f}\")\n",
    "print(f\"  Hard target loss: {hard.item():.4f}\")\n",
    "print(f\"  Total: {total.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eukaxhi8adi",
   "metadata": {},
   "source": [
    "Wait, why is the total loss (121.5) so much bigger than either component?\n",
    "\n",
    "Because of the $T^2$ scaling factor! When we use temperature $T=2$, we multiply the KL loss by $2^2 = 4$. This is standard practice in distillation—it balances the contribution of the soft targets against the hard targets.\n",
    "\n",
    "Breaking it down:\n",
    "- **KL loss (60.5)**: How different are the student's probabilities from the teacher's? This is naturally larger because we're comparing full probability distributions across thousands of tokens.\n",
    "- **Hard target loss (0.97)**: Standard cross-entropy against the actual next token. Same scale as the trace distillation loss above.\n",
    "- **Total (121.5)**: $\\alpha \\cdot T^2 \\cdot \\text{KL} + (1-\\alpha) \\cdot \\text{hard} = 0.5 \\cdot 4 \\cdot 60.5 + 0.5 \\cdot 0.97 \\approx 121.5$\n",
    "\n",
    "The total will decrease as training progresses and the student learns to match the teacher's distribution better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Training Loop for Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:17.812346Z",
     "iopub.status.busy": "2026-01-22T02:58:17.812346Z",
     "iopub.status.idle": "2026-01-22T02:58:19.951877Z",
     "shell.execute_reply": "2026-01-22T02:58:19.951877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training student with distillation...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 98.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 78.5417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 69.8750\n",
      "\n",
      "Distillation complete!\n"
     ]
    }
   ],
   "source": [
    "def train_distillation_epoch(teacher, student, tokenizer,\n",
    "                              traces: List[dict], optimizer,\n",
    "                              temperature: float = 2.0,\n",
    "                              alpha: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Train student for one epoch on teacher traces.\n",
    "    \"\"\"\n",
    "    student.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for trace in traces:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, _, _ = knowledge_distillation_loss(\n",
    "            teacher, student, tokenizer,\n",
    "            trace[\"full_text\"],\n",
    "            temperature=temperature,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(traces)\n",
    "\n",
    "\n",
    "# Train for a few epochs\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=1e-5)\n",
    "\n",
    "print(\"Training student with distillation...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = train_distillation_epoch(\n",
    "        teacher, student, tokenizer,\n",
    "        teacher_traces, optimizer\n",
    "    )\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(\"\\nDistillation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6tfoeiif8po",
   "metadata": {},
   "source": [
    "These losses look huge (110 → 84 → 75) but they're actually fine! Remember, we're averaging the distillation loss across all traces, and that loss includes the $T^2$ scaling we saw above.\n",
    "\n",
    "What matters is the **trend**: loss is decreasing consistently. The student is learning to match the teacher's reasoning patterns better with each epoch. In a real training run with thousands of traces, you'd see this drop much further (into the 20s or lower).\n",
    "\n",
    "Also note: we're using the knowledge distillation loss here (with the KL component), not the simpler trace distillation loss. So the scale is naturally larger. If we were just doing trace SFT, we'd see losses closer to the 0.97 we got in the first test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Quality Filtering\n",
    "\n",
    "Not all teacher traces are worth imitating. We should filter for:\n",
    "1. **Correct answers** — Don't teach wrong reasoning\n",
    "2. **Clear steps** — Mumbled reasoning is hard to learn from\n",
    "3. **Diverse approaches** — Multiple ways to solve problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:58:19.951877Z",
     "iopub.status.busy": "2026-01-22T02:58:19.951877Z",
     "iopub.status.idle": "2026-01-22T02:58:19.957407Z",
     "shell.execute_reply": "2026-01-22T02:58:19.957407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 5/6 traces kept\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def filter_traces(traces: List[dict], correct_answers: dict = None) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Filter teacher traces for quality.\n",
    "    \n",
    "    Args:\n",
    "        traces: List of trace dicts\n",
    "        correct_answers: Dict mapping problems to correct answers\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of high-quality traces\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    \n",
    "    for trace in traces:\n",
    "        text = trace[\"trace\"]\n",
    "        problem = trace[\"problem\"]\n",
    "        \n",
    "        # Filter 1: Must have step-by-step structure\n",
    "        has_steps = any(marker in text.lower() \n",
    "                       for marker in ['step', 'first', 'then', 'next', 'finally'])\n",
    "        if not has_steps:\n",
    "            continue\n",
    "        \n",
    "        # Filter 2: Must have reasonable length\n",
    "        if len(text.split()) < 20 or len(text.split()) > 300:\n",
    "            continue\n",
    "        \n",
    "        # Filter 3: Check correctness if we have answers\n",
    "        if correct_answers and problem in correct_answers:\n",
    "            correct = str(correct_answers[problem])\n",
    "            if correct not in text:\n",
    "                continue\n",
    "        \n",
    "        filtered.append(trace)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "# Example filtering\n",
    "correct_answers = {\n",
    "    \"What is 15 + 28?\": \"43\",\n",
    "    \"If a train travels 60 miles in 2 hours, what is its speed?\": \"30\",\n",
    "    \"A store has 50 items. They sell 20% of them. How many are left?\": \"40\",\n",
    "}\n",
    "\n",
    "filtered = filter_traces(teacher_traces, correct_answers)\n",
    "print(f\"Filtered: {len(filtered)}/{len(teacher_traces)} traces kept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Results from DeepSeek\n",
    "\n",
    "From the DeepSeek-R1 paper:\n",
    "\n",
    "| Model | Method | AIME 2024 | MATH |\n",
    "|-------|--------|-----------|------|\n",
    "| Qwen2.5-7B | Base | 3.3% | 75.5% |\n",
    "| Qwen2.5-7B | + RL alone | 10.0% | 79.3% |\n",
    "| Qwen2.5-7B | **+ R1 distillation** | **26.7%** | **83.9%** |\n",
    "| Qwen2.5-32B | + R1 distillation | 43.3% | 90.2% |\n",
    "\n",
    "Key insight: A 7B model with distillation dramatically outperforms a 7B model trained with RL alone. The reasoning patterns from the larger model transfer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## What We've Learned\n",
    "\n",
    "Reasoning distillation transfers thinking patterns from large to small models:\n",
    "\n",
    "1. **Generate** high-quality reasoning traces from teacher\n",
    "2. **Filter** for correctness and clarity\n",
    "3. **Train** student to reproduce the traces (SFT or KD)\n",
    "\n",
    "The key insight:\n",
    "> Small models can't discover complex reasoning on their own, but they *can* learn to imitate it.\n",
    "\n",
    "Two loss functions:\n",
    "- **Trace SFT**: $\\mathcal{L} = -\\log P_S(\\text{trace})$\n",
    "- **KD**: $\\mathcal{L} = \\alpha \\cdot T^2 \\cdot \\text{KL}(P_T/T || P_S/T) + (1-\\alpha) \\cdot \\text{CE}$\n",
    "\n",
    "This completes our journey through reasoning techniques!\n",
    "\n",
    "## Summary of the Section\n",
    "\n",
    "We covered:\n",
    "1. **Chain-of-Thought** — Think step by step\n",
    "2. **Self-Consistency** — Sample many, vote\n",
    "3. **Tree of Thoughts** — Explore and backtrack\n",
    "4. **Process Reward Models** — Score each step\n",
    "5. **Best-of-N** — Generate and verify\n",
    "6. **MCTS** — Smart search\n",
    "7. **Budget Forcing** — Control thinking length\n",
    "8. **GRPO** — RL without a critic\n",
    "9. **Distillation** — Transfer to smaller models\n",
    "\n",
    "These techniques, combined, power the reasoning capabilities of models like o1 and DeepSeek-R1."
   ]
  }
 ],
 "metadata": {
  "description": "Distills reasoning capabilities from large models into smaller ones by training on generated reasoning traces.",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
