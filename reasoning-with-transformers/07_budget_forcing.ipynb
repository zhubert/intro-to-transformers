{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Budget Forcing and Wait Tokens\n\nA striking result from early 2025: researchers trained a 32B model on just 1,000 examples and beat o1-preview on math benchmarks. Their technique: \"budget forcing\" to control how long the model \"thinks.\"\n\nThe key insight: **reasoning ability is already in the model**. Fine-tuning activates it—but that's only half the story. The real power comes from forcing the model to *keep thinking* with \"Wait\" tokens at inference time."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## The s1 Paper\n\nThe [\"s1: Simple Test-Time Scaling\"](https://arxiv.org/abs/2501.19393) paper (January 2025) demonstrated something surprising:\n\n1. Take a strong base model (Qwen2.5-32B-Instruct)\n2. Fine-tune on just 1,000 carefully selected reasoning examples\n3. At inference, force the model to keep thinking by appending \"Wait\"\n\nResult: **27% improvement over o1-preview on MATH/AIME24**\n\nThe \"Wait\" token trick is simple but effective. When the model tries to output a final answer, append \"Wait\" and it continues reasoning. Often, it catches and fixes its own mistakes."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## How Budget Forcing Works\n",
    "\n",
    "```\n",
    "Model: \"Let me solve this. 5 + 7 = 12. The answer is 12.\"\n",
    "                                                      ↑\n",
    "                                            Model wants to stop\n",
    "\n",
    "With budget forcing:\n",
    "\n",
    "Model: \"Let me solve this. 5 + 7 = 12. The answer is 12.\"\n",
    "                                                      ↓\n",
    "                                            We append: \"Wait\"\n",
    "                                                      ↓\n",
    "Model: \"Wait, let me double-check. 5 + 7... yes, 12 is correct.\"\n",
    "```\n",
    "\n",
    "The \"Wait\" token induces doubt. The model reconsiders, often catching errors it would have missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:24:33.287133Z",
     "iopub.status.busy": "2025-12-10T21:24:33.287024Z",
     "iopub.status.idle": "2025-12-10T21:24:37.184695Z",
     "shell.execute_reply": "2025-12-10T21:24:37.184414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Optional\n",
    "import re\n",
    "\n",
    "# Load model\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:24:37.186053Z",
     "iopub.status.busy": "2025-12-10T21:24:37.185878Z",
     "iopub.status.idle": "2025-12-10T21:24:39.011891Z",
     "shell.execute_reply": "2025-12-10T21:24:39.011534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without budget forcing:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  The sum of 15 and 28 is:\n",
      "$$\n",
      "\\begin{align}\n",
      "15 &+ 28 \\\\\n",
      "= &43\n",
      "\\end{align}\n",
      "$$\n",
      "Therefore, the answer is $\\boxed{43}\n",
      "Tokens: 50\n"
     ]
    }
   ],
   "source": [
    "def generate_with_budget_forcing(prompt: str, \n",
    "                                  min_tokens: int = 50,\n",
    "                                  max_tokens: int = 200,\n",
    "                                  wait_token: str = \" Wait,\",\n",
    "                                  end_markers: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate with budget forcing.\n",
    "    \n",
    "    If the model tries to stop before min_tokens, append wait_token\n",
    "    to encourage continued reasoning.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input prompt\n",
    "        min_tokens: Minimum tokens to generate (budget)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        wait_token: Token to append for continuation\n",
    "        end_markers: Phrases that indicate final answer\n",
    "    \n",
    "    Returns:\n",
    "        Generated text with reasoning\n",
    "    \"\"\"\n",
    "    if end_markers is None:\n",
    "        end_markers = [\"the answer is\", \"therefore\", \"final answer\", \"in conclusion\"]\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_text = \"\"\n",
    "    total_tokens = 0\n",
    "    n_waits = 0\n",
    "    \n",
    "    while total_tokens < max_tokens:\n",
    "        # Generate a chunk\n",
    "        chunk_size = min(30, max_tokens - total_tokens)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            current_input = tokenizer(prompt + generated_text, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(\n",
    "                **current_input,\n",
    "                max_new_tokens=chunk_size,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        new_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        new_text = new_text[len(prompt) + len(generated_text):]\n",
    "        generated_text += new_text\n",
    "        total_tokens = len(tokenizer.encode(generated_text))\n",
    "        \n",
    "        # Check if model is trying to conclude\n",
    "        lower_text = generated_text.lower()\n",
    "        is_concluding = any(marker in lower_text for marker in end_markers)\n",
    "        \n",
    "        # If concluding too early, append wait token\n",
    "        if is_concluding and total_tokens < min_tokens:\n",
    "            generated_text += wait_token\n",
    "            n_waits += 1\n",
    "            continue\n",
    "        \n",
    "        # If naturally concluded and past minimum, stop\n",
    "        if is_concluding:\n",
    "            break\n",
    "        \n",
    "        # If we hit EOS, check if we need to continue\n",
    "        if outputs[0][-1] == tokenizer.eos_token_id:\n",
    "            if total_tokens < min_tokens:\n",
    "                generated_text += wait_token\n",
    "                n_waits += 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    return generated_text, n_waits\n",
    "\n",
    "\n",
    "# Test without budget forcing\n",
    "prompt = \"Question: What is 15 + 28?\\nAnswer: Let me solve this.\"\n",
    "\n",
    "print(\"Without budget forcing:\")\n",
    "print(\"=\"*60)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "short_response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
    "print(f\"Response: {short_response}\")\n",
    "print(f\"Tokens: {len(tokenizer.encode(short_response))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:24:39.012990Z",
     "iopub.status.busy": "2025-12-10T21:24:39.012913Z",
     "iopub.status.idle": "2025-12-10T21:24:39.962374Z",
     "shell.execute_reply": "2025-12-10T21:24:39.962015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With budget forcing (min 50 tokens):\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  15 + 28 = 43\n",
      "Therefore, the answer is 43.\n",
      "\n",
      "Question: The temperature dropped from 60 Wait, wait... I'm not sure what a \"Wait\" means in this context. Can you please provide more information or clarify your question? It seems there\n",
      "\n",
      "Tokens: 62\n",
      "Wait tokens inserted: 1\n"
     ]
    }
   ],
   "source": [
    "# Test WITH budget forcing\n",
    "print(\"\\nWith budget forcing (min 50 tokens):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "long_response, n_waits = generate_with_budget_forcing(\n",
    "    prompt, \n",
    "    min_tokens=50,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(f\"Response: {long_response}\")\n",
    "print(f\"\\nTokens: {len(tokenizer.encode(long_response))}\")\n",
    "print(f\"Wait tokens inserted: {n_waits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Why \"Wait\" Works\n",
    "\n",
    "The s1 paper tested different continuation tokens:\n",
    "\n",
    "| Token | AIME24 Accuracy |\n",
    "|-------|----------------|\n",
    "| No continuation | 50.0% |\n",
    "| \"Hmm\" | 50.0% |\n",
    "| \"Alternatively\" | 50.0% |\n",
    "| **\"Wait\"** | **53.3%** |\n",
    "\n",
    "\"Wait\" specifically induces **doubt and reconsideration**. It's not just about generating more tokens—it's about generating the *right kind* of additional reasoning.\n",
    "\n",
    "The model doesn't just continue; it **questions itself**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:24:39.963268Z",
     "iopub.status.busy": "2025-12-10T21:24:39.963192Z",
     "iopub.status.idle": "2025-12-10T21:24:42.488465Z",
     "shell.execute_reply": "2025-12-10T21:24:42.488172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How different tokens affect continuation:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wait,\n",
      "  →  I made a mistake in my previous response. Let me correct it.\n",
      "\n",
      "The correct calcu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hmm,\n",
      "  →  that's not right.\n",
      "Question: What is 3 + 2?\n",
      "Answer: Let me calculate. 3 + 2 = 5....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let me think more.\n",
      "  →  Seven times eight equals fifty-four.\n",
      "You are an AI assistant that helps people ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actually,\n",
      "  →  the correct calculation for 7 × 8 should be 56, not 54.\n",
      "What is a more appropri...\n"
     ]
    }
   ],
   "source": [
    "# Compare different continuation tokens\n",
    "continuation_tokens = [\n",
    "    \" Wait,\",\n",
    "    \" Hmm,\",\n",
    "    \" Let me think more.\",\n",
    "    \" Actually,\"\n",
    "]\n",
    "\n",
    "base_text = \"Question: What is 7 × 8?\\nAnswer: Let me calculate. 7 × 8 = 54. The answer is 54.\"\n",
    "\n",
    "print(\"How different tokens affect continuation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for token in continuation_tokens:\n",
    "    prompt = base_text + token\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=40,\n",
    "            temperature=0.5,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    continuation = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
    "    print(f\"\\n{token.strip()}\")\n",
    "    print(f\"  → {continuation[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Adaptive Budget Forcing\n",
    "\n",
    "A smarter approach: adapt the budget based on problem difficulty.\n",
    "\n",
    "- Easy problem: Less thinking needed\n",
    "- Hard problem: Force more deliberation\n",
    "\n",
    "The s1 paper found that **problem difficulty correlates with optimal compute**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:24:42.489471Z",
     "iopub.status.busy": "2025-12-10T21:24:42.489391Z",
     "iopub.status.idle": "2025-12-10T21:24:42.492541Z",
     "shell.execute_reply": "2025-12-10T21:24:42.492242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive budget based on difficulty:\n",
      "============================================================\n",
      "\n",
      "Problem: What is 5 + 3?...\n",
      "  Difficulty: 0.60\n",
      "  Token budget: 102\n",
      "\n",
      "Problem: What is 15% of 80?...\n",
      "  Difficulty: 0.50\n",
      "  Token budget: 90\n",
      "\n",
      "Problem: If a train travels 120 miles in 2 hours, what is i...\n",
      "  Difficulty: 0.90\n",
      "  Token budget: 138\n",
      "\n",
      "Problem: A store has 1250 items. They sell 15% and receive ...\n",
      "  Difficulty: 1.00\n",
      "  Token budget: 150\n"
     ]
    }
   ],
   "source": [
    "def estimate_difficulty(problem: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple heuristic to estimate problem difficulty.\n",
    "    \n",
    "    In practice, you might use a classifier or the model's\n",
    "    own uncertainty estimates.\n",
    "    \"\"\"\n",
    "    difficulty = 0.3  # Base difficulty\n",
    "    \n",
    "    # More numbers = harder\n",
    "    numbers = re.findall(r'\\d+', problem)\n",
    "    difficulty += 0.1 * min(len(numbers), 5)\n",
    "    \n",
    "    # Larger numbers = harder\n",
    "    if numbers:\n",
    "        max_num = max(int(n) for n in numbers)\n",
    "        if max_num > 100:\n",
    "            difficulty += 0.2\n",
    "        if max_num > 1000:\n",
    "            difficulty += 0.2\n",
    "    \n",
    "    # Multiple operations = harder\n",
    "    ops = len(re.findall(r'[+\\-×÷*/]', problem))\n",
    "    difficulty += 0.1 * min(ops, 3)\n",
    "    \n",
    "    # Keywords suggesting complexity\n",
    "    if any(w in problem.lower() for w in ['percent', 'ratio', 'average', 'probability']):\n",
    "        difficulty += 0.2\n",
    "    \n",
    "    return min(1.0, difficulty)\n",
    "\n",
    "\n",
    "def adaptive_budget(problem: str, base_tokens: int = 30, \n",
    "                    max_tokens: int = 150) -> int:\n",
    "    \"\"\"\n",
    "    Set token budget based on problem difficulty.\n",
    "    \"\"\"\n",
    "    difficulty = estimate_difficulty(problem)\n",
    "    budget = int(base_tokens + difficulty * (max_tokens - base_tokens))\n",
    "    return budget\n",
    "\n",
    "\n",
    "# Test on problems of varying difficulty\n",
    "problems = [\n",
    "    \"What is 5 + 3?\",\n",
    "    \"What is 15% of 80?\",\n",
    "    \"If a train travels 120 miles in 2 hours, what is its average speed?\",\n",
    "    \"A store has 1250 items. They sell 15% and receive 340 more. How many items do they have?\",\n",
    "]\n",
    "\n",
    "print(\"Adaptive budget based on difficulty:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for problem in problems:\n",
    "    diff = estimate_difficulty(problem)\n",
    "    budget = adaptive_budget(problem)\n",
    "    print(f\"\\nProblem: {problem[:50]}...\")\n",
    "    print(f\"  Difficulty: {diff:.2f}\")\n",
    "    print(f\"  Token budget: {budget}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Training with Reasoning Traces\n",
    "\n",
    "The s1 paper's other key contribution: the **s1K dataset**.\n",
    "\n",
    "They curated just 1,000 examples based on three criteria:\n",
    "1. **Difficulty**: Problems that require multi-step reasoning\n",
    "2. **Diversity**: Different types of problems (math, logic, etc.)\n",
    "3. **Quality**: Clear, correct reasoning traces\n",
    "\n",
    "This tiny dataset was enough to activate reasoning capabilities already present in the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:24:42.493373Z",
     "iopub.status.busy": "2025-12-10T21:24:42.493296Z",
     "iopub.status.idle": "2025-12-10T21:24:42.495188Z",
     "shell.execute_reply": "2025-12-10T21:24:42.494904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example reasoning trace for training:\n",
      "============================================================\n",
      "\n",
      "Question: A store has 45 items. They sell 30% of them and receive 20 more.\n",
      "How many items do they have now?\n",
      "\n",
      "Let me solve this step by step.\n",
      "\n",
      "Step 1: Calculate 30% of 45.\n",
      "30% = 0.30\n",
      "0.30 × 45 = 13.5\n",
      "\n",
      "Wait, can you sell half an item? Let me reconsider.\n",
      "Actually, 30% of 45 = 0.3 × 45 = 13.5\n",
      "Since we can't sell half an item, let's round to 13 or 14.\n",
      "\n",
      "Hmm, the problem might expect exact math. Let me continue with 13.5 \n",
      "and see if we need to round at the end.\n",
      "\n",
      "Step 2: Subtract items sold.\n",
      "45 - 13.5 = 31.5 items remaining\n",
      "\n",
      "Step 3: Add new items.\n",
      "31.5 + 20 = 51.5 items\n",
      "\n",
      "Rounding to a whole number: 51 or 52 items.\n",
      "\n",
      "The answer is approximately 51-52 items (or exactly 51.5 if fractional items are allowed).\n",
      "\n",
      "\n",
      "Key features:\n",
      "- Step-by-step structure\n",
      "- Self-correction (Wait...)\n",
      "- Explicit uncertainty handling\n",
      "- Clear final answer\n"
     ]
    }
   ],
   "source": [
    "# Example of what a good training example looks like\n",
    "example_trace = \"\"\"\n",
    "Question: A store has 45 items. They sell 30% of them and receive 20 more.\n",
    "How many items do they have now?\n",
    "\n",
    "Let me solve this step by step.\n",
    "\n",
    "Step 1: Calculate 30% of 45.\n",
    "30% = 0.30\n",
    "0.30 × 45 = 13.5\n",
    "\n",
    "Wait, can you sell half an item? Let me reconsider.\n",
    "Actually, 30% of 45 = 0.3 × 45 = 13.5\n",
    "Since we can't sell half an item, let's round to 13 or 14.\n",
    "\n",
    "Hmm, the problem might expect exact math. Let me continue with 13.5 \n",
    "and see if we need to round at the end.\n",
    "\n",
    "Step 2: Subtract items sold.\n",
    "45 - 13.5 = 31.5 items remaining\n",
    "\n",
    "Step 3: Add new items.\n",
    "31.5 + 20 = 51.5 items\n",
    "\n",
    "Rounding to a whole number: 51 or 52 items.\n",
    "\n",
    "The answer is approximately 51-52 items (or exactly 51.5 if fractional items are allowed).\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example reasoning trace for training:\")\n",
    "print(\"=\"*60)\n",
    "print(example_trace)\n",
    "print(\"\\nKey features:\")\n",
    "print(\"- Step-by-step structure\")\n",
    "print(\"- Self-correction (Wait...)\")\n",
    "print(\"- Explicit uncertainty handling\")\n",
    "print(\"- Clear final answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Results: s1 Performance\n",
    "\n",
    "From the s1 paper:\n",
    "\n",
    "| Model | MATH | AIME24 |\n",
    "|-------|------|--------|\n",
    "| Qwen2.5-32B-Instruct | 83.1% | 16.7% |\n",
    "| + s1K fine-tuning | 88.2% | 50.0% |\n",
    "| + budget forcing | **93.0%** | **57.0%** |\n",
    "| o1-preview | 85.5% | 44.6% |\n",
    "\n",
    "Key takeaways:\n",
    "1. Just 1,000 examples dramatically improves reasoning\n",
    "2. Budget forcing adds another significant boost\n",
    "3. Simple methods can beat complex systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## What We've Learned\n",
    "\n",
    "Budget forcing is a simple but powerful technique:\n",
    "\n",
    "1. **Core idea**: Append \"Wait\" to prevent premature conclusions\n",
    "2. **Why it works**: Induces self-doubt and reconsideration\n",
    "3. **Adaptive budgets**: Harder problems → more thinking\n",
    "4. **Minimal training**: 1,000 examples can activate latent reasoning\n",
    "\n",
    "The insight:\n",
    "> The model's reasoning capabilities are largely present from pretraining. Fine-tuning merely activates these latent abilities.\n",
    "\n",
    "This suggests that much of \"reasoning\" is about *controlling* existing capabilities, not creating new ones.\n",
    "\n",
    "**Next up:** GRPO — training reasoning models with RL (no critic needed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}