{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Self-Consistency\n\n**Sample many reasoning paths, then vote**\n\nA simple idea makes chain-of-thought far more reliable: instead of generating one reasoning chain and hoping it's right, generate *many* chains and see which answer comes up most often.\n\nIt's like asking 10 experts to solve a problem independently, then going with the majority opinion."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Problem with Single Samples\n",
    "\n",
    "In the last notebook, we saw that chain-of-thought prompting dramatically improves reasoning. But there's a catch: the model might generate a wrong reasoning chain.\n",
    "\n",
    "Consider this:\n",
    "\n",
    "```\n",
    "Q: What's 23 × 17?\n",
    "\n",
    "Chain 1: 23 × 17 = 23 × 10 + 23 × 7 = 230 + 161 = 391 ✓\n",
    "Chain 2: 23 × 17 = 20 × 17 + 3 × 17 = 340 + 51 = 391 ✓  \n",
    "Chain 3: 23 × 17 = 23 × 20 - 23 × 3 = 460 - 69 = 391 ✓\n",
    "Chain 4: 23 × 17 = 25 × 17 - 2 × 17 = 425 - 34 = 389 ✗ (arithmetic error!)\n",
    "```\n",
    "\n",
    "If we only sample Chain 4, we get the wrong answer. But if we sample all four and take the majority, we get 391 (correct!).\n",
    "\n",
    "This is the core insight of **self-consistency**: the correct answer tends to be more robust across different reasoning paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## The Algorithm\n\nSelf-consistency is straightforward:\n\n1. **Sample** $N$ independent reasoning chains from the model\n2. **Extract** the final answer from each chain\n3. **Vote** — take the most common answer\n\nNo training. No new models. Just sample more.\n\n```\n┌─────────────────┐\n│    Question     │\n└────────┬────────┘\n         │\n         ▼  (sample N times with temperature > 0)\n┌─────────────────────────────────────────────────────┐\n│  Chain 1 → 391    Chain 2 → 391    Chain 3 → 391   │\n│  Chain 4 → 389    Chain 5 → 391    Chain 6 → 391   │\n└────────────────────────┬────────────────────────────┘\n                         │\n                         ▼  (majority vote)\n                   ┌───────────┐\n                   │    391    │\n                   └───────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load model\n",
    "model_name = \"gpt2-medium\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the final numerical answer from reasoning text.\n",
    "    \n",
    "    Tries several patterns, falls back to last number found.\n",
    "    \"\"\"\n",
    "    # Try common answer patterns\n",
    "    patterns = [\n",
    "        r\"(?:the answer is|therefore|thus|so|=)\\s*[:\\s]*([\\d,\\.]+)\",\n",
    "        r\"([\\d,\\.]+)\\s*(?:total|in all|altogether)\",\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            return match.group(1).replace(\",\", \"\")\n",
    "    \n",
    "    # Fallback: last number in the text\n",
    "    numbers = re.findall(r\"\\b\\d+\\.?\\d*\\b\", text)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_chain(prompt: str, max_new_tokens: int = 100, \n",
    "                   temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Generate a single reasoning chain.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The CoT prompt\n",
    "        max_new_tokens: Max tokens to generate\n",
    "        temperature: Sampling temperature (>0 for diversity)\n",
    "    \n",
    "    Returns:\n",
    "        The generated reasoning chain\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return full_response[len(prompt):].strip()\n",
    "\n",
    "\n",
    "# Test single generation\n",
    "test_prompt = \"Question: What is 5 + 7?\\nAnswer: Let's think step by step.\"\n",
    "chain = generate_chain(test_prompt)\n",
    "print(f\"Generated chain: {chain}\")\n",
    "print(f\"Extracted answer: {extract_number(chain)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Implementing Self-Consistency\n",
    "\n",
    "Now let's build the full self-consistency pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfConsistency:\n",
    "    \"\"\"\n",
    "    Self-Consistency for chain-of-thought reasoning.\n",
    "    \n",
    "    Sample multiple reasoning paths, extract answers, take majority vote.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device=\"cuda\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    \n",
    "    def sample_chains(self, prompt: str, n_samples: int = 5,\n",
    "                      max_new_tokens: int = 100,\n",
    "                      temperature: float = 0.7) -> list:\n",
    "        \"\"\"\n",
    "        Generate N independent reasoning chains.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The CoT prompt\n",
    "            n_samples: Number of chains to generate\n",
    "            max_new_tokens: Max tokens per chain\n",
    "            temperature: Sampling temperature (higher = more diversity)\n",
    "        \n",
    "        Returns:\n",
    "            List of (chain, extracted_answer) tuples\n",
    "        \"\"\"\n",
    "        chains = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            chain = generate_chain(\n",
    "                prompt, \n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            answer = extract_number(chain)\n",
    "            chains.append((chain, answer))\n",
    "        \n",
    "        return chains\n",
    "    \n",
    "    def majority_vote(self, answers: list) -> tuple:\n",
    "        \"\"\"\n",
    "        Take majority vote across answers.\n",
    "        \n",
    "        Args:\n",
    "            answers: List of extracted answers (some may be None)\n",
    "        \n",
    "        Returns:\n",
    "            (most_common_answer, vote_count, total_valid)\n",
    "        \"\"\"\n",
    "        # Filter out None answers\n",
    "        valid_answers = [a for a in answers if a is not None]\n",
    "        \n",
    "        if not valid_answers:\n",
    "            return None, 0, 0\n",
    "        \n",
    "        # Count votes\n",
    "        counter = Counter(valid_answers)\n",
    "        most_common, count = counter.most_common(1)[0]\n",
    "        \n",
    "        return most_common, count, len(valid_answers)\n",
    "    \n",
    "    def solve(self, question: str, n_samples: int = 5,\n",
    "              max_new_tokens: int = 100,\n",
    "              temperature: float = 0.7) -> dict:\n",
    "        \"\"\"\n",
    "        Solve a problem using self-consistency.\n",
    "        \n",
    "        Args:\n",
    "            question: The problem to solve\n",
    "            n_samples: Number of reasoning chains to sample\n",
    "            max_new_tokens: Max tokens per chain\n",
    "            temperature: Sampling temperature\n",
    "        \n",
    "        Returns:\n",
    "            Dict with answer, confidence, chains, and vote details\n",
    "        \"\"\"\n",
    "        # Build CoT prompt\n",
    "        prompt = f\"Question: {question}\\nAnswer: Let's think step by step.\"\n",
    "        \n",
    "        # Sample chains\n",
    "        chains = self.sample_chains(\n",
    "            prompt, \n",
    "            n_samples=n_samples,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Extract answers and vote\n",
    "        answers = [answer for _, answer in chains]\n",
    "        final_answer, votes, total_valid = self.majority_vote(answers)\n",
    "        \n",
    "        # Calculate confidence as vote fraction\n",
    "        confidence = votes / total_valid if total_valid > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"answer\": final_answer,\n",
    "            \"confidence\": confidence,\n",
    "            \"votes\": votes,\n",
    "            \"total_samples\": n_samples,\n",
    "            \"valid_samples\": total_valid,\n",
    "            \"chains\": chains,\n",
    "            \"vote_distribution\": Counter(answers)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create self-consistency solver\n",
    "sc = SelfConsistency(model, tokenizer, device=device)\n",
    "print(\"Self-consistency solver ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test self-consistency on a math problem\n",
    "question = \"A store has 45 books. They sell 12 and receive 30 more. How many books do they have?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Correct answer: 45 - 12 + 30 = 63\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Sampling 5 reasoning chains...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "result = sc.solve(question, n_samples=5, temperature=0.8)\n",
    "\n",
    "# Show each chain\n",
    "for i, (chain, answer) in enumerate(result[\"chains\"], 1):\n",
    "    print(f\"Chain {i}: {chain[:80]}...\")\n",
    "    print(f\"  → Extracted answer: {answer}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Vote distribution: {dict(result['vote_distribution'])}\")\n",
    "print(f\"Final answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.1%} ({result['votes']}/{result['valid_samples']} votes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## The Math Behind Self-Consistency\n",
    "\n",
    "Why does majority voting work? Let's think about it probabilistically.\n",
    "\n",
    "Suppose the model has probability $p$ of generating a correct reasoning chain. If we sample $N$ chains and vote:\n",
    "\n",
    "$$P(\\text{majority correct}) = \\sum_{k=\\lceil N/2 \\rceil}^{N} \\binom{N}{k} p^k (1-p)^{N-k}$$\n",
    "\n",
    "This is just the binomial distribution. For $p > 0.5$ (the model is more often right than wrong), majority voting *amplifies* accuracy.\n",
    "\n",
    "**Example:**\n",
    "- Model accuracy per chain: $p = 0.6$\n",
    "- With 1 sample: 60% chance of correct answer\n",
    "- With 5 samples (majority vote): ~68% chance of correct answer\n",
    "- With 10 samples: ~74% chance\n",
    "- With 40 samples: ~90% chance\n",
    "\n",
    "The more you sample, the more reliable the answer—as long as the model is better than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def majority_vote_accuracy(p, n):\n",
    "    \"\"\"\n",
    "    Calculate probability that majority vote gives correct answer.\n",
    "    \n",
    "    Args:\n",
    "        p: Probability each individual sample is correct\n",
    "        n: Number of samples\n",
    "    \n",
    "    Returns:\n",
    "        Probability of correct majority\n",
    "    \"\"\"\n",
    "    # Need more than half to be correct\n",
    "    # For n samples, we need at least ceil(n/2) correct\n",
    "    k_min = (n // 2) + 1\n",
    "    \n",
    "    # Sum probability of getting k_min, k_min+1, ..., n correct\n",
    "    return 1 - stats.binom.cdf(k_min - 1, n, p)\n",
    "\n",
    "\n",
    "# Plot the improvement from self-consistency\n",
    "sample_sizes = [1, 3, 5, 9, 15, 21, 41]\n",
    "base_accuracies = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for p in base_accuracies:\n",
    "    accuracies = [majority_vote_accuracy(p, n) for n in sample_sizes]\n",
    "    plt.plot(sample_sizes, accuracies, 'o-', label=f'Base p={p}', linewidth=2, markersize=8)\n",
    "\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Number of Samples (N)', fontsize=12)\n",
    "plt.ylabel('Probability of Correct Majority', fontsize=12)\n",
    "plt.title('Self-Consistency Improves Accuracy\\n(as long as base accuracy > 50%)', fontsize=14)\n",
    "plt.legend(title='Single-sample accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: If the model is right more often than not (p > 0.5),\")\n",
    "print(\"sampling more chains and voting will push accuracy toward 100%.\")\n",
    "print(\"\\nBut if p < 0.5, voting makes things WORSE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Diversity Matters: Temperature\n",
    "\n",
    "For self-consistency to work, we need the reasoning chains to be *different*. If we sample 10 identical chains, voting is useless.\n",
    "\n",
    "This is why **temperature** matters.\n",
    "\n",
    "Temperature controls how \"random\" the sampling is:\n",
    "- **Temperature = 0**: Always pick the most likely token (deterministic)\n",
    "- **Temperature = 0.7**: Some randomness (typical for creative tasks)\n",
    "- **Temperature = 1.0**: Full probability distribution\n",
    "- **Temperature > 1.0**: Even more random (often incoherent)\n",
    "\n",
    "For self-consistency, we typically use temperature 0.7-0.8. This gives enough diversity to explore different reasoning paths while keeping each chain coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of temperature on diversity\n",
    "question = \"What is 15 + 27?\"\n",
    "prompt = f\"Question: {question}\\nAnswer: Let's think step by step.\"\n",
    "\n",
    "temperatures = [0.3, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    chains = []\n",
    "    for i in range(3):\n",
    "        chain = generate_chain(prompt, max_new_tokens=50, temperature=temp)\n",
    "        chains.append(chain)\n",
    "        print(f\"\\nSample {i+1}: {chain[:60]}...\")\n",
    "    \n",
    "    # Check diversity: how many unique answers?\n",
    "    answers = [extract_number(c) for c in chains]\n",
    "    unique = len(set(a for a in answers if a is not None))\n",
    "    print(f\"\\nUnique answers: {unique}/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## The Cost Trade-off\n",
    "\n",
    "Self-consistency improves accuracy, but at a cost:\n",
    "\n",
    "| Metric | Single Chain | 5 Samples | 10 Samples | 40 Samples |\n",
    "|--------|-------------|-----------|------------|------------|\n",
    "| Tokens generated | 1× | 5× | 10× | 40× |\n",
    "| Latency | 1× | ~5× | ~10× | ~40× |\n",
    "| API cost | 1× | 5× | 10× | 40× |\n",
    "\n",
    "(Latency can be reduced with parallel sampling, but cost scales linearly.)\n",
    "\n",
    "Is it worth it? Depends on your use case:\n",
    "\n",
    "- **High-stakes decisions**: Yes! Sample 40+ chains.\n",
    "- **Math competitions/exams**: Absolutely.\n",
    "- **Casual chat**: Probably overkill.\n",
    "- **Real-time applications**: Probably not feasible.\n",
    "\n",
    "The original paper used 40 samples for their best results. But even 5-10 samples gives substantial improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Weighted Voting\n",
    "\n",
    "Basic self-consistency treats all chains equally. But what if some chains are more \"trustworthy\" than others?\n",
    "\n",
    "**Weighted self-consistency** assigns weights to each chain, typically based on:\n",
    "\n",
    "1. **Token probabilities**: Chains where the model was more confident get higher weight\n",
    "2. **Length**: Longer, more detailed chains might be more reliable\n",
    "3. **Verifier scores**: Use a trained model to score chain quality\n",
    "\n",
    "The third option leads us toward **Process Reward Models**—but that's a later notebook.\n",
    "\n",
    "For now, let's implement probability-weighted voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chain_with_score(prompt: str, max_new_tokens: int = 100,\n",
    "                               temperature: float = 0.7) -> tuple:\n",
    "    \"\"\"\n",
    "    Generate a chain and compute its log probability.\n",
    "    \n",
    "    The log probability indicates how \"confident\" the model was\n",
    "    when generating this particular chain.\n",
    "    \n",
    "    Returns:\n",
    "        (chain_text, log_probability)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "    \n",
    "    # Get generated token IDs (excluding prompt)\n",
    "    generated_ids = outputs.sequences[0, input_len:]\n",
    "    \n",
    "    # Compute log probability of the generated sequence\n",
    "    # scores[i] has shape (batch, vocab) for token i\n",
    "    log_prob = 0.0\n",
    "    for i, token_id in enumerate(generated_ids):\n",
    "        if i >= len(outputs.scores):\n",
    "            break\n",
    "        logits = outputs.scores[i][0]  # Shape: (vocab_size,)\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        log_prob += log_probs[token_id].item()\n",
    "    \n",
    "    # Normalize by length (average log prob per token)\n",
    "    avg_log_prob = log_prob / max(len(generated_ids), 1)\n",
    "    \n",
    "    # Decode the chain\n",
    "    chain = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return chain, avg_log_prob\n",
    "\n",
    "\n",
    "def weighted_majority_vote(chains_with_scores: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Weighted majority vote based on log probabilities.\n",
    "    \n",
    "    Args:\n",
    "        chains_with_scores: List of (chain, answer, log_prob) tuples\n",
    "    \n",
    "    Returns:\n",
    "        (best_answer, weight_sum, total_weight)\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Convert log probs to weights (using softmax for stability)\n",
    "    log_probs = [lp for _, _, lp in chains_with_scores]\n",
    "    \n",
    "    # Softmax: exp(log_prob) / sum(exp(log_probs))\n",
    "    max_lp = max(log_probs)\n",
    "    exp_lps = [np.exp(lp - max_lp) for lp in log_probs]  # Subtract max for stability\n",
    "    total = sum(exp_lps)\n",
    "    weights = [e / total for e in exp_lps]\n",
    "    \n",
    "    # Weighted vote\n",
    "    answer_weights = defaultdict(float)\n",
    "    for (_, answer, _), weight in zip(chains_with_scores, weights):\n",
    "        if answer is not None:\n",
    "            answer_weights[answer] += weight\n",
    "    \n",
    "    if not answer_weights:\n",
    "        return None, 0, 0\n",
    "    \n",
    "    best_answer = max(answer_weights, key=answer_weights.get)\n",
    "    return best_answer, answer_weights[best_answer], sum(answer_weights.values())\n",
    "\n",
    "\n",
    "# Demo weighted voting\n",
    "print(\"Generating chains with confidence scores...\\n\")\n",
    "\n",
    "question = \"What is 8 + 15?\"\n",
    "prompt = f\"Question: {question}\\nAnswer: Let's think step by step.\"\n",
    "\n",
    "chains_with_scores = []\n",
    "for i in range(5):\n",
    "    chain, log_prob = generate_chain_with_score(prompt, max_new_tokens=50)\n",
    "    answer = extract_number(chain)\n",
    "    chains_with_scores.append((chain, answer, log_prob))\n",
    "    print(f\"Chain {i+1}:\")\n",
    "    print(f\"  Text: {chain[:50]}...\")\n",
    "    print(f\"  Answer: {answer}\")\n",
    "    print(f\"  Avg log prob: {log_prob:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Compare voting methods\n",
    "answers = [a for _, a, _ in chains_with_scores]\n",
    "unweighted_answer, votes, _ = sc.majority_vote(answers)\n",
    "weighted_answer, weight, _ = weighted_majority_vote(chains_with_scores)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Unweighted vote: {unweighted_answer}\")\n",
    "print(f\"Weighted vote:   {weighted_answer}\")\n",
    "print(f\"Correct answer:  23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Benchmark Results\n",
    "\n",
    "From the original self-consistency paper (Wang et al., 2022):\n",
    "\n",
    "| Benchmark | CoT (1 sample) | Self-Consistency (40 samples) | Improvement |\n",
    "|-----------|----------------|------------------------------|-------------|\n",
    "| GSM8K | 56.5% | 74.4% | +17.9% |\n",
    "| SVAMP | 78.7% | 86.6% | +7.9% |\n",
    "| AQuA | 48.0% | 52.0% | +4.0% |\n",
    "| ASDiv | 79.6% | 87.0% | +7.4% |\n",
    "\n",
    "(Results with GPT-3 text-davinci-002)\n",
    "\n",
    "The gains are particularly strong on harder benchmarks (GSM8K) where the base CoT accuracy is lower. Self-consistency helps more when there's more room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Efficient Self-Consistency: Early Stopping\n",
    "\n",
    "One optimization: we don't always need all $N$ samples.\n",
    "\n",
    "If the first 5 samples all give the same answer, we can be pretty confident. Why sample 35 more?\n",
    "\n",
    "**RASC (Reasoning-Aware Self-Consistency)** does exactly this:\n",
    "1. Sample chains sequentially\n",
    "2. Track a running confidence score\n",
    "3. Stop when confidence exceeds a threshold\n",
    "\n",
    "This can reduce compute by 40%+ while maintaining accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency_early_stop(question: str, max_samples: int = 20,\n",
    "                                 confidence_threshold: float = 0.8,\n",
    "                                 min_samples: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Self-consistency with early stopping.\n",
    "    \n",
    "    Stop sampling when we're confident enough in the answer.\n",
    "    \n",
    "    Args:\n",
    "        question: The problem to solve\n",
    "        max_samples: Maximum chains to generate\n",
    "        confidence_threshold: Stop if vote share exceeds this\n",
    "        min_samples: Always generate at least this many\n",
    "    \n",
    "    Returns:\n",
    "        Dict with answer, samples_used, and chains\n",
    "    \"\"\"\n",
    "    prompt = f\"Question: {question}\\nAnswer: Let's think step by step.\"\n",
    "    \n",
    "    chains = []\n",
    "    answers = []\n",
    "    \n",
    "    for i in range(max_samples):\n",
    "        # Generate one chain\n",
    "        chain = generate_chain(prompt, max_new_tokens=80, temperature=0.7)\n",
    "        answer = extract_number(chain)\n",
    "        \n",
    "        chains.append(chain)\n",
    "        answers.append(answer)\n",
    "        \n",
    "        # Check if we can stop early\n",
    "        if i + 1 >= min_samples:\n",
    "            valid = [a for a in answers if a is not None]\n",
    "            if valid:\n",
    "                counter = Counter(valid)\n",
    "                top_answer, top_count = counter.most_common(1)[0]\n",
    "                confidence = top_count / len(valid)\n",
    "                \n",
    "                if confidence >= confidence_threshold:\n",
    "                    return {\n",
    "                        \"answer\": top_answer,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"samples_used\": i + 1,\n",
    "                        \"max_samples\": max_samples,\n",
    "                        \"early_stopped\": True,\n",
    "                        \"chains\": list(zip(chains, answers))\n",
    "                    }\n",
    "    \n",
    "    # Didn't stop early—use all samples\n",
    "    valid = [a for a in answers if a is not None]\n",
    "    if valid:\n",
    "        counter = Counter(valid)\n",
    "        top_answer, top_count = counter.most_common(1)[0]\n",
    "        confidence = top_count / len(valid)\n",
    "    else:\n",
    "        top_answer, confidence = None, 0.0\n",
    "    \n",
    "    return {\n",
    "        \"answer\": top_answer,\n",
    "        \"confidence\": confidence,\n",
    "        \"samples_used\": max_samples,\n",
    "        \"max_samples\": max_samples,\n",
    "        \"early_stopped\": False,\n",
    "        \"chains\": list(zip(chains, answers))\n",
    "    }\n",
    "\n",
    "\n",
    "# Demo early stopping\n",
    "print(\"Testing self-consistency with early stopping...\\n\")\n",
    "\n",
    "question = \"A farmer has 30 cows. He sells 10 and buys 15 more. How many cows does he have?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Correct: 30 - 10 + 15 = 35\\n\")\n",
    "\n",
    "result = self_consistency_early_stop(\n",
    "    question,\n",
    "    max_samples=10,\n",
    "    confidence_threshold=0.7\n",
    ")\n",
    "\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.1%}\")\n",
    "print(f\"Samples used: {result['samples_used']}/{result['max_samples']}\")\n",
    "print(f\"Early stopped: {result['early_stopped']}\")\n",
    "\n",
    "if result['early_stopped']:\n",
    "    savings = (result['max_samples'] - result['samples_used']) / result['max_samples']\n",
    "    print(f\"Compute saved: {savings:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Limitations of Self-Consistency\n",
    "\n",
    "Self-consistency is powerful but has limits:\n",
    "\n",
    "### 1. Requires p > 0.5\n",
    "\n",
    "If the model gets it wrong more often than right, voting makes things *worse*. The majority will be wrong!\n",
    "\n",
    "### 2. All-or-Nothing Answers\n",
    "\n",
    "Voting works great for problems with discrete answers (numbers, multiple choice). For open-ended generation, it's not clear how to \"vote\" between different phrasings.\n",
    "\n",
    "### 3. No Reasoning Verification\n",
    "\n",
    "We vote on the *answer*, not the *reasoning*. Two chains might reach the same answer for completely different reasons—one correct, one flawed. Voting doesn't distinguish them.\n",
    "\n",
    "### 4. Linear Compute Cost\n",
    "\n",
    "Sampling 40 chains costs 40× as much as sampling 1. For latency-sensitive applications, this can be prohibitive.\n",
    "\n",
    "---\n",
    "\n",
    "These limitations motivate more sophisticated approaches:\n",
    "\n",
    "- **Tree of Thoughts**: Explore reasoning paths more efficiently\n",
    "- **Process Reward Models**: Verify the reasoning, not just the answer\n",
    "- **MCTS**: Smart search instead of brute-force sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## What We've Learned\n",
    "\n",
    "Self-consistency is a simple, powerful technique:\n",
    "\n",
    "1. **Sample** multiple reasoning chains with temperature > 0\n",
    "2. **Extract** answers from each chain\n",
    "3. **Vote** — majority wins\n",
    "\n",
    "The math: if individual accuracy is $p > 0.5$, majority voting amplifies accuracy.\n",
    "\n",
    "$$P(\\text{majority correct}) = \\sum_{k=\\lceil N/2 \\rceil}^{N} \\binom{N}{k} p^k (1-p)^{N-k}$$\n",
    "\n",
    "Practical tips:\n",
    "- Use temperature 0.7-0.8 for diversity\n",
    "- 5-10 samples often enough; 40 for best results\n",
    "- Consider early stopping to save compute\n",
    "- Weighted voting can help but adds complexity\n",
    "\n",
    "**Next up:** Tree of Thoughts — what if we could explore reasoning paths more intelligently?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}