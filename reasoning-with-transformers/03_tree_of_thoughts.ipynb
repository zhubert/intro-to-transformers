{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tree of Thoughts\n",
    "\n",
    "Chain-of-thought is linear: one step follows another, marching toward an answer. But human problem-solving isn't linear. We explore options, hit dead ends, backtrack, and try different approaches.\n",
    "\n",
    "Tree of Thoughts (ToT) brings that flexibility to language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## From Chain to Tree\n",
    "\n",
    "Compare the two paradigms:\n",
    "\n",
    "**Chain-of-Thought:**\n",
    "```\n",
    "Question \u2192 Step 1 \u2192 Step 2 \u2192 Step 3 \u2192 Answer\n",
    "                                      (hope it's right!)\n",
    "```\n",
    "\n",
    "**Tree of Thoughts:**\n",
    "```\n",
    "                    Question\n",
    "                       \u2502\n",
    "           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "           \u25bc           \u25bc           \u25bc\n",
    "        Step 1a     Step 1b     Step 1c\n",
    "           \u2502           \u2502           \u2717 (bad idea, prune)\n",
    "       \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n",
    "       \u25bc       \u25bc   \u25bc       \u25bc\n",
    "    Step 2a  Step 2b  Step 2c  Step 2d\n",
    "       \u2502       \u2717       \u2502       \u2502\n",
    "       \u25bc               \u25bc       \u25bc\n",
    "    Answer          Answer  Answer\n",
    "                       \u2713\n",
    "```\n",
    "\n",
    "Key differences:\n",
    "\n",
    "1. **Branching**: At each step, generate *multiple* possible continuations\n",
    "2. **Evaluation**: Score each branch to identify promising vs. hopeless paths\n",
    "3. **Pruning**: Abandon bad branches early (save compute!)\n",
    "4. **Backtracking**: If a path leads nowhere, try a different one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The ToT Framework\n",
    "\n",
    "Tree of Thoughts has four key components:\n",
    "\n",
    "### 1. Thought Decomposition\n",
    "How do we break the problem into \"thoughts\" (intermediate steps)?\n",
    "\n",
    "- For math: each computation step\n",
    "- For writing: each paragraph or idea\n",
    "- For puzzles: each move or decision\n",
    "\n",
    "### 2. Thought Generation\n",
    "How do we generate candidate thoughts at each step?\n",
    "\n",
    "- **Sample**: Generate N thoughts independently (like self-consistency)\n",
    "- **Propose**: Have the model propose several alternatives in one call\n",
    "\n",
    "### 3. Thought Evaluation\n",
    "How do we know which thoughts are promising?\n",
    "\n",
    "- **Self-evaluation**: Ask the model to rate its own thoughts\n",
    "- **Voting**: Generate continuations, see which lead to success\n",
    "- **Heuristics**: Problem-specific scoring functions\n",
    "\n",
    "### 4. Search Algorithm\n",
    "How do we navigate the tree?\n",
    "\n",
    "- **Breadth-First Search (BFS)**: Explore all options at each level before going deeper\n",
    "- **Depth-First Search (DFS)**: Go deep first, backtrack on failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec125d7033541bd919952b1291b9d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3655898d90c4851a265f4fbc0c19019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc63361e6942462195e404f745c4ddd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e949a47a01da42e7906ca085dac761ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d21d01e18b418581044ee32987d3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce12b0a0a7f4070b645ee9ce3a02ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9604359529e4c1daa1f2d367d2149ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Callable\n",
    "import re\n",
    "\n",
    "# Load model\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:23:19.800241Z",
     "iopub.status.busy": "2025-12-10T21:23:19.800058Z",
     "iopub.status.idle": "2025-12-10T21:23:19.803443Z",
     "shell.execute_reply": "2025-12-10T21:23:19.803167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThoughtNode defined.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ThoughtNode:\n",
    "    \"\"\"\n",
    "    A node in the Tree of Thoughts.\n",
    "    \n",
    "    Each node represents a partial solution state.\n",
    "    \"\"\"\n",
    "    thought: str           # The reasoning step at this node\n",
    "    state: str             # Full state (all thoughts so far)\n",
    "    score: float = 0.0     # Evaluation score\n",
    "    depth: int = 0         # How deep in the tree\n",
    "    parent: Optional['ThoughtNode'] = None\n",
    "    children: List['ThoughtNode'] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.children is None:\n",
    "            self.children = []\n",
    "    \n",
    "    def get_path(self) -> List[str]:\n",
    "        \"\"\"Get the full reasoning path from root to this node.\"\"\"\n",
    "        path = []\n",
    "        node = self\n",
    "        while node is not None:\n",
    "            path.append(node.thought)\n",
    "            node = node.parent\n",
    "        return list(reversed(path))\n",
    "\n",
    "\n",
    "def generate_text(prompt: str, max_new_tokens: int = 50, \n",
    "                  temperature: float = 0.7) -> str:\n",
    "    \"\"\"Generate text continuation.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return full_response[len(prompt):].strip()\n",
    "\n",
    "\n",
    "print(\"ThoughtNode defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Example: Game of 24\n",
    "\n",
    "The original ToT paper used the \"Game of 24\" as a key benchmark. The rules:\n",
    "\n",
    "- You're given 4 numbers (e.g., 1, 2, 3, 4)\n",
    "- Use +, -, \u00d7, \u00f7 to make 24\n",
    "- Use each number exactly once\n",
    "\n",
    "Example: 1, 2, 3, 4 \u2192 (1 + 2 + 3) \u00d7 4 = 24 \u2713\n",
    "\n",
    "This is perfect for ToT because:\n",
    "- There are many possible paths (which operation to try first?)\n",
    "- Some paths are dead ends (can't reach 24)\n",
    "- We can evaluate intermediate states (\"is 24 still reachable?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:23:19.804195Z",
     "iopub.status.busy": "2025-12-10T21:23:19.804120Z",
     "iopub.status.idle": "2025-12-10T21:23:22.421340Z",
     "shell.execute_reply": "2025-12-10T21:23:22.421021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating candidate thoughts...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. To create the number 24 using the numbers 2, 3, 4, and 6 along with the operations of addition (+), subtraction (-), multiplication (\u00d7), and division (\u00f7), we need to find a combination that equals 24. Let's explore three different combinations:\n",
      "2. ### Combination 1:\n",
      "3. \\[ 6 \\times 4 - 3 - 2 = 24 \\]\n"
     ]
    }
   ],
   "source": [
    "def generate_thoughts(state: str, n_thoughts: int = 3,\n",
    "                      problem_type: str = \"math\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate multiple candidate next thoughts.\n",
    "    \n",
    "    We ask the model to propose several possible next steps.\n",
    "    \n",
    "    Args:\n",
    "        state: Current reasoning state\n",
    "        n_thoughts: Number of candidates to generate\n",
    "        problem_type: Type of problem (affects prompting)\n",
    "    \n",
    "    Returns:\n",
    "        List of candidate next thoughts\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"{state}\n",
    "\n",
    "What are {n_thoughts} different possible next steps? List them as:\n",
    "1.\n",
    "2.\n",
    "3.\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, max_new_tokens=100, temperature=0.8)\n",
    "    \n",
    "    # Parse numbered list\n",
    "    thoughts = []\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        # Match lines starting with a number\n",
    "        match = re.match(r'^\\d+\\.\\s*(.+)', line.strip())\n",
    "        if match:\n",
    "            thoughts.append(match.group(1).strip())\n",
    "    \n",
    "    # If parsing failed, just split by newlines\n",
    "    if len(thoughts) < n_thoughts and lines:\n",
    "        thoughts = [l.strip() for l in lines if l.strip()][:n_thoughts]\n",
    "    \n",
    "    return thoughts[:n_thoughts]\n",
    "\n",
    "\n",
    "# Test thought generation\n",
    "test_state = \"\"\"Problem: Use 2, 3, 4, 6 with +, -, \u00d7, \u00f7 to make 24.\n",
    "Current: I have numbers 2, 3, 4, 6.\"\"\"\n",
    "\n",
    "print(\"Generating candidate thoughts...\\n\")\n",
    "thoughts = generate_thoughts(test_state, n_thoughts=3)\n",
    "for i, t in enumerate(thoughts, 1):\n",
    "    print(f\"{i}. {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Self-Evaluation\n",
    "\n",
    "The key to ToT is evaluating which thoughts are promising. We can ask the model to rate its own ideas.\n",
    "\n",
    "For Game of 24, we might ask:\n",
    "- \"Can you still make 24 from the remaining numbers?\"\n",
    "- Rate as: sure / likely / unlikely / impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:23:22.422523Z",
     "iopub.status.busy": "2025-12-10T21:23:22.422426Z",
     "iopub.status.idle": "2025-12-10T21:23:22.907149Z",
     "shell.execute_reply": "2025-12-10T21:23:22.906830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating thoughts:\n",
      "\n",
      "  The word 'Awesome' might work.\n",
      "  \u2192 Score: 0.40\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The word 'Sad' could be it.\n",
      "  \u2192 Score: 0.70\n",
      "\n",
      "  I'll think about 'Afraid'.\n",
      "  \u2192 Score: 0.40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_thought(state: str, thought: str,\n",
    "                     evaluation_prompt: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Have the model evaluate a thought on a 0-1 scale.\n",
    "    \n",
    "    We ask the model to rate the thought as:\n",
    "    - sure (1.0): This will definitely work\n",
    "    - likely (0.7): This looks promising\n",
    "    - possible (0.4): Maybe, not sure\n",
    "    - unlikely (0.1): Probably won't work\n",
    "    \n",
    "    Args:\n",
    "        state: Current problem state\n",
    "        thought: The candidate thought to evaluate\n",
    "        evaluation_prompt: Custom evaluation prompt\n",
    "    \n",
    "    Returns:\n",
    "        Score from 0.0 to 1.0\n",
    "    \"\"\"\n",
    "    if evaluation_prompt is None:\n",
    "        evaluation_prompt = f\"\"\"{state}\n",
    "\n",
    "Proposed next step: {thought}\n",
    "\n",
    "Evaluate this step. Is it a good approach to solving the problem?\n",
    "Rate as: sure / likely / possible / unlikely\n",
    "\n",
    "Rating:\"\"\"\n",
    "    \n",
    "    response = generate_text(evaluation_prompt, max_new_tokens=10, temperature=0.3)\n",
    "    response_lower = response.lower().strip()\n",
    "    \n",
    "    # Parse the rating\n",
    "    if 'sure' in response_lower:\n",
    "        return 1.0\n",
    "    elif 'likely' in response_lower:\n",
    "        return 0.7\n",
    "    elif 'possible' in response_lower:\n",
    "        return 0.4\n",
    "    elif 'unlikely' in response_lower or 'impossible' in response_lower:\n",
    "        return 0.1\n",
    "    else:\n",
    "        # Default to middle score if we can't parse\n",
    "        return 0.5\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "state = \"Problem: Find a word that starts with 'A' and means 'happy'.\"\n",
    "thoughts = [\"The word 'Awesome' might work.\", \"The word 'Sad' could be it.\", \"I'll think about 'Afraid'.\"]\n",
    "\n",
    "print(\"Evaluating thoughts:\\n\")\n",
    "for thought in thoughts:\n",
    "    score = evaluate_thought(state, thought)\n",
    "    print(f\"  {thought}\")\n",
    "    print(f\"  \u2192 Score: {score:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Breadth-First Search (BFS)\n",
    "\n",
    "BFS explores the tree level by level:\n",
    "1. Start with the initial state\n",
    "2. Generate all possible first steps\n",
    "3. Evaluate and keep the best $k$ (\"beam width\")\n",
    "4. For each kept state, generate next steps\n",
    "5. Repeat until reaching a solution or max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:23:22.908091Z",
     "iopub.status.busy": "2025-12-10T21:23:22.908018Z",
     "iopub.status.idle": "2025-12-10T21:23:22.911771Z",
     "shell.execute_reply": "2025-12-10T21:23:22.911532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFS Tree of Thoughts solver ready.\n"
     ]
    }
   ],
   "source": [
    "class TreeOfThoughtsBFS:\n",
    "    \"\"\"\n",
    "    Tree of Thoughts with Breadth-First Search.\n",
    "    \n",
    "    Explores all options at each level, keeps top-k,\n",
    "    then goes deeper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device=\"cuda\",\n",
    "                 n_candidates: int = 3, beam_width: int = 2, max_depth: int = 3):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.n_candidates = n_candidates  # How many thoughts to generate per node\n",
    "        self.beam_width = beam_width      # How many nodes to keep at each level\n",
    "        self.max_depth = max_depth        # How deep to search\n",
    "    \n",
    "    def solve(self, problem: str, is_solution: Callable[[str], bool] = None) -> dict:\n",
    "        \"\"\"\n",
    "        Solve a problem using BFS over thoughts.\n",
    "        \n",
    "        Args:\n",
    "            problem: The problem statement\n",
    "            is_solution: Function to check if a state is a solution\n",
    "        \n",
    "        Returns:\n",
    "            Dict with best solution, path, and all explored nodes\n",
    "        \"\"\"\n",
    "        # Initialize with root node\n",
    "        root = ThoughtNode(\n",
    "            thought=\"Start\",\n",
    "            state=f\"Problem: {problem}\\n\\nLet me think step by step.\",\n",
    "            score=1.0,\n",
    "            depth=0\n",
    "        )\n",
    "        \n",
    "        current_level = [root]\n",
    "        all_nodes = [root]\n",
    "        best_solution = None\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        for depth in range(self.max_depth):\n",
    "            print(f\"\\nDepth {depth + 1}: Exploring {len(current_level)} nodes...\")\n",
    "            \n",
    "            next_level = []\n",
    "            \n",
    "            for node in current_level:\n",
    "                # Generate candidate thoughts\n",
    "                thoughts = generate_thoughts(\n",
    "                    node.state, \n",
    "                    n_thoughts=self.n_candidates\n",
    "                )\n",
    "                \n",
    "                for thought in thoughts:\n",
    "                    # Create new state\n",
    "                    new_state = f\"{node.state}\\n\\nStep {depth + 1}: {thought}\"\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    score = evaluate_thought(node.state, thought)\n",
    "                    \n",
    "                    # Create child node\n",
    "                    child = ThoughtNode(\n",
    "                        thought=thought,\n",
    "                        state=new_state,\n",
    "                        score=score,\n",
    "                        depth=depth + 1,\n",
    "                        parent=node\n",
    "                    )\n",
    "                    node.children.append(child)\n",
    "                    all_nodes.append(child)\n",
    "                    next_level.append(child)\n",
    "                    \n",
    "                    # Check if solution\n",
    "                    if is_solution and is_solution(new_state):\n",
    "                        if score > best_score:\n",
    "                            best_solution = child\n",
    "                            best_score = score\n",
    "            \n",
    "            # Keep only top-k nodes for next level\n",
    "            next_level.sort(key=lambda n: n.score, reverse=True)\n",
    "            current_level = next_level[:self.beam_width]\n",
    "            \n",
    "            print(f\"  Generated {len(next_level)} candidates, kept {len(current_level)}\")\n",
    "            for node in current_level:\n",
    "                print(f\"    Score {node.score:.2f}: {node.thought[:50]}...\")\n",
    "        \n",
    "        # If no explicit solution found, return best final node\n",
    "        if best_solution is None and current_level:\n",
    "            best_solution = max(current_level, key=lambda n: n.score)\n",
    "        \n",
    "        return {\n",
    "            \"solution\": best_solution,\n",
    "            \"path\": best_solution.get_path() if best_solution else [],\n",
    "            \"final_state\": best_solution.state if best_solution else None,\n",
    "            \"all_nodes\": all_nodes,\n",
    "            \"nodes_explored\": len(all_nodes)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create ToT solver\n",
    "tot_bfs = TreeOfThoughtsBFS(\n",
    "    model, tokenizer, device=device,\n",
    "    n_candidates=2,  # Generate 2 candidates per node\n",
    "    beam_width=2,    # Keep top 2 at each level\n",
    "    max_depth=2      # Go 2 levels deep\n",
    ")\n",
    "\n",
    "print(\"BFS Tree of Thoughts solver ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:23:22.912540Z",
     "iopub.status.busy": "2025-12-10T21:23:22.912467Z",
     "iopub.status.idle": "2025-12-10T21:23:28.620850Z",
     "shell.execute_reply": "2025-12-10T21:23:28.620477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: What is 15 + 28? Show your reasoning.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Depth 1: Exploring 1 nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 2 candidates, kept 2\n",
      "    Score 0.70: Count up 28 more units from 15....\n",
      "    Score 0.50: Start with 15 on the right side of the plus sign....\n",
      "\n",
      "Depth 2: Exploring 2 nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 4 candidates, kept 2\n",
      "    Score 0.70: Step 2: Determine which of the two sums you get in...\n",
      "    Score 0.70: Which sum do I want to use for my final answer?...\n",
      "\n",
      "============================================================\n",
      "SOLUTION PATH:\n",
      "============================================================\n",
      "Step 0: Start\n",
      "Step 1: Count up 28 more units from 15.\n",
      "Step 2: Step 2: Determine which of the two sums you get in Step 1 is correct.\n",
      "\n",
      "Total nodes explored: 7\n"
     ]
    }
   ],
   "source": [
    "# Test ToT-BFS on a simple problem\n",
    "problem = \"What is 15 + 28? Show your reasoning.\"\n",
    "\n",
    "print(f\"Problem: {problem}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "result = tot_bfs.solve(problem)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SOLUTION PATH:\")\n",
    "print(\"=\"*60)\n",
    "for i, step in enumerate(result[\"path\"]):\n",
    "    print(f\"Step {i}: {step}\")\n",
    "\n",
    "print(f\"\\nTotal nodes explored: {result['nodes_explored']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Depth-First Search (DFS)\n",
    "\n",
    "DFS explores deeply first, then backtracks:\n",
    "\n",
    "1. Start with initial state\n",
    "2. Generate candidates for next step\n",
    "3. Pick the best one and go deeper\n",
    "4. If stuck (bad evaluation), backtrack and try another branch\n",
    "\n",
    "DFS is more memory-efficient than BFS and can find solutions faster for some problems. But it might miss better solutions on other branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:23:28.621773Z",
     "iopub.status.busy": "2025-12-10T21:23:28.621700Z",
     "iopub.status.idle": "2025-12-10T21:23:28.625205Z",
     "shell.execute_reply": "2025-12-10T21:23:28.624938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFS Tree of Thoughts solver ready.\n"
     ]
    }
   ],
   "source": [
    "class TreeOfThoughtsDFS:\n",
    "    \"\"\"\n",
    "    Tree of Thoughts with Depth-First Search.\n",
    "    \n",
    "    Explores deeply first, backtracks on failure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device=\"cuda\",\n",
    "                 n_candidates: int = 3, max_depth: int = 4,\n",
    "                 prune_threshold: float = 0.2):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.n_candidates = n_candidates\n",
    "        self.max_depth = max_depth\n",
    "        self.prune_threshold = prune_threshold  # Don't explore if score < this\n",
    "        self.nodes_explored = 0\n",
    "    \n",
    "    def dfs(self, node: ThoughtNode, is_solution: Callable = None) -> Optional[ThoughtNode]:\n",
    "        \"\"\"\n",
    "        Recursive DFS exploration.\n",
    "        \n",
    "        Returns the first solution found, or None.\n",
    "        \"\"\"\n",
    "        self.nodes_explored += 1\n",
    "        \n",
    "        # Check depth limit\n",
    "        if node.depth >= self.max_depth:\n",
    "            return node if is_solution is None or is_solution(node.state) else None\n",
    "        \n",
    "        # Check if already a solution\n",
    "        if is_solution and is_solution(node.state):\n",
    "            return node\n",
    "        \n",
    "        # Generate and evaluate candidates\n",
    "        thoughts = generate_thoughts(node.state, n_thoughts=self.n_candidates)\n",
    "        candidates = []\n",
    "        \n",
    "        for thought in thoughts:\n",
    "            new_state = f\"{node.state}\\n\\nStep {node.depth + 1}: {thought}\"\n",
    "            score = evaluate_thought(node.state, thought)\n",
    "            \n",
    "            # Prune low-scoring candidates\n",
    "            if score < self.prune_threshold:\n",
    "                continue\n",
    "                \n",
    "            candidates.append((thought, new_state, score))\n",
    "        \n",
    "        # Sort by score, explore best first\n",
    "        candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        for thought, new_state, score in candidates:\n",
    "            child = ThoughtNode(\n",
    "                thought=thought,\n",
    "                state=new_state,\n",
    "                score=score,\n",
    "                depth=node.depth + 1,\n",
    "                parent=node\n",
    "            )\n",
    "            node.children.append(child)\n",
    "            \n",
    "            # Recurse\n",
    "            result = self.dfs(child, is_solution)\n",
    "            if result is not None:\n",
    "                return result\n",
    "        \n",
    "        # No solution found in this subtree\n",
    "        return None\n",
    "    \n",
    "    def solve(self, problem: str, is_solution: Callable = None) -> dict:\n",
    "        \"\"\"\n",
    "        Solve a problem using DFS.\n",
    "        \"\"\"\n",
    "        self.nodes_explored = 0\n",
    "        \n",
    "        root = ThoughtNode(\n",
    "            thought=\"Start\",\n",
    "            state=f\"Problem: {problem}\\n\\nLet me think step by step.\",\n",
    "            score=1.0,\n",
    "            depth=0\n",
    "        )\n",
    "        \n",
    "        solution = self.dfs(root, is_solution)\n",
    "        \n",
    "        return {\n",
    "            \"solution\": solution,\n",
    "            \"path\": solution.get_path() if solution else [],\n",
    "            \"final_state\": solution.state if solution else None,\n",
    "            \"nodes_explored\": self.nodes_explored\n",
    "        }\n",
    "\n",
    "\n",
    "# Create DFS solver\n",
    "tot_dfs = TreeOfThoughtsDFS(\n",
    "    model, tokenizer, device=device,\n",
    "    n_candidates=2,\n",
    "    max_depth=3,\n",
    "    prune_threshold=0.2\n",
    ")\n",
    "\n",
    "print(\"DFS Tree of Thoughts solver ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:23:28.626048Z",
     "iopub.status.busy": "2025-12-10T21:23:28.625975Z",
     "iopub.status.idle": "2025-12-10T21:23:39.558499Z",
     "shell.execute_reply": "2025-12-10T21:23:39.558092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: A store has 50 items. They sell 15 and receive 20 more. How many items do they have?\n",
      "Correct answer: 50 - 15 + 20 = 55\n",
      "\n",
      "============================================================\n",
      "BFS APPROACH:\n",
      "============================================================\n",
      "\n",
      "Depth 1: Exploring 1 nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 2 candidates, kept 2\n",
      "    Score 0.70: Start with the initial number of items, which is 5...\n",
      "    Score 0.70: Subtract the number of items sold, which is 15....\n",
      "\n",
      "Depth 2: Exploring 2 nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 4 candidates, kept 2\n",
      "    Score 0.70: Now let's solve the problem:...\n",
      "    Score 0.70: We start with 50 items in the store. The store sel...\n",
      "\n",
      "Nodes explored: 7\n",
      "\n",
      "============================================================\n",
      "DFS APPROACH:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nodes explored: 4\n"
     ]
    }
   ],
   "source": [
    "# Compare BFS vs DFS\n",
    "problem = \"A store has 50 items. They sell 15 and receive 20 more. How many items do they have?\"\n",
    "\n",
    "print(f\"Problem: {problem}\")\n",
    "print(f\"Correct answer: 50 - 15 + 20 = 55\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BFS APPROACH:\")\n",
    "print(\"=\"*60)\n",
    "bfs_result = tot_bfs.solve(problem)\n",
    "print(f\"\\nNodes explored: {bfs_result['nodes_explored']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DFS APPROACH:\")\n",
    "print(\"=\"*60)\n",
    "dfs_result = tot_dfs.solve(problem)\n",
    "print(f\"\\nNodes explored: {dfs_result['nodes_explored']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## A Simpler Approach: Prompt-Based ToT\n",
    "\n",
    "The full ToT framework with explicit search algorithms is powerful but complex. There's a simpler version that works surprisingly well:\n",
    "\n",
    "Just... ask the model to do it!\n",
    "\n",
    "The idea (from Hulbert, 2023): prompt the model to imagine multiple experts, each contributing one step, with explicit backtracking.\n",
    "\n",
    "```\n",
    "Imagine three different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realizes they're wrong at any point then they leave.\n",
    "```\n",
    "\n",
    "This encodes the ToT idea entirely in the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:23:39.559414Z",
     "iopub.status.busy": "2025-12-10T21:23:39.559333Z",
     "iopub.status.idle": "2025-12-10T21:23:44.354349Z",
     "shell.execute_reply": "2025-12-10T21:23:44.353966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROMPT-BASED TREE OF THOUGHTS\n",
      "============================================================\n",
      "Problem: If I have 3 apples and buy 2 bags with 4 apples each, how many apples do I have?\n",
      "Correct: 3 + 2\u00d74 = 11\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start by adding the number of apples you already have (3) to the number of apples in the bags (2*4=8). So, 3+8 = 11 apples. \n",
      "\n",
      "Expert 2: Let's break it down step-by-step: First, add the number of apples from your bag (2) to the number you already have (3), which gives us 5 apples. Then, add the additional apples from the second bag (which is 4 more than what we just calculated) to get a total of 9 apples.\n",
      "\n",
      "Expert 3: Let's approach this logically: You started with 3 apples. When you bought 2 bags with 4 apples each, that means you added 4 apples to each bag twice, making it 4*2=8 apples. Adding these new apples to your original amount of 3 apples gives us 3+8=11 apples.\n",
      "\n",
      "The final expert left after realizing the mistake in Expert 3's calculation.\n",
      "\n",
      "What was the correct answer? How did you arrive at it?\n",
      "- **Correct Answer:** The correct answer is 11 apples. This can be derived by simply performing the arithmetic operation mentioned in Expert 3's method: \\(3 + 4 \\times 2 = 3 + 8 = 11\\).\n",
      "\n",
      "**Explanation of Calculation:** \n",
      "1. You start with 3 apples.\n",
      "2. Each bag contains 4 apples, so for two bags, you add\n"
     ]
    }
   ],
   "source": [
    "def prompt_based_tot(problem: str, n_experts: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Tree of Thoughts through clever prompting.\n",
    "    \n",
    "    No explicit search algorithm \u2014 just ask the model to simulate\n",
    "    multiple reasoning paths with self-critique.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Imagine {n_experts} different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking, then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realizes they're wrong at any point, they leave.\n",
    "Continue until only one expert remains or all agree on the answer.\n",
    "\n",
    "Question: {problem}\n",
    "\n",
    "Expert 1:\"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, max_new_tokens=300, temperature=0.7)\n",
    "    return prompt + response\n",
    "\n",
    "\n",
    "# Test prompt-based ToT\n",
    "problem = \"If I have 3 apples and buy 2 bags with 4 apples each, how many apples do I have?\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROMPT-BASED TREE OF THOUGHTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Problem: {problem}\")\n",
    "print(f\"Correct: 3 + 2\u00d74 = 11\")\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "result = prompt_based_tot(problem, n_experts=3)\n",
    "# Just show the response part\n",
    "print(result.split(\"Expert 1:\")[1] if \"Expert 1:\" in result else result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## When to Use ToT\n",
    "\n",
    "Tree of Thoughts shines in specific scenarios:\n",
    "\n",
    "### Good for ToT:\n",
    "- **Planning problems** \u2014 Multiple strategies, some better than others\n",
    "- **Creative tasks** \u2014 Brainstorming with refinement\n",
    "- **Puzzles** \u2014 Clear win/lose states, backtracking helps\n",
    "- **Math with multiple approaches** \u2014 Different paths to same answer\n",
    "\n",
    "### Overkill for ToT:\n",
    "- **Simple factual questions** \u2014 No need for exploration\n",
    "- **Single-step problems** \u2014 Tree is trivial\n",
    "- **Time-sensitive applications** \u2014 Exploration takes time\n",
    "\n",
    "### Compared to Self-Consistency:\n",
    "- **ToT**: Smart exploration, can backtrack, uses evaluation\n",
    "- **Self-Consistency**: Dumb sampling, just vote on final answers\n",
    "\n",
    "ToT is more powerful but more complex. Self-consistency is simpler but wastes compute exploring bad paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Benchmark Results\n",
    "\n",
    "From the original ToT paper (Yao et al., 2023):\n",
    "\n",
    "| Task | Standard | CoT | CoT-SC | ToT |\n",
    "|------|----------|-----|--------|-----|\n",
    "| Game of 24 | 7.3% | 4.0% | 9.0% | **74%** |\n",
    "| Creative Writing (coherence) | 6.2 | 6.9 | 7.0 | **7.6** |\n",
    "| Mini Crosswords | 4.4% | 15.6% | 16.4% | **60%** |\n",
    "\n",
    "(Using GPT-4)\n",
    "\n",
    "The Game of 24 result is striking: from 4% with CoT to 74% with ToT! That's the power of exploration and backtracking.\n",
    "\n",
    "But note: ToT uses many more API calls. The paper reports ~100 LLM calls per problem for Game of 24. That's 100\u00d7 the cost of single-shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## What We've Learned\n",
    "\n",
    "Tree of Thoughts extends chain-of-thought reasoning:\n",
    "\n",
    "1. **Branch**: Generate multiple candidate thoughts at each step\n",
    "2. **Evaluate**: Score each candidate (self-evaluation or heuristics)\n",
    "3. **Search**: Use BFS or DFS to explore the tree\n",
    "4. **Prune**: Abandon low-scoring branches early\n",
    "5. **Backtrack**: When stuck, try different paths\n",
    "\n",
    "Key trade-offs:\n",
    "- **BFS**: Thorough but memory-intensive\n",
    "- **DFS**: Efficient but might miss good solutions\n",
    "- **Prompt-based**: Simple but less controllable\n",
    "\n",
    "When evaluation is unreliable (the model can't score its own thoughts well), ToT becomes less useful. That's why the next section on **Process Reward Models** is so important\u2014we'll train a dedicated model to evaluate reasoning steps.\n",
    "\n",
    "**Next up:** Process Reward Models \u2014 training verifiers to score each step"
   ]
  }
 ],
 "metadata": {
  "description": "Explores multiple reasoning branches in parallel, evaluates paths, and backtracks when needed.",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}